05/24/2022 06:33:48 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/24/2022 06:33:48 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli
05/24/2022 06:33:48 - INFO - __main__ - Namespace(task_dir='data_32/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down32shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/24/2022 06:33:48 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli
05/24/2022 06:33:49 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/24/2022 06:33:49 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/24/2022 06:33:49 - INFO - __main__ - args.device: cuda:0
05/24/2022 06:33:49 - INFO - __main__ - Using 2 gpus
05/24/2022 06:33:49 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/24/2022 06:33:49 - INFO - __main__ - args.device: cuda:1
05/24/2022 06:33:49 - INFO - __main__ - Using 2 gpus
05/24/2022 06:33:49 - INFO - __main__ - Fine-tuning the following samples: ['anli_32_100', 'anli_32_13', 'anli_32_21', 'anli_32_42', 'anli_32_87']
05/24/2022 06:33:54 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.5, bsz=8 ...
05/24/2022 06:33:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:33:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:33:55 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 06:33:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:33:55 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 06:33:55 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:33:55 - INFO - __main__ - Printing 3 examples
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 06:33:55 - INFO - __main__ - ['neutral']
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:33:55 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:33:55 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 06:33:55 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 06:34:13 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 06:34:13 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 06:34:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 06:34:14 - INFO - __main__ - Starting training!
05/24/2022 06:34:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 06:34:18 - INFO - __main__ - Starting training!
05/24/2022 06:34:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=1
05/24/2022 06:34:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=3
05/24/2022 06:34:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=4
05/24/2022 06:34:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=6
05/24/2022 06:34:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=8
05/24/2022 06:34:35 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 06:34:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 06:34:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=9
05/24/2022 06:34:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=11
05/24/2022 06:34:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=13
05/24/2022 06:34:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=14
05/24/2022 06:34:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=16
05/24/2022 06:34:51 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 06:34:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
05/24/2022 06:34:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=19
05/24/2022 06:34:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=21
05/24/2022 06:35:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 06:35:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=24
05/24/2022 06:35:07 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 06:35:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=26
05/24/2022 06:35:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
05/24/2022 06:35:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=29
05/24/2022 06:35:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=31
05/24/2022 06:35:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.41 on epoch=33
05/24/2022 06:35:23 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 06:35:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
05/24/2022 06:35:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=36
05/24/2022 06:35:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
05/24/2022 06:35:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/24/2022 06:35:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=41
05/24/2022 06:35:39 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 06:35:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=43
05/24/2022 06:35:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=44
05/24/2022 06:35:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/24/2022 06:35:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=48
05/24/2022 06:35:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=49
05/24/2022 06:35:55 - INFO - __main__ - Global step 300 Train loss 0.40 Classification-F1 0.22918586789554532 on epoch=49
05/24/2022 06:35:55 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22918586789554532 on epoch=49, global_step=300
05/24/2022 06:35:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=51
05/24/2022 06:36:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=53
05/24/2022 06:36:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=54
05/24/2022 06:36:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=56
05/24/2022 06:36:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.35 on epoch=58
05/24/2022 06:36:11 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3233489958167291 on epoch=58
05/24/2022 06:36:11 - INFO - __main__ - Saving model with best Classification-F1: 0.22918586789554532 -> 0.3233489958167291 on epoch=58, global_step=350
05/24/2022 06:36:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=59
05/24/2022 06:36:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=61
05/24/2022 06:36:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=63
05/24/2022 06:36:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.33 on epoch=64
05/24/2022 06:36:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=66
05/24/2022 06:36:27 - INFO - __main__ - Global step 400 Train loss 0.39 Classification-F1 0.2877492877492877 on epoch=66
05/24/2022 06:36:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=68
05/24/2022 06:36:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=69
05/24/2022 06:36:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.31 on epoch=71
05/24/2022 06:36:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=73
05/24/2022 06:36:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=74
05/24/2022 06:36:42 - INFO - __main__ - Global step 450 Train loss 0.32 Classification-F1 0.2751412429378531 on epoch=74
05/24/2022 06:36:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.33 on epoch=76
05/24/2022 06:36:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=78
05/24/2022 06:36:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=79
05/24/2022 06:36:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=81
05/24/2022 06:36:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=83
05/24/2022 06:36:58 - INFO - __main__ - Global step 500 Train loss 0.30 Classification-F1 0.32785829307568437 on epoch=83
05/24/2022 06:36:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3233489958167291 -> 0.32785829307568437 on epoch=83, global_step=500
05/24/2022 06:37:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=84
05/24/2022 06:37:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=86
05/24/2022 06:37:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=88
05/24/2022 06:37:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=89
05/24/2022 06:37:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.28 on epoch=91
05/24/2022 06:37:13 - INFO - __main__ - Global step 550 Train loss 0.31 Classification-F1 0.2889623811492352 on epoch=91
05/24/2022 06:37:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=93
05/24/2022 06:37:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=94
05/24/2022 06:37:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=96
05/24/2022 06:37:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.30 on epoch=98
05/24/2022 06:37:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=99
05/24/2022 06:37:29 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.31769580326116015 on epoch=99
05/24/2022 06:37:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=101
05/24/2022 06:37:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=103
05/24/2022 06:37:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=104
05/24/2022 06:37:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=106
05/24/2022 06:37:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=108
05/24/2022 06:37:44 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3294346978557505 on epoch=108
05/24/2022 06:37:44 - INFO - __main__ - Saving model with best Classification-F1: 0.32785829307568437 -> 0.3294346978557505 on epoch=108, global_step=650
05/24/2022 06:37:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=109
05/24/2022 06:37:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=111
05/24/2022 06:37:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=113
05/24/2022 06:37:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=114
05/24/2022 06:37:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=116
05/24/2022 06:38:00 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.35257985257985264 on epoch=116
05/24/2022 06:38:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3294346978557505 -> 0.35257985257985264 on epoch=116, global_step=700
05/24/2022 06:38:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=118
05/24/2022 06:38:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=119
05/24/2022 06:38:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=121
05/24/2022 06:38:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=123
05/24/2022 06:38:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=124
05/24/2022 06:38:16 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.3680858370885325 on epoch=124
05/24/2022 06:38:16 - INFO - __main__ - Saving model with best Classification-F1: 0.35257985257985264 -> 0.3680858370885325 on epoch=124, global_step=750
05/24/2022 06:38:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=126
05/24/2022 06:38:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=128
05/24/2022 06:38:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=129
05/24/2022 06:38:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=131
05/24/2022 06:38:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=133
05/24/2022 06:38:32 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.44053650364329977 on epoch=133
05/24/2022 06:38:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3680858370885325 -> 0.44053650364329977 on epoch=133, global_step=800
05/24/2022 06:38:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=134
05/24/2022 06:38:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=136
05/24/2022 06:38:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=138
05/24/2022 06:38:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=139
05/24/2022 06:38:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=141
05/24/2022 06:38:48 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.4100490428729631 on epoch=141
05/24/2022 06:38:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=143
05/24/2022 06:38:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=144
05/24/2022 06:38:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=146
05/24/2022 06:38:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=148
05/24/2022 06:39:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=149
05/24/2022 06:39:03 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.4474610356963297 on epoch=149
05/24/2022 06:39:03 - INFO - __main__ - Saving model with best Classification-F1: 0.44053650364329977 -> 0.4474610356963297 on epoch=149, global_step=900
05/24/2022 06:39:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=151
05/24/2022 06:39:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=153
05/24/2022 06:39:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=154
05/24/2022 06:39:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=156
05/24/2022 06:39:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=158
05/24/2022 06:39:19 - INFO - __main__ - Global step 950 Train loss 0.14 Classification-F1 0.43928238977743933 on epoch=158
05/24/2022 06:39:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=159
05/24/2022 06:39:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=161
05/24/2022 06:39:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=163
05/24/2022 06:39:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=164
05/24/2022 06:39:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=166
05/24/2022 06:39:35 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.20075757575757575 on epoch=166
05/24/2022 06:39:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=168
05/24/2022 06:39:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=169
05/24/2022 06:39:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=171
05/24/2022 06:39:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=173
05/24/2022 06:39:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=174
05/24/2022 06:39:50 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.19519812609798945 on epoch=174
05/24/2022 06:39:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=176
05/24/2022 06:39:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=178
05/24/2022 06:39:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=179
05/24/2022 06:40:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=181
05/24/2022 06:40:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=183
05/24/2022 06:40:06 - INFO - __main__ - Global step 1100 Train loss 0.16 Classification-F1 0.3299591211037302 on epoch=183
05/24/2022 06:40:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=184
05/24/2022 06:40:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=186
05/24/2022 06:40:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=188
05/24/2022 06:40:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.12 on epoch=189
05/24/2022 06:40:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=191
05/24/2022 06:40:22 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.32846153846153847 on epoch=191
05/24/2022 06:40:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=193
05/24/2022 06:40:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=194
05/24/2022 06:40:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=196
05/24/2022 06:40:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=198
05/24/2022 06:40:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=199
05/24/2022 06:40:38 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.2588703949603187 on epoch=199
05/24/2022 06:40:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=201
05/24/2022 06:40:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=203
05/24/2022 06:40:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=204
05/24/2022 06:40:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=206
05/24/2022 06:40:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=208
05/24/2022 06:40:54 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.2195951551352248 on epoch=208
05/24/2022 06:40:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=209
05/24/2022 06:40:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=211
05/24/2022 06:41:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=213
05/24/2022 06:41:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=214
05/24/2022 06:41:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=216
05/24/2022 06:41:10 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.21830288195355071 on epoch=216
05/24/2022 06:41:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=218
05/24/2022 06:41:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=219
05/24/2022 06:41:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=221
05/24/2022 06:41:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=223
05/24/2022 06:41:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=224
05/24/2022 06:41:26 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.18881875966137068 on epoch=224
05/24/2022 06:41:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=226
05/24/2022 06:41:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=228
05/24/2022 06:41:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=229
05/24/2022 06:41:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=231
05/24/2022 06:41:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=233
05/24/2022 06:41:42 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.19154051274951361 on epoch=233
05/24/2022 06:41:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=234
05/24/2022 06:41:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=236
05/24/2022 06:41:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=238
05/24/2022 06:41:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=239
05/24/2022 06:41:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=241
05/24/2022 06:41:58 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.18666053154622556 on epoch=241
05/24/2022 06:42:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=243
05/24/2022 06:42:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=244
05/24/2022 06:42:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=246
05/24/2022 06:42:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=248
05/24/2022 06:42:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=249
05/24/2022 06:42:14 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.20887067575129192 on epoch=249
05/24/2022 06:42:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=251
05/24/2022 06:42:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=253
05/24/2022 06:42:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=254
05/24/2022 06:42:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=256
05/24/2022 06:42:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=258
05/24/2022 06:42:29 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.14616865614170196 on epoch=258
05/24/2022 06:42:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=259
05/24/2022 06:42:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=261
05/24/2022 06:42:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=263
05/24/2022 06:42:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=264
05/24/2022 06:42:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=266
05/24/2022 06:42:46 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.09497066639923783 on epoch=266
05/24/2022 06:42:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=268
05/24/2022 06:42:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=269
05/24/2022 06:42:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=271
05/24/2022 06:42:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=273
05/24/2022 06:42:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=274
05/24/2022 06:43:02 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.15061202887289843 on epoch=274
05/24/2022 06:43:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=276
05/24/2022 06:43:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=278
05/24/2022 06:43:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=279
05/24/2022 06:43:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=281
05/24/2022 06:43:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=283
05/24/2022 06:43:18 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.12806722689075628 on epoch=283
05/24/2022 06:43:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=284
05/24/2022 06:43:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=286
05/24/2022 06:43:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=288
05/24/2022 06:43:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=289
05/24/2022 06:43:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=291
05/24/2022 06:43:34 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.13877551020408163 on epoch=291
05/24/2022 06:43:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=293
05/24/2022 06:43:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=294
05/24/2022 06:43:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=296
05/24/2022 06:43:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=298
05/24/2022 06:43:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=299
05/24/2022 06:43:50 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.11604214471929551 on epoch=299
05/24/2022 06:43:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=301
05/24/2022 06:43:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=303
05/24/2022 06:43:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=304
05/24/2022 06:44:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=306
05/24/2022 06:44:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=308
05/24/2022 06:44:06 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.11834045584045584 on epoch=308
05/24/2022 06:44:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=309
05/24/2022 06:44:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=311
05/24/2022 06:44:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=313
05/24/2022 06:44:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=314
05/24/2022 06:44:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=316
05/24/2022 06:44:22 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.13080810899642178 on epoch=316
05/24/2022 06:44:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=318
05/24/2022 06:44:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=319
05/24/2022 06:44:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=321
05/24/2022 06:44:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=323
05/24/2022 06:44:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=324
05/24/2022 06:44:38 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.15947940947940947 on epoch=324
05/24/2022 06:44:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=326
05/24/2022 06:44:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=328
05/24/2022 06:44:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=329
05/24/2022 06:44:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=331
05/24/2022 06:44:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=333
05/24/2022 06:44:54 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.13246507629486354 on epoch=333
05/24/2022 06:44:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=334
05/24/2022 06:44:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=336
05/24/2022 06:45:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=338
05/24/2022 06:45:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=339
05/24/2022 06:45:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=341
05/24/2022 06:45:10 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.13229528848576466 on epoch=341
05/24/2022 06:45:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=343
05/24/2022 06:45:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=344
05/24/2022 06:45:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=346
05/24/2022 06:45:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=348
05/24/2022 06:45:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=349
05/24/2022 06:45:26 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.14953389314835097 on epoch=349
05/24/2022 06:45:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=351
05/24/2022 06:45:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=353
05/24/2022 06:45:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=354
05/24/2022 06:45:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=356
05/24/2022 06:45:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=358
05/24/2022 06:45:42 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.08781409631794963 on epoch=358
05/24/2022 06:45:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=359
05/24/2022 06:45:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=361
05/24/2022 06:45:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=363
05/24/2022 06:45:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=364
05/24/2022 06:45:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=366
05/24/2022 06:45:58 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.12940540001850784 on epoch=366
05/24/2022 06:46:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=368
05/24/2022 06:46:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=369
05/24/2022 06:46:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=371
05/24/2022 06:46:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=373
05/24/2022 06:46:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=374
05/24/2022 06:46:14 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.08271411128553985 on epoch=374
05/24/2022 06:46:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=376
05/24/2022 06:46:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=378
05/24/2022 06:46:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=379
05/24/2022 06:46:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=381
05/24/2022 06:46:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=383
05/24/2022 06:46:30 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.07057043553207236 on epoch=383
05/24/2022 06:46:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=384
05/24/2022 06:46:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.02 on epoch=386
05/24/2022 06:46:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=388
05/24/2022 06:46:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=389
05/24/2022 06:46:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=391
05/24/2022 06:46:46 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.09628968296653255 on epoch=391
05/24/2022 06:46:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=393
05/24/2022 06:46:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=394
05/24/2022 06:46:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=396
05/24/2022 06:46:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=398
05/24/2022 06:47:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=399
05/24/2022 06:47:03 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.1331372549019608 on epoch=399
05/24/2022 06:47:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=401
05/24/2022 06:47:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=403
05/24/2022 06:47:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=404
05/24/2022 06:47:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=406
05/24/2022 06:47:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=408
05/24/2022 06:47:19 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.09678416821273965 on epoch=408
05/24/2022 06:47:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=409
05/24/2022 06:47:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=411
05/24/2022 06:47:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=413
05/24/2022 06:47:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
05/24/2022 06:47:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=416
05/24/2022 06:47:35 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.07560622181689214 on epoch=416
05/24/2022 06:47:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=418
05/24/2022 06:47:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=419
05/24/2022 06:47:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=421
05/24/2022 06:47:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=423
05/24/2022 06:47:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=424
05/24/2022 06:47:51 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.07952380952380952 on epoch=424
05/24/2022 06:47:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=426
05/24/2022 06:47:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=428
05/24/2022 06:47:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=429
05/24/2022 06:48:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=431
05/24/2022 06:48:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=433
05/24/2022 06:48:07 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.11433925846126225 on epoch=433
05/24/2022 06:48:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=434
05/24/2022 06:48:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 06:48:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=438
05/24/2022 06:48:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
05/24/2022 06:48:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=441
05/24/2022 06:48:23 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.10662815445424141 on epoch=441
05/24/2022 06:48:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=443
05/24/2022 06:48:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 06:48:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
05/24/2022 06:48:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=448
05/24/2022 06:48:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 06:48:39 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.14922832043842682 on epoch=449
05/24/2022 06:48:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=451
05/24/2022 06:48:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 06:48:47 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=454
05/24/2022 06:48:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/24/2022 06:48:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=458
05/24/2022 06:48:55 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.11752798925212718 on epoch=458
05/24/2022 06:48:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=459
05/24/2022 06:49:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=461
05/24/2022 06:49:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=463
05/24/2022 06:49:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=464
05/24/2022 06:49:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=466
05/24/2022 06:49:11 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.11843094528123964 on epoch=466
05/24/2022 06:49:14 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
05/24/2022 06:49:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 06:49:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=471
05/24/2022 06:49:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
05/24/2022 06:49:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=474
05/24/2022 06:49:27 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.06460750341412362 on epoch=474
05/24/2022 06:49:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=476
05/24/2022 06:49:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
05/24/2022 06:49:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=479
05/24/2022 06:49:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
05/24/2022 06:49:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
05/24/2022 06:49:43 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.10755937755937756 on epoch=483
05/24/2022 06:49:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=484
05/24/2022 06:49:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=486
05/24/2022 06:49:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=488
05/24/2022 06:49:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 06:49:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=491
05/24/2022 06:49:59 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.1116757638496769 on epoch=491
05/24/2022 06:50:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
05/24/2022 06:50:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=494
05/24/2022 06:50:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=496
05/24/2022 06:50:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=498
05/24/2022 06:50:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
05/24/2022 06:50:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:50:14 - INFO - __main__ - Printing 3 examples
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:50:14 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:50:14 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 06:50:14 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:50:14 - INFO - __main__ - Printing 3 examples
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 06:50:14 - INFO - __main__ - ['neutral']
05/24/2022 06:50:14 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:50:15 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:50:15 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 06:50:15 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.09436076381227725 on epoch=499
05/24/2022 06:50:16 - INFO - __main__ - save last model!
05/24/2022 06:50:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 06:50:16 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 06:50:16 - INFO - __main__ - Printing 3 examples
05/24/2022 06:50:16 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 06:50:16 - INFO - __main__ - ['contradiction']
05/24/2022 06:50:16 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 06:50:16 - INFO - __main__ - ['entailment']
05/24/2022 06:50:16 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 06:50:16 - INFO - __main__ - ['contradiction']
05/24/2022 06:50:16 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:50:16 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:50:17 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 06:50:33 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 06:50:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 06:50:34 - INFO - __main__ - Starting training!
05/24/2022 06:50:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_100_0.5_8_predictions.txt
05/24/2022 06:50:46 - INFO - __main__ - Classification-F1 on test data: 0.0178
05/24/2022 06:50:46 - INFO - __main__ - prefix=anli_32_100, lr=0.5, bsz=8, dev_performance=0.4474610356963297, test_performance=0.01784280331087516
05/24/2022 06:50:46 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.4, bsz=8 ...
05/24/2022 06:50:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:50:47 - INFO - __main__ - Printing 3 examples
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:50:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:50:47 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 06:50:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 06:50:47 - INFO - __main__ - Printing 3 examples
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 06:50:47 - INFO - __main__ - ['neutral']
05/24/2022 06:50:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 06:50:48 - INFO - __main__ - Tokenizing Output ...
05/24/2022 06:50:48 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 06:51:03 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 06:51:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 06:51:04 - INFO - __main__ - Starting training!
05/24/2022 06:51:07 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=1
05/24/2022 06:51:10 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=3
05/24/2022 06:51:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=4
05/24/2022 06:51:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=6
05/24/2022 06:51:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=8
05/24/2022 06:51:20 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 06:51:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 06:51:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
05/24/2022 06:51:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=11
05/24/2022 06:51:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=13
05/24/2022 06:51:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=14
05/24/2022 06:51:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=16
05/24/2022 06:51:36 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 06:51:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
05/24/2022 06:51:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=19
05/24/2022 06:51:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=21
05/24/2022 06:51:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 06:51:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=24
05/24/2022 06:51:52 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 06:51:54 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/24/2022 06:51:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
05/24/2022 06:52:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=29
05/24/2022 06:52:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 06:52:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=33
05/24/2022 06:52:08 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=33
05/24/2022 06:52:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2085278555866791 on epoch=33, global_step=200
05/24/2022 06:52:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=34
05/24/2022 06:52:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=36
05/24/2022 06:52:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=38
05/24/2022 06:52:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
05/24/2022 06:52:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/24/2022 06:52:23 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 06:52:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/24/2022 06:52:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=44
05/24/2022 06:52:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
05/24/2022 06:52:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=48
05/24/2022 06:52:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=49
05/24/2022 06:52:39 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=49
05/24/2022 06:52:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=51
05/24/2022 06:52:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=53
05/24/2022 06:52:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=54
05/24/2022 06:52:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=56
05/24/2022 06:52:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=58
05/24/2022 06:52:54 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.22780952380952382 on epoch=58
05/24/2022 06:52:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.22780952380952382 on epoch=58, global_step=350
05/24/2022 06:52:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=59
05/24/2022 06:53:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
05/24/2022 06:53:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.32 on epoch=63
05/24/2022 06:53:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=64
05/24/2022 06:53:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=66
05/24/2022 06:53:10 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.24611708482676223 on epoch=66
05/24/2022 06:53:10 - INFO - __main__ - Saving model with best Classification-F1: 0.22780952380952382 -> 0.24611708482676223 on epoch=66, global_step=400
05/24/2022 06:53:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=68
05/24/2022 06:53:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=69
05/24/2022 06:53:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=71
05/24/2022 06:53:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=73
05/24/2022 06:53:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/24/2022 06:53:25 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.24611708482676223 on epoch=74
05/24/2022 06:53:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=76
05/24/2022 06:53:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=78
05/24/2022 06:53:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=79
05/24/2022 06:53:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=81
05/24/2022 06:53:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.29 on epoch=83
05/24/2022 06:53:41 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.27123044339687097 on epoch=83
05/24/2022 06:53:41 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.27123044339687097 on epoch=83, global_step=500
05/24/2022 06:53:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=84
05/24/2022 06:53:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=86
05/24/2022 06:53:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=88
05/24/2022 06:53:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=89
05/24/2022 06:53:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=91
05/24/2022 06:53:56 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.25626898354171085 on epoch=91
05/24/2022 06:53:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=93
05/24/2022 06:54:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=94
05/24/2022 06:54:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=96
05/24/2022 06:54:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=98
05/24/2022 06:54:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=99
05/24/2022 06:54:12 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.25626898354171085 on epoch=99
05/24/2022 06:54:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=101
05/24/2022 06:54:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=103
05/24/2022 06:54:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=104
05/24/2022 06:54:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=106
05/24/2022 06:54:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=108
05/24/2022 06:54:27 - INFO - __main__ - Global step 650 Train loss 0.29 Classification-F1 0.3146198830409357 on epoch=108
05/24/2022 06:54:27 - INFO - __main__ - Saving model with best Classification-F1: 0.27123044339687097 -> 0.3146198830409357 on epoch=108, global_step=650
05/24/2022 06:54:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=109
05/24/2022 06:54:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=111
05/24/2022 06:54:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=113
05/24/2022 06:54:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=114
05/24/2022 06:54:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=116
05/24/2022 06:54:43 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.33103900360537525 on epoch=116
05/24/2022 06:54:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3146198830409357 -> 0.33103900360537525 on epoch=116, global_step=700
05/24/2022 06:54:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=118
05/24/2022 06:54:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=119
05/24/2022 06:54:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=121
05/24/2022 06:54:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=123
05/24/2022 06:54:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=124
05/24/2022 06:54:59 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.3066942719116632 on epoch=124
05/24/2022 06:55:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=126
05/24/2022 06:55:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=128
05/24/2022 06:55:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=129
05/24/2022 06:55:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=131
05/24/2022 06:55:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=133
05/24/2022 06:55:15 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.2889623811492352 on epoch=133
05/24/2022 06:55:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=134
05/24/2022 06:55:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=136
05/24/2022 06:55:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=138
05/24/2022 06:55:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=139
05/24/2022 06:55:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=141
05/24/2022 06:55:31 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.34609782981876 on epoch=141
05/24/2022 06:55:31 - INFO - __main__ - Saving model with best Classification-F1: 0.33103900360537525 -> 0.34609782981876 on epoch=141, global_step=850
05/24/2022 06:55:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=143
05/24/2022 06:55:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=144
05/24/2022 06:55:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=146
05/24/2022 06:55:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=148
05/24/2022 06:55:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=149
05/24/2022 06:55:47 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.3816272238738115 on epoch=149
05/24/2022 06:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.34609782981876 -> 0.3816272238738115 on epoch=149, global_step=900
05/24/2022 06:55:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=151
05/24/2022 06:55:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=153
05/24/2022 06:55:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=154
05/24/2022 06:55:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=156
05/24/2022 06:56:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=158
05/24/2022 06:56:02 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.37687400318979264 on epoch=158
05/24/2022 06:56:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=159
05/24/2022 06:56:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=161
05/24/2022 06:56:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=163
05/24/2022 06:56:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=164
05/24/2022 06:56:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=166
05/24/2022 06:56:18 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.36168402590305654 on epoch=166
05/24/2022 06:56:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=168
05/24/2022 06:56:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=169
05/24/2022 06:56:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=171
05/24/2022 06:56:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=173
05/24/2022 06:56:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=174
05/24/2022 06:56:34 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3644050451556784 on epoch=174
05/24/2022 06:56:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=176
05/24/2022 06:56:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=178
05/24/2022 06:56:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=179
05/24/2022 06:56:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=181
05/24/2022 06:56:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=183
05/24/2022 06:56:51 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.3817391304347826 on epoch=183
05/24/2022 06:56:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3816272238738115 -> 0.3817391304347826 on epoch=183, global_step=1100
05/24/2022 06:56:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=184
05/24/2022 06:56:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=186
05/24/2022 06:56:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=188
05/24/2022 06:57:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=189
05/24/2022 06:57:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=191
05/24/2022 06:57:07 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.3682155431645436 on epoch=191
05/24/2022 06:57:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=193
05/24/2022 06:57:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=194
05/24/2022 06:57:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=196
05/24/2022 06:57:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=198
05/24/2022 06:57:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=199
05/24/2022 06:57:23 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.26834202237428045 on epoch=199
05/24/2022 06:57:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=201
05/24/2022 06:57:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.12 on epoch=203
05/24/2022 06:57:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=204
05/24/2022 06:57:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=206
05/24/2022 06:57:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=208
05/24/2022 06:57:39 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.30646279730947923 on epoch=208
05/24/2022 06:57:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=209
05/24/2022 06:57:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=211
05/24/2022 06:57:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=213
05/24/2022 06:57:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=214
05/24/2022 06:57:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=216
05/24/2022 06:57:55 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.25489742613048916 on epoch=216
05/24/2022 06:57:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=218
05/24/2022 06:58:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=219
05/24/2022 06:58:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=221
05/24/2022 06:58:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=223
05/24/2022 06:58:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=224
05/24/2022 06:58:11 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.2815784134712923 on epoch=224
05/24/2022 06:58:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=226
05/24/2022 06:58:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=228
05/24/2022 06:58:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=229
05/24/2022 06:58:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=231
05/24/2022 06:58:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=233
05/24/2022 06:58:27 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.23302154195011335 on epoch=233
05/24/2022 06:58:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=234
05/24/2022 06:58:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=236
05/24/2022 06:58:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=238
05/24/2022 06:58:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=239
05/24/2022 06:58:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=241
05/24/2022 06:58:43 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.34024455077086657 on epoch=241
05/24/2022 06:58:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=243
05/24/2022 06:58:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=244
05/24/2022 06:58:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=246
05/24/2022 06:58:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=248
05/24/2022 06:58:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=249
05/24/2022 06:58:59 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.3877426749767175 on epoch=249
05/24/2022 06:58:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3817391304347826 -> 0.3877426749767175 on epoch=249, global_step=1500
05/24/2022 06:59:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=251
05/24/2022 06:59:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=253
05/24/2022 06:59:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=254
05/24/2022 06:59:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=256
05/24/2022 06:59:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=258
05/24/2022 06:59:15 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.23085290720061608 on epoch=258
05/24/2022 06:59:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=259
05/24/2022 06:59:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=261
05/24/2022 06:59:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=263
05/24/2022 06:59:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=264
05/24/2022 06:59:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=266
05/24/2022 06:59:31 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.22277186241239175 on epoch=266
05/24/2022 06:59:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=268
05/24/2022 06:59:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=269
05/24/2022 06:59:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=271
05/24/2022 06:59:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=273
05/24/2022 06:59:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=274
05/24/2022 06:59:47 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.3912420535917298 on epoch=274
05/24/2022 06:59:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3877426749767175 -> 0.3912420535917298 on epoch=274, global_step=1650
05/24/2022 06:59:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=276
05/24/2022 06:59:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=278
05/24/2022 06:59:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=279
05/24/2022 06:59:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=281
05/24/2022 07:00:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=283
05/24/2022 07:00:04 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.13581670190887235 on epoch=283
05/24/2022 07:00:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=284
05/24/2022 07:00:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=286
05/24/2022 07:00:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=288
05/24/2022 07:00:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=289
05/24/2022 07:00:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=291
05/24/2022 07:00:20 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.2632992327365729 on epoch=291
05/24/2022 07:00:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=293
05/24/2022 07:00:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=294
05/24/2022 07:00:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=296
05/24/2022 07:00:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=298
05/24/2022 07:00:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=299
05/24/2022 07:00:36 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.1920454963912211 on epoch=299
05/24/2022 07:00:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=301
05/24/2022 07:00:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=303
05/24/2022 07:00:44 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=304
05/24/2022 07:00:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=306
05/24/2022 07:00:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=308
05/24/2022 07:00:52 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.2110271760648968 on epoch=308
05/24/2022 07:00:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=309
05/24/2022 07:00:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=311
05/24/2022 07:01:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=313
05/24/2022 07:01:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=314
05/24/2022 07:01:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=316
05/24/2022 07:01:08 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.2467749529190207 on epoch=316
05/24/2022 07:01:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=318
05/24/2022 07:01:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=319
05/24/2022 07:01:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=321
05/24/2022 07:01:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=323
05/24/2022 07:01:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=324
05/24/2022 07:01:24 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.20630363706750174 on epoch=324
05/24/2022 07:01:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=326
05/24/2022 07:01:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=328
05/24/2022 07:01:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=329
05/24/2022 07:01:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=331
05/24/2022 07:01:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
05/24/2022 07:01:40 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.14303555030827758 on epoch=333
05/24/2022 07:01:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=334
05/24/2022 07:01:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=336
05/24/2022 07:01:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=338
05/24/2022 07:01:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=339
05/24/2022 07:01:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=341
05/24/2022 07:01:56 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.1796701388888889 on epoch=341
05/24/2022 07:01:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=343
05/24/2022 07:02:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=344
05/24/2022 07:02:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=346
05/24/2022 07:02:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=348
05/24/2022 07:02:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=349
05/24/2022 07:02:12 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.09997185477061636 on epoch=349
05/24/2022 07:02:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=351
05/24/2022 07:02:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=353
05/24/2022 07:02:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=354
05/24/2022 07:02:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=356
05/24/2022 07:02:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=358
05/24/2022 07:02:28 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.12858493752110772 on epoch=358
05/24/2022 07:02:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=359
05/24/2022 07:02:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=361
05/24/2022 07:02:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=363
05/24/2022 07:02:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=364
05/24/2022 07:02:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=366
05/24/2022 07:02:44 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.07675485008818342 on epoch=366
05/24/2022 07:02:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=368
05/24/2022 07:02:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=369
05/24/2022 07:02:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=371
05/24/2022 07:02:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=373
05/24/2022 07:02:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=374
05/24/2022 07:03:01 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.13066440373383917 on epoch=374
05/24/2022 07:03:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=376
05/24/2022 07:03:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=378
05/24/2022 07:03:09 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=379
05/24/2022 07:03:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=381
05/24/2022 07:03:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=383
05/24/2022 07:03:17 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.19670277749654078 on epoch=383
05/24/2022 07:03:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=384
05/24/2022 07:03:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=386
05/24/2022 07:03:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=388
05/24/2022 07:03:28 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=389
05/24/2022 07:03:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=391
05/24/2022 07:03:33 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.12037043679117378 on epoch=391
05/24/2022 07:03:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=393
05/24/2022 07:03:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=394
05/24/2022 07:03:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=396
05/24/2022 07:03:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=398
05/24/2022 07:03:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=399
05/24/2022 07:03:49 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.08454106280193237 on epoch=399
05/24/2022 07:03:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=401
05/24/2022 07:03:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=403
05/24/2022 07:03:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=404
05/24/2022 07:04:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=406
05/24/2022 07:04:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=408
05/24/2022 07:04:05 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.1163160355306218 on epoch=408
05/24/2022 07:04:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=409
05/24/2022 07:04:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=411
05/24/2022 07:04:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=413
05/24/2022 07:04:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=414
05/24/2022 07:04:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=416
05/24/2022 07:04:21 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.1303247636148505 on epoch=416
05/24/2022 07:04:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=418
05/24/2022 07:04:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=419
05/24/2022 07:04:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=421
05/24/2022 07:04:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=423
05/24/2022 07:04:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
05/24/2022 07:04:37 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.13787716797339036 on epoch=424
05/24/2022 07:04:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=426
05/24/2022 07:04:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=428
05/24/2022 07:04:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=429
05/24/2022 07:04:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=431
05/24/2022 07:04:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=433
05/24/2022 07:04:54 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.09728678929765885 on epoch=433
05/24/2022 07:04:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=434
05/24/2022 07:04:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 07:05:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=438
05/24/2022 07:05:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
05/24/2022 07:05:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=441
05/24/2022 07:05:10 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.12710980775496905 on epoch=441
05/24/2022 07:05:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=443
05/24/2022 07:05:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 07:05:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=446
05/24/2022 07:05:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=448
05/24/2022 07:05:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=449
05/24/2022 07:05:26 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.11948413787280748 on epoch=449
05/24/2022 07:05:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=451
05/24/2022 07:05:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=453
05/24/2022 07:05:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=454
05/24/2022 07:05:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=456
05/24/2022 07:05:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=458
05/24/2022 07:05:42 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.15001908615490323 on epoch=458
05/24/2022 07:05:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/24/2022 07:05:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=461
05/24/2022 07:05:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=463
05/24/2022 07:05:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=464
05/24/2022 07:05:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=466
05/24/2022 07:05:58 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.1161777415701378 on epoch=466
05/24/2022 07:06:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=468
05/24/2022 07:06:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=469
05/24/2022 07:06:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=471
05/24/2022 07:06:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=473
05/24/2022 07:06:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=474
05/24/2022 07:06:14 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.10986366949377153 on epoch=474
05/24/2022 07:06:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=476
05/24/2022 07:06:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=478
05/24/2022 07:06:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=479
05/24/2022 07:06:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=481
05/24/2022 07:06:27 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=483
05/24/2022 07:06:30 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.11753364088793772 on epoch=483
05/24/2022 07:06:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=484
05/24/2022 07:06:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 07:06:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
05/24/2022 07:06:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 07:06:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=491
05/24/2022 07:06:46 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.10322128851540616 on epoch=491
05/24/2022 07:06:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
05/24/2022 07:06:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 07:06:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=496
05/24/2022 07:06:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=498
05/24/2022 07:06:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
05/24/2022 07:07:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:07:01 - INFO - __main__ - Printing 3 examples
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:07:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:07:01 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:07:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:07:01 - INFO - __main__ - Printing 3 examples
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 07:07:01 - INFO - __main__ - ['neutral']
05/24/2022 07:07:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:07:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:07:01 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:07:02 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.12519847302455997 on epoch=499
05/24/2022 07:07:02 - INFO - __main__ - save last model!
05/24/2022 07:07:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 07:07:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 07:07:02 - INFO - __main__ - Printing 3 examples
05/24/2022 07:07:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 07:07:02 - INFO - __main__ - ['contradiction']
05/24/2022 07:07:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 07:07:02 - INFO - __main__ - ['entailment']
05/24/2022 07:07:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 07:07:02 - INFO - __main__ - ['contradiction']
05/24/2022 07:07:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:07:03 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:07:04 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 07:07:16 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:07:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:07:17 - INFO - __main__ - Starting training!
05/24/2022 07:07:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_100_0.4_8_predictions.txt
05/24/2022 07:07:34 - INFO - __main__ - Classification-F1 on test data: 0.0204
05/24/2022 07:07:34 - INFO - __main__ - prefix=anli_32_100, lr=0.4, bsz=8, dev_performance=0.3912420535917298, test_performance=0.020420116780076335
05/24/2022 07:07:34 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.3, bsz=8 ...
05/24/2022 07:07:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:07:35 - INFO - __main__ - Printing 3 examples
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:07:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:07:35 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:07:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:07:35 - INFO - __main__ - Printing 3 examples
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 07:07:35 - INFO - __main__ - ['neutral']
05/24/2022 07:07:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:07:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:07:35 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:07:50 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:07:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:07:51 - INFO - __main__ - Starting training!
05/24/2022 07:07:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.51 on epoch=1
05/24/2022 07:07:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=3
05/24/2022 07:08:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=4
05/24/2022 07:08:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=6
05/24/2022 07:08:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=8
05/24/2022 07:08:08 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 07:08:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 07:08:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=9
05/24/2022 07:08:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=11
05/24/2022 07:08:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=13
05/24/2022 07:08:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=14
05/24/2022 07:08:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=16
05/24/2022 07:08:23 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 07:08:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=18
05/24/2022 07:08:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=19
05/24/2022 07:08:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=21
05/24/2022 07:08:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
05/24/2022 07:08:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
05/24/2022 07:08:38 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 07:08:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=26
05/24/2022 07:08:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=28
05/24/2022 07:08:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=29
05/24/2022 07:08:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
05/24/2022 07:08:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=33
05/24/2022 07:08:54 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 07:08:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=34
05/24/2022 07:08:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=36
05/24/2022 07:09:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
05/24/2022 07:09:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
05/24/2022 07:09:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=41
05/24/2022 07:09:10 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 07:09:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=43
05/24/2022 07:09:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=44
05/24/2022 07:09:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=46
05/24/2022 07:09:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=48
05/24/2022 07:09:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=49
05/24/2022 07:09:25 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 07:09:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=51
05/24/2022 07:09:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=53
05/24/2022 07:09:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=54
05/24/2022 07:09:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=56
05/24/2022 07:09:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=58
05/24/2022 07:09:41 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 07:09:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=59
05/24/2022 07:09:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=61
05/24/2022 07:09:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=63
05/24/2022 07:09:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=64
05/24/2022 07:09:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=66
05/24/2022 07:09:56 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 07:09:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=68
05/24/2022 07:10:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=69
05/24/2022 07:10:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=71
05/24/2022 07:10:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/24/2022 07:10:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/24/2022 07:10:12 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.22899728997289973 on epoch=74
05/24/2022 07:10:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22899728997289973 on epoch=74, global_step=450
05/24/2022 07:10:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=76
05/24/2022 07:10:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=78
05/24/2022 07:10:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=79
05/24/2022 07:10:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
05/24/2022 07:10:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=83
05/24/2022 07:10:27 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.26786092896490843 on epoch=83
05/24/2022 07:10:27 - INFO - __main__ - Saving model with best Classification-F1: 0.22899728997289973 -> 0.26786092896490843 on epoch=83, global_step=500
05/24/2022 07:10:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=84
05/24/2022 07:10:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=86
05/24/2022 07:10:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=88
05/24/2022 07:10:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=89
05/24/2022 07:10:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=91
05/24/2022 07:10:43 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.23041894353369763 on epoch=91
05/24/2022 07:10:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=93
05/24/2022 07:10:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=94
05/24/2022 07:10:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=96
05/24/2022 07:10:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=98
05/24/2022 07:10:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=99
05/24/2022 07:10:58 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.33003800260437427 on epoch=99
05/24/2022 07:10:58 - INFO - __main__ - Saving model with best Classification-F1: 0.26786092896490843 -> 0.33003800260437427 on epoch=99, global_step=600
05/24/2022 07:11:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=101
05/24/2022 07:11:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=103
05/24/2022 07:11:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=104
05/24/2022 07:11:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=106
05/24/2022 07:11:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=108
05/24/2022 07:11:14 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.3593301435406699 on epoch=108
05/24/2022 07:11:14 - INFO - __main__ - Saving model with best Classification-F1: 0.33003800260437427 -> 0.3593301435406699 on epoch=108, global_step=650
05/24/2022 07:11:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=109
05/24/2022 07:11:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=111
05/24/2022 07:11:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=113
05/24/2022 07:11:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=114
05/24/2022 07:11:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=116
05/24/2022 07:11:29 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.3233489958167291 on epoch=116
05/24/2022 07:11:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=118
05/24/2022 07:11:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=119
05/24/2022 07:11:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=121
05/24/2022 07:11:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=123
05/24/2022 07:11:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=124
05/24/2022 07:11:44 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.3445908533117836 on epoch=124
05/24/2022 07:11:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=126
05/24/2022 07:11:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=128
05/24/2022 07:11:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=129
05/24/2022 07:11:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=131
05/24/2022 07:11:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=133
05/24/2022 07:12:00 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.3243651765071594 on epoch=133
05/24/2022 07:12:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=134
05/24/2022 07:12:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=136
05/24/2022 07:12:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=138
05/24/2022 07:12:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=139
05/24/2022 07:12:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=141
05/24/2022 07:12:15 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.31519795657726696 on epoch=141
05/24/2022 07:12:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=143
05/24/2022 07:12:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=144
05/24/2022 07:12:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=146
05/24/2022 07:12:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=148
05/24/2022 07:12:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=149
05/24/2022 07:12:30 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.34609782981876 on epoch=149
05/24/2022 07:12:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=151
05/24/2022 07:12:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=153
05/24/2022 07:12:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=154
05/24/2022 07:12:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=156
05/24/2022 07:12:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=158
05/24/2022 07:12:46 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.3517195767195767 on epoch=158
05/24/2022 07:12:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=159
05/24/2022 07:12:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=161
05/24/2022 07:12:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=163
05/24/2022 07:12:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=164
05/24/2022 07:12:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=166
05/24/2022 07:13:01 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.3265728352937655 on epoch=166
05/24/2022 07:13:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=168
05/24/2022 07:13:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=169
05/24/2022 07:13:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=171
05/24/2022 07:13:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=173
05/24/2022 07:13:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=174
05/24/2022 07:13:16 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.3517195767195767 on epoch=174
05/24/2022 07:13:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=176
05/24/2022 07:13:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=178
05/24/2022 07:13:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=179
05/24/2022 07:13:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=181
05/24/2022 07:13:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=183
05/24/2022 07:13:32 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.34591101847739014 on epoch=183
05/24/2022 07:13:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=184
05/24/2022 07:13:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=186
05/24/2022 07:13:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=188
05/24/2022 07:13:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=189
05/24/2022 07:13:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=191
05/24/2022 07:13:47 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.36704075435627087 on epoch=191
05/24/2022 07:13:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3593301435406699 -> 0.36704075435627087 on epoch=191, global_step=1150
05/24/2022 07:13:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=193
05/24/2022 07:13:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=194
05/24/2022 07:13:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=196
05/24/2022 07:13:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=198
05/24/2022 07:14:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=199
05/24/2022 07:14:03 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.3392301392301393 on epoch=199
05/24/2022 07:14:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=201
05/24/2022 07:14:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=203
05/24/2022 07:14:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=204
05/24/2022 07:14:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=206
05/24/2022 07:14:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=208
05/24/2022 07:14:18 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.34438832772166106 on epoch=208
05/24/2022 07:14:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=209
05/24/2022 07:14:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=211
05/24/2022 07:14:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=213
05/24/2022 07:14:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=214
05/24/2022 07:14:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=216
05/24/2022 07:14:33 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.32789300045937214 on epoch=216
05/24/2022 07:14:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=218
05/24/2022 07:14:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=219
05/24/2022 07:14:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=221
05/24/2022 07:14:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=223
05/24/2022 07:14:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=224
05/24/2022 07:14:48 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.3737373737373737 on epoch=224
05/24/2022 07:14:48 - INFO - __main__ - Saving model with best Classification-F1: 0.36704075435627087 -> 0.3737373737373737 on epoch=224, global_step=1350
05/24/2022 07:14:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=226
05/24/2022 07:14:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=228
05/24/2022 07:14:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=229
05/24/2022 07:14:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=231
05/24/2022 07:15:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=233
05/24/2022 07:15:04 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.3799245653465602 on epoch=233
05/24/2022 07:15:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3737373737373737 -> 0.3799245653465602 on epoch=233, global_step=1400
05/24/2022 07:15:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=234
05/24/2022 07:15:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=236
05/24/2022 07:15:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=238
05/24/2022 07:15:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=239
05/24/2022 07:15:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=241
05/24/2022 07:15:19 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.41165089224095436 on epoch=241
05/24/2022 07:15:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3799245653465602 -> 0.41165089224095436 on epoch=241, global_step=1450
05/24/2022 07:15:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=243
05/24/2022 07:15:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=244
05/24/2022 07:15:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=246
05/24/2022 07:15:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=248
05/24/2022 07:15:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=249
05/24/2022 07:15:35 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.4205516705516706 on epoch=249
05/24/2022 07:15:35 - INFO - __main__ - Saving model with best Classification-F1: 0.41165089224095436 -> 0.4205516705516706 on epoch=249, global_step=1500
05/24/2022 07:15:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=251
05/24/2022 07:15:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=253
05/24/2022 07:15:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=254
05/24/2022 07:15:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=256
05/24/2022 07:15:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=258
05/24/2022 07:15:51 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.4349128540305011 on epoch=258
05/24/2022 07:15:51 - INFO - __main__ - Saving model with best Classification-F1: 0.4205516705516706 -> 0.4349128540305011 on epoch=258, global_step=1550
05/24/2022 07:15:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=259
05/24/2022 07:15:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=261
05/24/2022 07:15:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=263
05/24/2022 07:16:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=264
05/24/2022 07:16:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=266
05/24/2022 07:16:08 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.40148774567379214 on epoch=266
05/24/2022 07:16:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=268
05/24/2022 07:16:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=269
05/24/2022 07:16:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=271
05/24/2022 07:16:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=273
05/24/2022 07:16:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=274
05/24/2022 07:16:23 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.3949106449106449 on epoch=274
05/24/2022 07:16:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=276
05/24/2022 07:16:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=278
05/24/2022 07:16:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=279
05/24/2022 07:16:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=281
05/24/2022 07:16:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=283
05/24/2022 07:16:39 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.42544446856756885 on epoch=283
05/24/2022 07:16:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=284
05/24/2022 07:16:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=286
05/24/2022 07:16:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=288
05/24/2022 07:16:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=289
05/24/2022 07:16:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=291
05/24/2022 07:16:54 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.43165838024907993 on epoch=291
05/24/2022 07:16:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=293
05/24/2022 07:16:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=294
05/24/2022 07:17:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=296
05/24/2022 07:17:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=298
05/24/2022 07:17:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=299
05/24/2022 07:17:10 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.40972222222222215 on epoch=299
05/24/2022 07:17:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=301
05/24/2022 07:17:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=303
05/24/2022 07:17:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=304
05/24/2022 07:17:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=306
05/24/2022 07:17:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=308
05/24/2022 07:17:26 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.3406593406593406 on epoch=308
05/24/2022 07:17:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=309
05/24/2022 07:17:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=311
05/24/2022 07:17:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=313
05/24/2022 07:17:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=314
05/24/2022 07:17:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=316
05/24/2022 07:17:41 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.4470254383297861 on epoch=316
05/24/2022 07:17:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4349128540305011 -> 0.4470254383297861 on epoch=316, global_step=1900
05/24/2022 07:17:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=318
05/24/2022 07:17:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=319
05/24/2022 07:17:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=321
05/24/2022 07:17:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=323
05/24/2022 07:17:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=324
05/24/2022 07:17:57 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.32257683215130023 on epoch=324
05/24/2022 07:17:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=326
05/24/2022 07:18:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=328
05/24/2022 07:18:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=329
05/24/2022 07:18:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=331
05/24/2022 07:18:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=333
05/24/2022 07:18:12 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.17334269662921348 on epoch=333
05/24/2022 07:18:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=334
05/24/2022 07:18:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=336
05/24/2022 07:18:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=338
05/24/2022 07:18:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=339
05/24/2022 07:18:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=341
05/24/2022 07:18:28 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.18652501787508377 on epoch=341
05/24/2022 07:18:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=343
05/24/2022 07:18:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=344
05/24/2022 07:18:36 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=346
05/24/2022 07:18:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=348
05/24/2022 07:18:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=349
05/24/2022 07:18:44 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.33297194296602933 on epoch=349
05/24/2022 07:18:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=351
05/24/2022 07:18:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=353
05/24/2022 07:18:52 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=354
05/24/2022 07:18:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=356
05/24/2022 07:18:57 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=358
05/24/2022 07:19:00 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.31418385536032595 on epoch=358
05/24/2022 07:19:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=359
05/24/2022 07:19:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=361
05/24/2022 07:19:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=363
05/24/2022 07:19:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=364
05/24/2022 07:19:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=366
05/24/2022 07:19:15 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.2668266516092603 on epoch=366
05/24/2022 07:19:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=368
05/24/2022 07:19:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=369
05/24/2022 07:19:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=371
05/24/2022 07:19:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=373
05/24/2022 07:19:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=374
05/24/2022 07:19:31 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.19786729397881267 on epoch=374
05/24/2022 07:19:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=376
05/24/2022 07:19:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=378
05/24/2022 07:19:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=379
05/24/2022 07:19:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=381
05/24/2022 07:19:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=383
05/24/2022 07:19:47 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.19927971188475388 on epoch=383
05/24/2022 07:19:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=384
05/24/2022 07:19:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=386
05/24/2022 07:19:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=388
05/24/2022 07:19:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=389
05/24/2022 07:20:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=391
05/24/2022 07:20:03 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.16769354852120696 on epoch=391
05/24/2022 07:20:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=393
05/24/2022 07:20:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=394
05/24/2022 07:20:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=396
05/24/2022 07:20:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=398
05/24/2022 07:20:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=399
05/24/2022 07:20:19 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.17392676767676765 on epoch=399
05/24/2022 07:20:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=401
05/24/2022 07:20:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=403
05/24/2022 07:20:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=404
05/24/2022 07:20:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=406
05/24/2022 07:20:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=408
05/24/2022 07:20:35 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.19976813338525257 on epoch=408
05/24/2022 07:20:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=409
05/24/2022 07:20:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=411
05/24/2022 07:20:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=413
05/24/2022 07:20:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=414
05/24/2022 07:20:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=416
05/24/2022 07:20:50 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.1373779199866156 on epoch=416
05/24/2022 07:20:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=418
05/24/2022 07:20:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
05/24/2022 07:20:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=421
05/24/2022 07:21:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.11 on epoch=423
05/24/2022 07:21:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=424
05/24/2022 07:21:06 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.14444444444444443 on epoch=424
05/24/2022 07:21:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=426
05/24/2022 07:21:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=428
05/24/2022 07:21:14 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=429
05/24/2022 07:21:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=431
05/24/2022 07:21:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=433
05/24/2022 07:21:22 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.17915184318719804 on epoch=433
05/24/2022 07:21:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=434
05/24/2022 07:21:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=436
05/24/2022 07:21:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=438
05/24/2022 07:21:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=439
05/24/2022 07:21:35 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=441
05/24/2022 07:21:38 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.14268733850129198 on epoch=441
05/24/2022 07:21:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=443
05/24/2022 07:21:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
05/24/2022 07:21:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=446
05/24/2022 07:21:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=448
05/24/2022 07:21:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
05/24/2022 07:21:54 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.15906380441264162 on epoch=449
05/24/2022 07:21:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=451
05/24/2022 07:21:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=453
05/24/2022 07:22:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=454
05/24/2022 07:22:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=456
05/24/2022 07:22:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=458
05/24/2022 07:22:10 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.12897625317517653 on epoch=458
05/24/2022 07:22:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/24/2022 07:22:15 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=461
05/24/2022 07:22:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=463
05/24/2022 07:22:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=464
05/24/2022 07:22:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=466
05/24/2022 07:22:25 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.12880836765218104 on epoch=466
05/24/2022 07:22:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=468
05/24/2022 07:22:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=469
05/24/2022 07:22:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=471
05/24/2022 07:22:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=473
05/24/2022 07:22:38 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/24/2022 07:22:41 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.10166825927695491 on epoch=474
05/24/2022 07:22:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=476
05/24/2022 07:22:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=478
05/24/2022 07:22:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/24/2022 07:22:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=481
05/24/2022 07:22:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=483
05/24/2022 07:22:57 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.16800404829032026 on epoch=483
05/24/2022 07:22:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=484
05/24/2022 07:23:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=486
05/24/2022 07:23:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
05/24/2022 07:23:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=489
05/24/2022 07:23:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=491
05/24/2022 07:23:13 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.1264614039308823 on epoch=491
05/24/2022 07:23:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=493
05/24/2022 07:23:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=494
05/24/2022 07:23:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
05/24/2022 07:23:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=498
05/24/2022 07:23:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=499
05/24/2022 07:23:27 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:23:27 - INFO - __main__ - Printing 3 examples
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:23:27 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:23:27 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:23:27 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:23:27 - INFO - __main__ - Printing 3 examples
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 07:23:27 - INFO - __main__ - ['neutral']
05/24/2022 07:23:27 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:23:27 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:23:27 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:23:28 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.081994761728981 on epoch=499
05/24/2022 07:23:28 - INFO - __main__ - save last model!
05/24/2022 07:23:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 07:23:28 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 07:23:28 - INFO - __main__ - Printing 3 examples
05/24/2022 07:23:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 07:23:28 - INFO - __main__ - ['contradiction']
05/24/2022 07:23:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 07:23:28 - INFO - __main__ - ['entailment']
05/24/2022 07:23:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 07:23:28 - INFO - __main__ - ['contradiction']
05/24/2022 07:23:28 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:23:29 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:23:30 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 07:23:43 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:23:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:23:44 - INFO - __main__ - Starting training!
05/24/2022 07:24:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_100_0.3_8_predictions.txt
05/24/2022 07:24:00 - INFO - __main__ - Classification-F1 on test data: 0.0353
05/24/2022 07:24:00 - INFO - __main__ - prefix=anli_32_100, lr=0.3, bsz=8, dev_performance=0.4470254383297861, test_performance=0.035315831252675844
05/24/2022 07:24:00 - INFO - __main__ - Running ... prefix=anli_32_100, lr=0.2, bsz=8 ...
05/24/2022 07:24:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:24:01 - INFO - __main__ - Printing 3 examples
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:24:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:24:01 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:24:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:24:01 - INFO - __main__ - Printing 3 examples
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: Manchester City Football Club is a football club in Manchester, England. Founded in 1880 as St. Mark's (West Gorton), they became Ardwick Association Football Club in 1887 and Manchester City in 1894. The club moved to the City of Manchester Stadium in 2003, having played at Maine Road since 1923. [SEP] hypothesis: Manchester City Football Club plays in a bigger stadium now than they did before 2003. 
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: Luis Walter Alvarez (June 13, 1911 – September 1, 1988) was an American experimental physicist, inventor, and professor who was awarded the Nobel Prize in Physics in 1968. The American Journal of Physics commented, "Luis Alvarez was one of the most brilliant and productive experimental physicists of the twentieth century." [SEP] hypothesis: Luis Walter unfortunately died right after his Nobel Prize was awarded.
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ -  [anli] premise: The 29th Los Angeles Film Critics Association Awards, given by the Los Angeles Film Critics Association (LAFCA) on January 7, 2004, honored the best in film for 2003. The ceremony was originally called off because of the MPAA screener ban as members felt they could not see all the movies in time for their awards but when that was removed the show was back on. [SEP] hypothesis: Brad Pitt's 2003 movie won at the LAFCAA in January 2004.
05/24/2022 07:24:01 - INFO - __main__ - ['neutral']
05/24/2022 07:24:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:24:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:24:01 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:24:16 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:24:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:24:17 - INFO - __main__ - Starting training!
05/24/2022 07:24:20 - INFO - __main__ - Step 10 Global step 10 Train loss 0.47 on epoch=1
05/24/2022 07:24:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=3
05/24/2022 07:24:26 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=4
05/24/2022 07:24:28 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=6
05/24/2022 07:24:31 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=8
05/24/2022 07:24:34 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 07:24:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 07:24:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=9
05/24/2022 07:24:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=11
05/24/2022 07:24:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=13
05/24/2022 07:24:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
05/24/2022 07:24:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=16
05/24/2022 07:24:49 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 07:24:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.47 on epoch=18
05/24/2022 07:24:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=19
05/24/2022 07:24:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=21
05/24/2022 07:24:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=23
05/24/2022 07:25:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
05/24/2022 07:25:05 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 07:25:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
05/24/2022 07:25:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
05/24/2022 07:25:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=29
05/24/2022 07:25:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=31
05/24/2022 07:25:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=33
05/24/2022 07:25:20 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.1679790026246719 on epoch=33
05/24/2022 07:25:20 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1679790026246719 on epoch=33, global_step=200
05/24/2022 07:25:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
05/24/2022 07:25:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/24/2022 07:25:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
05/24/2022 07:25:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=39
05/24/2022 07:25:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=41
05/24/2022 07:25:36 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 07:25:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=43
05/24/2022 07:25:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=44
05/24/2022 07:25:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=46
05/24/2022 07:25:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=48
05/24/2022 07:25:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
05/24/2022 07:25:52 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 07:25:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=51
05/24/2022 07:25:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=53
05/24/2022 07:25:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=54
05/24/2022 07:26:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=56
05/24/2022 07:26:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/24/2022 07:26:07 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 07:26:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=59
05/24/2022 07:26:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=61
05/24/2022 07:26:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
05/24/2022 07:26:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=64
05/24/2022 07:26:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=66
05/24/2022 07:26:23 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 07:26:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=68
05/24/2022 07:26:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=69
05/24/2022 07:26:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
05/24/2022 07:26:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=73
05/24/2022 07:26:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/24/2022 07:26:39 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=74
05/24/2022 07:26:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=76
05/24/2022 07:26:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=78
05/24/2022 07:26:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=79
05/24/2022 07:26:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
05/24/2022 07:26:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=83
05/24/2022 07:26:54 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 07:26:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/24/2022 07:26:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=86
05/24/2022 07:27:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=88
05/24/2022 07:27:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=89
05/24/2022 07:27:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=91
05/24/2022 07:27:10 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.1679790026246719 on epoch=91
05/24/2022 07:27:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
05/24/2022 07:27:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=94
05/24/2022 07:27:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=96
05/24/2022 07:27:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/24/2022 07:27:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=99
05/24/2022 07:27:25 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=99
05/24/2022 07:27:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=101
05/24/2022 07:27:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=103
05/24/2022 07:27:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=104
05/24/2022 07:27:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=106
05/24/2022 07:27:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=108
05/24/2022 07:27:41 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=108
05/24/2022 07:27:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=109
05/24/2022 07:27:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
05/24/2022 07:27:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=113
05/24/2022 07:27:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=114
05/24/2022 07:27:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=116
05/24/2022 07:27:57 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.21013824884792628 on epoch=116
05/24/2022 07:27:57 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.21013824884792628 on epoch=116, global_step=700
05/24/2022 07:27:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/24/2022 07:28:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=119
05/24/2022 07:28:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=121
05/24/2022 07:28:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=123
05/24/2022 07:28:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=124
05/24/2022 07:28:13 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.211258697027198 on epoch=124
05/24/2022 07:28:13 - INFO - __main__ - Saving model with best Classification-F1: 0.21013824884792628 -> 0.211258697027198 on epoch=124, global_step=750
05/24/2022 07:28:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=126
05/24/2022 07:28:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=128
05/24/2022 07:28:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=129
05/24/2022 07:28:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=131
05/24/2022 07:28:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=133
05/24/2022 07:28:28 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.25105386416861825 on epoch=133
05/24/2022 07:28:28 - INFO - __main__ - Saving model with best Classification-F1: 0.211258697027198 -> 0.25105386416861825 on epoch=133, global_step=800
05/24/2022 07:28:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=134
05/24/2022 07:28:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/24/2022 07:28:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=138
05/24/2022 07:28:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=139
05/24/2022 07:28:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=141
05/24/2022 07:28:44 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.22780952380952382 on epoch=141
05/24/2022 07:28:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=143
05/24/2022 07:28:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=144
05/24/2022 07:28:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=146
05/24/2022 07:28:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=148
05/24/2022 07:28:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.29 on epoch=149
05/24/2022 07:28:59 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.275721895149986 on epoch=149
05/24/2022 07:28:59 - INFO - __main__ - Saving model with best Classification-F1: 0.25105386416861825 -> 0.275721895149986 on epoch=149, global_step=900
05/24/2022 07:29:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=151
05/24/2022 07:29:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=153
05/24/2022 07:29:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=154
05/24/2022 07:29:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=156
05/24/2022 07:29:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
05/24/2022 07:29:15 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.34530491323636975 on epoch=158
05/24/2022 07:29:15 - INFO - __main__ - Saving model with best Classification-F1: 0.275721895149986 -> 0.34530491323636975 on epoch=158, global_step=950
05/24/2022 07:29:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=159
05/24/2022 07:29:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=161
05/24/2022 07:29:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=163
05/24/2022 07:29:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=164
05/24/2022 07:29:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=166
05/24/2022 07:29:31 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.2576923076923077 on epoch=166
05/24/2022 07:29:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=168
05/24/2022 07:29:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=169
05/24/2022 07:29:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=171
05/24/2022 07:29:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=173
05/24/2022 07:29:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=174
05/24/2022 07:29:46 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.28625235404896415 on epoch=174
05/24/2022 07:29:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=176
05/24/2022 07:29:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=178
05/24/2022 07:29:54 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=179
05/24/2022 07:29:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=181
05/24/2022 07:29:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=183
05/24/2022 07:30:01 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.28625235404896415 on epoch=183
05/24/2022 07:30:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=184
05/24/2022 07:30:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=186
05/24/2022 07:30:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=188
05/24/2022 07:30:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=189
05/24/2022 07:30:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=191
05/24/2022 07:30:16 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.3006691843901146 on epoch=191
05/24/2022 07:30:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=193
05/24/2022 07:30:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=194
05/24/2022 07:30:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=196
05/24/2022 07:30:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=198
05/24/2022 07:30:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=199
05/24/2022 07:30:31 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.3006691843901146 on epoch=199
05/24/2022 07:30:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=201
05/24/2022 07:30:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=203
05/24/2022 07:30:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=204
05/24/2022 07:30:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=206
05/24/2022 07:30:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=208
05/24/2022 07:30:47 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.3145245559038663 on epoch=208
05/24/2022 07:30:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=209
05/24/2022 07:30:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=211
05/24/2022 07:30:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=213
05/24/2022 07:30:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=214
05/24/2022 07:30:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=216
05/24/2022 07:31:02 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.3053175574595403 on epoch=216
05/24/2022 07:31:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=218
05/24/2022 07:31:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=219
05/24/2022 07:31:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=221
05/24/2022 07:31:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.31 on epoch=223
05/24/2022 07:31:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=224
05/24/2022 07:31:17 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.3037411526794742 on epoch=224
05/24/2022 07:31:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=226
05/24/2022 07:31:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=228
05/24/2022 07:31:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=229
05/24/2022 07:31:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=231
05/24/2022 07:31:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=233
05/24/2022 07:31:32 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.3192544918208635 on epoch=233
05/24/2022 07:31:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=234
05/24/2022 07:31:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=236
05/24/2022 07:31:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=238
05/24/2022 07:31:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=239
05/24/2022 07:31:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=241
05/24/2022 07:31:47 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.3066942719116632 on epoch=241
05/24/2022 07:31:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=243
05/24/2022 07:31:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=244
05/24/2022 07:31:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=246
05/24/2022 07:31:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=248
05/24/2022 07:32:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=249
05/24/2022 07:32:03 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.33111372241807024 on epoch=249
05/24/2022 07:32:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=251
05/24/2022 07:32:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=253
05/24/2022 07:32:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=254
05/24/2022 07:32:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=256
05/24/2022 07:32:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=258
05/24/2022 07:32:18 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.33280632411067196 on epoch=258
05/24/2022 07:32:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=259
05/24/2022 07:32:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=261
05/24/2022 07:32:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=263
05/24/2022 07:32:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=264
05/24/2022 07:32:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=266
05/24/2022 07:32:33 - INFO - __main__ - Global step 1600 Train loss 0.24 Classification-F1 0.3345299827150645 on epoch=266
05/24/2022 07:32:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=268
05/24/2022 07:32:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=269
05/24/2022 07:32:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=271
05/24/2022 07:32:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=273
05/24/2022 07:32:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=274
05/24/2022 07:32:49 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.3610549943883277 on epoch=274
05/24/2022 07:32:49 - INFO - __main__ - Saving model with best Classification-F1: 0.34530491323636975 -> 0.3610549943883277 on epoch=274, global_step=1650
05/24/2022 07:32:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=276
05/24/2022 07:32:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=278
05/24/2022 07:32:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=279
05/24/2022 07:32:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=281
05/24/2022 07:33:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=283
05/24/2022 07:33:04 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.38363777653180664 on epoch=283
05/24/2022 07:33:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3610549943883277 -> 0.38363777653180664 on epoch=283, global_step=1700
05/24/2022 07:33:07 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=284
05/24/2022 07:33:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=286
05/24/2022 07:33:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=288
05/24/2022 07:33:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=289
05/24/2022 07:33:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=291
05/24/2022 07:33:20 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.34910689867055944 on epoch=291
05/24/2022 07:33:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=293
05/24/2022 07:33:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=294
05/24/2022 07:33:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=296
05/24/2022 07:33:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=298
05/24/2022 07:33:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=299
05/24/2022 07:33:35 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.35027344818156964 on epoch=299
05/24/2022 07:33:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=301
05/24/2022 07:33:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=303
05/24/2022 07:33:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=304
05/24/2022 07:33:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=306
05/24/2022 07:33:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=308
05/24/2022 07:33:50 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.355167970771384 on epoch=308
05/24/2022 07:33:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=309
05/24/2022 07:33:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=311
05/24/2022 07:33:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=313
05/24/2022 07:34:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=314
05/24/2022 07:34:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=316
05/24/2022 07:34:06 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.38340425531914896 on epoch=316
05/24/2022 07:34:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=318
05/24/2022 07:34:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=319
05/24/2022 07:34:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=321
05/24/2022 07:34:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=323
05/24/2022 07:34:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=324
05/24/2022 07:34:22 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.38650958119067713 on epoch=324
05/24/2022 07:34:22 - INFO - __main__ - Saving model with best Classification-F1: 0.38363777653180664 -> 0.38650958119067713 on epoch=324, global_step=1950
05/24/2022 07:34:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=326
05/24/2022 07:34:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=328
05/24/2022 07:34:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=329
05/24/2022 07:34:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=331
05/24/2022 07:34:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=333
05/24/2022 07:34:38 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.36823073611709 on epoch=333
05/24/2022 07:34:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=334
05/24/2022 07:34:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=336
05/24/2022 07:34:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=338
05/24/2022 07:34:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=339
05/24/2022 07:34:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=341
05/24/2022 07:34:53 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.36823073611709 on epoch=341
05/24/2022 07:34:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=343
05/24/2022 07:34:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=344
05/24/2022 07:35:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=346
05/24/2022 07:35:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=348
05/24/2022 07:35:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=349
05/24/2022 07:35:09 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.404848171152519 on epoch=349
05/24/2022 07:35:09 - INFO - __main__ - Saving model with best Classification-F1: 0.38650958119067713 -> 0.404848171152519 on epoch=349, global_step=2100
05/24/2022 07:35:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=351
05/24/2022 07:35:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=353
05/24/2022 07:35:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=354
05/24/2022 07:35:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=356
05/24/2022 07:35:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=358
05/24/2022 07:35:25 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.3856139214204142 on epoch=358
05/24/2022 07:35:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=359
05/24/2022 07:35:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=361
05/24/2022 07:35:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=363
05/24/2022 07:35:35 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=364
05/24/2022 07:35:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=366
05/24/2022 07:35:41 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.3384610221819524 on epoch=366
05/24/2022 07:35:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=368
05/24/2022 07:35:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=369
05/24/2022 07:35:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=371
05/24/2022 07:35:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=373
05/24/2022 07:35:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=374
05/24/2022 07:35:56 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.36823073611709 on epoch=374
05/24/2022 07:35:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=376
05/24/2022 07:36:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=378
05/24/2022 07:36:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=379
05/24/2022 07:36:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=381
05/24/2022 07:36:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=383
05/24/2022 07:36:12 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.2578642079942779 on epoch=383
05/24/2022 07:36:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=384
05/24/2022 07:36:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=386
05/24/2022 07:36:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=388
05/24/2022 07:36:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=389
05/24/2022 07:36:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=391
05/24/2022 07:36:28 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.38697588126159554 on epoch=391
05/24/2022 07:36:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=393
05/24/2022 07:36:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=394
05/24/2022 07:36:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=396
05/24/2022 07:36:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=398
05/24/2022 07:36:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=399
05/24/2022 07:36:43 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.2732241442767759 on epoch=399
05/24/2022 07:36:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=401
05/24/2022 07:36:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=403
05/24/2022 07:36:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=404
05/24/2022 07:36:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=406
05/24/2022 07:36:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=408
05/24/2022 07:36:59 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.3121446154912974 on epoch=408
05/24/2022 07:37:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=409
05/24/2022 07:37:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=411
05/24/2022 07:37:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=413
05/24/2022 07:37:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=414
05/24/2022 07:37:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=416
05/24/2022 07:37:15 - INFO - __main__ - Global step 2500 Train loss 0.12 Classification-F1 0.3997606975665933 on epoch=416
05/24/2022 07:37:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=418
05/24/2022 07:37:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=419
05/24/2022 07:37:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=421
05/24/2022 07:37:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=423
05/24/2022 07:37:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=424
05/24/2022 07:37:31 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.2986989946777055 on epoch=424
05/24/2022 07:37:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=426
05/24/2022 07:37:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=428
05/24/2022 07:37:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=429
05/24/2022 07:37:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=431
05/24/2022 07:37:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=433
05/24/2022 07:37:46 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.30704823077364224 on epoch=433
05/24/2022 07:37:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=434
05/24/2022 07:37:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=436
05/24/2022 07:37:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=438
05/24/2022 07:37:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=439
05/24/2022 07:37:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=441
05/24/2022 07:38:02 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.3077620173364854 on epoch=441
05/24/2022 07:38:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=443
05/24/2022 07:38:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=444
05/24/2022 07:38:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=446
05/24/2022 07:38:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=448
05/24/2022 07:38:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=449
05/24/2022 07:38:18 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.22032802504423574 on epoch=449
05/24/2022 07:38:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=451
05/24/2022 07:38:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=453
05/24/2022 07:38:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=454
05/24/2022 07:38:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=456
05/24/2022 07:38:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=458
05/24/2022 07:38:34 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.3313356310464263 on epoch=458
05/24/2022 07:38:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=459
05/24/2022 07:38:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=461
05/24/2022 07:38:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=463
05/24/2022 07:38:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=464
05/24/2022 07:38:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=466
05/24/2022 07:38:50 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.2541349863453119 on epoch=466
05/24/2022 07:38:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=468
05/24/2022 07:38:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=469
05/24/2022 07:38:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=471
05/24/2022 07:39:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=473
05/24/2022 07:39:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=474
05/24/2022 07:39:05 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.2650496421260541 on epoch=474
05/24/2022 07:39:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=476
05/24/2022 07:39:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=478
05/24/2022 07:39:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=479
05/24/2022 07:39:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=481
05/24/2022 07:39:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=483
05/24/2022 07:39:21 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.30034106033717833 on epoch=483
05/24/2022 07:39:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=484
05/24/2022 07:39:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=486
05/24/2022 07:39:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
05/24/2022 07:39:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=489
05/24/2022 07:39:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=491
05/24/2022 07:39:37 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.20979310426386152 on epoch=491
05/24/2022 07:39:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=493
05/24/2022 07:39:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=494
05/24/2022 07:39:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=496
05/24/2022 07:39:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=498
05/24/2022 07:39:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=499
05/24/2022 07:39:51 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:39:51 - INFO - __main__ - Printing 3 examples
05/24/2022 07:39:51 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 07:39:51 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:51 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 07:39:51 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:51 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 07:39:51 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:39:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:39:52 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:39:52 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:39:52 - INFO - __main__ - Printing 3 examples
05/24/2022 07:39:52 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 07:39:52 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:52 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 07:39:52 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:52 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 07:39:52 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:39:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:39:52 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:39:53 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.2581624758220503 on epoch=499
05/24/2022 07:39:53 - INFO - __main__ - save last model!
05/24/2022 07:39:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 07:39:53 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 07:39:53 - INFO - __main__ - Printing 3 examples
05/24/2022 07:39:53 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 07:39:53 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:53 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 07:39:53 - INFO - __main__ - ['entailment']
05/24/2022 07:39:53 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 07:39:53 - INFO - __main__ - ['contradiction']
05/24/2022 07:39:53 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:39:53 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:39:54 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 07:40:07 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:40:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:40:08 - INFO - __main__ - Starting training!
05/24/2022 07:40:24 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_100_0.2_8_predictions.txt
05/24/2022 07:40:24 - INFO - __main__ - Classification-F1 on test data: 0.0433
05/24/2022 07:40:24 - INFO - __main__ - prefix=anli_32_100, lr=0.2, bsz=8, dev_performance=0.404848171152519, test_performance=0.04325096019673295
05/24/2022 07:40:24 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.5, bsz=8 ...
05/24/2022 07:40:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:40:25 - INFO - __main__ - Printing 3 examples
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:40:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:40:25 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:40:25 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:40:25 - INFO - __main__ - Printing 3 examples
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 07:40:25 - INFO - __main__ - ['contradiction']
05/24/2022 07:40:25 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:40:25 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:40:25 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:40:44 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:40:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:40:45 - INFO - __main__ - Starting training!
05/24/2022 07:40:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.43 on epoch=1
05/24/2022 07:40:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.42 on epoch=3
05/24/2022 07:40:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=4
05/24/2022 07:40:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=6
05/24/2022 07:40:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=8
05/24/2022 07:41:01 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 07:41:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 07:41:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=9
05/24/2022 07:41:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=11
05/24/2022 07:41:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=13
05/24/2022 07:41:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=14
05/24/2022 07:41:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=16
05/24/2022 07:41:18 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.1881810228266921 on epoch=16
05/24/2022 07:41:18 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1881810228266921 on epoch=16, global_step=100
05/24/2022 07:41:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=18
05/24/2022 07:41:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=19
05/24/2022 07:41:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=21
05/24/2022 07:41:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
05/24/2022 07:41:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
05/24/2022 07:41:34 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 07:41:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=26
05/24/2022 07:41:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
05/24/2022 07:41:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=29
05/24/2022 07:41:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
05/24/2022 07:41:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=33
05/24/2022 07:41:50 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 07:41:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=34
05/24/2022 07:41:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=36
05/24/2022 07:41:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=38
05/24/2022 07:42:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=39
05/24/2022 07:42:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=41
05/24/2022 07:42:06 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.1881810228266921 on epoch=41
05/24/2022 07:42:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=43
05/24/2022 07:42:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
05/24/2022 07:42:14 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=46
05/24/2022 07:42:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=48
05/24/2022 07:42:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
05/24/2022 07:42:22 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 07:42:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=51
05/24/2022 07:42:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
05/24/2022 07:42:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=54
05/24/2022 07:42:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/24/2022 07:42:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=58
05/24/2022 07:42:38 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=58
05/24/2022 07:42:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=59
05/24/2022 07:42:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=61
05/24/2022 07:42:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
05/24/2022 07:42:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=64
05/24/2022 07:42:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=66
05/24/2022 07:42:55 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3108722612649855 on epoch=66
05/24/2022 07:42:55 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.3108722612649855 on epoch=66, global_step=400
05/24/2022 07:42:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=68
05/24/2022 07:43:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=69
05/24/2022 07:43:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=71
05/24/2022 07:43:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=73
05/24/2022 07:43:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=74
05/24/2022 07:43:10 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.32364810330912025 on epoch=74
05/24/2022 07:43:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3108722612649855 -> 0.32364810330912025 on epoch=74, global_step=450
05/24/2022 07:43:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=76
05/24/2022 07:43:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=78
05/24/2022 07:43:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=79
05/24/2022 07:43:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=81
05/24/2022 07:43:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=83
05/24/2022 07:43:26 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3551995931858632 on epoch=83
05/24/2022 07:43:26 - INFO - __main__ - Saving model with best Classification-F1: 0.32364810330912025 -> 0.3551995931858632 on epoch=83, global_step=500
05/24/2022 07:43:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=84
05/24/2022 07:43:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=86
05/24/2022 07:43:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=88
05/24/2022 07:43:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=89
05/24/2022 07:43:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=91
05/24/2022 07:43:42 - INFO - __main__ - Global step 550 Train loss 0.34 Classification-F1 0.3766623766623766 on epoch=91
05/24/2022 07:43:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3551995931858632 -> 0.3766623766623766 on epoch=91, global_step=550
05/24/2022 07:43:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=93
05/24/2022 07:43:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=94
05/24/2022 07:43:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=96
05/24/2022 07:43:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=98
05/24/2022 07:43:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=99
05/24/2022 07:43:57 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.3650793650793651 on epoch=99
05/24/2022 07:44:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=101
05/24/2022 07:44:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=103
05/24/2022 07:44:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=104
05/24/2022 07:44:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=106
05/24/2022 07:44:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=108
05/24/2022 07:44:13 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.3878787878787879 on epoch=108
05/24/2022 07:44:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3766623766623766 -> 0.3878787878787879 on epoch=108, global_step=650
05/24/2022 07:44:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=109
05/24/2022 07:44:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=111
05/24/2022 07:44:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=113
05/24/2022 07:44:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=114
05/24/2022 07:44:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=116
05/24/2022 07:44:29 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.3878787878787879 on epoch=116
05/24/2022 07:44:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=118
05/24/2022 07:44:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=119
05/24/2022 07:44:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=121
05/24/2022 07:44:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=123
05/24/2022 07:44:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=124
05/24/2022 07:44:45 - INFO - __main__ - Global step 750 Train loss 0.26 Classification-F1 0.3878787878787879 on epoch=124
05/24/2022 07:44:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=126
05/24/2022 07:44:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=128
05/24/2022 07:44:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=129
05/24/2022 07:44:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=131
05/24/2022 07:44:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=133
05/24/2022 07:45:02 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.3766623766623766 on epoch=133
05/24/2022 07:45:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=134
05/24/2022 07:45:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=136
05/24/2022 07:45:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=138
05/24/2022 07:45:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=139
05/24/2022 07:45:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=141
05/24/2022 07:45:18 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.4069828954329043 on epoch=141
05/24/2022 07:45:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3878787878787879 -> 0.4069828954329043 on epoch=141, global_step=850
05/24/2022 07:45:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=143
05/24/2022 07:45:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=144
05/24/2022 07:45:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=146
05/24/2022 07:45:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=148
05/24/2022 07:45:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=149
05/24/2022 07:45:34 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.4271284271284271 on epoch=149
05/24/2022 07:45:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4069828954329043 -> 0.4271284271284271 on epoch=149, global_step=900
05/24/2022 07:45:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=151
05/24/2022 07:45:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=153
05/24/2022 07:45:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=154
05/24/2022 07:45:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=156
05/24/2022 07:45:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=158
05/24/2022 07:45:50 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.3878787878787879 on epoch=158
05/24/2022 07:45:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=159
05/24/2022 07:45:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=161
05/24/2022 07:45:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=163
05/24/2022 07:46:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=164
05/24/2022 07:46:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=166
05/24/2022 07:46:06 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.45894450748819676 on epoch=166
05/24/2022 07:46:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4271284271284271 -> 0.45894450748819676 on epoch=166, global_step=1000
05/24/2022 07:46:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=168
05/24/2022 07:46:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=169
05/24/2022 07:46:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=171
05/24/2022 07:46:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=173
05/24/2022 07:46:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=174
05/24/2022 07:46:22 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.4415856680007623 on epoch=174
05/24/2022 07:46:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=176
05/24/2022 07:46:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=178
05/24/2022 07:46:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=179
05/24/2022 07:46:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=181
05/24/2022 07:46:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=183
05/24/2022 07:46:38 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.4710126474832357 on epoch=183
05/24/2022 07:46:38 - INFO - __main__ - Saving model with best Classification-F1: 0.45894450748819676 -> 0.4710126474832357 on epoch=183, global_step=1100
05/24/2022 07:46:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=184
05/24/2022 07:46:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=186
05/24/2022 07:46:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=188
05/24/2022 07:46:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=189
05/24/2022 07:46:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=191
05/24/2022 07:46:54 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.44494511370641704 on epoch=191
05/24/2022 07:46:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=193
05/24/2022 07:47:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=194
05/24/2022 07:47:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=196
05/24/2022 07:47:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=198
05/24/2022 07:47:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=199
05/24/2022 07:47:11 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.4481068796534826 on epoch=199
05/24/2022 07:47:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=201
05/24/2022 07:47:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=203
05/24/2022 07:47:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=204
05/24/2022 07:47:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=206
05/24/2022 07:47:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=208
05/24/2022 07:47:27 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.4789895701452945 on epoch=208
05/24/2022 07:47:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4710126474832357 -> 0.4789895701452945 on epoch=208, global_step=1250
05/24/2022 07:47:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=209
05/24/2022 07:47:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=211
05/24/2022 07:47:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=213
05/24/2022 07:47:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=214
05/24/2022 07:47:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=216
05/24/2022 07:47:43 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.43276068276068286 on epoch=216
05/24/2022 07:47:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=218
05/24/2022 07:47:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=219
05/24/2022 07:47:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=221
05/24/2022 07:47:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=223
05/24/2022 07:47:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=224
05/24/2022 07:47:59 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.4253372705691622 on epoch=224
05/24/2022 07:48:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=226
05/24/2022 07:48:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=228
05/24/2022 07:48:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=229
05/24/2022 07:48:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=231
05/24/2022 07:48:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=233
05/24/2022 07:48:16 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.2545761567038163 on epoch=233
05/24/2022 07:48:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=234
05/24/2022 07:48:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=236
05/24/2022 07:48:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=238
05/24/2022 07:48:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=239
05/24/2022 07:48:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=241
05/24/2022 07:48:32 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.1908453303802141 on epoch=241
05/24/2022 07:48:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=243
05/24/2022 07:48:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=244
05/24/2022 07:48:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=246
05/24/2022 07:48:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=248
05/24/2022 07:48:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=249
05/24/2022 07:48:48 - INFO - __main__ - Global step 1500 Train loss 0.09 Classification-F1 0.30797101449275366 on epoch=249
05/24/2022 07:48:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=251
05/24/2022 07:48:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=253
05/24/2022 07:48:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=254
05/24/2022 07:48:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=256
05/24/2022 07:49:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=258
05/24/2022 07:49:04 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.19379220164353914 on epoch=258
05/24/2022 07:49:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=259
05/24/2022 07:49:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=261
05/24/2022 07:49:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=263
05/24/2022 07:49:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=264
05/24/2022 07:49:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=266
05/24/2022 07:49:20 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.2807518796992481 on epoch=266
05/24/2022 07:49:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=268
05/24/2022 07:49:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=269
05/24/2022 07:49:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=271
05/24/2022 07:49:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=273
05/24/2022 07:49:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=274
05/24/2022 07:49:36 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.1309609326404077 on epoch=274
05/24/2022 07:49:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=276
05/24/2022 07:49:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=278
05/24/2022 07:49:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=279
05/24/2022 07:49:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=281
05/24/2022 07:49:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=283
05/24/2022 07:49:52 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.15754511573720612 on epoch=283
05/24/2022 07:49:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=284
05/24/2022 07:49:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=286
05/24/2022 07:50:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=288
05/24/2022 07:50:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=289
05/24/2022 07:50:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=291
05/24/2022 07:50:08 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.22962548214507902 on epoch=291
05/24/2022 07:50:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=293
05/24/2022 07:50:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=294
05/24/2022 07:50:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=296
05/24/2022 07:50:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=298
05/24/2022 07:50:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=299
05/24/2022 07:50:24 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.2028197028197028 on epoch=299
05/24/2022 07:50:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=301
05/24/2022 07:50:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=303
05/24/2022 07:50:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=304
05/24/2022 07:50:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=306
05/24/2022 07:50:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=308
05/24/2022 07:50:40 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.14784519808324798 on epoch=308
05/24/2022 07:50:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=309
05/24/2022 07:50:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=311
05/24/2022 07:50:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=313
05/24/2022 07:50:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=314
05/24/2022 07:50:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=316
05/24/2022 07:50:56 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.17124594155844158 on epoch=316
05/24/2022 07:50:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=318
05/24/2022 07:51:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=319
05/24/2022 07:51:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=321
05/24/2022 07:51:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=323
05/24/2022 07:51:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=324
05/24/2022 07:51:12 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.1602512509870497 on epoch=324
05/24/2022 07:51:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=326
05/24/2022 07:51:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=328
05/24/2022 07:51:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=329
05/24/2022 07:51:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=331
05/24/2022 07:51:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=333
05/24/2022 07:51:28 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.1430231351799979 on epoch=333
05/24/2022 07:51:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.04 on epoch=334
05/24/2022 07:51:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=336
05/24/2022 07:51:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=338
05/24/2022 07:51:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=339
05/24/2022 07:51:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=341
05/24/2022 07:51:44 - INFO - __main__ - Global step 2050 Train loss 0.03 Classification-F1 0.13724525780466978 on epoch=341
05/24/2022 07:51:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=343
05/24/2022 07:51:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=344
05/24/2022 07:51:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=346
05/24/2022 07:51:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=348
05/24/2022 07:51:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=349
05/24/2022 07:52:01 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.11746732799364377 on epoch=349
05/24/2022 07:52:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=351
05/24/2022 07:52:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=353
05/24/2022 07:52:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=354
05/24/2022 07:52:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=356
05/24/2022 07:52:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=358
05/24/2022 07:52:17 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.18681318681318682 on epoch=358
05/24/2022 07:52:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=359
05/24/2022 07:52:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=361
05/24/2022 07:52:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=363
05/24/2022 07:52:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=364
05/24/2022 07:52:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=366
05/24/2022 07:52:33 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.10993898988225875 on epoch=366
05/24/2022 07:52:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=368
05/24/2022 07:52:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=369
05/24/2022 07:52:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=371
05/24/2022 07:52:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=373
05/24/2022 07:52:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=374
05/24/2022 07:52:49 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.10053100351171654 on epoch=374
05/24/2022 07:52:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=376
05/24/2022 07:52:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=378
05/24/2022 07:52:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=379
05/24/2022 07:52:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=381
05/24/2022 07:53:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=383
05/24/2022 07:53:05 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.09766875711148157 on epoch=383
05/24/2022 07:53:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=384
05/24/2022 07:53:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=386
05/24/2022 07:53:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=388
05/24/2022 07:53:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=389
05/24/2022 07:53:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=391
05/24/2022 07:53:21 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.08606811145510837 on epoch=391
05/24/2022 07:53:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=393
05/24/2022 07:53:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=394
05/24/2022 07:53:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=396
05/24/2022 07:53:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=398
05/24/2022 07:53:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=399
05/24/2022 07:53:37 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.11272338990671012 on epoch=399
05/24/2022 07:53:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=401
05/24/2022 07:53:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=403
05/24/2022 07:53:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=404
05/24/2022 07:53:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=406
05/24/2022 07:53:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=408
05/24/2022 07:53:52 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.11350574712643678 on epoch=408
05/24/2022 07:53:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=409
05/24/2022 07:53:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=411
05/24/2022 07:54:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=413
05/24/2022 07:54:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
05/24/2022 07:54:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=416
05/24/2022 07:54:08 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.096426410854723 on epoch=416
05/24/2022 07:54:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=418
05/24/2022 07:54:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=419
05/24/2022 07:54:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=421
05/24/2022 07:54:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=423
05/24/2022 07:54:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
05/24/2022 07:54:25 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.08482743683388974 on epoch=424
05/24/2022 07:54:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=426
05/24/2022 07:54:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=428
05/24/2022 07:54:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=429
05/24/2022 07:54:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=431
05/24/2022 07:54:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=433
05/24/2022 07:54:41 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.17107783827384554 on epoch=433
05/24/2022 07:54:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=434
05/24/2022 07:54:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=436
05/24/2022 07:54:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=438
05/24/2022 07:54:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=439
05/24/2022 07:54:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=441
05/24/2022 07:54:56 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.1555609619837987 on epoch=441
05/24/2022 07:54:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=443
05/24/2022 07:55:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=444
05/24/2022 07:55:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=446
05/24/2022 07:55:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=448
05/24/2022 07:55:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 07:55:12 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.10843060381953724 on epoch=449
05/24/2022 07:55:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=451
05/24/2022 07:55:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=453
05/24/2022 07:55:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=454
05/24/2022 07:55:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/24/2022 07:55:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 07:55:28 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.09025525525525525 on epoch=458
05/24/2022 07:55:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=459
05/24/2022 07:55:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=461
05/24/2022 07:55:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=463
05/24/2022 07:55:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=464
05/24/2022 07:55:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=466
05/24/2022 07:55:44 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.12494449552600302 on epoch=466
05/24/2022 07:55:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
05/24/2022 07:55:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 07:55:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=471
05/24/2022 07:55:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=473
05/24/2022 07:55:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=474
05/24/2022 07:56:00 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.13980392156862745 on epoch=474
05/24/2022 07:56:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=476
05/24/2022 07:56:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=478
05/24/2022 07:56:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=479
05/24/2022 07:56:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=481
05/24/2022 07:56:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=483
05/24/2022 07:56:16 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.0786342123056119 on epoch=483
05/24/2022 07:56:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=484
05/24/2022 07:56:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 07:56:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=488
05/24/2022 07:56:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 07:56:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=491
05/24/2022 07:56:32 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.1417778410799348 on epoch=491
05/24/2022 07:56:35 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=493
05/24/2022 07:56:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=494
05/24/2022 07:56:40 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=496
05/24/2022 07:56:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=498
05/24/2022 07:56:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=499
05/24/2022 07:56:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:56:47 - INFO - __main__ - Printing 3 examples
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:56:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:56:47 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:56:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:56:47 - INFO - __main__ - Printing 3 examples
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 07:56:47 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:56:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:56:47 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:56:48 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.12195018096657441 on epoch=499
05/24/2022 07:56:48 - INFO - __main__ - save last model!
05/24/2022 07:56:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 07:56:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 07:56:48 - INFO - __main__ - Printing 3 examples
05/24/2022 07:56:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 07:56:48 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 07:56:48 - INFO - __main__ - ['entailment']
05/24/2022 07:56:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 07:56:48 - INFO - __main__ - ['contradiction']
05/24/2022 07:56:48 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:56:48 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:56:49 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 07:57:06 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:57:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:57:07 - INFO - __main__ - Starting training!
05/24/2022 07:57:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_13_0.5_8_predictions.txt
05/24/2022 07:57:16 - INFO - __main__ - Classification-F1 on test data: 0.0213
05/24/2022 07:57:17 - INFO - __main__ - prefix=anli_32_13, lr=0.5, bsz=8, dev_performance=0.4789895701452945, test_performance=0.02130391715915204
05/24/2022 07:57:17 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.4, bsz=8 ...
05/24/2022 07:57:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:57:18 - INFO - __main__ - Printing 3 examples
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:57:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:57:18 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 07:57:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 07:57:18 - INFO - __main__ - Printing 3 examples
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 07:57:18 - INFO - __main__ - ['contradiction']
05/24/2022 07:57:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 07:57:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 07:57:18 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 07:57:33 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 07:57:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 07:57:34 - INFO - __main__ - Starting training!
05/24/2022 07:57:37 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=1
05/24/2022 07:57:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=3
05/24/2022 07:57:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=4
05/24/2022 07:57:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=6
05/24/2022 07:57:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=8
05/24/2022 07:57:51 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 07:57:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 07:57:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
05/24/2022 07:57:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=11
05/24/2022 07:57:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=13
05/24/2022 07:58:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=14
05/24/2022 07:58:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=16
05/24/2022 07:58:06 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.2085278555866791 on epoch=16
05/24/2022 07:58:06 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2085278555866791 on epoch=16, global_step=100
05/24/2022 07:58:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=18
05/24/2022 07:58:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=19
05/24/2022 07:58:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=21
05/24/2022 07:58:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 07:58:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=24
05/24/2022 07:58:22 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 07:58:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/24/2022 07:58:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/24/2022 07:58:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=29
05/24/2022 07:58:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=31
05/24/2022 07:58:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
05/24/2022 07:58:38 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 07:58:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
05/24/2022 07:58:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=36
05/24/2022 07:58:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=38
05/24/2022 07:58:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/24/2022 07:58:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/24/2022 07:58:54 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 07:58:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=43
05/24/2022 07:58:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=44
05/24/2022 07:59:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=46
05/24/2022 07:59:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.41 on epoch=48
05/24/2022 07:59:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=49
05/24/2022 07:59:10 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.1881810228266921 on epoch=49
05/24/2022 07:59:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/24/2022 07:59:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=53
05/24/2022 07:59:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
05/24/2022 07:59:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=56
05/24/2022 07:59:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
05/24/2022 07:59:26 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 07:59:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=59
05/24/2022 07:59:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=61
05/24/2022 07:59:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
05/24/2022 07:59:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=64
05/24/2022 07:59:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
05/24/2022 07:59:42 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.370158002038736 on epoch=66
05/24/2022 07:59:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.370158002038736 on epoch=66, global_step=400
05/24/2022 07:59:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/24/2022 07:59:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=69
05/24/2022 07:59:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
05/24/2022 07:59:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=73
05/24/2022 07:59:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=74
05/24/2022 07:59:58 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2085278555866791 on epoch=74
05/24/2022 08:00:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=76
05/24/2022 08:00:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=78
05/24/2022 08:00:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
05/24/2022 08:00:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=81
05/24/2022 08:00:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=83
05/24/2022 08:00:13 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.22780952380952382 on epoch=83
05/24/2022 08:00:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=84
05/24/2022 08:00:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
05/24/2022 08:00:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=88
05/24/2022 08:00:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=89
05/24/2022 08:00:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=91
05/24/2022 08:00:29 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.38366571699905033 on epoch=91
05/24/2022 08:00:29 - INFO - __main__ - Saving model with best Classification-F1: 0.370158002038736 -> 0.38366571699905033 on epoch=91, global_step=550
05/24/2022 08:00:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=93
05/24/2022 08:00:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=94
05/24/2022 08:00:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=96
05/24/2022 08:00:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=98
05/24/2022 08:00:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=99
05/24/2022 08:00:44 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.303750768600123 on epoch=99
05/24/2022 08:00:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=101
05/24/2022 08:00:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=103
05/24/2022 08:00:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=104
05/24/2022 08:00:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/24/2022 08:00:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=108
05/24/2022 08:00:59 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.25711820534943913 on epoch=108
05/24/2022 08:01:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=109
05/24/2022 08:01:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=111
05/24/2022 08:01:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=113
05/24/2022 08:01:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=114
05/24/2022 08:01:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=116
05/24/2022 08:01:15 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.3878787878787879 on epoch=116
05/24/2022 08:01:15 - INFO - __main__ - Saving model with best Classification-F1: 0.38366571699905033 -> 0.3878787878787879 on epoch=116, global_step=700
05/24/2022 08:01:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=118
05/24/2022 08:01:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=119
05/24/2022 08:01:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=121
05/24/2022 08:01:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=123
05/24/2022 08:01:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=124
05/24/2022 08:01:30 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.3511904761904762 on epoch=124
05/24/2022 08:01:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=126
05/24/2022 08:01:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=128
05/24/2022 08:01:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=129
05/24/2022 08:01:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=131
05/24/2022 08:01:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=133
05/24/2022 08:01:45 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.3316769363280991 on epoch=133
05/24/2022 08:01:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=134
05/24/2022 08:01:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=136
05/24/2022 08:01:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=138
05/24/2022 08:01:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=139
05/24/2022 08:01:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=141
05/24/2022 08:02:01 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.37896825396825395 on epoch=141
05/24/2022 08:02:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=143
05/24/2022 08:02:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=144
05/24/2022 08:02:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=146
05/24/2022 08:02:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=148
05/24/2022 08:02:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=149
05/24/2022 08:02:16 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.36728801857779453 on epoch=149
05/24/2022 08:02:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=151
05/24/2022 08:02:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=153
05/24/2022 08:02:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=154
05/24/2022 08:02:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=156
05/24/2022 08:02:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=158
05/24/2022 08:02:32 - INFO - __main__ - Global step 950 Train loss 0.26 Classification-F1 0.39855699855699855 on epoch=158
05/24/2022 08:02:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3878787878787879 -> 0.39855699855699855 on epoch=158, global_step=950
05/24/2022 08:02:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=159
05/24/2022 08:02:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=161
05/24/2022 08:02:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=163
05/24/2022 08:02:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=164
05/24/2022 08:02:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=166
05/24/2022 08:02:47 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.38181818181818183 on epoch=166
05/24/2022 08:02:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=168
05/24/2022 08:02:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=169
05/24/2022 08:02:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=171
05/24/2022 08:02:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=173
05/24/2022 08:03:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=174
05/24/2022 08:03:03 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.39647113335186734 on epoch=174
05/24/2022 08:03:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=176
05/24/2022 08:03:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=178
05/24/2022 08:03:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=179
05/24/2022 08:03:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=181
05/24/2022 08:03:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=183
05/24/2022 08:03:19 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.3746246246246246 on epoch=183
05/24/2022 08:03:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=184
05/24/2022 08:03:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=186
05/24/2022 08:03:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=188
05/24/2022 08:03:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=189
05/24/2022 08:03:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=191
05/24/2022 08:03:34 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.3878787878787879 on epoch=191
05/24/2022 08:03:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=193
05/24/2022 08:03:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=194
05/24/2022 08:03:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=196
05/24/2022 08:03:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=198
05/24/2022 08:03:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=199
05/24/2022 08:03:50 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.4098044666852007 on epoch=199
05/24/2022 08:03:50 - INFO - __main__ - Saving model with best Classification-F1: 0.39855699855699855 -> 0.4098044666852007 on epoch=199, global_step=1200
05/24/2022 08:03:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=201
05/24/2022 08:03:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=203
05/24/2022 08:03:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=204
05/24/2022 08:04:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=206
05/24/2022 08:04:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=208
05/24/2022 08:04:06 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.39855699855699855 on epoch=208
05/24/2022 08:04:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=209
05/24/2022 08:04:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=211
05/24/2022 08:04:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=213
05/24/2022 08:04:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=214
05/24/2022 08:04:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=216
05/24/2022 08:04:22 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.4197046017245795 on epoch=216
05/24/2022 08:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4098044666852007 -> 0.4197046017245795 on epoch=216, global_step=1300
05/24/2022 08:04:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=218
05/24/2022 08:04:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=219
05/24/2022 08:04:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=221
05/24/2022 08:04:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=223
05/24/2022 08:04:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=224
05/24/2022 08:04:37 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.43843691051001166 on epoch=224
05/24/2022 08:04:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4197046017245795 -> 0.43843691051001166 on epoch=224, global_step=1350
05/24/2022 08:04:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=226
05/24/2022 08:04:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=228
05/24/2022 08:04:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=229
05/24/2022 08:04:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=231
05/24/2022 08:04:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=233
05/24/2022 08:04:53 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.4302697179500732 on epoch=233
05/24/2022 08:04:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=234
05/24/2022 08:04:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=236
05/24/2022 08:05:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=238
05/24/2022 08:05:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=239
05/24/2022 08:05:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=241
05/24/2022 08:05:09 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.4249642625081222 on epoch=241
05/24/2022 08:05:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=243
05/24/2022 08:05:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=244
05/24/2022 08:05:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=246
05/24/2022 08:05:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=248
05/24/2022 08:05:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=249
05/24/2022 08:05:25 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.4299719887955182 on epoch=249
05/24/2022 08:05:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=251
05/24/2022 08:05:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=253
05/24/2022 08:05:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=254
05/24/2022 08:05:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=256
05/24/2022 08:05:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=258
05/24/2022 08:05:41 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.30799112097669257 on epoch=258
05/24/2022 08:05:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=259
05/24/2022 08:05:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=261
05/24/2022 08:05:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=263
05/24/2022 08:05:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=264
05/24/2022 08:05:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=266
05/24/2022 08:05:57 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.34072264369294075 on epoch=266
05/24/2022 08:05:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=268
05/24/2022 08:06:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=269
05/24/2022 08:06:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=271
05/24/2022 08:06:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=273
05/24/2022 08:06:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=274
05/24/2022 08:06:12 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.33401989242721764 on epoch=274
05/24/2022 08:06:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=276
05/24/2022 08:06:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=278
05/24/2022 08:06:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=279
05/24/2022 08:06:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=281
05/24/2022 08:06:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=283
05/24/2022 08:06:28 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.33401989242721764 on epoch=283
05/24/2022 08:06:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=284
05/24/2022 08:06:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=286
05/24/2022 08:06:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=288
05/24/2022 08:06:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=289
05/24/2022 08:06:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.10 on epoch=291
05/24/2022 08:06:44 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.26125891866180434 on epoch=291
05/24/2022 08:06:47 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=293
05/24/2022 08:06:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=294
05/24/2022 08:06:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=296
05/24/2022 08:06:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=298
05/24/2022 08:06:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=299
05/24/2022 08:07:00 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.21638296344178698 on epoch=299
05/24/2022 08:07:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=301
05/24/2022 08:07:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=303
05/24/2022 08:07:08 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=304
05/24/2022 08:07:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=306
05/24/2022 08:07:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=308
05/24/2022 08:07:15 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.31678969178969174 on epoch=308
05/24/2022 08:07:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=309
05/24/2022 08:07:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=311
05/24/2022 08:07:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=313
05/24/2022 08:07:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=314
05/24/2022 08:07:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=316
05/24/2022 08:07:31 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.2485561497326203 on epoch=316
05/24/2022 08:07:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=318
05/24/2022 08:07:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=319
05/24/2022 08:07:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=321
05/24/2022 08:07:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=323
05/24/2022 08:07:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=324
05/24/2022 08:07:47 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.2365319865319865 on epoch=324
05/24/2022 08:07:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=326
05/24/2022 08:07:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=328
05/24/2022 08:07:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=329
05/24/2022 08:07:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=331
05/24/2022 08:08:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=333
05/24/2022 08:08:03 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.2997908622908623 on epoch=333
05/24/2022 08:08:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=334
05/24/2022 08:08:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=336
05/24/2022 08:08:11 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=338
05/24/2022 08:08:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=339
05/24/2022 08:08:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=341
05/24/2022 08:08:18 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.2460157726115173 on epoch=341
05/24/2022 08:08:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=343
05/24/2022 08:08:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=344
05/24/2022 08:08:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=346
05/24/2022 08:08:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=348
05/24/2022 08:08:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=349
05/24/2022 08:08:34 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.26274509803921564 on epoch=349
05/24/2022 08:08:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=351
05/24/2022 08:08:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=353
05/24/2022 08:08:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=354
05/24/2022 08:08:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=356
05/24/2022 08:08:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=358
05/24/2022 08:08:49 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.26184218421842187 on epoch=358
05/24/2022 08:08:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=359
05/24/2022 08:08:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=361
05/24/2022 08:08:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=363
05/24/2022 08:09:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=364
05/24/2022 08:09:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=366
05/24/2022 08:09:05 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.2150392817059484 on epoch=366
05/24/2022 08:09:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=368
05/24/2022 08:09:10 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=369
05/24/2022 08:09:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=371
05/24/2022 08:09:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=373
05/24/2022 08:09:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=374
05/24/2022 08:09:21 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.2671590296495957 on epoch=374
05/24/2022 08:09:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=376
05/24/2022 08:09:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=378
05/24/2022 08:09:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=379
05/24/2022 08:09:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=381
05/24/2022 08:09:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=383
05/24/2022 08:09:36 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.17903656627060882 on epoch=383
05/24/2022 08:09:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=384
05/24/2022 08:09:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=386
05/24/2022 08:09:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=388
05/24/2022 08:09:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=389
05/24/2022 08:09:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=391
05/24/2022 08:09:52 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.24938078347127624 on epoch=391
05/24/2022 08:09:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=393
05/24/2022 08:09:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=394
05/24/2022 08:10:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=396
05/24/2022 08:10:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=398
05/24/2022 08:10:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=399
05/24/2022 08:10:08 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.2540471380471381 on epoch=399
05/24/2022 08:10:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=401
05/24/2022 08:10:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=403
05/24/2022 08:10:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=404
05/24/2022 08:10:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=406
05/24/2022 08:10:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=408
05/24/2022 08:10:23 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.2129885480949311 on epoch=408
05/24/2022 08:10:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=409
05/24/2022 08:10:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=411
05/24/2022 08:10:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=413
05/24/2022 08:10:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
05/24/2022 08:10:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=416
05/24/2022 08:10:39 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.1787075941487706 on epoch=416
05/24/2022 08:10:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=418
05/24/2022 08:10:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=419
05/24/2022 08:10:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
05/24/2022 08:10:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=423
05/24/2022 08:10:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=424
05/24/2022 08:10:55 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.13000584286476918 on epoch=424
05/24/2022 08:10:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=426
05/24/2022 08:11:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=428
05/24/2022 08:11:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=429
05/24/2022 08:11:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=431
05/24/2022 08:11:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=433
05/24/2022 08:11:10 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.10985539088146679 on epoch=433
05/24/2022 08:11:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=434
05/24/2022 08:11:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 08:11:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=438
05/24/2022 08:11:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=439
05/24/2022 08:11:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=441
05/24/2022 08:11:26 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.2505050505050505 on epoch=441
05/24/2022 08:11:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=443
05/24/2022 08:11:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
05/24/2022 08:11:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=446
05/24/2022 08:11:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=448
05/24/2022 08:11:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 08:11:42 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.2557044125465178 on epoch=449
05/24/2022 08:11:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=451
05/24/2022 08:11:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=453
05/24/2022 08:11:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=454
05/24/2022 08:11:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=456
05/24/2022 08:11:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 08:11:58 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.18424242424242424 on epoch=458
05/24/2022 08:12:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=459
05/24/2022 08:12:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=461
05/24/2022 08:12:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=463
05/24/2022 08:12:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=464
05/24/2022 08:12:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=466
05/24/2022 08:12:14 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.19042071197411004 on epoch=466
05/24/2022 08:12:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=468
05/24/2022 08:12:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=469
05/24/2022 08:12:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=471
05/24/2022 08:12:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
05/24/2022 08:12:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=474
05/24/2022 08:12:29 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.2267379679144385 on epoch=474
05/24/2022 08:12:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=476
05/24/2022 08:12:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
05/24/2022 08:12:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=479
05/24/2022 08:12:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=481
05/24/2022 08:12:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
05/24/2022 08:12:45 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.1428785103785104 on epoch=483
05/24/2022 08:12:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/24/2022 08:12:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=486
05/24/2022 08:12:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=488
05/24/2022 08:12:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 08:12:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=491
05/24/2022 08:13:01 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.2764661654135338 on epoch=491
05/24/2022 08:13:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
05/24/2022 08:13:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=494
05/24/2022 08:13:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=496
05/24/2022 08:13:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=498
05/24/2022 08:13:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=499
05/24/2022 08:13:16 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:13:16 - INFO - __main__ - Printing 3 examples
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:13:16 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:13:16 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:13:16 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:13:16 - INFO - __main__ - Printing 3 examples
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 08:13:16 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:16 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:13:16 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:13:16 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:13:17 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.21628364607491743 on epoch=499
05/24/2022 08:13:17 - INFO - __main__ - save last model!
05/24/2022 08:13:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 08:13:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 08:13:17 - INFO - __main__ - Printing 3 examples
05/24/2022 08:13:17 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 08:13:17 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:17 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 08:13:17 - INFO - __main__ - ['entailment']
05/24/2022 08:13:17 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 08:13:17 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:17 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:13:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:13:19 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 08:13:35 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:13:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:13:36 - INFO - __main__ - Starting training!
05/24/2022 08:13:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_13_0.4_8_predictions.txt
05/24/2022 08:13:46 - INFO - __main__ - Classification-F1 on test data: 0.0196
05/24/2022 08:13:46 - INFO - __main__ - prefix=anli_32_13, lr=0.4, bsz=8, dev_performance=0.43843691051001166, test_performance=0.01960631755714131
05/24/2022 08:13:46 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.3, bsz=8 ...
05/24/2022 08:13:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:13:47 - INFO - __main__ - Printing 3 examples
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:13:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:13:47 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:13:47 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:13:47 - INFO - __main__ - Printing 3 examples
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 08:13:47 - INFO - __main__ - ['contradiction']
05/24/2022 08:13:47 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:13:47 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:13:48 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:14:06 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:14:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:14:07 - INFO - __main__ - Starting training!
05/24/2022 08:14:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=1
05/24/2022 08:14:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=3
05/24/2022 08:14:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=4
05/24/2022 08:14:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=6
05/24/2022 08:14:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=8
05/24/2022 08:14:24 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 08:14:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 08:14:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=9
05/24/2022 08:14:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=11
05/24/2022 08:14:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.47 on epoch=13
05/24/2022 08:14:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=14
05/24/2022 08:14:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=16
05/24/2022 08:14:40 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 08:14:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=18
05/24/2022 08:14:45 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=19
05/24/2022 08:14:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=21
05/24/2022 08:14:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=23
05/24/2022 08:14:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=24
05/24/2022 08:14:56 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 08:14:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=26
05/24/2022 08:15:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/24/2022 08:15:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
05/24/2022 08:15:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/24/2022 08:15:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=33
05/24/2022 08:15:11 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 08:15:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=34
05/24/2022 08:15:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=36
05/24/2022 08:15:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=38
05/24/2022 08:15:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=39
05/24/2022 08:15:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/24/2022 08:15:27 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 08:15:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/24/2022 08:15:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/24/2022 08:15:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/24/2022 08:15:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=48
05/24/2022 08:15:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=49
05/24/2022 08:15:43 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 08:15:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=51
05/24/2022 08:15:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=53
05/24/2022 08:15:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=54
05/24/2022 08:15:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=56
05/24/2022 08:15:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=58
05/24/2022 08:16:00 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 08:16:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
05/24/2022 08:16:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
05/24/2022 08:16:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=63
05/24/2022 08:16:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/24/2022 08:16:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=66
05/24/2022 08:16:16 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 08:16:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=68
05/24/2022 08:16:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/24/2022 08:16:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=71
05/24/2022 08:16:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/24/2022 08:16:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=74
05/24/2022 08:16:32 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
05/24/2022 08:16:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
05/24/2022 08:16:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=78
05/24/2022 08:16:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/24/2022 08:16:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=81
05/24/2022 08:16:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=83
05/24/2022 08:16:48 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.1881810228266921 on epoch=83
05/24/2022 08:16:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1881810228266921 on epoch=83, global_step=500
05/24/2022 08:16:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=84
05/24/2022 08:16:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=86
05/24/2022 08:16:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
05/24/2022 08:16:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=89
05/24/2022 08:17:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=91
05/24/2022 08:17:04 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.3649122807017544 on epoch=91
05/24/2022 08:17:04 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.3649122807017544 on epoch=91, global_step=550
05/24/2022 08:17:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=93
05/24/2022 08:17:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=94
05/24/2022 08:17:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
05/24/2022 08:17:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=98
05/24/2022 08:17:17 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=99
05/24/2022 08:17:20 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.29444444444444445 on epoch=99
05/24/2022 08:17:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=101
05/24/2022 08:17:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=103
05/24/2022 08:17:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=104
05/24/2022 08:17:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/24/2022 08:17:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=108
05/24/2022 08:17:36 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.26258268622375613 on epoch=108
05/24/2022 08:17:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=109
05/24/2022 08:17:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=111
05/24/2022 08:17:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=113
05/24/2022 08:17:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=114
05/24/2022 08:17:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=116
05/24/2022 08:17:52 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.38568087785572946 on epoch=116
05/24/2022 08:17:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3649122807017544 -> 0.38568087785572946 on epoch=116, global_step=700
05/24/2022 08:17:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=118
05/24/2022 08:17:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=119
05/24/2022 08:18:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=121
05/24/2022 08:18:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=123
05/24/2022 08:18:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=124
05/24/2022 08:18:08 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3021253699219801 on epoch=124
05/24/2022 08:18:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=126
05/24/2022 08:18:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=128
05/24/2022 08:18:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=129
05/24/2022 08:18:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=131
05/24/2022 08:18:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=133
05/24/2022 08:18:24 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.34070683956267483 on epoch=133
05/24/2022 08:18:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=134
05/24/2022 08:18:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=136
05/24/2022 08:18:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
05/24/2022 08:18:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=139
05/24/2022 08:18:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=141
05/24/2022 08:18:39 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3745454545454545 on epoch=141
05/24/2022 08:18:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=143
05/24/2022 08:18:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=144
05/24/2022 08:18:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=146
05/24/2022 08:18:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=148
05/24/2022 08:18:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=149
05/24/2022 08:18:55 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.3745454545454545 on epoch=149
05/24/2022 08:18:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=151
05/24/2022 08:19:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=153
05/24/2022 08:19:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=154
05/24/2022 08:19:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=156
05/24/2022 08:19:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=158
05/24/2022 08:19:10 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.38568087785572946 on epoch=158
05/24/2022 08:19:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=159
05/24/2022 08:19:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=161
05/24/2022 08:19:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=163
05/24/2022 08:19:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=164
05/24/2022 08:19:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=166
05/24/2022 08:19:26 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.4069828954329043 on epoch=166
05/24/2022 08:19:26 - INFO - __main__ - Saving model with best Classification-F1: 0.38568087785572946 -> 0.4069828954329043 on epoch=166, global_step=1000
05/24/2022 08:19:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=168
05/24/2022 08:19:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=169
05/24/2022 08:19:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=171
05/24/2022 08:19:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=173
05/24/2022 08:19:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=174
05/24/2022 08:19:41 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3630569344855059 on epoch=174
05/24/2022 08:19:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=176
05/24/2022 08:19:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=178
05/24/2022 08:19:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=179
05/24/2022 08:19:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=181
05/24/2022 08:19:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=183
05/24/2022 08:19:57 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.40484509666899604 on epoch=183
05/24/2022 08:20:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=184
05/24/2022 08:20:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=186
05/24/2022 08:20:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=188
05/24/2022 08:20:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=189
05/24/2022 08:20:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=191
05/24/2022 08:20:13 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.38366571699905033 on epoch=191
05/24/2022 08:20:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=193
05/24/2022 08:20:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=194
05/24/2022 08:20:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=196
05/24/2022 08:20:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=198
05/24/2022 08:20:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=199
05/24/2022 08:20:28 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.38366571699905033 on epoch=199
05/24/2022 08:20:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=201
05/24/2022 08:20:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=203
05/24/2022 08:20:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=204
05/24/2022 08:20:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=206
05/24/2022 08:20:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=208
05/24/2022 08:20:44 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.3630569344855059 on epoch=208
05/24/2022 08:20:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=209
05/24/2022 08:20:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=211
05/24/2022 08:20:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=213
05/24/2022 08:20:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=214
05/24/2022 08:20:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=216
05/24/2022 08:21:00 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.4164284352963598 on epoch=216
05/24/2022 08:21:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4069828954329043 -> 0.4164284352963598 on epoch=216, global_step=1300
05/24/2022 08:21:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=218
05/24/2022 08:21:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=219
05/24/2022 08:21:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=221
05/24/2022 08:21:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=223
05/24/2022 08:21:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=224
05/24/2022 08:21:15 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.39440427908070297 on epoch=224
05/24/2022 08:21:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=226
05/24/2022 08:21:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=228
05/24/2022 08:21:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=229
05/24/2022 08:21:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=231
05/24/2022 08:21:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=233
05/24/2022 08:21:31 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.40484509666899604 on epoch=233
05/24/2022 08:21:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=234
05/24/2022 08:21:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=236
05/24/2022 08:21:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=238
05/24/2022 08:21:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=239
05/24/2022 08:21:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=241
05/24/2022 08:21:47 - INFO - __main__ - Global step 1450 Train loss 0.24 Classification-F1 0.42690396023729366 on epoch=241
05/24/2022 08:21:47 - INFO - __main__ - Saving model with best Classification-F1: 0.4164284352963598 -> 0.42690396023729366 on epoch=241, global_step=1450
05/24/2022 08:21:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=243
05/24/2022 08:21:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=244
05/24/2022 08:21:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=246
05/24/2022 08:21:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=248
05/24/2022 08:22:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=249
05/24/2022 08:22:02 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.43729894473512676 on epoch=249
05/24/2022 08:22:02 - INFO - __main__ - Saving model with best Classification-F1: 0.42690396023729366 -> 0.43729894473512676 on epoch=249, global_step=1500
05/24/2022 08:22:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=251
05/24/2022 08:22:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=253
05/24/2022 08:22:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=254
05/24/2022 08:22:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=256
05/24/2022 08:22:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=258
05/24/2022 08:22:18 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.4400488400488401 on epoch=258
05/24/2022 08:22:18 - INFO - __main__ - Saving model with best Classification-F1: 0.43729894473512676 -> 0.4400488400488401 on epoch=258, global_step=1550
05/24/2022 08:22:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=259
05/24/2022 08:22:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=261
05/24/2022 08:22:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=263
05/24/2022 08:22:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=264
05/24/2022 08:22:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=266
05/24/2022 08:22:34 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.42820233506508015 on epoch=266
05/24/2022 08:22:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=268
05/24/2022 08:22:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=269
05/24/2022 08:22:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=271
05/24/2022 08:22:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=273
05/24/2022 08:22:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=274
05/24/2022 08:22:50 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.43807104844840694 on epoch=274
05/24/2022 08:22:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=276
05/24/2022 08:22:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=278
05/24/2022 08:22:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=279
05/24/2022 08:23:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=281
05/24/2022 08:23:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=283
05/24/2022 08:23:06 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.30655569920275805 on epoch=283
05/24/2022 08:23:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=284
05/24/2022 08:23:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=286
05/24/2022 08:23:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=288
05/24/2022 08:23:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=289
05/24/2022 08:23:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=291
05/24/2022 08:23:22 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.30655569920275805 on epoch=291
05/24/2022 08:23:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=293
05/24/2022 08:23:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=294
05/24/2022 08:23:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=296
05/24/2022 08:23:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=298
05/24/2022 08:23:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=299
05/24/2022 08:23:38 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.43807104844840694 on epoch=299
05/24/2022 08:23:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=301
05/24/2022 08:23:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=303
05/24/2022 08:23:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=304
05/24/2022 08:23:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=306
05/24/2022 08:23:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=308
05/24/2022 08:23:54 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.4197046017245795 on epoch=308
05/24/2022 08:23:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=309
05/24/2022 08:23:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=311
05/24/2022 08:24:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=313
05/24/2022 08:24:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=314
05/24/2022 08:24:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=316
05/24/2022 08:24:10 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.4197046017245795 on epoch=316
05/24/2022 08:24:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=318
05/24/2022 08:24:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=319
05/24/2022 08:24:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=321
05/24/2022 08:24:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=323
05/24/2022 08:24:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=324
05/24/2022 08:24:26 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.31236263736263736 on epoch=324
05/24/2022 08:24:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=326
05/24/2022 08:24:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=328
05/24/2022 08:24:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=329
05/24/2022 08:24:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=331
05/24/2022 08:24:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=333
05/24/2022 08:24:42 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.3051784816490699 on epoch=333
05/24/2022 08:24:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=334
05/24/2022 08:24:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=336
05/24/2022 08:24:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=338
05/24/2022 08:24:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=339
05/24/2022 08:24:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=341
05/24/2022 08:24:58 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.33999878825210417 on epoch=341
05/24/2022 08:25:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=343
05/24/2022 08:25:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=344
05/24/2022 08:25:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=346
05/24/2022 08:25:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=348
05/24/2022 08:25:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=349
05/24/2022 08:25:14 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.3479479479479479 on epoch=349
05/24/2022 08:25:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=351
05/24/2022 08:25:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=353
05/24/2022 08:25:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=354
05/24/2022 08:25:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=356
05/24/2022 08:25:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=358
05/24/2022 08:25:30 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.3479479479479479 on epoch=358
05/24/2022 08:25:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=359
05/24/2022 08:25:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=361
05/24/2022 08:25:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=363
05/24/2022 08:25:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=364
05/24/2022 08:25:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=366
05/24/2022 08:25:45 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.34080253723110865 on epoch=366
05/24/2022 08:25:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=368
05/24/2022 08:25:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=369
05/24/2022 08:25:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=371
05/24/2022 08:25:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=373
05/24/2022 08:25:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=374
05/24/2022 08:26:00 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.3134343434343434 on epoch=374
05/24/2022 08:26:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=376
05/24/2022 08:26:06 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=378
05/24/2022 08:26:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=379
05/24/2022 08:26:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=381
05/24/2022 08:26:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=383
05/24/2022 08:26:16 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.23097651222651225 on epoch=383
05/24/2022 08:26:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=384
05/24/2022 08:26:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=386
05/24/2022 08:26:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=388
05/24/2022 08:26:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=389
05/24/2022 08:26:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=391
05/24/2022 08:26:31 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.2324712643678161 on epoch=391
05/24/2022 08:26:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=393
05/24/2022 08:26:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=394
05/24/2022 08:26:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=396
05/24/2022 08:26:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=398
05/24/2022 08:26:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=399
05/24/2022 08:26:46 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.26100088932007437 on epoch=399
05/24/2022 08:26:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=401
05/24/2022 08:26:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=403
05/24/2022 08:26:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=404
05/24/2022 08:26:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=406
05/24/2022 08:26:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=408
05/24/2022 08:27:01 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.20752765752765753 on epoch=408
05/24/2022 08:27:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=409
05/24/2022 08:27:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=411
05/24/2022 08:27:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=413
05/24/2022 08:27:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=414
05/24/2022 08:27:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=416
05/24/2022 08:27:17 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.14809630459126538 on epoch=416
05/24/2022 08:27:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=418
05/24/2022 08:27:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=419
05/24/2022 08:27:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=421
05/24/2022 08:27:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=423
05/24/2022 08:27:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=424
05/24/2022 08:27:32 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.2828339723098745 on epoch=424
05/24/2022 08:27:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=426
05/24/2022 08:27:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=428
05/24/2022 08:27:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=429
05/24/2022 08:27:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=431
05/24/2022 08:27:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=433
05/24/2022 08:27:47 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.1535919637041529 on epoch=433
05/24/2022 08:27:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=434
05/24/2022 08:27:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=436
05/24/2022 08:27:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=438
05/24/2022 08:27:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=439
05/24/2022 08:28:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=441
05/24/2022 08:28:03 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.27414141414141413 on epoch=441
05/24/2022 08:28:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=443
05/24/2022 08:28:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=444
05/24/2022 08:28:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
05/24/2022 08:28:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=448
05/24/2022 08:28:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=449
05/24/2022 08:28:18 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.1317065287653523 on epoch=449
05/24/2022 08:28:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=451
05/24/2022 08:28:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=453
05/24/2022 08:28:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=454
05/24/2022 08:28:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=456
05/24/2022 08:28:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=458
05/24/2022 08:28:33 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.18385093167701866 on epoch=458
05/24/2022 08:28:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=459
05/24/2022 08:28:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=461
05/24/2022 08:28:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=463
05/24/2022 08:28:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=464
05/24/2022 08:28:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=466
05/24/2022 08:28:48 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.15462802304907564 on epoch=466
05/24/2022 08:28:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=468
05/24/2022 08:28:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 08:28:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=471
05/24/2022 08:28:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=473
05/24/2022 08:29:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=474
05/24/2022 08:29:03 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.13091850678057573 on epoch=474
05/24/2022 08:29:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=476
05/24/2022 08:29:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
05/24/2022 08:29:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/24/2022 08:29:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=481
05/24/2022 08:29:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=483
05/24/2022 08:29:19 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.12483940790392403 on epoch=483
05/24/2022 08:29:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=484
05/24/2022 08:29:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=486
05/24/2022 08:29:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=488
05/24/2022 08:29:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=489
05/24/2022 08:29:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=491
05/24/2022 08:29:34 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.23807338747916 on epoch=491
05/24/2022 08:29:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=493
05/24/2022 08:29:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 08:29:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=496
05/24/2022 08:29:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=498
05/24/2022 08:29:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=499
05/24/2022 08:29:48 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:29:48 - INFO - __main__ - Printing 3 examples
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:29:48 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:29:48 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:29:48 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:29:48 - INFO - __main__ - Printing 3 examples
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 08:29:48 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:48 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:29:48 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:29:49 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:29:50 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.18494872921359867 on epoch=499
05/24/2022 08:29:50 - INFO - __main__ - save last model!
05/24/2022 08:29:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 08:29:50 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 08:29:50 - INFO - __main__ - Printing 3 examples
05/24/2022 08:29:50 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 08:29:50 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:50 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 08:29:50 - INFO - __main__ - ['entailment']
05/24/2022 08:29:50 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 08:29:50 - INFO - __main__ - ['contradiction']
05/24/2022 08:29:50 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:29:50 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:29:51 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 08:30:04 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:30:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:30:05 - INFO - __main__ - Starting training!
05/24/2022 08:30:18 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_13_0.3_8_predictions.txt
05/24/2022 08:30:18 - INFO - __main__ - Classification-F1 on test data: 0.0321
05/24/2022 08:30:18 - INFO - __main__ - prefix=anli_32_13, lr=0.3, bsz=8, dev_performance=0.4400488400488401, test_performance=0.032060328359574564
05/24/2022 08:30:18 - INFO - __main__ - Running ... prefix=anli_32_13, lr=0.2, bsz=8 ...
05/24/2022 08:30:19 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:30:19 - INFO - __main__ - Printing 3 examples
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:30:19 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:30:19 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:30:19 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:30:19 - INFO - __main__ - Printing 3 examples
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: Marion Mitchell (born 1941 in Seaham, County Durham), better known by her stage name, Janie Jones, was an English singer. She became renowned for holding sex parties at her home during the 1970s, and was jailed for her involvement in 'controlling prostitutes'. She first achieved notoriety in August 1964, when she attended the film premiere of "London in the Raw", wearing a topless dress. [SEP] hypothesis: When she was 30 years old, Janie Jones attended a film premiere in a scandalous outfit.
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: Islands in the Stream is a 1977 American drama film, an adaptation of Ernest Hemingway's novel of the same name. The film was directed by Franklin J. Schaffner and starred George C. Scott, Hart Bochner, Claire Bloom, Gilbert Roland, and David Hemmings. [SEP] hypothesis: Islands in the Stream is a 1977 germandrama film
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ -  [anli] premise: William George "Billy" Zane, Jr. (born February 24, 1966) is an American actor and producer. He is best known for playing Hughie in the thriller "Dead Calm" (1989), Kit Walker / The Phantom in the superhero film "The Phantom" (1996), Caledon Hockley in the epic romantic disaster film "Titanic" (1997), and for his television role as John Wheeler in the serial drama series "Twin Peaks". [SEP] hypothesis: Billy Zane was born in 1985.
05/24/2022 08:30:19 - INFO - __main__ - ['contradiction']
05/24/2022 08:30:19 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:30:19 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:30:19 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:30:38 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:30:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:30:39 - INFO - __main__ - Starting training!
05/24/2022 08:30:42 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=1
05/24/2022 08:30:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=3
05/24/2022 08:30:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=4
05/24/2022 08:30:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=6
05/24/2022 08:30:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=8
05/24/2022 08:30:56 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 08:30:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 08:30:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=9
05/24/2022 08:31:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=11
05/24/2022 08:31:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=13
05/24/2022 08:31:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=14
05/24/2022 08:31:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=16
05/24/2022 08:31:12 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 08:31:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=18
05/24/2022 08:31:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=19
05/24/2022 08:31:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=21
05/24/2022 08:31:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
05/24/2022 08:31:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=24
05/24/2022 08:31:28 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 08:31:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=26
05/24/2022 08:31:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=28
05/24/2022 08:31:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
05/24/2022 08:31:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=31
05/24/2022 08:31:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
05/24/2022 08:31:44 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 08:31:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=34
05/24/2022 08:31:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=36
05/24/2022 08:31:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=38
05/24/2022 08:31:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=39
05/24/2022 08:31:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=41
05/24/2022 08:31:59 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 08:32:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/24/2022 08:32:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
05/24/2022 08:32:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=46
05/24/2022 08:32:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
05/24/2022 08:32:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/24/2022 08:32:15 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 08:32:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/24/2022 08:32:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
05/24/2022 08:32:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=54
05/24/2022 08:32:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/24/2022 08:32:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
05/24/2022 08:32:32 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 08:32:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=59
05/24/2022 08:32:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=61
05/24/2022 08:32:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=63
05/24/2022 08:32:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=64
05/24/2022 08:32:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=66
05/24/2022 08:32:48 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 08:32:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=68
05/24/2022 08:32:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=69
05/24/2022 08:32:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
05/24/2022 08:32:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=73
05/24/2022 08:33:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=74
05/24/2022 08:33:04 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=74
05/24/2022 08:33:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=76
05/24/2022 08:33:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=78
05/24/2022 08:33:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=79
05/24/2022 08:33:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
05/24/2022 08:33:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
05/24/2022 08:33:20 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=83
05/24/2022 08:33:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=84
05/24/2022 08:33:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=86
05/24/2022 08:33:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=88
05/24/2022 08:33:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=89
05/24/2022 08:33:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=91
05/24/2022 08:33:36 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
05/24/2022 08:33:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
05/24/2022 08:33:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=94
05/24/2022 08:33:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=96
05/24/2022 08:33:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=98
05/24/2022 08:33:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=99
05/24/2022 08:33:52 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/24/2022 08:33:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=101
05/24/2022 08:33:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=103
05/24/2022 08:34:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=104
05/24/2022 08:34:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=106
05/24/2022 08:34:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=108
05/24/2022 08:34:09 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=108
05/24/2022 08:34:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=109
05/24/2022 08:34:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=111
05/24/2022 08:34:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=113
05/24/2022 08:34:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=114
05/24/2022 08:34:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=116
05/24/2022 08:34:25 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/24/2022 08:34:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=118
05/24/2022 08:34:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=119
05/24/2022 08:34:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/24/2022 08:34:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=123
05/24/2022 08:34:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=124
05/24/2022 08:34:41 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=124
05/24/2022 08:34:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=126
05/24/2022 08:34:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=128
05/24/2022 08:34:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
05/24/2022 08:34:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=131
05/24/2022 08:34:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
05/24/2022 08:34:57 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=133
05/24/2022 08:35:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=134
05/24/2022 08:35:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=136
05/24/2022 08:35:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=138
05/24/2022 08:35:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=139
05/24/2022 08:35:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=141
05/24/2022 08:35:13 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.22780952380952382 on epoch=141
05/24/2022 08:35:13 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22780952380952382 on epoch=141, global_step=850
05/24/2022 08:35:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=143
05/24/2022 08:35:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=144
05/24/2022 08:35:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=146
05/24/2022 08:35:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=148
05/24/2022 08:35:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=149
05/24/2022 08:35:29 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.22780952380952382 on epoch=149
05/24/2022 08:35:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
05/24/2022 08:35:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=153
05/24/2022 08:35:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=154
05/24/2022 08:35:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=156
05/24/2022 08:35:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=158
05/24/2022 08:35:45 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.2635318245074343 on epoch=158
05/24/2022 08:35:45 - INFO - __main__ - Saving model with best Classification-F1: 0.22780952380952382 -> 0.2635318245074343 on epoch=158, global_step=950
05/24/2022 08:35:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=159
05/24/2022 08:35:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=161
05/24/2022 08:35:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=163
05/24/2022 08:35:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=164
05/24/2022 08:35:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=166
05/24/2022 08:36:01 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.28012654587287894 on epoch=166
05/24/2022 08:36:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2635318245074343 -> 0.28012654587287894 on epoch=166, global_step=1000
05/24/2022 08:36:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=168
05/24/2022 08:36:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=169
05/24/2022 08:36:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=171
05/24/2022 08:36:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=173
05/24/2022 08:36:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=174
05/24/2022 08:36:17 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.29596665960302326 on epoch=174
05/24/2022 08:36:17 - INFO - __main__ - Saving model with best Classification-F1: 0.28012654587287894 -> 0.29596665960302326 on epoch=174, global_step=1050
05/24/2022 08:36:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=176
05/24/2022 08:36:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=178
05/24/2022 08:36:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=179
05/24/2022 08:36:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=181
05/24/2022 08:36:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=183
05/24/2022 08:36:33 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.303750768600123 on epoch=183
05/24/2022 08:36:33 - INFO - __main__ - Saving model with best Classification-F1: 0.29596665960302326 -> 0.303750768600123 on epoch=183, global_step=1100
05/24/2022 08:36:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=184
05/24/2022 08:36:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=186
05/24/2022 08:36:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=188
05/24/2022 08:36:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=189
05/24/2022 08:36:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=191
05/24/2022 08:36:48 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.37896825396825395 on epoch=191
05/24/2022 08:36:48 - INFO - __main__ - Saving model with best Classification-F1: 0.303750768600123 -> 0.37896825396825395 on epoch=191, global_step=1150
05/24/2022 08:36:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=193
05/24/2022 08:36:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=194
05/24/2022 08:36:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=196
05/24/2022 08:36:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=198
05/24/2022 08:37:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=199
05/24/2022 08:37:04 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.36728801857779453 on epoch=199
05/24/2022 08:37:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=201
05/24/2022 08:37:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=203
05/24/2022 08:37:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=204
05/24/2022 08:37:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=206
05/24/2022 08:37:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=208
05/24/2022 08:37:20 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3426731078904992 on epoch=208
05/24/2022 08:37:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=209
05/24/2022 08:37:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
05/24/2022 08:37:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=213
05/24/2022 08:37:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=214
05/24/2022 08:37:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=216
05/24/2022 08:37:36 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.3902678188392474 on epoch=216
05/24/2022 08:37:36 - INFO - __main__ - Saving model with best Classification-F1: 0.37896825396825395 -> 0.3902678188392474 on epoch=216, global_step=1300
05/24/2022 08:37:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=218
05/24/2022 08:37:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=219
05/24/2022 08:37:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=221
05/24/2022 08:37:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=223
05/24/2022 08:37:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=224
05/24/2022 08:37:51 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.4093067426400759 on epoch=224
05/24/2022 08:37:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3902678188392474 -> 0.4093067426400759 on epoch=224, global_step=1350
05/24/2022 08:37:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=226
05/24/2022 08:37:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=228
05/24/2022 08:37:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=229
05/24/2022 08:38:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=231
05/24/2022 08:38:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=233
05/24/2022 08:38:07 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.3902678188392474 on epoch=233
05/24/2022 08:38:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=234
05/24/2022 08:38:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=236
05/24/2022 08:38:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=238
05/24/2022 08:38:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=239
05/24/2022 08:38:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=241
05/24/2022 08:38:23 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.41182466870540263 on epoch=241
05/24/2022 08:38:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4093067426400759 -> 0.41182466870540263 on epoch=241, global_step=1450
05/24/2022 08:38:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=243
05/24/2022 08:38:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=244
05/24/2022 08:38:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=246
05/24/2022 08:38:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=248
05/24/2022 08:38:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=249
05/24/2022 08:38:38 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.41182466870540263 on epoch=249
05/24/2022 08:38:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=251
05/24/2022 08:38:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=253
05/24/2022 08:38:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=254
05/24/2022 08:38:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=256
05/24/2022 08:38:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=258
05/24/2022 08:38:54 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.4093067426400759 on epoch=258
05/24/2022 08:38:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=259
05/24/2022 08:38:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.34 on epoch=261
05/24/2022 08:39:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.30 on epoch=263
05/24/2022 08:39:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=264
05/24/2022 08:39:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=266
05/24/2022 08:39:09 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.4069828954329043 on epoch=266
05/24/2022 08:39:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=268
05/24/2022 08:39:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=269
05/24/2022 08:39:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=271
05/24/2022 08:39:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=273
05/24/2022 08:39:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=274
05/24/2022 08:39:25 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.41182466870540263 on epoch=274
05/24/2022 08:39:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=276
05/24/2022 08:39:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=278
05/24/2022 08:39:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=279
05/24/2022 08:39:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=281
05/24/2022 08:39:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=283
05/24/2022 08:39:41 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.4012121212121212 on epoch=283
05/24/2022 08:39:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=284
05/24/2022 08:39:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.28 on epoch=286
05/24/2022 08:39:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=288
05/24/2022 08:39:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=289
05/24/2022 08:39:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=291
05/24/2022 08:39:57 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.3987527732805661 on epoch=291
05/24/2022 08:39:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=293
05/24/2022 08:40:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=294
05/24/2022 08:40:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=296
05/24/2022 08:40:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=298
05/24/2022 08:40:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=299
05/24/2022 08:40:12 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.39031339031339035 on epoch=299
05/24/2022 08:40:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=301
05/24/2022 08:40:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=303
05/24/2022 08:40:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=304
05/24/2022 08:40:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=306
05/24/2022 08:40:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.34 on epoch=308
05/24/2022 08:40:28 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.3987527732805661 on epoch=308
05/24/2022 08:40:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=309
05/24/2022 08:40:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=311
05/24/2022 08:40:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=313
05/24/2022 08:40:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=314
05/24/2022 08:40:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=316
05/24/2022 08:40:44 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.3987527732805661 on epoch=316
05/24/2022 08:40:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.26 on epoch=318
05/24/2022 08:40:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=319
05/24/2022 08:40:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=321
05/24/2022 08:40:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=323
05/24/2022 08:40:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=324
05/24/2022 08:40:59 - INFO - __main__ - Global step 1950 Train loss 0.27 Classification-F1 0.4093067426400759 on epoch=324
05/24/2022 08:41:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=326
05/24/2022 08:41:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=328
05/24/2022 08:41:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=329
05/24/2022 08:41:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=331
05/24/2022 08:41:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=333
05/24/2022 08:41:15 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.42071037169076386 on epoch=333
05/24/2022 08:41:15 - INFO - __main__ - Saving model with best Classification-F1: 0.41182466870540263 -> 0.42071037169076386 on epoch=333, global_step=2000
05/24/2022 08:41:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=334
05/24/2022 08:41:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=336
05/24/2022 08:41:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=338
05/24/2022 08:41:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=339
05/24/2022 08:41:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=341
05/24/2022 08:41:31 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.4262358063689917 on epoch=341
05/24/2022 08:41:31 - INFO - __main__ - Saving model with best Classification-F1: 0.42071037169076386 -> 0.4262358063689917 on epoch=341, global_step=2050
05/24/2022 08:41:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=343
05/24/2022 08:41:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=344
05/24/2022 08:41:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=346
05/24/2022 08:41:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=348
05/24/2022 08:41:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=349
05/24/2022 08:41:47 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.3881737494856874 on epoch=349
05/24/2022 08:41:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=351
05/24/2022 08:41:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=353
05/24/2022 08:41:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=354
05/24/2022 08:41:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=356
05/24/2022 08:42:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=358
05/24/2022 08:42:03 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.42392051160086686 on epoch=358
05/24/2022 08:42:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=359
05/24/2022 08:42:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=361
05/24/2022 08:42:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=363
05/24/2022 08:42:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=364
05/24/2022 08:42:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=366
05/24/2022 08:42:19 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.42055475388808733 on epoch=366
05/24/2022 08:42:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=368
05/24/2022 08:42:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.23 on epoch=369
05/24/2022 08:42:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=371
05/24/2022 08:42:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=373
05/24/2022 08:42:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=374
05/24/2022 08:42:35 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.3853848457622042 on epoch=374
05/24/2022 08:42:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=376
05/24/2022 08:42:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=378
05/24/2022 08:42:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=379
05/24/2022 08:42:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=381
05/24/2022 08:42:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=383
05/24/2022 08:42:51 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.41013912712025924 on epoch=383
05/24/2022 08:42:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=384
05/24/2022 08:42:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=386
05/24/2022 08:42:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.22 on epoch=388
05/24/2022 08:43:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=389
05/24/2022 08:43:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=391
05/24/2022 08:43:06 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.42055475388808733 on epoch=391
05/24/2022 08:43:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=393
05/24/2022 08:43:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=394
05/24/2022 08:43:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=396
05/24/2022 08:43:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=398
05/24/2022 08:43:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=399
05/24/2022 08:43:22 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.39560308972073677 on epoch=399
05/24/2022 08:43:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=401
05/24/2022 08:43:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.25 on epoch=403
05/24/2022 08:43:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=404
05/24/2022 08:43:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=406
05/24/2022 08:43:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=408
05/24/2022 08:43:38 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.3799208013044491 on epoch=408
05/24/2022 08:43:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=409
05/24/2022 08:43:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=411
05/24/2022 08:43:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=413
05/24/2022 08:43:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=414
05/24/2022 08:43:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=416
05/24/2022 08:43:54 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.3799208013044491 on epoch=416
05/24/2022 08:43:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=418
05/24/2022 08:43:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=419
05/24/2022 08:44:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=421
05/24/2022 08:44:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=423
05/24/2022 08:44:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=424
05/24/2022 08:44:10 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.40035273368606705 on epoch=424
05/24/2022 08:44:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=426
05/24/2022 08:44:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=428
05/24/2022 08:44:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=429
05/24/2022 08:44:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=431
05/24/2022 08:44:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=433
05/24/2022 08:44:26 - INFO - __main__ - Global step 2600 Train loss 0.17 Classification-F1 0.4197046017245795 on epoch=433
05/24/2022 08:44:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.18 on epoch=434
05/24/2022 08:44:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=436
05/24/2022 08:44:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=438
05/24/2022 08:44:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=439
05/24/2022 08:44:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=441
05/24/2022 08:44:42 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.40198797965788263 on epoch=441
05/24/2022 08:44:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=443
05/24/2022 08:44:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=444
05/24/2022 08:44:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.20 on epoch=446
05/24/2022 08:44:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=448
05/24/2022 08:44:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.16 on epoch=449
05/24/2022 08:44:57 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.39195804195804196 on epoch=449
05/24/2022 08:45:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=451
05/24/2022 08:45:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=453
05/24/2022 08:45:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=454
05/24/2022 08:45:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=456
05/24/2022 08:45:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=458
05/24/2022 08:45:13 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.40588408355398653 on epoch=458
05/24/2022 08:45:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=459
05/24/2022 08:45:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.13 on epoch=461
05/24/2022 08:45:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=463
05/24/2022 08:45:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=464
05/24/2022 08:45:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=466
05/24/2022 08:45:29 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.31917211328976036 on epoch=466
05/24/2022 08:45:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=468
05/24/2022 08:45:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=469
05/24/2022 08:45:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=471
05/24/2022 08:45:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=473
05/24/2022 08:45:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=474
05/24/2022 08:45:45 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.32394461859979096 on epoch=474
05/24/2022 08:45:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=476
05/24/2022 08:45:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.17 on epoch=478
05/24/2022 08:45:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=479
05/24/2022 08:45:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=481
05/24/2022 08:45:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=483
05/24/2022 08:46:01 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.3254010695187166 on epoch=483
05/24/2022 08:46:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=484
05/24/2022 08:46:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=486
05/24/2022 08:46:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=488
05/24/2022 08:46:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=489
05/24/2022 08:46:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=491
05/24/2022 08:46:17 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.3180226636108989 on epoch=491
05/24/2022 08:46:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=493
05/24/2022 08:46:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=494
05/24/2022 08:46:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=496
05/24/2022 08:46:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=498
05/24/2022 08:46:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=499
05/24/2022 08:46:32 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:46:32 - INFO - __main__ - Printing 3 examples
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:46:32 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:46:32 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:46:32 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:46:32 - INFO - __main__ - Printing 3 examples
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 08:46:32 - INFO - __main__ - ['entailment']
05/24/2022 08:46:32 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:46:32 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:46:32 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:46:33 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.3287495416208287 on epoch=499
05/24/2022 08:46:33 - INFO - __main__ - save last model!
05/24/2022 08:46:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 08:46:33 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 08:46:33 - INFO - __main__ - Printing 3 examples
05/24/2022 08:46:33 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 08:46:33 - INFO - __main__ - ['contradiction']
05/24/2022 08:46:33 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 08:46:33 - INFO - __main__ - ['entailment']
05/24/2022 08:46:33 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 08:46:33 - INFO - __main__ - ['contradiction']
05/24/2022 08:46:33 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:46:34 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:46:35 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 08:46:47 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:46:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:46:48 - INFO - __main__ - Starting training!
05/24/2022 08:47:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_13_0.2_8_predictions.txt
05/24/2022 08:47:03 - INFO - __main__ - Classification-F1 on test data: 0.2118
05/24/2022 08:47:04 - INFO - __main__ - prefix=anli_32_13, lr=0.2, bsz=8, dev_performance=0.4262358063689917, test_performance=0.2118014543726059
05/24/2022 08:47:04 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.5, bsz=8 ...
05/24/2022 08:47:05 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:47:05 - INFO - __main__ - Printing 3 examples
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:47:05 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:47:05 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 08:47:05 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 08:47:05 - INFO - __main__ - Printing 3 examples
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 08:47:05 - INFO - __main__ - ['entailment']
05/24/2022 08:47:05 - INFO - __main__ - Tokenizing Input ...
05/24/2022 08:47:05 - INFO - __main__ - Tokenizing Output ...
05/24/2022 08:47:05 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 08:47:23 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 08:47:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 08:47:24 - INFO - __main__ - Starting training!
05/24/2022 08:47:28 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=1
05/24/2022 08:47:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=3
05/24/2022 08:47:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=4
05/24/2022 08:47:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=6
05/24/2022 08:47:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.40 on epoch=8
05/24/2022 08:47:41 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 08:47:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 08:47:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=9
05/24/2022 08:47:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=11
05/24/2022 08:47:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=13
05/24/2022 08:47:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=14
05/24/2022 08:47:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=16
05/24/2022 08:47:56 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 08:47:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=18
05/24/2022 08:48:02 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=19
05/24/2022 08:48:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=21
05/24/2022 08:48:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=23
05/24/2022 08:48:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=24
05/24/2022 08:48:12 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 08:48:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/24/2022 08:48:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=28
05/24/2022 08:48:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
05/24/2022 08:48:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=31
05/24/2022 08:48:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
05/24/2022 08:48:28 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 08:48:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
05/24/2022 08:48:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=36
05/24/2022 08:48:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=38
05/24/2022 08:48:39 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=39
05/24/2022 08:48:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=41
05/24/2022 08:48:44 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 08:48:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=43
05/24/2022 08:48:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=44
05/24/2022 08:48:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=46
05/24/2022 08:48:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=48
05/24/2022 08:48:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
05/24/2022 08:49:00 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 08:49:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=51
05/24/2022 08:49:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
05/24/2022 08:49:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=54
05/24/2022 08:49:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/24/2022 08:49:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=58
05/24/2022 08:49:16 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.20448662640207071 on epoch=58
05/24/2022 08:49:16 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20448662640207071 on epoch=58, global_step=350
05/24/2022 08:49:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=59
05/24/2022 08:49:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
05/24/2022 08:49:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=63
05/24/2022 08:49:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=64
05/24/2022 08:49:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=66
05/24/2022 08:49:32 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.24161628175901148 on epoch=66
05/24/2022 08:49:32 - INFO - __main__ - Saving model with best Classification-F1: 0.20448662640207071 -> 0.24161628175901148 on epoch=66, global_step=400
05/24/2022 08:49:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=68
05/24/2022 08:49:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=69
05/24/2022 08:49:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=71
05/24/2022 08:49:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=73
05/24/2022 08:49:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=74
05/24/2022 08:49:47 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.3166046429204324 on epoch=74
05/24/2022 08:49:48 - INFO - __main__ - Saving model with best Classification-F1: 0.24161628175901148 -> 0.3166046429204324 on epoch=74, global_step=450
05/24/2022 08:49:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=76
05/24/2022 08:49:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=78
05/24/2022 08:49:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=79
05/24/2022 08:49:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=81
05/24/2022 08:50:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=83
05/24/2022 08:50:03 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.30231065468549423 on epoch=83
05/24/2022 08:50:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=84
05/24/2022 08:50:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.35 on epoch=86
05/24/2022 08:50:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=88
05/24/2022 08:50:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=89
05/24/2022 08:50:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=91
05/24/2022 08:50:18 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.2502374169040836 on epoch=91
05/24/2022 08:50:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=93
05/24/2022 08:50:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=94
05/24/2022 08:50:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=96
05/24/2022 08:50:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=98
05/24/2022 08:50:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=99
05/24/2022 08:50:34 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.27991250263546275 on epoch=99
05/24/2022 08:50:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=101
05/24/2022 08:50:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=103
05/24/2022 08:50:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=104
05/24/2022 08:50:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=106
05/24/2022 08:50:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=108
05/24/2022 08:50:49 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.28565360518211746 on epoch=108
05/24/2022 08:50:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=109
05/24/2022 08:50:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=111
05/24/2022 08:50:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=113
05/24/2022 08:51:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=114
05/24/2022 08:51:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=116
05/24/2022 08:51:05 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.2991129785247432 on epoch=116
05/24/2022 08:51:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=118
05/24/2022 08:51:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=119
05/24/2022 08:51:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=121
05/24/2022 08:51:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=123
05/24/2022 08:51:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=124
05/24/2022 08:51:21 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.33431675433957714 on epoch=124
05/24/2022 08:51:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3166046429204324 -> 0.33431675433957714 on epoch=124, global_step=750
05/24/2022 08:51:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=126
05/24/2022 08:51:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=128
05/24/2022 08:51:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=129
05/24/2022 08:51:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=131
05/24/2022 08:51:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=133
05/24/2022 08:51:37 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.3338164251207729 on epoch=133
05/24/2022 08:51:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=134
05/24/2022 08:51:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=136
05/24/2022 08:51:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=138
05/24/2022 08:51:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=139
05/24/2022 08:51:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=141
05/24/2022 08:51:52 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.12149779093721227 on epoch=141
05/24/2022 08:51:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=143
05/24/2022 08:51:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=144
05/24/2022 08:52:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=146
05/24/2022 08:52:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=148
05/24/2022 08:52:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=149
05/24/2022 08:52:08 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.14585553295230713 on epoch=149
05/24/2022 08:52:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=151
05/24/2022 08:52:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=153
05/24/2022 08:52:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=154
05/24/2022 08:52:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=156
05/24/2022 08:52:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=158
05/24/2022 08:52:24 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.13075123666487873 on epoch=158
05/24/2022 08:52:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=159
05/24/2022 08:52:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=161
05/24/2022 08:52:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=163
05/24/2022 08:52:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=164
05/24/2022 08:52:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=166
05/24/2022 08:52:39 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.1451382654131795 on epoch=166
05/24/2022 08:52:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=168
05/24/2022 08:52:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=169
05/24/2022 08:52:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=171
05/24/2022 08:52:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=173
05/24/2022 08:52:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=174
05/24/2022 08:52:55 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.11529953917050692 on epoch=174
05/24/2022 08:52:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=176
05/24/2022 08:53:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=178
05/24/2022 08:53:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=179
05/24/2022 08:53:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=181
05/24/2022 08:53:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=183
05/24/2022 08:53:11 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.0924061124061124 on epoch=183
05/24/2022 08:53:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=184
05/24/2022 08:53:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=186
05/24/2022 08:53:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=188
05/24/2022 08:53:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=189
05/24/2022 08:53:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=191
05/24/2022 08:53:27 - INFO - __main__ - Global step 1150 Train loss 0.15 Classification-F1 0.08725525525525527 on epoch=191
05/24/2022 08:53:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=193
05/24/2022 08:53:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=194
05/24/2022 08:53:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=196
05/24/2022 08:53:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=198
05/24/2022 08:53:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=199
05/24/2022 08:53:42 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.11945468636246215 on epoch=199
05/24/2022 08:53:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=201
05/24/2022 08:53:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=203
05/24/2022 08:53:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=204
05/24/2022 08:53:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=206
05/24/2022 08:53:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=208
05/24/2022 08:53:58 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.11209846209846208 on epoch=208
05/24/2022 08:54:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=209
05/24/2022 08:54:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=211
05/24/2022 08:54:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=213
05/24/2022 08:54:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=214
05/24/2022 08:54:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=216
05/24/2022 08:54:13 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.08766760431317394 on epoch=216
05/24/2022 08:54:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=218
05/24/2022 08:54:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=219
05/24/2022 08:54:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=221
05/24/2022 08:54:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=223
05/24/2022 08:54:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=224
05/24/2022 08:54:29 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.09125431530494822 on epoch=224
05/24/2022 08:54:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=226
05/24/2022 08:54:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=228
05/24/2022 08:54:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=229
05/24/2022 08:54:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=231
05/24/2022 08:54:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=233
05/24/2022 08:54:44 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.08346883468834689 on epoch=233
05/24/2022 08:54:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=234
05/24/2022 08:54:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=236
05/24/2022 08:54:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=238
05/24/2022 08:54:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=239
05/24/2022 08:54:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=241
05/24/2022 08:55:00 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.08474424838061202 on epoch=241
05/24/2022 08:55:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=243
05/24/2022 08:55:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=244
05/24/2022 08:55:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=246
05/24/2022 08:55:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=248
05/24/2022 08:55:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=249
05/24/2022 08:55:15 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.11659355196096959 on epoch=249
05/24/2022 08:55:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=251
05/24/2022 08:55:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=253
05/24/2022 08:55:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.07 on epoch=254
05/24/2022 08:55:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=256
05/24/2022 08:55:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=258
05/24/2022 08:55:31 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.10450980392156864 on epoch=258
05/24/2022 08:55:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=259
05/24/2022 08:55:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=261
05/24/2022 08:55:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=263
05/24/2022 08:55:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=264
05/24/2022 08:55:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=266
05/24/2022 08:55:47 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.12084236275282376 on epoch=266
05/24/2022 08:55:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=268
05/24/2022 08:55:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=269
05/24/2022 08:55:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=271
05/24/2022 08:55:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=273
05/24/2022 08:55:59 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=274
05/24/2022 08:56:02 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.13288255340821048 on epoch=274
05/24/2022 08:56:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=276
05/24/2022 08:56:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=278
05/24/2022 08:56:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=279
05/24/2022 08:56:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=281
05/24/2022 08:56:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=283
05/24/2022 08:56:18 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.07238139058899128 on epoch=283
05/24/2022 08:56:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=284
05/24/2022 08:56:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=286
05/24/2022 08:56:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=288
05/24/2022 08:56:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=289
05/24/2022 08:56:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=291
05/24/2022 08:56:33 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.10040391156462584 on epoch=291
05/24/2022 08:56:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=293
05/24/2022 08:56:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=294
05/24/2022 08:56:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=296
05/24/2022 08:56:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=298
05/24/2022 08:56:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=299
05/24/2022 08:56:49 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.06170545733027003 on epoch=299
05/24/2022 08:56:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=301
05/24/2022 08:56:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=303
05/24/2022 08:56:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=304
05/24/2022 08:56:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=306
05/24/2022 08:57:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=308
05/24/2022 08:57:04 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.08745317028345231 on epoch=308
05/24/2022 08:57:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=309
05/24/2022 08:57:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=311
05/24/2022 08:57:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=313
05/24/2022 08:57:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=314
05/24/2022 08:57:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=316
05/24/2022 08:57:20 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.07860027580772262 on epoch=316
05/24/2022 08:57:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=318
05/24/2022 08:57:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=319
05/24/2022 08:57:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=321
05/24/2022 08:57:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=323
05/24/2022 08:57:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=324
05/24/2022 08:57:35 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.10853357422463114 on epoch=324
05/24/2022 08:57:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=326
05/24/2022 08:57:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=328
05/24/2022 08:57:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=329
05/24/2022 08:57:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=331
05/24/2022 08:57:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=333
05/24/2022 08:57:51 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.0794146433276868 on epoch=333
05/24/2022 08:57:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=334
05/24/2022 08:57:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=336
05/24/2022 08:57:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=338
05/24/2022 08:58:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=339
05/24/2022 08:58:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=341
05/24/2022 08:58:06 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.08295740192117784 on epoch=341
05/24/2022 08:58:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=343
05/24/2022 08:58:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=344
05/24/2022 08:58:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=346
05/24/2022 08:58:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=348
05/24/2022 08:58:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=349
05/24/2022 08:58:22 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.08950948800572861 on epoch=349
05/24/2022 08:58:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=351
05/24/2022 08:58:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=353
05/24/2022 08:58:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=354
05/24/2022 08:58:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=356
05/24/2022 08:58:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=358
05/24/2022 08:58:37 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.10522727272727272 on epoch=358
05/24/2022 08:58:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=359
05/24/2022 08:58:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=361
05/24/2022 08:58:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=363
05/24/2022 08:58:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=364
05/24/2022 08:58:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=366
05/24/2022 08:58:53 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.09919066440805571 on epoch=366
05/24/2022 08:58:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=368
05/24/2022 08:58:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=369
05/24/2022 08:59:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=371
05/24/2022 08:59:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=373
05/24/2022 08:59:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=374
05/24/2022 08:59:09 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.0957542908762421 on epoch=374
05/24/2022 08:59:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=376
05/24/2022 08:59:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=378
05/24/2022 08:59:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=379
05/24/2022 08:59:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=381
05/24/2022 08:59:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=383
05/24/2022 08:59:24 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.09917243250576585 on epoch=383
05/24/2022 08:59:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=384
05/24/2022 08:59:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=386
05/24/2022 08:59:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=388
05/24/2022 08:59:35 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=389
05/24/2022 08:59:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
05/24/2022 08:59:40 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.13346171802054155 on epoch=391
05/24/2022 08:59:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=393
05/24/2022 08:59:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=394
05/24/2022 08:59:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=396
05/24/2022 08:59:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=398
05/24/2022 08:59:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=399
05/24/2022 08:59:56 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.0826982727658602 on epoch=399
05/24/2022 08:59:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=401
05/24/2022 09:00:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=403
05/24/2022 09:00:03 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=404
05/24/2022 09:00:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=406
05/24/2022 09:00:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=408
05/24/2022 09:00:11 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.06507771718283552 on epoch=408
05/24/2022 09:00:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=409
05/24/2022 09:00:16 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=411
05/24/2022 09:00:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=413
05/24/2022 09:00:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=414
05/24/2022 09:00:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
05/24/2022 09:00:27 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.07977648828712659 on epoch=416
05/24/2022 09:00:29 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=418
05/24/2022 09:00:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=419
05/24/2022 09:00:35 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=421
05/24/2022 09:00:37 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=423
05/24/2022 09:00:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=424
05/24/2022 09:00:42 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.08417562164868414 on epoch=424
05/24/2022 09:00:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=426
05/24/2022 09:00:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=428
05/24/2022 09:00:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=429
05/24/2022 09:00:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=431
05/24/2022 09:00:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=433
05/24/2022 09:00:58 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.08167708123362226 on epoch=433
05/24/2022 09:01:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=434
05/24/2022 09:01:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 09:01:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=438
05/24/2022 09:01:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=439
05/24/2022 09:01:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=441
05/24/2022 09:01:13 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.10153554175293306 on epoch=441
05/24/2022 09:01:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=443
05/24/2022 09:01:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=444
05/24/2022 09:01:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=446
05/24/2022 09:01:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=448
05/24/2022 09:01:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 09:01:29 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.11626818684591168 on epoch=449
05/24/2022 09:01:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=451
05/24/2022 09:01:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 09:01:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=454
05/24/2022 09:01:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/24/2022 09:01:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=458
05/24/2022 09:01:45 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.09186965252313603 on epoch=458
05/24/2022 09:01:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/24/2022 09:01:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
05/24/2022 09:01:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=463
05/24/2022 09:01:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=464
05/24/2022 09:01:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=466
05/24/2022 09:02:00 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.12168881506090809 on epoch=466
05/24/2022 09:02:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
05/24/2022 09:02:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=469
05/24/2022 09:02:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=471
05/24/2022 09:02:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=473
05/24/2022 09:02:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=474
05/24/2022 09:02:16 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.10998343911241298 on epoch=474
05/24/2022 09:02:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
05/24/2022 09:02:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=478
05/24/2022 09:02:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=479
05/24/2022 09:02:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=481
05/24/2022 09:02:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=483
05/24/2022 09:02:31 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.1123702137655626 on epoch=483
05/24/2022 09:02:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=484
05/24/2022 09:02:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 09:02:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=488
05/24/2022 09:02:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=489
05/24/2022 09:02:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=491
05/24/2022 09:02:47 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.10883116883116883 on epoch=491
05/24/2022 09:02:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=493
05/24/2022 09:02:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=494
05/24/2022 09:02:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
05/24/2022 09:02:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=498
05/24/2022 09:03:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
05/24/2022 09:03:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:03:01 - INFO - __main__ - Printing 3 examples
05/24/2022 09:03:01 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:03:01 - INFO - __main__ - ['entailment']
05/24/2022 09:03:01 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:03:01 - INFO - __main__ - ['entailment']
05/24/2022 09:03:01 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:03:01 - INFO - __main__ - ['entailment']
05/24/2022 09:03:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:03:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:03:02 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:03:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:03:02 - INFO - __main__ - Printing 3 examples
05/24/2022 09:03:02 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:03:02 - INFO - __main__ - ['entailment']
05/24/2022 09:03:02 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:03:02 - INFO - __main__ - ['entailment']
05/24/2022 09:03:02 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:03:02 - INFO - __main__ - ['entailment']
05/24/2022 09:03:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:03:02 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:03:02 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:03:03 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.10686331465552244 on epoch=499
05/24/2022 09:03:03 - INFO - __main__ - save last model!
05/24/2022 09:03:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 09:03:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 09:03:03 - INFO - __main__ - Printing 3 examples
05/24/2022 09:03:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 09:03:03 - INFO - __main__ - ['contradiction']
05/24/2022 09:03:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 09:03:03 - INFO - __main__ - ['entailment']
05/24/2022 09:03:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 09:03:03 - INFO - __main__ - ['contradiction']
05/24/2022 09:03:03 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:03:03 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:03:05 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 09:03:17 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:03:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:03:18 - INFO - __main__ - Starting training!
05/24/2022 09:03:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_21_0.5_8_predictions.txt
05/24/2022 09:03:34 - INFO - __main__ - Classification-F1 on test data: 0.0534
05/24/2022 09:03:34 - INFO - __main__ - prefix=anli_32_21, lr=0.5, bsz=8, dev_performance=0.33431675433957714, test_performance=0.053374528274709766
05/24/2022 09:03:34 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.4, bsz=8 ...
05/24/2022 09:03:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:03:35 - INFO - __main__ - Printing 3 examples
05/24/2022 09:03:35 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:03:35 - INFO - __main__ - ['entailment']
05/24/2022 09:03:35 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:03:35 - INFO - __main__ - ['entailment']
05/24/2022 09:03:35 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:03:35 - INFO - __main__ - ['entailment']
05/24/2022 09:03:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:03:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:03:36 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:03:36 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:03:36 - INFO - __main__ - Printing 3 examples
05/24/2022 09:03:36 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:03:36 - INFO - __main__ - ['entailment']
05/24/2022 09:03:36 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:03:36 - INFO - __main__ - ['entailment']
05/24/2022 09:03:36 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:03:36 - INFO - __main__ - ['entailment']
05/24/2022 09:03:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:03:36 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:03:36 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:03:52 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:03:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:03:53 - INFO - __main__ - Starting training!
05/24/2022 09:03:57 - INFO - __main__ - Step 10 Global step 10 Train loss 0.56 on epoch=1
05/24/2022 09:03:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=3
05/24/2022 09:04:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=4
05/24/2022 09:04:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=6
05/24/2022 09:04:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=8
05/24/2022 09:04:09 - INFO - __main__ - Global step 50 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 09:04:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 09:04:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=9
05/24/2022 09:04:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=11
05/24/2022 09:04:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=13
05/24/2022 09:04:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=14
05/24/2022 09:04:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=16
05/24/2022 09:04:24 - INFO - __main__ - Global step 100 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 09:04:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=18
05/24/2022 09:04:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=19
05/24/2022 09:04:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=21
05/24/2022 09:04:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=23
05/24/2022 09:04:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=24
05/24/2022 09:04:40 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 09:04:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
05/24/2022 09:04:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=28
05/24/2022 09:04:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
05/24/2022 09:04:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/24/2022 09:04:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=33
05/24/2022 09:04:55 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 09:04:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
05/24/2022 09:05:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=36
05/24/2022 09:05:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=38
05/24/2022 09:05:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
05/24/2022 09:05:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=41
05/24/2022 09:05:11 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 09:05:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=43
05/24/2022 09:05:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
05/24/2022 09:05:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/24/2022 09:05:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=48
05/24/2022 09:05:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/24/2022 09:05:26 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.293859649122807 on epoch=49
05/24/2022 09:05:26 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.293859649122807 on epoch=49, global_step=300
05/24/2022 09:05:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.37 on epoch=51
05/24/2022 09:05:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=53
05/24/2022 09:05:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/24/2022 09:05:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
05/24/2022 09:05:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=58
05/24/2022 09:05:42 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.3590225276823215 on epoch=58
05/24/2022 09:05:42 - INFO - __main__ - Saving model with best Classification-F1: 0.293859649122807 -> 0.3590225276823215 on epoch=58, global_step=350
05/24/2022 09:05:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.39 on epoch=59
05/24/2022 09:05:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
05/24/2022 09:05:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=63
05/24/2022 09:05:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/24/2022 09:05:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=66
05/24/2022 09:05:57 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.28883666274970626 on epoch=66
05/24/2022 09:06:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=68
05/24/2022 09:06:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=69
05/24/2022 09:06:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=71
05/24/2022 09:06:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=73
05/24/2022 09:06:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=74
05/24/2022 09:06:13 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.3081663223507195 on epoch=74
05/24/2022 09:06:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
05/24/2022 09:06:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=78
05/24/2022 09:06:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=79
05/24/2022 09:06:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=81
05/24/2022 09:06:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=83
05/24/2022 09:06:28 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3482368633117514 on epoch=83
05/24/2022 09:06:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=84
05/24/2022 09:06:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
05/24/2022 09:06:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=88
05/24/2022 09:06:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
05/24/2022 09:06:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=91
05/24/2022 09:06:44 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3092011008592782 on epoch=91
05/24/2022 09:06:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=93
05/24/2022 09:06:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=94
05/24/2022 09:06:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=96
05/24/2022 09:06:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=98
05/24/2022 09:06:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
05/24/2022 09:06:59 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.34345649199066247 on epoch=99
05/24/2022 09:07:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=101
05/24/2022 09:07:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=103
05/24/2022 09:07:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=104
05/24/2022 09:07:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=106
05/24/2022 09:07:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=108
05/24/2022 09:07:15 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.2965986394557823 on epoch=108
05/24/2022 09:07:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=109
05/24/2022 09:07:20 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=111
05/24/2022 09:07:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=113
05/24/2022 09:07:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=114
05/24/2022 09:07:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=116
05/24/2022 09:07:30 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.28265905657210005 on epoch=116
05/24/2022 09:07:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=118
05/24/2022 09:07:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=119
05/24/2022 09:07:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=121
05/24/2022 09:07:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=123
05/24/2022 09:07:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=124
05/24/2022 09:07:45 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.3238925199709513 on epoch=124
05/24/2022 09:07:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=126
05/24/2022 09:07:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=128
05/24/2022 09:07:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=129
05/24/2022 09:07:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=131
05/24/2022 09:07:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=133
05/24/2022 09:08:01 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.33274097600322644 on epoch=133
05/24/2022 09:08:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=134
05/24/2022 09:08:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=136
05/24/2022 09:08:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
05/24/2022 09:08:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=139
05/24/2022 09:08:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=141
05/24/2022 09:08:16 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.3151515151515151 on epoch=141
05/24/2022 09:08:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=143
05/24/2022 09:08:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=144
05/24/2022 09:08:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=146
05/24/2022 09:08:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=148
05/24/2022 09:08:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=149
05/24/2022 09:08:31 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.3221593587930222 on epoch=149
05/24/2022 09:08:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=151
05/24/2022 09:08:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=153
05/24/2022 09:08:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=154
05/24/2022 09:08:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=156
05/24/2022 09:08:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=158
05/24/2022 09:08:46 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.3363515645462907 on epoch=158
05/24/2022 09:08:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=159
05/24/2022 09:08:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=161
05/24/2022 09:08:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=163
05/24/2022 09:08:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=164
05/24/2022 09:08:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=166
05/24/2022 09:09:01 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.2612406927475421 on epoch=166
05/24/2022 09:09:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=168
05/24/2022 09:09:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=169
05/24/2022 09:09:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=171
05/24/2022 09:09:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=173
05/24/2022 09:09:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=174
05/24/2022 09:09:16 - INFO - __main__ - Global step 1050 Train loss 0.26 Classification-F1 0.25722789115646255 on epoch=174
05/24/2022 09:09:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=176
05/24/2022 09:09:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=178
05/24/2022 09:09:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=179
05/24/2022 09:09:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=181
05/24/2022 09:09:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=183
05/24/2022 09:09:31 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.24798471588808085 on epoch=183
05/24/2022 09:09:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=184
05/24/2022 09:09:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=186
05/24/2022 09:09:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=188
05/24/2022 09:09:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=189
05/24/2022 09:09:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=191
05/24/2022 09:09:47 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.22724586288416077 on epoch=191
05/24/2022 09:09:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=193
05/24/2022 09:09:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=194
05/24/2022 09:09:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=196
05/24/2022 09:09:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=198
05/24/2022 09:09:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=199
05/24/2022 09:10:02 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.16513810513810515 on epoch=199
05/24/2022 09:10:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=201
05/24/2022 09:10:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=203
05/24/2022 09:10:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=204
05/24/2022 09:10:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=206
05/24/2022 09:10:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=208
05/24/2022 09:10:17 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.2552886863020103 on epoch=208
05/24/2022 09:10:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=209
05/24/2022 09:10:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=211
05/24/2022 09:10:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=213
05/24/2022 09:10:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=214
05/24/2022 09:10:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=216
05/24/2022 09:10:32 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.22597692733706926 on epoch=216
05/24/2022 09:10:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=218
05/24/2022 09:10:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=219
05/24/2022 09:10:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=221
05/24/2022 09:10:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=223
05/24/2022 09:10:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=224
05/24/2022 09:10:48 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.113585291113381 on epoch=224
05/24/2022 09:10:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=226
05/24/2022 09:10:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=228
05/24/2022 09:10:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=229
05/24/2022 09:10:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=231
05/24/2022 09:11:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=233
05/24/2022 09:11:03 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.22067480960845387 on epoch=233
05/24/2022 09:11:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=234
05/24/2022 09:11:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=236
05/24/2022 09:11:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=238
05/24/2022 09:11:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=239
05/24/2022 09:11:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=241
05/24/2022 09:11:18 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.1466133958080576 on epoch=241
05/24/2022 09:11:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=243
05/24/2022 09:11:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=244
05/24/2022 09:11:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=246
05/24/2022 09:11:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=248
05/24/2022 09:11:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=249
05/24/2022 09:11:33 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.1729032948224219 on epoch=249
05/24/2022 09:11:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=251
05/24/2022 09:11:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=253
05/24/2022 09:11:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=254
05/24/2022 09:11:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=256
05/24/2022 09:11:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=258
05/24/2022 09:11:49 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.14356725146198832 on epoch=258
05/24/2022 09:11:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=259
05/24/2022 09:11:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=261
05/24/2022 09:11:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=263
05/24/2022 09:11:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=264
05/24/2022 09:12:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=266
05/24/2022 09:12:04 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.2091969696969697 on epoch=266
05/24/2022 09:12:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=268
05/24/2022 09:12:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=269
05/24/2022 09:12:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=271
05/24/2022 09:12:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=273
05/24/2022 09:12:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=274
05/24/2022 09:12:20 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.1363972407369677 on epoch=274
05/24/2022 09:12:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=276
05/24/2022 09:12:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=278
05/24/2022 09:12:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=279
05/24/2022 09:12:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=281
05/24/2022 09:12:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=283
05/24/2022 09:12:35 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.16515406162464985 on epoch=283
05/24/2022 09:12:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=284
05/24/2022 09:12:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=286
05/24/2022 09:12:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=288
05/24/2022 09:12:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=289
05/24/2022 09:12:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=291
05/24/2022 09:12:51 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.2513354700854701 on epoch=291
05/24/2022 09:12:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=293
05/24/2022 09:12:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=294
05/24/2022 09:12:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=296
05/24/2022 09:13:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=298
05/24/2022 09:13:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=299
05/24/2022 09:13:06 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.17705128205128207 on epoch=299
05/24/2022 09:13:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=301
05/24/2022 09:13:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=303
05/24/2022 09:13:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=304
05/24/2022 09:13:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=306
05/24/2022 09:13:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=308
05/24/2022 09:13:22 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.20518125352907965 on epoch=308
05/24/2022 09:13:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=309
05/24/2022 09:13:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=311
05/24/2022 09:13:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=313
05/24/2022 09:13:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=314
05/24/2022 09:13:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=316
05/24/2022 09:13:37 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.18702576532902412 on epoch=316
05/24/2022 09:13:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=318
05/24/2022 09:13:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=319
05/24/2022 09:13:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=321
05/24/2022 09:13:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=323
05/24/2022 09:13:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=324
05/24/2022 09:13:53 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.2869822652841521 on epoch=324
05/24/2022 09:13:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=326
05/24/2022 09:13:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=328
05/24/2022 09:14:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=329
05/24/2022 09:14:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=331
05/24/2022 09:14:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=333
05/24/2022 09:14:09 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.1756400276768725 on epoch=333
05/24/2022 09:14:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=334
05/24/2022 09:14:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=336
05/24/2022 09:14:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=338
05/24/2022 09:14:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=339
05/24/2022 09:14:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=341
05/24/2022 09:14:24 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.11499921606069329 on epoch=341
05/24/2022 09:14:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=343
05/24/2022 09:14:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=344
05/24/2022 09:14:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=346
05/24/2022 09:14:35 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=348
05/24/2022 09:14:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=349
05/24/2022 09:14:40 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.14501742160278747 on epoch=349
05/24/2022 09:14:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=351
05/24/2022 09:14:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=353
05/24/2022 09:14:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=354
05/24/2022 09:14:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=356
05/24/2022 09:14:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=358
05/24/2022 09:14:56 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.14374233455980376 on epoch=358
05/24/2022 09:14:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=359
05/24/2022 09:15:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=361
05/24/2022 09:15:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=363
05/24/2022 09:15:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=364
05/24/2022 09:15:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=366
05/24/2022 09:15:11 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.12295362589480237 on epoch=366
05/24/2022 09:15:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=368
05/24/2022 09:15:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=369
05/24/2022 09:15:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=371
05/24/2022 09:15:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=373
05/24/2022 09:15:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=374
05/24/2022 09:15:27 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.2715943021650489 on epoch=374
05/24/2022 09:15:29 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=376
05/24/2022 09:15:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=378
05/24/2022 09:15:34 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=379
05/24/2022 09:15:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=381
05/24/2022 09:15:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=383
05/24/2022 09:15:42 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.15059987631416202 on epoch=383
05/24/2022 09:15:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=384
05/24/2022 09:15:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=386
05/24/2022 09:15:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=388
05/24/2022 09:15:53 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=389
05/24/2022 09:15:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=391
05/24/2022 09:15:58 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.1604637240525742 on epoch=391
05/24/2022 09:16:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=393
05/24/2022 09:16:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=394
05/24/2022 09:16:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=396
05/24/2022 09:16:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=398
05/24/2022 09:16:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=399
05/24/2022 09:16:13 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.1491960420531849 on epoch=399
05/24/2022 09:16:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=401
05/24/2022 09:16:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=403
05/24/2022 09:16:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=404
05/24/2022 09:16:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=406
05/24/2022 09:16:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=408
05/24/2022 09:16:29 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.17327098970509827 on epoch=408
05/24/2022 09:16:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=409
05/24/2022 09:16:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=411
05/24/2022 09:16:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=413
05/24/2022 09:16:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=414
05/24/2022 09:16:42 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=416
05/24/2022 09:16:44 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.2309522044417144 on epoch=416
05/24/2022 09:16:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=418
05/24/2022 09:16:50 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
05/24/2022 09:16:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
05/24/2022 09:16:55 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=423
05/24/2022 09:16:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=424
05/24/2022 09:17:00 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.1621964991530209 on epoch=424
05/24/2022 09:17:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=426
05/24/2022 09:17:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=428
05/24/2022 09:17:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=429
05/24/2022 09:17:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=431
05/24/2022 09:17:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=433
05/24/2022 09:17:16 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.18666666666666665 on epoch=433
05/24/2022 09:17:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=434
05/24/2022 09:17:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=436
05/24/2022 09:17:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=438
05/24/2022 09:17:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=439
05/24/2022 09:17:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=441
05/24/2022 09:17:31 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.2044480904201987 on epoch=441
05/24/2022 09:17:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=443
05/24/2022 09:17:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 09:17:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=446
05/24/2022 09:17:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=448
05/24/2022 09:17:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=449
05/24/2022 09:17:47 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.192016210739615 on epoch=449
05/24/2022 09:17:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=451
05/24/2022 09:17:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 09:17:54 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=454
05/24/2022 09:17:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=456
05/24/2022 09:17:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 09:18:02 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.1411354286230155 on epoch=458
05/24/2022 09:18:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/24/2022 09:18:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=461
05/24/2022 09:18:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=463
05/24/2022 09:18:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=464
05/24/2022 09:18:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=466
05/24/2022 09:18:17 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.23836382113821142 on epoch=466
05/24/2022 09:18:20 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=468
05/24/2022 09:18:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=469
05/24/2022 09:18:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=471
05/24/2022 09:18:28 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=473
05/24/2022 09:18:30 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/24/2022 09:18:33 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.17425861088667446 on epoch=474
05/24/2022 09:18:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
05/24/2022 09:18:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=478
05/24/2022 09:18:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=479
05/24/2022 09:18:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=481
05/24/2022 09:18:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=483
05/24/2022 09:18:48 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.13066378066378065 on epoch=483
05/24/2022 09:18:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/24/2022 09:18:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=486
05/24/2022 09:18:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=488
05/24/2022 09:18:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=489
05/24/2022 09:19:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=491
05/24/2022 09:19:04 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.11800496251715764 on epoch=491
05/24/2022 09:19:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=493
05/24/2022 09:19:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 09:19:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
05/24/2022 09:19:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=498
05/24/2022 09:19:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=499
05/24/2022 09:19:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:19:18 - INFO - __main__ - Printing 3 examples
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:19:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:19:18 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:19:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:19:18 - INFO - __main__ - Printing 3 examples
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:19:18 - INFO - __main__ - ['entailment']
05/24/2022 09:19:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:19:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:19:19 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:19:19 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.20542802486848397 on epoch=499
05/24/2022 09:19:19 - INFO - __main__ - save last model!
05/24/2022 09:19:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 09:19:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 09:19:19 - INFO - __main__ - Printing 3 examples
05/24/2022 09:19:19 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 09:19:19 - INFO - __main__ - ['contradiction']
05/24/2022 09:19:19 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 09:19:19 - INFO - __main__ - ['entailment']
05/24/2022 09:19:19 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 09:19:19 - INFO - __main__ - ['contradiction']
05/24/2022 09:19:19 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:19:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:19:21 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 09:19:34 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:19:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:19:35 - INFO - __main__ - Starting training!
05/24/2022 09:19:50 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_21_0.4_8_predictions.txt
05/24/2022 09:19:50 - INFO - __main__ - Classification-F1 on test data: 0.0406
05/24/2022 09:19:50 - INFO - __main__ - prefix=anli_32_21, lr=0.4, bsz=8, dev_performance=0.3590225276823215, test_performance=0.04063964242425236
05/24/2022 09:19:50 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.3, bsz=8 ...
05/24/2022 09:19:51 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:19:51 - INFO - __main__ - Printing 3 examples
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:19:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:19:51 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:19:51 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:19:51 - INFO - __main__ - Printing 3 examples
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:19:51 - INFO - __main__ - ['entailment']
05/24/2022 09:19:51 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:19:51 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:19:52 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:20:10 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:20:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:20:11 - INFO - __main__ - Starting training!
05/24/2022 09:20:14 - INFO - __main__ - Step 10 Global step 10 Train loss 0.48 on epoch=1
05/24/2022 09:20:17 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=3
05/24/2022 09:20:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=4
05/24/2022 09:20:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=6
05/24/2022 09:20:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=8
05/24/2022 09:20:27 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 09:20:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 09:20:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=9
05/24/2022 09:20:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=11
05/24/2022 09:20:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=13
05/24/2022 09:20:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=14
05/24/2022 09:20:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=16
05/24/2022 09:20:43 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 09:20:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=18
05/24/2022 09:20:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=19
05/24/2022 09:20:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=21
05/24/2022 09:20:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 09:20:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=24
05/24/2022 09:20:59 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 09:21:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=26
05/24/2022 09:21:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=28
05/24/2022 09:21:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
05/24/2022 09:21:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 09:21:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=33
05/24/2022 09:21:15 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 09:21:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=34
05/24/2022 09:21:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=36
05/24/2022 09:21:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
05/24/2022 09:21:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=39
05/24/2022 09:21:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=41
05/24/2022 09:21:31 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 09:21:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=43
05/24/2022 09:21:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=44
05/24/2022 09:21:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
05/24/2022 09:21:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=48
05/24/2022 09:21:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/24/2022 09:21:47 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 09:21:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=51
05/24/2022 09:21:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=53
05/24/2022 09:21:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=54
05/24/2022 09:21:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/24/2022 09:22:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/24/2022 09:22:03 - INFO - __main__ - Global step 350 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 09:22:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=59
05/24/2022 09:22:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=61
05/24/2022 09:22:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=63
05/24/2022 09:22:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=64
05/24/2022 09:22:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=66
05/24/2022 09:22:19 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 09:22:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/24/2022 09:22:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=69
05/24/2022 09:22:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=71
05/24/2022 09:22:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=73
05/24/2022 09:22:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=74
05/24/2022 09:22:36 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.26476055887820593 on epoch=74
05/24/2022 09:22:36 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26476055887820593 on epoch=74, global_step=450
05/24/2022 09:22:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=76
05/24/2022 09:22:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=78
05/24/2022 09:22:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=79
05/24/2022 09:22:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=81
05/24/2022 09:22:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=83
05/24/2022 09:22:52 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.2812612224376931 on epoch=83
05/24/2022 09:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.26476055887820593 -> 0.2812612224376931 on epoch=83, global_step=500
05/24/2022 09:22:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=84
05/24/2022 09:22:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
05/24/2022 09:23:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=88
05/24/2022 09:23:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=89
05/24/2022 09:23:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=91
05/24/2022 09:23:08 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.20285539376448466 on epoch=91
05/24/2022 09:23:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=93
05/24/2022 09:23:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=94
05/24/2022 09:23:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=96
05/24/2022 09:23:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=98
05/24/2022 09:23:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=99
05/24/2022 09:23:24 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.3118874972533509 on epoch=99
05/24/2022 09:23:25 - INFO - __main__ - Saving model with best Classification-F1: 0.2812612224376931 -> 0.3118874972533509 on epoch=99, global_step=600
05/24/2022 09:23:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
05/24/2022 09:23:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=103
05/24/2022 09:23:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=104
05/24/2022 09:23:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=106
05/24/2022 09:23:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=108
05/24/2022 09:23:41 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3114809931883103 on epoch=108
05/24/2022 09:23:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=109
05/24/2022 09:23:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=111
05/24/2022 09:23:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=113
05/24/2022 09:23:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=114
05/24/2022 09:23:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=116
05/24/2022 09:23:57 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.336400912281996 on epoch=116
05/24/2022 09:23:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3118874972533509 -> 0.336400912281996 on epoch=116, global_step=700
05/24/2022 09:24:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=118
05/24/2022 09:24:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=119
05/24/2022 09:24:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=121
05/24/2022 09:24:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=123
05/24/2022 09:24:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
05/24/2022 09:24:13 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3356934783901076 on epoch=124
05/24/2022 09:24:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=126
05/24/2022 09:24:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=128
05/24/2022 09:24:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=129
05/24/2022 09:24:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=131
05/24/2022 09:24:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=133
05/24/2022 09:24:29 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.3826546003016591 on epoch=133
05/24/2022 09:24:29 - INFO - __main__ - Saving model with best Classification-F1: 0.336400912281996 -> 0.3826546003016591 on epoch=133, global_step=800
05/24/2022 09:24:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=134
05/24/2022 09:24:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
05/24/2022 09:24:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=138
05/24/2022 09:24:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=139
05/24/2022 09:24:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=141
05/24/2022 09:24:46 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.31345466863759547 on epoch=141
05/24/2022 09:24:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=143
05/24/2022 09:24:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=144
05/24/2022 09:24:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=146
05/24/2022 09:24:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=148
05/24/2022 09:24:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=149
05/24/2022 09:25:02 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.3202651515151515 on epoch=149
05/24/2022 09:25:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=151
05/24/2022 09:25:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=153
05/24/2022 09:25:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=154
05/24/2022 09:25:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=156
05/24/2022 09:25:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=158
05/24/2022 09:25:18 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.3575153492434557 on epoch=158
05/24/2022 09:25:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=159
05/24/2022 09:25:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=161
05/24/2022 09:25:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=163
05/24/2022 09:25:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=164
05/24/2022 09:25:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=166
05/24/2022 09:25:34 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.31063600797009755 on epoch=166
05/24/2022 09:25:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=168
05/24/2022 09:25:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=169
05/24/2022 09:25:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=171
05/24/2022 09:25:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=173
05/24/2022 09:25:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=174
05/24/2022 09:25:50 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.3101301943998573 on epoch=174
05/24/2022 09:25:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=176
05/24/2022 09:25:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=178
05/24/2022 09:25:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=179
05/24/2022 09:26:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=181
05/24/2022 09:26:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=183
05/24/2022 09:26:07 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.2243838028169014 on epoch=183
05/24/2022 09:26:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=184
05/24/2022 09:26:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=186
05/24/2022 09:26:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=188
05/24/2022 09:26:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=189
05/24/2022 09:26:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=191
05/24/2022 09:26:23 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.2355262460935431 on epoch=191
05/24/2022 09:26:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=193
05/24/2022 09:26:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=194
05/24/2022 09:26:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=196
05/24/2022 09:26:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=198
05/24/2022 09:26:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=199
05/24/2022 09:26:39 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.2898653437278526 on epoch=199
05/24/2022 09:26:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=201
05/24/2022 09:26:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=203
05/24/2022 09:26:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=204
05/24/2022 09:26:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=206
05/24/2022 09:26:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=208
05/24/2022 09:26:55 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.32214590442438545 on epoch=208
05/24/2022 09:26:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=209
05/24/2022 09:27:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=211
05/24/2022 09:27:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=213
05/24/2022 09:27:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=214
05/24/2022 09:27:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=216
05/24/2022 09:27:11 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.17714491708723865 on epoch=216
05/24/2022 09:27:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=218
05/24/2022 09:27:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=219
05/24/2022 09:27:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=221
05/24/2022 09:27:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=223
05/24/2022 09:27:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=224
05/24/2022 09:27:27 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.18214285714285713 on epoch=224
05/24/2022 09:27:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=226
05/24/2022 09:27:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=228
05/24/2022 09:27:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=229
05/24/2022 09:27:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=231
05/24/2022 09:27:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=233
05/24/2022 09:27:43 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.1723529411764706 on epoch=233
05/24/2022 09:27:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=234
05/24/2022 09:27:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=236
05/24/2022 09:27:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=238
05/24/2022 09:27:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=239
05/24/2022 09:27:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=241
05/24/2022 09:27:59 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.22364929362352043 on epoch=241
05/24/2022 09:28:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=243
05/24/2022 09:28:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=244
05/24/2022 09:28:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=246
05/24/2022 09:28:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=248
05/24/2022 09:28:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=249
05/24/2022 09:28:15 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.1972253408179631 on epoch=249
05/24/2022 09:28:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=251
05/24/2022 09:28:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=253
05/24/2022 09:28:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=254
05/24/2022 09:28:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=256
05/24/2022 09:28:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=258
05/24/2022 09:28:31 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.14269694421082144 on epoch=258
05/24/2022 09:28:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=259
05/24/2022 09:28:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=261
05/24/2022 09:28:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=263
05/24/2022 09:28:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=264
05/24/2022 09:28:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=266
05/24/2022 09:28:47 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.18395909936568858 on epoch=266
05/24/2022 09:28:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=268
05/24/2022 09:28:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=269
05/24/2022 09:28:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=271
05/24/2022 09:28:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=273
05/24/2022 09:29:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=274
05/24/2022 09:29:03 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.12305919045359007 on epoch=274
05/24/2022 09:29:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=276
05/24/2022 09:29:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=278
05/24/2022 09:29:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=279
05/24/2022 09:29:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=281
05/24/2022 09:29:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=283
05/24/2022 09:29:19 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.21117227530812904 on epoch=283
05/24/2022 09:29:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=284
05/24/2022 09:29:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=286
05/24/2022 09:29:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=288
05/24/2022 09:29:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=289
05/24/2022 09:29:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=291
05/24/2022 09:29:35 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.17169312169312168 on epoch=291
05/24/2022 09:29:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=293
05/24/2022 09:29:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=294
05/24/2022 09:29:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=296
05/24/2022 09:29:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=298
05/24/2022 09:29:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=299
05/24/2022 09:29:51 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.1509614416591161 on epoch=299
05/24/2022 09:29:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=301
05/24/2022 09:29:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=303
05/24/2022 09:29:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=304
05/24/2022 09:30:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=306
05/24/2022 09:30:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=308
05/24/2022 09:30:07 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.2725490196078431 on epoch=308
05/24/2022 09:30:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=309
05/24/2022 09:30:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=311
05/24/2022 09:30:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=313
05/24/2022 09:30:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=314
05/24/2022 09:30:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=316
05/24/2022 09:30:23 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.19201817001946786 on epoch=316
05/24/2022 09:30:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=318
05/24/2022 09:30:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=319
05/24/2022 09:30:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=321
05/24/2022 09:30:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=323
05/24/2022 09:30:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=324
05/24/2022 09:30:39 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.17163839318498933 on epoch=324
05/24/2022 09:30:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=326
05/24/2022 09:30:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=328
05/24/2022 09:30:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=329
05/24/2022 09:30:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=331
05/24/2022 09:30:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
05/24/2022 09:30:56 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.1675452196382429 on epoch=333
05/24/2022 09:30:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=334
05/24/2022 09:31:01 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=336
05/24/2022 09:31:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=338
05/24/2022 09:31:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=339
05/24/2022 09:31:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=341
05/24/2022 09:31:12 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.14296157059314954 on epoch=341
05/24/2022 09:31:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=343
05/24/2022 09:31:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=344
05/24/2022 09:31:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=346
05/24/2022 09:31:22 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=348
05/24/2022 09:31:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=349
05/24/2022 09:31:28 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.16076388888888887 on epoch=349
05/24/2022 09:31:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=351
05/24/2022 09:31:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=353
05/24/2022 09:31:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=354
05/24/2022 09:31:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=356
05/24/2022 09:31:41 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=358
05/24/2022 09:31:44 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.13437226318614573 on epoch=358
05/24/2022 09:31:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=359
05/24/2022 09:31:49 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=361
05/24/2022 09:31:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=363
05/24/2022 09:31:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=364
05/24/2022 09:31:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=366
05/24/2022 09:32:00 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.11768380670819695 on epoch=366
05/24/2022 09:32:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=368
05/24/2022 09:32:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=369
05/24/2022 09:32:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=371
05/24/2022 09:32:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=373
05/24/2022 09:32:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=374
05/24/2022 09:32:16 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.13307050092764378 on epoch=374
05/24/2022 09:32:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=376
05/24/2022 09:32:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=378
05/24/2022 09:32:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=379
05/24/2022 09:32:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=381
05/24/2022 09:32:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=383
05/24/2022 09:32:33 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.13432337782476542 on epoch=383
05/24/2022 09:32:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=384
05/24/2022 09:32:38 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=386
05/24/2022 09:32:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=388
05/24/2022 09:32:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=389
05/24/2022 09:32:46 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=391
05/24/2022 09:32:49 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.12382032589308906 on epoch=391
05/24/2022 09:32:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=393
05/24/2022 09:32:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=394
05/24/2022 09:32:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=396
05/24/2022 09:32:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=398
05/24/2022 09:33:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=399
05/24/2022 09:33:05 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.11301969065379551 on epoch=399
05/24/2022 09:33:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=401
05/24/2022 09:33:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=403
05/24/2022 09:33:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=404
05/24/2022 09:33:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=406
05/24/2022 09:33:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=408
05/24/2022 09:33:21 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.11554621848739496 on epoch=408
05/24/2022 09:33:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=409
05/24/2022 09:33:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=411
05/24/2022 09:33:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=413
05/24/2022 09:33:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=414
05/24/2022 09:33:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=416
05/24/2022 09:33:37 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.082281284606866 on epoch=416
05/24/2022 09:33:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=418
05/24/2022 09:33:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=419
05/24/2022 09:33:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=421
05/24/2022 09:33:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=423
05/24/2022 09:33:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=424
05/24/2022 09:33:53 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.10414846801707514 on epoch=424
05/24/2022 09:33:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=426
05/24/2022 09:33:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=428
05/24/2022 09:34:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=429
05/24/2022 09:34:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=431
05/24/2022 09:34:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=433
05/24/2022 09:34:09 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.09961141939461722 on epoch=433
05/24/2022 09:34:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=434
05/24/2022 09:34:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=436
05/24/2022 09:34:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=438
05/24/2022 09:34:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=439
05/24/2022 09:34:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=441
05/24/2022 09:34:25 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.09263492063492065 on epoch=441
05/24/2022 09:34:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=443
05/24/2022 09:34:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 09:34:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=446
05/24/2022 09:34:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=448
05/24/2022 09:34:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=449
05/24/2022 09:34:41 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.10696055657918363 on epoch=449
05/24/2022 09:34:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=451
05/24/2022 09:34:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=453
05/24/2022 09:34:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=454
05/24/2022 09:34:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=456
05/24/2022 09:34:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=458
05/24/2022 09:34:58 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.1107983193277311 on epoch=458
05/24/2022 09:35:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=459
05/24/2022 09:35:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=461
05/24/2022 09:35:06 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=463
05/24/2022 09:35:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=464
05/24/2022 09:35:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=466
05/24/2022 09:35:14 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.0692469351391778 on epoch=466
05/24/2022 09:35:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=468
05/24/2022 09:35:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=469
05/24/2022 09:35:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=471
05/24/2022 09:35:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=473
05/24/2022 09:35:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/24/2022 09:35:30 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.10843867720602951 on epoch=474
05/24/2022 09:35:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=476
05/24/2022 09:35:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
05/24/2022 09:35:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=479
05/24/2022 09:35:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=481
05/24/2022 09:35:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=483
05/24/2022 09:35:47 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.09102664576802508 on epoch=483
05/24/2022 09:35:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=484
05/24/2022 09:35:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=486
05/24/2022 09:35:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=488
05/24/2022 09:35:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=489
05/24/2022 09:36:00 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=491
05/24/2022 09:36:03 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.07522073156748081 on epoch=491
05/24/2022 09:36:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=493
05/24/2022 09:36:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 09:36:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=496
05/24/2022 09:36:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=498
05/24/2022 09:36:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=499
05/24/2022 09:36:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:36:18 - INFO - __main__ - Printing 3 examples
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:36:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:36:18 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:36:18 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:36:18 - INFO - __main__ - Printing 3 examples
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:36:18 - INFO - __main__ - ['entailment']
05/24/2022 09:36:18 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:36:18 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:36:18 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:36:19 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.09885920061575677 on epoch=499
05/24/2022 09:36:19 - INFO - __main__ - save last model!
05/24/2022 09:36:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 09:36:19 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 09:36:19 - INFO - __main__ - Printing 3 examples
05/24/2022 09:36:19 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 09:36:19 - INFO - __main__ - ['contradiction']
05/24/2022 09:36:19 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 09:36:19 - INFO - __main__ - ['entailment']
05/24/2022 09:36:19 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 09:36:19 - INFO - __main__ - ['contradiction']
05/24/2022 09:36:19 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:36:20 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:36:21 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 09:36:37 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:36:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:36:38 - INFO - __main__ - Starting training!
05/24/2022 09:36:50 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_21_0.3_8_predictions.txt
05/24/2022 09:36:50 - INFO - __main__ - Classification-F1 on test data: 0.0425
05/24/2022 09:36:51 - INFO - __main__ - prefix=anli_32_21, lr=0.3, bsz=8, dev_performance=0.3826546003016591, test_performance=0.04253508442624832
05/24/2022 09:36:51 - INFO - __main__ - Running ... prefix=anli_32_21, lr=0.2, bsz=8 ...
05/24/2022 09:36:52 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:36:52 - INFO - __main__ - Printing 3 examples
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:36:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:36:52 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:36:52 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:36:52 - INFO - __main__ - Printing 3 examples
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: Gawsworth is a civil parish and village in the unitary authority of Cheshire East and the ceremonial county of Cheshire, England. The population of the civil parish as of the 2011 census was 1,705. It is one of the eight ancient parishes of Macclesfield Hundred. Twenty acres of the civil parish were transferred to Macclesfield civil parish in 1936 [SEP] hypothesis: Gawsworth is a civil parish and village in England.
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: Hunan University of Science and Technology () is an institution of higher learning in Xiangtan, Hunan Province of the People's Republic of China. It is under the jointly jurisdiction of central government and provincial government, and is mainly administered by Hunan Province. Hunan University of Science and Technology was formed by the merger of two earlier universities. [SEP] hypothesis: The Hunan University was formed by the merger of two universities in Xiangtan.
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ -  [anli] premise: Jean-Baptiste Poquelin, known by his stage name Molière ( or ; ] ; 15 January 162217 February 1673), was a French playwright and actor who is considered to be one of the greatest masters of comedy in Western literature. Among Molière's best known works are "The Misanthrope", "The School for Wives", "Tartuffe", "The Miser", "The Imaginary Invalid", and "The Bourgeois Gentleman". [SEP] hypothesis: Tartuffe was a work by Molière.
05/24/2022 09:36:52 - INFO - __main__ - ['entailment']
05/24/2022 09:36:52 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:36:52 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:36:52 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:37:11 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:37:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:37:11 - INFO - __main__ - Starting training!
05/24/2022 09:37:15 - INFO - __main__ - Step 10 Global step 10 Train loss 0.52 on epoch=1
05/24/2022 09:37:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=3
05/24/2022 09:37:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=4
05/24/2022 09:37:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=6
05/24/2022 09:37:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=8
05/24/2022 09:37:28 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 09:37:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 09:37:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=9
05/24/2022 09:37:34 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=11
05/24/2022 09:37:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=13
05/24/2022 09:37:39 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=14
05/24/2022 09:37:42 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=16
05/24/2022 09:37:44 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 09:37:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=18
05/24/2022 09:37:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=19
05/24/2022 09:37:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=21
05/24/2022 09:37:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=23
05/24/2022 09:37:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=24
05/24/2022 09:38:00 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 09:38:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=26
05/24/2022 09:38:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=28
05/24/2022 09:38:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=29
05/24/2022 09:38:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 09:38:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=33
05/24/2022 09:38:16 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 09:38:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=34
05/24/2022 09:38:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=36
05/24/2022 09:38:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=38
05/24/2022 09:38:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
05/24/2022 09:38:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=41
05/24/2022 09:38:32 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 09:38:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=43
05/24/2022 09:38:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/24/2022 09:38:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=46
05/24/2022 09:38:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=48
05/24/2022 09:38:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=49
05/24/2022 09:38:48 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 09:38:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=51
05/24/2022 09:38:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
05/24/2022 09:38:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=54
05/24/2022 09:38:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=56
05/24/2022 09:39:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=58
05/24/2022 09:39:04 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
05/24/2022 09:39:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=59
05/24/2022 09:39:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/24/2022 09:39:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=63
05/24/2022 09:39:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=64
05/24/2022 09:39:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=66
05/24/2022 09:39:20 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.18892001244942422 on epoch=66
05/24/2022 09:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18892001244942422 on epoch=66, global_step=400
05/24/2022 09:39:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/24/2022 09:39:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=69
05/24/2022 09:39:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=71
05/24/2022 09:39:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=73
05/24/2022 09:39:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=74
05/24/2022 09:39:36 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.18892001244942422 on epoch=74
05/24/2022 09:39:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=76
05/24/2022 09:39:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=78
05/24/2022 09:39:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
05/24/2022 09:39:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
05/24/2022 09:39:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=83
05/24/2022 09:39:52 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.1881810228266921 on epoch=83
05/24/2022 09:39:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/24/2022 09:39:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=86
05/24/2022 09:40:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
05/24/2022 09:40:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=89
05/24/2022 09:40:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=91
05/24/2022 09:40:08 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.1881810228266921 on epoch=91
05/24/2022 09:40:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=93
05/24/2022 09:40:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=94
05/24/2022 09:40:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=96
05/24/2022 09:40:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/24/2022 09:40:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=99
05/24/2022 09:40:24 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.18971428571428572 on epoch=99
05/24/2022 09:40:24 - INFO - __main__ - Saving model with best Classification-F1: 0.18892001244942422 -> 0.18971428571428572 on epoch=99, global_step=600
05/24/2022 09:40:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=101
05/24/2022 09:40:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=103
05/24/2022 09:40:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=104
05/24/2022 09:40:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=106
05/24/2022 09:40:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=108
05/24/2022 09:40:40 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.2987912556326878 on epoch=108
05/24/2022 09:40:40 - INFO - __main__ - Saving model with best Classification-F1: 0.18971428571428572 -> 0.2987912556326878 on epoch=108, global_step=650
05/24/2022 09:40:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=109
05/24/2022 09:40:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=111
05/24/2022 09:40:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=113
05/24/2022 09:40:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=114
05/24/2022 09:40:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=116
05/24/2022 09:40:56 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.28102359587902953 on epoch=116
05/24/2022 09:40:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=118
05/24/2022 09:41:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
05/24/2022 09:41:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=121
05/24/2022 09:41:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=123
05/24/2022 09:41:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=124
05/24/2022 09:41:12 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.2975569706435981 on epoch=124
05/24/2022 09:41:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=126
05/24/2022 09:41:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=128
05/24/2022 09:41:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=129
05/24/2022 09:41:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=131
05/24/2022 09:41:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=133
05/24/2022 09:41:28 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.2814000814000814 on epoch=133
05/24/2022 09:41:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=134
05/24/2022 09:41:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=136
05/24/2022 09:41:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=138
05/24/2022 09:41:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=139
05/24/2022 09:41:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=141
05/24/2022 09:41:44 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.2829422829422829 on epoch=141
05/24/2022 09:41:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=143
05/24/2022 09:41:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=144
05/24/2022 09:41:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=146
05/24/2022 09:41:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=148
05/24/2022 09:41:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=149
05/24/2022 09:42:00 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.2827378579590969 on epoch=149
05/24/2022 09:42:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=151
05/24/2022 09:42:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=153
05/24/2022 09:42:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=154
05/24/2022 09:42:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=156
05/24/2022 09:42:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=158
05/24/2022 09:42:16 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.26524316501120165 on epoch=158
05/24/2022 09:42:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=159
05/24/2022 09:42:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=161
05/24/2022 09:42:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=163
05/24/2022 09:42:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=164
05/24/2022 09:42:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=166
05/24/2022 09:42:32 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.2665610892675113 on epoch=166
05/24/2022 09:42:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=168
05/24/2022 09:42:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=169
05/24/2022 09:42:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=171
05/24/2022 09:42:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=173
05/24/2022 09:42:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=174
05/24/2022 09:42:48 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.33154162000746545 on epoch=174
05/24/2022 09:42:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2987912556326878 -> 0.33154162000746545 on epoch=174, global_step=1050
05/24/2022 09:42:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=176
05/24/2022 09:42:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=178
05/24/2022 09:42:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=179
05/24/2022 09:42:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=181
05/24/2022 09:43:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=183
05/24/2022 09:43:04 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.32078501338090987 on epoch=183
05/24/2022 09:43:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=184
05/24/2022 09:43:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=186
05/24/2022 09:43:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=188
05/24/2022 09:43:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=189
05/24/2022 09:43:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=191
05/24/2022 09:43:20 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.29820788530465947 on epoch=191
05/24/2022 09:43:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=193
05/24/2022 09:43:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=194
05/24/2022 09:43:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=196
05/24/2022 09:43:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=198
05/24/2022 09:43:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=199
05/24/2022 09:43:36 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3189814814814815 on epoch=199
05/24/2022 09:43:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=201
05/24/2022 09:43:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=203
05/24/2022 09:43:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=204
05/24/2022 09:43:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=206
05/24/2022 09:43:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=208
05/24/2022 09:43:52 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.31546801316168555 on epoch=208
05/24/2022 09:43:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=209
05/24/2022 09:43:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=211
05/24/2022 09:44:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=213
05/24/2022 09:44:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=214
05/24/2022 09:44:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=216
05/24/2022 09:44:08 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.27695906432748535 on epoch=216
05/24/2022 09:44:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=218
05/24/2022 09:44:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=219
05/24/2022 09:44:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=221
05/24/2022 09:44:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=223
05/24/2022 09:44:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=224
05/24/2022 09:44:24 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.32675020885547207 on epoch=224
05/24/2022 09:44:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=226
05/24/2022 09:44:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=228
05/24/2022 09:44:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=229
05/24/2022 09:44:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=231
05/24/2022 09:44:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=233
05/24/2022 09:44:39 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.29247259041073476 on epoch=233
05/24/2022 09:44:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=234
05/24/2022 09:44:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=236
05/24/2022 09:44:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=238
05/24/2022 09:44:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=239
05/24/2022 09:44:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=241
05/24/2022 09:44:55 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.2791759401928893 on epoch=241
05/24/2022 09:44:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=243
05/24/2022 09:45:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=244
05/24/2022 09:45:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=246
05/24/2022 09:45:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=248
05/24/2022 09:45:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=249
05/24/2022 09:45:11 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.307702589807853 on epoch=249
05/24/2022 09:45:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=251
05/24/2022 09:45:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=253
05/24/2022 09:45:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=254
05/24/2022 09:45:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=256
05/24/2022 09:45:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=258
05/24/2022 09:45:27 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.31435087074184825 on epoch=258
05/24/2022 09:45:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=259
05/24/2022 09:45:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=261
05/24/2022 09:45:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=263
05/24/2022 09:45:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=264
05/24/2022 09:45:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=266
05/24/2022 09:45:43 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.26575476306790424 on epoch=266
05/24/2022 09:45:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=268
05/24/2022 09:45:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=269
05/24/2022 09:45:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=271
05/24/2022 09:45:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.29 on epoch=273
05/24/2022 09:45:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=274
05/24/2022 09:45:58 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.307702589807853 on epoch=274
05/24/2022 09:46:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=276
05/24/2022 09:46:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=278
05/24/2022 09:46:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=279
05/24/2022 09:46:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=281
05/24/2022 09:46:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=283
05/24/2022 09:46:14 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.31327931554765387 on epoch=283
05/24/2022 09:46:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=284
05/24/2022 09:46:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.31 on epoch=286
05/24/2022 09:46:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=288
05/24/2022 09:46:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=289
05/24/2022 09:46:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=291
05/24/2022 09:46:30 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.29204397606036947 on epoch=291
05/24/2022 09:46:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=293
05/24/2022 09:46:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=294
05/24/2022 09:46:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=296
05/24/2022 09:46:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=298
05/24/2022 09:46:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=299
05/24/2022 09:46:45 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.29066519080750575 on epoch=299
05/24/2022 09:46:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=301
05/24/2022 09:46:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=303
05/24/2022 09:46:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=304
05/24/2022 09:46:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=306
05/24/2022 09:46:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=308
05/24/2022 09:47:01 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.23461936628831298 on epoch=308
05/24/2022 09:47:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=309
05/24/2022 09:47:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=311
05/24/2022 09:47:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=313
05/24/2022 09:47:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=314
05/24/2022 09:47:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=316
05/24/2022 09:47:16 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.22926167878713177 on epoch=316
05/24/2022 09:47:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=318
05/24/2022 09:47:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=319
05/24/2022 09:47:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=321
05/24/2022 09:47:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.30 on epoch=323
05/24/2022 09:47:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=324
05/24/2022 09:47:32 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.2291716081593928 on epoch=324
05/24/2022 09:47:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=326
05/24/2022 09:47:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=328
05/24/2022 09:47:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=329
05/24/2022 09:47:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=331
05/24/2022 09:47:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=333
05/24/2022 09:47:48 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.19660390369107592 on epoch=333
05/24/2022 09:47:50 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=334
05/24/2022 09:47:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=336
05/24/2022 09:47:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=338
05/24/2022 09:47:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=339
05/24/2022 09:48:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=341
05/24/2022 09:48:03 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.24750375375375377 on epoch=341
05/24/2022 09:48:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=343
05/24/2022 09:48:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=344
05/24/2022 09:48:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=346
05/24/2022 09:48:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=348
05/24/2022 09:48:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=349
05/24/2022 09:48:19 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.2081453634085213 on epoch=349
05/24/2022 09:48:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=351
05/24/2022 09:48:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=353
05/24/2022 09:48:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=354
05/24/2022 09:48:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=356
05/24/2022 09:48:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=358
05/24/2022 09:48:35 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.21696178937558247 on epoch=358
05/24/2022 09:48:38 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=359
05/24/2022 09:48:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=361
05/24/2022 09:48:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.23 on epoch=363
05/24/2022 09:48:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=364
05/24/2022 09:48:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=366
05/24/2022 09:48:51 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.21588525912482354 on epoch=366
05/24/2022 09:48:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=368
05/24/2022 09:48:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.24 on epoch=369
05/24/2022 09:48:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.24 on epoch=371
05/24/2022 09:49:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=373
05/24/2022 09:49:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=374
05/24/2022 09:49:06 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.204763597582652 on epoch=374
05/24/2022 09:49:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=376
05/24/2022 09:49:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=378
05/24/2022 09:49:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=379
05/24/2022 09:49:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=381
05/24/2022 09:49:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=383
05/24/2022 09:49:22 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.21953612845673504 on epoch=383
05/24/2022 09:49:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=384
05/24/2022 09:49:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=386
05/24/2022 09:49:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=388
05/24/2022 09:49:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=389
05/24/2022 09:49:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=391
05/24/2022 09:49:37 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.213199558173785 on epoch=391
05/24/2022 09:49:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=393
05/24/2022 09:49:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=394
05/24/2022 09:49:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=396
05/24/2022 09:49:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=398
05/24/2022 09:49:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=399
05/24/2022 09:49:52 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.2047574123989218 on epoch=399
05/24/2022 09:49:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=401
05/24/2022 09:49:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=403
05/24/2022 09:50:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=404
05/24/2022 09:50:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=406
05/24/2022 09:50:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=408
05/24/2022 09:50:08 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.16296777546777547 on epoch=408
05/24/2022 09:50:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=409
05/24/2022 09:50:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=411
05/24/2022 09:50:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=413
05/24/2022 09:50:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=414
05/24/2022 09:50:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=416
05/24/2022 09:50:24 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.1417854463615904 on epoch=416
05/24/2022 09:50:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=418
05/24/2022 09:50:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=419
05/24/2022 09:50:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=421
05/24/2022 09:50:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=423
05/24/2022 09:50:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=424
05/24/2022 09:50:40 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.18176898068464334 on epoch=424
05/24/2022 09:50:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=426
05/24/2022 09:50:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=428
05/24/2022 09:50:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=429
05/24/2022 09:50:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=431
05/24/2022 09:50:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=433
05/24/2022 09:50:55 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.1598738104001262 on epoch=433
05/24/2022 09:50:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=434
05/24/2022 09:51:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=436
05/24/2022 09:51:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=438
05/24/2022 09:51:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=439
05/24/2022 09:51:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=441
05/24/2022 09:51:11 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.16473856209150328 on epoch=441
05/24/2022 09:51:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=443
05/24/2022 09:51:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=444
05/24/2022 09:51:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=446
05/24/2022 09:51:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=448
05/24/2022 09:51:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=449
05/24/2022 09:51:27 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.13663009699595066 on epoch=449
05/24/2022 09:51:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=451
05/24/2022 09:51:32 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=453
05/24/2022 09:51:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=454
05/24/2022 09:51:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=456
05/24/2022 09:51:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=458
05/24/2022 09:51:42 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.13783825993128318 on epoch=458
05/24/2022 09:51:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=459
05/24/2022 09:51:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=461
05/24/2022 09:51:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=463
05/24/2022 09:51:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=464
05/24/2022 09:51:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=466
05/24/2022 09:51:58 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.12931371087399457 on epoch=466
05/24/2022 09:52:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=468
05/24/2022 09:52:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=469
05/24/2022 09:52:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=471
05/24/2022 09:52:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=473
05/24/2022 09:52:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=474
05/24/2022 09:52:13 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.10542193067703272 on epoch=474
05/24/2022 09:52:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=476
05/24/2022 09:52:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=478
05/24/2022 09:52:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=479
05/24/2022 09:52:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=481
05/24/2022 09:52:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=483
05/24/2022 09:52:29 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.12400144413210969 on epoch=483
05/24/2022 09:52:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=484
05/24/2022 09:52:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=486
05/24/2022 09:52:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
05/24/2022 09:52:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=489
05/24/2022 09:52:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.17 on epoch=491
05/24/2022 09:52:44 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.11637569246264898 on epoch=491
05/24/2022 09:52:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=493
05/24/2022 09:52:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=494
05/24/2022 09:52:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=496
05/24/2022 09:52:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=498
05/24/2022 09:52:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=499
05/24/2022 09:52:59 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:52:59 - INFO - __main__ - Printing 3 examples
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:52:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:52:59 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:52:59 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:52:59 - INFO - __main__ - Printing 3 examples
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 09:52:59 - INFO - __main__ - ['neutral']
05/24/2022 09:52:59 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:52:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:52:59 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:53:00 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.11810057356275842 on epoch=499
05/24/2022 09:53:00 - INFO - __main__ - save last model!
05/24/2022 09:53:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 09:53:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 09:53:00 - INFO - __main__ - Printing 3 examples
05/24/2022 09:53:00 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 09:53:00 - INFO - __main__ - ['contradiction']
05/24/2022 09:53:00 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 09:53:00 - INFO - __main__ - ['entailment']
05/24/2022 09:53:00 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 09:53:00 - INFO - __main__ - ['contradiction']
05/24/2022 09:53:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:53:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:53:02 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 09:53:15 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:53:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:53:16 - INFO - __main__ - Starting training!
05/24/2022 09:53:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_21_0.2_8_predictions.txt
05/24/2022 09:53:29 - INFO - __main__ - Classification-F1 on test data: 0.0553
05/24/2022 09:53:29 - INFO - __main__ - prefix=anli_32_21, lr=0.2, bsz=8, dev_performance=0.33154162000746545, test_performance=0.05526959500228954
05/24/2022 09:53:29 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.5, bsz=8 ...
05/24/2022 09:53:30 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:53:30 - INFO - __main__ - Printing 3 examples
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:53:30 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:53:30 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 09:53:30 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 09:53:30 - INFO - __main__ - Printing 3 examples
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 09:53:30 - INFO - __main__ - ['neutral']
05/24/2022 09:53:30 - INFO - __main__ - Tokenizing Input ...
05/24/2022 09:53:30 - INFO - __main__ - Tokenizing Output ...
05/24/2022 09:53:30 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 09:53:49 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 09:53:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 09:53:50 - INFO - __main__ - Starting training!
05/24/2022 09:53:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=1
05/24/2022 09:53:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=3
05/24/2022 09:53:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=4
05/24/2022 09:54:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=6
05/24/2022 09:54:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=8
05/24/2022 09:54:07 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 09:54:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 09:54:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
05/24/2022 09:54:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=11
05/24/2022 09:54:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=13
05/24/2022 09:54:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=14
05/24/2022 09:54:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=16
05/24/2022 09:54:23 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 09:54:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=18
05/24/2022 09:54:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=19
05/24/2022 09:54:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=21
05/24/2022 09:54:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=23
05/24/2022 09:54:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=24
05/24/2022 09:54:39 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 09:54:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=26
05/24/2022 09:54:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
05/24/2022 09:54:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=29
05/24/2022 09:54:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=31
05/24/2022 09:54:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
05/24/2022 09:54:55 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 09:54:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
05/24/2022 09:55:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=36
05/24/2022 09:55:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=38
05/24/2022 09:55:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=39
05/24/2022 09:55:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=41
05/24/2022 09:55:11 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 09:55:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=43
05/24/2022 09:55:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=44
05/24/2022 09:55:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=46
05/24/2022 09:55:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=48
05/24/2022 09:55:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=49
05/24/2022 09:55:28 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.3120498016574369 on epoch=49
05/24/2022 09:55:28 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.3120498016574369 on epoch=49, global_step=300
05/24/2022 09:55:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=51
05/24/2022 09:55:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=53
05/24/2022 09:55:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=54
05/24/2022 09:55:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=56
05/24/2022 09:55:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/24/2022 09:55:44 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.35310362141467394 on epoch=58
05/24/2022 09:55:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3120498016574369 -> 0.35310362141467394 on epoch=58, global_step=350
05/24/2022 09:55:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=59
05/24/2022 09:55:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
05/24/2022 09:55:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=63
05/24/2022 09:55:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=64
05/24/2022 09:55:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=66
05/24/2022 09:56:00 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.2085278555866791 on epoch=66
05/24/2022 09:56:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/24/2022 09:56:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=69
05/24/2022 09:56:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
05/24/2022 09:56:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=73
05/24/2022 09:56:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/24/2022 09:56:16 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.35104492247349395 on epoch=74
05/24/2022 09:56:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=76
05/24/2022 09:56:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/24/2022 09:56:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=79
05/24/2022 09:56:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=81
05/24/2022 09:56:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=83
05/24/2022 09:56:32 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.3849960276926569 on epoch=83
05/24/2022 09:56:33 - INFO - __main__ - Saving model with best Classification-F1: 0.35310362141467394 -> 0.3849960276926569 on epoch=83, global_step=500
05/24/2022 09:56:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=84
05/24/2022 09:56:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=86
05/24/2022 09:56:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=88
05/24/2022 09:56:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=89
05/24/2022 09:56:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=91
05/24/2022 09:56:49 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.35906432748538014 on epoch=91
05/24/2022 09:56:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=93
05/24/2022 09:56:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
05/24/2022 09:56:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=96
05/24/2022 09:56:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=98
05/24/2022 09:57:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=99
05/24/2022 09:57:05 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.46103071845646104 on epoch=99
05/24/2022 09:57:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3849960276926569 -> 0.46103071845646104 on epoch=99, global_step=600
05/24/2022 09:57:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=101
05/24/2022 09:57:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=103
05/24/2022 09:57:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=104
05/24/2022 09:57:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=106
05/24/2022 09:57:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=108
05/24/2022 09:57:21 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.45932560415319035 on epoch=108
05/24/2022 09:57:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=109
05/24/2022 09:57:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=111
05/24/2022 09:57:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=113
05/24/2022 09:57:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=114
05/24/2022 09:57:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=116
05/24/2022 09:57:37 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.47087672278213616 on epoch=116
05/24/2022 09:57:37 - INFO - __main__ - Saving model with best Classification-F1: 0.46103071845646104 -> 0.47087672278213616 on epoch=116, global_step=700
05/24/2022 09:57:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=118
05/24/2022 09:57:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=119
05/24/2022 09:57:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=121
05/24/2022 09:57:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=123
05/24/2022 09:57:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=124
05/24/2022 09:57:53 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.430562714970865 on epoch=124
05/24/2022 09:57:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=126
05/24/2022 09:57:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=128
05/24/2022 09:58:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=129
05/24/2022 09:58:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=131
05/24/2022 09:58:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=133
05/24/2022 09:58:09 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.4132153728087194 on epoch=133
05/24/2022 09:58:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=134
05/24/2022 09:58:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=136
05/24/2022 09:58:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=138
05/24/2022 09:58:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=139
05/24/2022 09:58:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=141
05/24/2022 09:58:26 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.4798879551820729 on epoch=141
05/24/2022 09:58:26 - INFO - __main__ - Saving model with best Classification-F1: 0.47087672278213616 -> 0.4798879551820729 on epoch=141, global_step=850
05/24/2022 09:58:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=143
05/24/2022 09:58:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=144
05/24/2022 09:58:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=146
05/24/2022 09:58:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=148
05/24/2022 09:58:39 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=149
05/24/2022 09:58:42 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.48318870578244827 on epoch=149
05/24/2022 09:58:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4798879551820729 -> 0.48318870578244827 on epoch=149, global_step=900
05/24/2022 09:58:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=151
05/24/2022 09:58:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=153
05/24/2022 09:58:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=154
05/24/2022 09:58:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=156
05/24/2022 09:58:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=158
05/24/2022 09:58:58 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.4230769230769231 on epoch=158
05/24/2022 09:59:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=159
05/24/2022 09:59:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=161
05/24/2022 09:59:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=163
05/24/2022 09:59:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=164
05/24/2022 09:59:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=166
05/24/2022 09:59:14 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.4383976564551119 on epoch=166
05/24/2022 09:59:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=168
05/24/2022 09:59:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=169
05/24/2022 09:59:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=171
05/24/2022 09:59:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=173
05/24/2022 09:59:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=174
05/24/2022 09:59:31 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.4241038892201683 on epoch=174
05/24/2022 09:59:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=176
05/24/2022 09:59:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=178
05/24/2022 09:59:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=179
05/24/2022 09:59:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=181
05/24/2022 09:59:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=183
05/24/2022 09:59:47 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.3795144844673146 on epoch=183
05/24/2022 09:59:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=184
05/24/2022 09:59:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=186
05/24/2022 09:59:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=188
05/24/2022 09:59:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=189
05/24/2022 10:00:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=191
05/24/2022 10:00:03 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.3481280337391581 on epoch=191
05/24/2022 10:00:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=193
05/24/2022 10:00:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=194
05/24/2022 10:00:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=196
05/24/2022 10:00:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=198
05/24/2022 10:00:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=199
05/24/2022 10:00:19 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.3337068160597572 on epoch=199
05/24/2022 10:00:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=201
05/24/2022 10:00:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=203
05/24/2022 10:00:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=204
05/24/2022 10:00:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=206
05/24/2022 10:00:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=208
05/24/2022 10:00:35 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.3351282051282051 on epoch=208
05/24/2022 10:00:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.12 on epoch=209
05/24/2022 10:00:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=211
05/24/2022 10:00:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=213
05/24/2022 10:00:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=214
05/24/2022 10:00:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=216
05/24/2022 10:00:52 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.25393740031897927 on epoch=216
05/24/2022 10:00:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=218
05/24/2022 10:00:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=219
05/24/2022 10:01:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=221
05/24/2022 10:01:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=223
05/24/2022 10:01:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=224
05/24/2022 10:01:08 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.3037894030851777 on epoch=224
05/24/2022 10:01:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=226
05/24/2022 10:01:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=228
05/24/2022 10:01:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=229
05/24/2022 10:01:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=231
05/24/2022 10:01:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=233
05/24/2022 10:01:24 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.2747983870967742 on epoch=233
05/24/2022 10:01:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=234
05/24/2022 10:01:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=236
05/24/2022 10:01:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=238
05/24/2022 10:01:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=239
05/24/2022 10:01:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=241
05/24/2022 10:01:40 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.2768703989042972 on epoch=241
05/24/2022 10:01:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=243
05/24/2022 10:01:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=244
05/24/2022 10:01:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=246
05/24/2022 10:01:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=248
05/24/2022 10:01:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=249
05/24/2022 10:01:57 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.29364949855515887 on epoch=249
05/24/2022 10:01:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=251
05/24/2022 10:02:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=253
05/24/2022 10:02:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=254
05/24/2022 10:02:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=256
05/24/2022 10:02:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=258
05/24/2022 10:02:13 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.24465837256810194 on epoch=258
05/24/2022 10:02:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=259
05/24/2022 10:02:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=261
05/24/2022 10:02:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=263
05/24/2022 10:02:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=264
05/24/2022 10:02:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=266
05/24/2022 10:02:29 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.23092221056715898 on epoch=266
05/24/2022 10:02:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=268
05/24/2022 10:02:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=269
05/24/2022 10:02:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=271
05/24/2022 10:02:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=273
05/24/2022 10:02:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=274
05/24/2022 10:02:46 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.180302497549333 on epoch=274
05/24/2022 10:02:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=276
05/24/2022 10:02:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=278
05/24/2022 10:02:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=279
05/24/2022 10:02:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=281
05/24/2022 10:02:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=283
05/24/2022 10:03:02 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.1802586000955566 on epoch=283
05/24/2022 10:03:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=284
05/24/2022 10:03:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=286
05/24/2022 10:03:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=288
05/24/2022 10:03:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=289
05/24/2022 10:03:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=291
05/24/2022 10:03:18 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.16594516594516595 on epoch=291
05/24/2022 10:03:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=293
05/24/2022 10:03:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=294
05/24/2022 10:03:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=296
05/24/2022 10:03:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=298
05/24/2022 10:03:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=299
05/24/2022 10:03:35 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.19839650145772594 on epoch=299
05/24/2022 10:03:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=301
05/24/2022 10:03:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=303
05/24/2022 10:03:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=304
05/24/2022 10:03:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=306
05/24/2022 10:03:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=308
05/24/2022 10:03:51 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.17056737588652482 on epoch=308
05/24/2022 10:03:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=309
05/24/2022 10:03:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=311
05/24/2022 10:03:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=313
05/24/2022 10:04:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=314
05/24/2022 10:04:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=316
05/24/2022 10:04:07 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.19258976317799847 on epoch=316
05/24/2022 10:04:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=318
05/24/2022 10:04:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=319
05/24/2022 10:04:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=321
05/24/2022 10:04:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=323
05/24/2022 10:04:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=324
05/24/2022 10:04:23 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.15227272727272728 on epoch=324
05/24/2022 10:04:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=326
05/24/2022 10:04:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=328
05/24/2022 10:04:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=329
05/24/2022 10:04:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=331
05/24/2022 10:04:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=333
05/24/2022 10:04:39 - INFO - __main__ - Global step 2000 Train loss 0.05 Classification-F1 0.12902201864352297 on epoch=333
05/24/2022 10:04:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=334
05/24/2022 10:04:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=336
05/24/2022 10:04:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=338
05/24/2022 10:04:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=339
05/24/2022 10:04:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=341
05/24/2022 10:04:56 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.14987967229902716 on epoch=341
05/24/2022 10:04:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=343
05/24/2022 10:05:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=344
05/24/2022 10:05:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=346
05/24/2022 10:05:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=348
05/24/2022 10:05:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=349
05/24/2022 10:05:12 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.16522768670309654 on epoch=349
05/24/2022 10:05:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=351
05/24/2022 10:05:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=353
05/24/2022 10:05:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=354
05/24/2022 10:05:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=356
05/24/2022 10:05:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=358
05/24/2022 10:05:28 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.1707766439909297 on epoch=358
05/24/2022 10:05:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.02 on epoch=359
05/24/2022 10:05:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=361
05/24/2022 10:05:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=363
05/24/2022 10:05:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=364
05/24/2022 10:05:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=366
05/24/2022 10:05:44 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.1380984643179765 on epoch=366
05/24/2022 10:05:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=368
05/24/2022 10:05:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=369
05/24/2022 10:05:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=371
05/24/2022 10:05:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=373
05/24/2022 10:05:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=374
05/24/2022 10:06:00 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.1309439914091077 on epoch=374
05/24/2022 10:06:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=376
05/24/2022 10:06:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=378
05/24/2022 10:06:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=379
05/24/2022 10:06:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=381
05/24/2022 10:06:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=383
05/24/2022 10:06:16 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.14690342516429475 on epoch=383
05/24/2022 10:06:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=384
05/24/2022 10:06:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=386
05/24/2022 10:06:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=388
05/24/2022 10:06:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=389
05/24/2022 10:06:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=391
05/24/2022 10:06:32 - INFO - __main__ - Global step 2350 Train loss 0.03 Classification-F1 0.15672950238167632 on epoch=391
05/24/2022 10:06:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=393
05/24/2022 10:06:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=394
05/24/2022 10:06:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=396
05/24/2022 10:06:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=398
05/24/2022 10:06:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=399
05/24/2022 10:06:49 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.19655403144864506 on epoch=399
05/24/2022 10:06:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=401
05/24/2022 10:06:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=403
05/24/2022 10:06:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=404
05/24/2022 10:06:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=406
05/24/2022 10:07:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=408
05/24/2022 10:07:05 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.20023163111398404 on epoch=408
05/24/2022 10:07:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=409
05/24/2022 10:07:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=411
05/24/2022 10:07:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=413
05/24/2022 10:07:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=414
05/24/2022 10:07:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=416
05/24/2022 10:07:21 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.18835134951059562 on epoch=416
05/24/2022 10:07:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=418
05/24/2022 10:07:26 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=419
05/24/2022 10:07:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
05/24/2022 10:07:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=423
05/24/2022 10:07:34 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=424
05/24/2022 10:07:37 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.18307566284049315 on epoch=424
05/24/2022 10:07:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=426
05/24/2022 10:07:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=428
05/24/2022 10:07:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=429
05/24/2022 10:07:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=431
05/24/2022 10:07:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=433
05/24/2022 10:07:54 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.22771577858162942 on epoch=433
05/24/2022 10:07:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=434
05/24/2022 10:07:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 10:08:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=438
05/24/2022 10:08:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=439
05/24/2022 10:08:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=441
05/24/2022 10:08:10 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.15582214215525714 on epoch=441
05/24/2022 10:08:12 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=443
05/24/2022 10:08:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
05/24/2022 10:08:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=446
05/24/2022 10:08:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=448
05/24/2022 10:08:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 10:08:26 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.19846911708980677 on epoch=449
05/24/2022 10:08:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=451
05/24/2022 10:08:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 10:08:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=454
05/24/2022 10:08:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=456
05/24/2022 10:08:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=458
05/24/2022 10:08:42 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.14867724867724869 on epoch=458
05/24/2022 10:08:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=459
05/24/2022 10:08:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
05/24/2022 10:08:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=463
05/24/2022 10:08:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=464
05/24/2022 10:08:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=466
05/24/2022 10:08:59 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.14873563218390803 on epoch=466
05/24/2022 10:09:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=468
05/24/2022 10:09:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 10:09:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=471
05/24/2022 10:09:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=473
05/24/2022 10:09:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=474
05/24/2022 10:09:15 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.20209072296916405 on epoch=474
05/24/2022 10:09:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=476
05/24/2022 10:09:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=478
05/24/2022 10:09:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/24/2022 10:09:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=481
05/24/2022 10:09:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=483
05/24/2022 10:09:31 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.16066263021508803 on epoch=483
05/24/2022 10:09:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=484
05/24/2022 10:09:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 10:09:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
05/24/2022 10:09:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=489
05/24/2022 10:09:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=491
05/24/2022 10:09:48 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.17278112353311764 on epoch=491
05/24/2022 10:09:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=493
05/24/2022 10:09:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=494
05/24/2022 10:09:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=496
05/24/2022 10:09:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=498
05/24/2022 10:10:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=499
05/24/2022 10:10:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:10:02 - INFO - __main__ - Printing 3 examples
05/24/2022 10:10:02 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:10:02 - INFO - __main__ - ['neutral']
05/24/2022 10:10:02 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:10:02 - INFO - __main__ - ['neutral']
05/24/2022 10:10:02 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:10:02 - INFO - __main__ - ['neutral']
05/24/2022 10:10:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:10:02 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:10:03 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:10:03 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:10:03 - INFO - __main__ - Printing 3 examples
05/24/2022 10:10:03 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:10:03 - INFO - __main__ - ['neutral']
05/24/2022 10:10:03 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:10:03 - INFO - __main__ - ['neutral']
05/24/2022 10:10:03 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:10:03 - INFO - __main__ - ['neutral']
05/24/2022 10:10:03 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:10:03 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:10:03 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:10:04 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.19802949664925845 on epoch=499
05/24/2022 10:10:04 - INFO - __main__ - save last model!
05/24/2022 10:10:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 10:10:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 10:10:04 - INFO - __main__ - Printing 3 examples
05/24/2022 10:10:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 10:10:04 - INFO - __main__ - ['contradiction']
05/24/2022 10:10:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 10:10:04 - INFO - __main__ - ['entailment']
05/24/2022 10:10:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 10:10:04 - INFO - __main__ - ['contradiction']
05/24/2022 10:10:04 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:10:05 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:10:06 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 10:10:18 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:10:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:10:19 - INFO - __main__ - Starting training!
05/24/2022 10:10:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_42_0.5_8_predictions.txt
05/24/2022 10:10:35 - INFO - __main__ - Classification-F1 on test data: 0.0347
05/24/2022 10:10:36 - INFO - __main__ - prefix=anli_32_42, lr=0.5, bsz=8, dev_performance=0.48318870578244827, test_performance=0.0347068884838669
05/24/2022 10:10:36 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.4, bsz=8 ...
05/24/2022 10:10:36 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:10:36 - INFO - __main__ - Printing 3 examples
05/24/2022 10:10:36 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:10:36 - INFO - __main__ - ['neutral']
05/24/2022 10:10:36 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:10:36 - INFO - __main__ - ['neutral']
05/24/2022 10:10:36 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:10:36 - INFO - __main__ - ['neutral']
05/24/2022 10:10:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:10:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:10:37 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:10:37 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:10:37 - INFO - __main__ - Printing 3 examples
05/24/2022 10:10:37 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:10:37 - INFO - __main__ - ['neutral']
05/24/2022 10:10:37 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:10:37 - INFO - __main__ - ['neutral']
05/24/2022 10:10:37 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:10:37 - INFO - __main__ - ['neutral']
05/24/2022 10:10:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:10:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:10:37 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:10:55 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:10:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:10:56 - INFO - __main__ - Starting training!
05/24/2022 10:10:59 - INFO - __main__ - Step 10 Global step 10 Train loss 0.59 on epoch=1
05/24/2022 10:11:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=3
05/24/2022 10:11:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=4
05/24/2022 10:11:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=6
05/24/2022 10:11:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=8
05/24/2022 10:11:13 - INFO - __main__ - Global step 50 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 10:11:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 10:11:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=9
05/24/2022 10:11:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=11
05/24/2022 10:11:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=13
05/24/2022 10:11:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=14
05/24/2022 10:11:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=16
05/24/2022 10:11:29 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 10:11:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=18
05/24/2022 10:11:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=19
05/24/2022 10:11:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=21
05/24/2022 10:11:39 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=23
05/24/2022 10:11:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=24
05/24/2022 10:11:45 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 10:11:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/24/2022 10:11:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=28
05/24/2022 10:11:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=29
05/24/2022 10:11:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/24/2022 10:11:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
05/24/2022 10:12:01 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.22891744203219613 on epoch=33
05/24/2022 10:12:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22891744203219613 on epoch=33, global_step=200
05/24/2022 10:12:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=34
05/24/2022 10:12:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=36
05/24/2022 10:12:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=38
05/24/2022 10:12:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=39
05/24/2022 10:12:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=41
05/24/2022 10:12:17 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 10:12:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=43
05/24/2022 10:12:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/24/2022 10:12:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=46
05/24/2022 10:12:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=48
05/24/2022 10:12:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=49
05/24/2022 10:12:33 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.34819624819624817 on epoch=49
05/24/2022 10:12:33 - INFO - __main__ - Saving model with best Classification-F1: 0.22891744203219613 -> 0.34819624819624817 on epoch=49, global_step=300
05/24/2022 10:12:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=51
05/24/2022 10:12:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=53
05/24/2022 10:12:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=54
05/24/2022 10:12:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=56
05/24/2022 10:12:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=58
05/24/2022 10:12:49 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.2860064731722486 on epoch=58
05/24/2022 10:12:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=59
05/24/2022 10:12:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=61
05/24/2022 10:12:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=63
05/24/2022 10:13:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=64
05/24/2022 10:13:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=66
05/24/2022 10:13:05 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.2085278555866791 on epoch=66
05/24/2022 10:13:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=68
05/24/2022 10:13:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=69
05/24/2022 10:13:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=71
05/24/2022 10:13:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=73
05/24/2022 10:13:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/24/2022 10:13:20 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.2888888888888889 on epoch=74
05/24/2022 10:13:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=76
05/24/2022 10:13:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=78
05/24/2022 10:13:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/24/2022 10:13:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=81
05/24/2022 10:13:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=83
05/24/2022 10:13:35 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.29647565240785584 on epoch=83
05/24/2022 10:13:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=84
05/24/2022 10:13:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=86
05/24/2022 10:13:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=88
05/24/2022 10:13:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=89
05/24/2022 10:13:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=91
05/24/2022 10:13:51 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.3111111111111111 on epoch=91
05/24/2022 10:13:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=93
05/24/2022 10:13:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=94
05/24/2022 10:13:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=96
05/24/2022 10:14:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=98
05/24/2022 10:14:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=99
05/24/2022 10:14:06 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.3819432198906718 on epoch=99
05/24/2022 10:14:06 - INFO - __main__ - Saving model with best Classification-F1: 0.34819624819624817 -> 0.3819432198906718 on epoch=99, global_step=600
05/24/2022 10:14:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=101
05/24/2022 10:14:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=103
05/24/2022 10:14:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=104
05/24/2022 10:14:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=106
05/24/2022 10:14:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=108
05/24/2022 10:14:22 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.3734265734265734 on epoch=108
05/24/2022 10:14:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=109
05/24/2022 10:14:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=111
05/24/2022 10:14:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=113
05/24/2022 10:14:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=114
05/24/2022 10:14:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=116
05/24/2022 10:14:38 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.3891330891330891 on epoch=116
05/24/2022 10:14:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3819432198906718 -> 0.3891330891330891 on epoch=116, global_step=700
05/24/2022 10:14:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=118
05/24/2022 10:14:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=119
05/24/2022 10:14:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=121
05/24/2022 10:14:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=123
05/24/2022 10:14:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=124
05/24/2022 10:14:54 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.4046236976638841 on epoch=124
05/24/2022 10:14:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3891330891330891 -> 0.4046236976638841 on epoch=124, global_step=750
05/24/2022 10:14:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=126
05/24/2022 10:14:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=128
05/24/2022 10:15:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=129
05/24/2022 10:15:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=131
05/24/2022 10:15:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=133
05/24/2022 10:15:10 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.3878486537384842 on epoch=133
05/24/2022 10:15:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=134
05/24/2022 10:15:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=136
05/24/2022 10:15:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=138
05/24/2022 10:15:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=139
05/24/2022 10:15:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=141
05/24/2022 10:15:25 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.4033087633087633 on epoch=141
05/24/2022 10:15:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=143
05/24/2022 10:15:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=144
05/24/2022 10:15:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=146
05/24/2022 10:15:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=148
05/24/2022 10:15:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=149
05/24/2022 10:15:41 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.4168081494057725 on epoch=149
05/24/2022 10:15:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4046236976638841 -> 0.4168081494057725 on epoch=149, global_step=900
05/24/2022 10:15:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=151
05/24/2022 10:15:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=153
05/24/2022 10:15:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=154
05/24/2022 10:15:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=156
05/24/2022 10:15:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=158
05/24/2022 10:15:57 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.40267506990155094 on epoch=158
05/24/2022 10:16:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=159
05/24/2022 10:16:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=161
05/24/2022 10:16:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=163
05/24/2022 10:16:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=164
05/24/2022 10:16:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=166
05/24/2022 10:16:13 - INFO - __main__ - Global step 1000 Train loss 0.23 Classification-F1 0.40964264493676256 on epoch=166
05/24/2022 10:16:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=168
05/24/2022 10:16:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=169
05/24/2022 10:16:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=171
05/24/2022 10:16:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=173
05/24/2022 10:16:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=174
05/24/2022 10:16:29 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.47973403231300077 on epoch=174
05/24/2022 10:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4168081494057725 -> 0.47973403231300077 on epoch=174, global_step=1050
05/24/2022 10:16:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=176
05/24/2022 10:16:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=178
05/24/2022 10:16:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=179
05/24/2022 10:16:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=181
05/24/2022 10:16:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=183
05/24/2022 10:16:44 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.4592380041447474 on epoch=183
05/24/2022 10:16:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=184
05/24/2022 10:16:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=186
05/24/2022 10:16:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=188
05/24/2022 10:16:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=189
05/24/2022 10:16:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=191
05/24/2022 10:17:01 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.33006967990792296 on epoch=191
05/24/2022 10:17:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=193
05/24/2022 10:17:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=194
05/24/2022 10:17:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=196
05/24/2022 10:17:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=198
05/24/2022 10:17:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=199
05/24/2022 10:17:17 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.4844579369898132 on epoch=199
05/24/2022 10:17:17 - INFO - __main__ - Saving model with best Classification-F1: 0.47973403231300077 -> 0.4844579369898132 on epoch=199, global_step=1200
05/24/2022 10:17:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=201
05/24/2022 10:17:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=203
05/24/2022 10:17:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=204
05/24/2022 10:17:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=206
05/24/2022 10:17:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=208
05/24/2022 10:17:33 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.48971232093485906 on epoch=208
05/24/2022 10:17:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4844579369898132 -> 0.48971232093485906 on epoch=208, global_step=1250
05/24/2022 10:17:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=209
05/24/2022 10:17:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=211
05/24/2022 10:17:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=213
05/24/2022 10:17:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=214
05/24/2022 10:17:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=216
05/24/2022 10:17:49 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.4922990505182286 on epoch=216
05/24/2022 10:17:49 - INFO - __main__ - Saving model with best Classification-F1: 0.48971232093485906 -> 0.4922990505182286 on epoch=216, global_step=1300
05/24/2022 10:17:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=218
05/24/2022 10:17:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=219
05/24/2022 10:17:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=221
05/24/2022 10:18:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=223
05/24/2022 10:18:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=224
05/24/2022 10:18:05 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.38845551378446114 on epoch=224
05/24/2022 10:18:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=226
05/24/2022 10:18:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=228
05/24/2022 10:18:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=229
05/24/2022 10:18:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=231
05/24/2022 10:18:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=233
05/24/2022 10:18:22 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.3500549555112804 on epoch=233
05/24/2022 10:18:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=234
05/24/2022 10:18:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=236
05/24/2022 10:18:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=238
05/24/2022 10:18:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=239
05/24/2022 10:18:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=241
05/24/2022 10:18:38 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.5418637795996286 on epoch=241
05/24/2022 10:18:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4922990505182286 -> 0.5418637795996286 on epoch=241, global_step=1450
05/24/2022 10:18:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=243
05/24/2022 10:18:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=244
05/24/2022 10:18:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=246
05/24/2022 10:18:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=248
05/24/2022 10:18:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=249
05/24/2022 10:18:54 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.38207413695529147 on epoch=249
05/24/2022 10:18:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=251
05/24/2022 10:18:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=253
05/24/2022 10:19:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=254
05/24/2022 10:19:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=256
05/24/2022 10:19:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=258
05/24/2022 10:19:10 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.17870946713210245 on epoch=258
05/24/2022 10:19:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=259
05/24/2022 10:19:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=261
05/24/2022 10:19:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=263
05/24/2022 10:19:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=264
05/24/2022 10:19:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=266
05/24/2022 10:19:27 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.2129120879120879 on epoch=266
05/24/2022 10:19:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=268
05/24/2022 10:19:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=269
05/24/2022 10:19:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=271
05/24/2022 10:19:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=273
05/24/2022 10:19:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=274
05/24/2022 10:19:43 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.29641173641173635 on epoch=274
05/24/2022 10:19:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=276
05/24/2022 10:19:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=278
05/24/2022 10:19:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=279
05/24/2022 10:19:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=281
05/24/2022 10:19:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=283
05/24/2022 10:19:59 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.20617443034922817 on epoch=283
05/24/2022 10:20:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=284
05/24/2022 10:20:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=286
05/24/2022 10:20:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=288
05/24/2022 10:20:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=289
05/24/2022 10:20:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=291
05/24/2022 10:20:15 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.2037090266466524 on epoch=291
05/24/2022 10:20:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=293
05/24/2022 10:20:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=294
05/24/2022 10:20:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=296
05/24/2022 10:20:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=298
05/24/2022 10:20:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=299
05/24/2022 10:20:31 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.16708126036484244 on epoch=299
05/24/2022 10:20:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=301
05/24/2022 10:20:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=303
05/24/2022 10:20:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=304
05/24/2022 10:20:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=306
05/24/2022 10:20:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=308
05/24/2022 10:20:47 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.16839163041977354 on epoch=308
05/24/2022 10:20:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=309
05/24/2022 10:20:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=311
05/24/2022 10:20:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=313
05/24/2022 10:20:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=314
05/24/2022 10:21:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=316
05/24/2022 10:21:04 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.15778688524590162 on epoch=316
05/24/2022 10:21:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=318
05/24/2022 10:21:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=319
05/24/2022 10:21:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=321
05/24/2022 10:21:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=323
05/24/2022 10:21:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=324
05/24/2022 10:21:20 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.24780818360558432 on epoch=324
05/24/2022 10:21:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=326
05/24/2022 10:21:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=328
05/24/2022 10:21:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=329
05/24/2022 10:21:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=331
05/24/2022 10:21:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=333
05/24/2022 10:21:36 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.23730285885610955 on epoch=333
05/24/2022 10:21:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=334
05/24/2022 10:21:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=336
05/24/2022 10:21:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=338
05/24/2022 10:21:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=339
05/24/2022 10:21:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=341
05/24/2022 10:21:52 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.14837622549019608 on epoch=341
05/24/2022 10:21:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=343
05/24/2022 10:21:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=344
05/24/2022 10:22:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=346
05/24/2022 10:22:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=348
05/24/2022 10:22:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=349
05/24/2022 10:22:09 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.19117063492063494 on epoch=349
05/24/2022 10:22:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=351
05/24/2022 10:22:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=353
05/24/2022 10:22:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=354
05/24/2022 10:22:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=356
05/24/2022 10:22:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=358
05/24/2022 10:22:25 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.1513545551796918 on epoch=358
05/24/2022 10:22:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=359
05/24/2022 10:22:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=361
05/24/2022 10:22:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=363
05/24/2022 10:22:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.03 on epoch=364
05/24/2022 10:22:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=366
05/24/2022 10:22:41 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.2631689547955675 on epoch=366
05/24/2022 10:22:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=368
05/24/2022 10:22:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=369
05/24/2022 10:22:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=371
05/24/2022 10:22:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=373
05/24/2022 10:22:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=374
05/24/2022 10:22:58 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.12651467086344273 on epoch=374
05/24/2022 10:23:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=376
05/24/2022 10:23:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=378
05/24/2022 10:23:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=379
05/24/2022 10:23:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=381
05/24/2022 10:23:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=383
05/24/2022 10:23:14 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.1447107700257484 on epoch=383
05/24/2022 10:23:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=384
05/24/2022 10:23:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=386
05/24/2022 10:23:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=388
05/24/2022 10:23:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=389
05/24/2022 10:23:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=391
05/24/2022 10:23:30 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.15393336821908252 on epoch=391
05/24/2022 10:23:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=393
05/24/2022 10:23:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=394
05/24/2022 10:23:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=396
05/24/2022 10:23:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=398
05/24/2022 10:23:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=399
05/24/2022 10:23:46 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.13937768839729625 on epoch=399
05/24/2022 10:23:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=401
05/24/2022 10:23:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=403
05/24/2022 10:23:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=404
05/24/2022 10:23:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=406
05/24/2022 10:24:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=408
05/24/2022 10:24:03 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.10398114816218264 on epoch=408
05/24/2022 10:24:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=409
05/24/2022 10:24:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=411
05/24/2022 10:24:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=413
05/24/2022 10:24:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
05/24/2022 10:24:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
05/24/2022 10:24:19 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.12323232323232323 on epoch=416
05/24/2022 10:24:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=418
05/24/2022 10:24:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=419
05/24/2022 10:24:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=421
05/24/2022 10:24:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=423
05/24/2022 10:24:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
05/24/2022 10:24:35 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.13965853160601985 on epoch=424
05/24/2022 10:24:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=426
05/24/2022 10:24:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=428
05/24/2022 10:24:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=429
05/24/2022 10:24:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=431
05/24/2022 10:24:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=433
05/24/2022 10:24:52 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.12174707602339181 on epoch=433
05/24/2022 10:24:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=434
05/24/2022 10:24:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=436
05/24/2022 10:25:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=438
05/24/2022 10:25:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=439
05/24/2022 10:25:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=441
05/24/2022 10:25:08 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.1286861063464837 on epoch=441
05/24/2022 10:25:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=443
05/24/2022 10:25:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
05/24/2022 10:25:16 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=446
05/24/2022 10:25:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=448
05/24/2022 10:25:21 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=449
05/24/2022 10:25:24 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.11215481002787513 on epoch=449
05/24/2022 10:25:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=451
05/24/2022 10:25:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 10:25:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=454
05/24/2022 10:25:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/24/2022 10:25:38 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=458
05/24/2022 10:25:41 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.08146381919966825 on epoch=458
05/24/2022 10:25:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=459
05/24/2022 10:25:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
05/24/2022 10:25:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=463
05/24/2022 10:25:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=464
05/24/2022 10:25:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=466
05/24/2022 10:25:57 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.1267056530214425 on epoch=466
05/24/2022 10:25:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=468
05/24/2022 10:26:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 10:26:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=471
05/24/2022 10:26:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=473
05/24/2022 10:26:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/24/2022 10:26:13 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.17203864210906464 on epoch=474
05/24/2022 10:26:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=476
05/24/2022 10:26:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=478
05/24/2022 10:26:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=479
05/24/2022 10:26:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=481
05/24/2022 10:26:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
05/24/2022 10:26:29 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.13561551292824792 on epoch=483
05/24/2022 10:26:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/24/2022 10:26:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 10:26:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=488
05/24/2022 10:26:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 10:26:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=491
05/24/2022 10:26:46 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.10334076969649109 on epoch=491
05/24/2022 10:26:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=493
05/24/2022 10:26:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 10:26:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=496
05/24/2022 10:26:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=498
05/24/2022 10:26:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=499
05/24/2022 10:27:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:27:01 - INFO - __main__ - Printing 3 examples
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:27:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:27:01 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:27:01 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:27:01 - INFO - __main__ - Printing 3 examples
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:27:01 - INFO - __main__ - ['neutral']
05/24/2022 10:27:01 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:27:01 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:27:01 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:27:02 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.16291338847729825 on epoch=499
05/24/2022 10:27:02 - INFO - __main__ - save last model!
05/24/2022 10:27:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 10:27:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 10:27:02 - INFO - __main__ - Printing 3 examples
05/24/2022 10:27:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 10:27:02 - INFO - __main__ - ['contradiction']
05/24/2022 10:27:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 10:27:02 - INFO - __main__ - ['entailment']
05/24/2022 10:27:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 10:27:02 - INFO - __main__ - ['contradiction']
05/24/2022 10:27:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:27:03 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:27:04 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 10:27:16 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:27:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:27:17 - INFO - __main__ - Starting training!
05/24/2022 10:27:34 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_42_0.4_8_predictions.txt
05/24/2022 10:27:34 - INFO - __main__ - Classification-F1 on test data: 0.0270
05/24/2022 10:27:34 - INFO - __main__ - prefix=anli_32_42, lr=0.4, bsz=8, dev_performance=0.5418637795996286, test_performance=0.026993966993966993
05/24/2022 10:27:34 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.3, bsz=8 ...
05/24/2022 10:27:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:27:35 - INFO - __main__ - Printing 3 examples
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:27:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:27:35 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:27:35 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:27:35 - INFO - __main__ - Printing 3 examples
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:27:35 - INFO - __main__ - ['neutral']
05/24/2022 10:27:35 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:27:35 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:27:35 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:27:54 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:27:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:27:54 - INFO - __main__ - Starting training!
05/24/2022 10:27:58 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=1
05/24/2022 10:28:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=3
05/24/2022 10:28:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=4
05/24/2022 10:28:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=6
05/24/2022 10:28:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=8
05/24/2022 10:28:11 - INFO - __main__ - Global step 50 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 10:28:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 10:28:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
05/24/2022 10:28:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=11
05/24/2022 10:28:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=13
05/24/2022 10:28:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=14
05/24/2022 10:28:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=16
05/24/2022 10:28:27 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 10:28:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
05/24/2022 10:28:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=19
05/24/2022 10:28:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=21
05/24/2022 10:28:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=23
05/24/2022 10:28:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=24
05/24/2022 10:28:44 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 10:28:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=26
05/24/2022 10:28:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=28
05/24/2022 10:28:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=29
05/24/2022 10:28:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 10:28:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=33
05/24/2022 10:29:00 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 10:29:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=34
05/24/2022 10:29:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=36
05/24/2022 10:29:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
05/24/2022 10:29:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=39
05/24/2022 10:29:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=41
05/24/2022 10:29:16 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 10:29:19 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=43
05/24/2022 10:29:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=44
05/24/2022 10:29:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=46
05/24/2022 10:29:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.60 on epoch=48
05/24/2022 10:29:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=49
05/24/2022 10:29:32 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.1881810228266921 on epoch=49
05/24/2022 10:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1881810228266921 on epoch=49, global_step=300
05/24/2022 10:29:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=51
05/24/2022 10:29:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=53
05/24/2022 10:29:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=54
05/24/2022 10:29:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=56
05/24/2022 10:29:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=58
05/24/2022 10:29:48 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.1881810228266921 on epoch=58
05/24/2022 10:29:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=59
05/24/2022 10:29:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=61
05/24/2022 10:29:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=63
05/24/2022 10:29:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=64
05/24/2022 10:30:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=66
05/24/2022 10:30:04 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 10:30:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=68
05/24/2022 10:30:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=69
05/24/2022 10:30:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=71
05/24/2022 10:30:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=73
05/24/2022 10:30:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=74
05/24/2022 10:30:20 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=74
05/24/2022 10:30:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.2085278555866791 on epoch=74, global_step=450
05/24/2022 10:30:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=76
05/24/2022 10:30:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=78
05/24/2022 10:30:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/24/2022 10:30:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=81
05/24/2022 10:30:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=83
05/24/2022 10:30:36 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.2788726425090062 on epoch=83
05/24/2022 10:30:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.2788726425090062 on epoch=83, global_step=500
05/24/2022 10:30:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=84
05/24/2022 10:30:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=86
05/24/2022 10:30:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=88
05/24/2022 10:30:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=89
05/24/2022 10:30:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=91
05/24/2022 10:30:52 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.22780952380952382 on epoch=91
05/24/2022 10:30:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=93
05/24/2022 10:30:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=94
05/24/2022 10:31:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=96
05/24/2022 10:31:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=98
05/24/2022 10:31:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=99
05/24/2022 10:31:08 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.29596665960302326 on epoch=99
05/24/2022 10:31:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2788726425090062 -> 0.29596665960302326 on epoch=99, global_step=600
05/24/2022 10:31:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=101
05/24/2022 10:31:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=103
05/24/2022 10:31:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
05/24/2022 10:31:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=106
05/24/2022 10:31:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=108
05/24/2022 10:31:24 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.29596665960302326 on epoch=108
05/24/2022 10:31:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=109
05/24/2022 10:31:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=111
05/24/2022 10:31:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=113
05/24/2022 10:31:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=114
05/24/2022 10:31:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=116
05/24/2022 10:31:40 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.3395211191821361 on epoch=116
05/24/2022 10:31:40 - INFO - __main__ - Saving model with best Classification-F1: 0.29596665960302326 -> 0.3395211191821361 on epoch=116, global_step=700
05/24/2022 10:31:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=118
05/24/2022 10:31:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=119
05/24/2022 10:31:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=121
05/24/2022 10:31:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=123
05/24/2022 10:31:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=124
05/24/2022 10:31:55 - INFO - __main__ - Global step 750 Train loss 0.40 Classification-F1 0.3395211191821361 on epoch=124
05/24/2022 10:31:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=126
05/24/2022 10:32:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=128
05/24/2022 10:32:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=129
05/24/2022 10:32:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=131
05/24/2022 10:32:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=133
05/24/2022 10:32:11 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.3657262277951933 on epoch=133
05/24/2022 10:32:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3395211191821361 -> 0.3657262277951933 on epoch=133, global_step=800
05/24/2022 10:32:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=134
05/24/2022 10:32:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=136
05/24/2022 10:32:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=138
05/24/2022 10:32:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=139
05/24/2022 10:32:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=141
05/24/2022 10:32:27 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.36328502415458946 on epoch=141
05/24/2022 10:32:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=143
05/24/2022 10:32:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=144
05/24/2022 10:32:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=146
05/24/2022 10:32:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=148
05/24/2022 10:32:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=149
05/24/2022 10:32:43 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.36328502415458946 on epoch=149
05/24/2022 10:32:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=151
05/24/2022 10:32:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=153
05/24/2022 10:32:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=154
05/24/2022 10:32:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=156
05/24/2022 10:32:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=158
05/24/2022 10:32:59 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3988095238095239 on epoch=158
05/24/2022 10:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3657262277951933 -> 0.3988095238095239 on epoch=158, global_step=950
05/24/2022 10:33:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=159
05/24/2022 10:33:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=161
05/24/2022 10:33:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=163
05/24/2022 10:33:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=164
05/24/2022 10:33:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=166
05/24/2022 10:33:15 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.4072727272727273 on epoch=166
05/24/2022 10:33:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3988095238095239 -> 0.4072727272727273 on epoch=166, global_step=1000
05/24/2022 10:33:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=168
05/24/2022 10:33:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=169
05/24/2022 10:33:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=171
05/24/2022 10:33:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=173
05/24/2022 10:33:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=174
05/24/2022 10:33:31 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.4116264687693259 on epoch=174
05/24/2022 10:33:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4072727272727273 -> 0.4116264687693259 on epoch=174, global_step=1050
05/24/2022 10:33:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=176
05/24/2022 10:33:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=178
05/24/2022 10:33:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=179
05/24/2022 10:33:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=181
05/24/2022 10:33:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=183
05/24/2022 10:33:47 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.3771963575734256 on epoch=183
05/24/2022 10:33:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=184
05/24/2022 10:33:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=186
05/24/2022 10:33:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=188
05/24/2022 10:33:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=189
05/24/2022 10:34:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=191
05/24/2022 10:34:03 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.38055555555555554 on epoch=191
05/24/2022 10:34:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=193
05/24/2022 10:34:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=194
05/24/2022 10:34:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=196
05/24/2022 10:34:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=198
05/24/2022 10:34:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=199
05/24/2022 10:34:19 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.47536656891495604 on epoch=199
05/24/2022 10:34:19 - INFO - __main__ - Saving model with best Classification-F1: 0.4116264687693259 -> 0.47536656891495604 on epoch=199, global_step=1200
05/24/2022 10:34:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=201
05/24/2022 10:34:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=203
05/24/2022 10:34:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=204
05/24/2022 10:34:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=206
05/24/2022 10:34:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=208
05/24/2022 10:34:35 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.4142602495543672 on epoch=208
05/24/2022 10:34:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=209
05/24/2022 10:34:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=211
05/24/2022 10:34:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=213
05/24/2022 10:34:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=214
05/24/2022 10:34:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=216
05/24/2022 10:34:51 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.43390280204491033 on epoch=216
05/24/2022 10:34:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=218
05/24/2022 10:34:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=219
05/24/2022 10:34:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=221
05/24/2022 10:35:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=223
05/24/2022 10:35:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=224
05/24/2022 10:35:07 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.4819172334736586 on epoch=224
05/24/2022 10:35:07 - INFO - __main__ - Saving model with best Classification-F1: 0.47536656891495604 -> 0.4819172334736586 on epoch=224, global_step=1350
05/24/2022 10:35:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=226
05/24/2022 10:35:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=228
05/24/2022 10:35:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=229
05/24/2022 10:35:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=231
05/24/2022 10:35:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=233
05/24/2022 10:35:23 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.29294798934358834 on epoch=233
05/24/2022 10:35:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=234
05/24/2022 10:35:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=236
05/24/2022 10:35:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=238
05/24/2022 10:35:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=239
05/24/2022 10:35:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=241
05/24/2022 10:35:40 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.27327768945416003 on epoch=241
05/24/2022 10:35:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=243
05/24/2022 10:35:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=244
05/24/2022 10:35:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=246
05/24/2022 10:35:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=248
05/24/2022 10:35:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=249
05/24/2022 10:35:56 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.24635913076629815 on epoch=249
05/24/2022 10:35:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=251
05/24/2022 10:36:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=253
05/24/2022 10:36:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=254
05/24/2022 10:36:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=256
05/24/2022 10:36:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=258
05/24/2022 10:36:12 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.24632218844984802 on epoch=258
05/24/2022 10:36:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=259
05/24/2022 10:36:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=261
05/24/2022 10:36:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=263
05/24/2022 10:36:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=264
05/24/2022 10:36:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=266
05/24/2022 10:36:28 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.3100577613743566 on epoch=266
05/24/2022 10:36:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=268
05/24/2022 10:36:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=269
05/24/2022 10:36:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=271
05/24/2022 10:36:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=273
05/24/2022 10:36:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=274
05/24/2022 10:36:45 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.3192221493737171 on epoch=274
05/24/2022 10:36:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=276
05/24/2022 10:36:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=278
05/24/2022 10:36:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=279
05/24/2022 10:36:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=281
05/24/2022 10:36:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=283
05/24/2022 10:37:02 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.45679072171609486 on epoch=283
05/24/2022 10:37:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=284
05/24/2022 10:37:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=286
05/24/2022 10:37:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=288
05/24/2022 10:37:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=289
05/24/2022 10:37:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=291
05/24/2022 10:37:18 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.2808745684695052 on epoch=291
05/24/2022 10:37:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=293
05/24/2022 10:37:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=294
05/24/2022 10:37:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=296
05/24/2022 10:37:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=298
05/24/2022 10:37:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=299
05/24/2022 10:37:34 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.31249903391184125 on epoch=299
05/24/2022 10:37:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=301
05/24/2022 10:37:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=303
05/24/2022 10:37:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=304
05/24/2022 10:37:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=306
05/24/2022 10:37:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=308
05/24/2022 10:37:51 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.2689019607843138 on epoch=308
05/24/2022 10:37:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=309
05/24/2022 10:37:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=311
05/24/2022 10:37:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=313
05/24/2022 10:38:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=314
05/24/2022 10:38:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=316
05/24/2022 10:38:07 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.3540621472469585 on epoch=316
05/24/2022 10:38:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=318
05/24/2022 10:38:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=319
05/24/2022 10:38:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=321
05/24/2022 10:38:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=323
05/24/2022 10:38:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=324
05/24/2022 10:38:24 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.28856228517245464 on epoch=324
05/24/2022 10:38:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=326
05/24/2022 10:38:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=328
05/24/2022 10:38:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=329
05/24/2022 10:38:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=331
05/24/2022 10:38:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
05/24/2022 10:38:40 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.4230769230769231 on epoch=333
05/24/2022 10:38:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=334
05/24/2022 10:38:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=336
05/24/2022 10:38:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=338
05/24/2022 10:38:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=339
05/24/2022 10:38:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=341
05/24/2022 10:38:56 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.21247664826514978 on epoch=341
05/24/2022 10:38:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=343
05/24/2022 10:39:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=344
05/24/2022 10:39:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=346
05/24/2022 10:39:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=348
05/24/2022 10:39:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=349
05/24/2022 10:39:13 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.19135577250274255 on epoch=349
05/24/2022 10:39:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=351
05/24/2022 10:39:18 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=353
05/24/2022 10:39:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=354
05/24/2022 10:39:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=356
05/24/2022 10:39:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=358
05/24/2022 10:39:29 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.18544748112499448 on epoch=358
05/24/2022 10:39:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=359
05/24/2022 10:39:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=361
05/24/2022 10:39:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=363
05/24/2022 10:39:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=364
05/24/2022 10:39:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=366
05/24/2022 10:39:45 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.18696488696488697 on epoch=366
05/24/2022 10:39:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=368
05/24/2022 10:39:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=369
05/24/2022 10:39:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=371
05/24/2022 10:39:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=373
05/24/2022 10:39:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=374
05/24/2022 10:40:02 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.1988061507835519 on epoch=374
05/24/2022 10:40:04 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=376
05/24/2022 10:40:07 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=378
05/24/2022 10:40:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=379
05/24/2022 10:40:12 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=381
05/24/2022 10:40:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=383
05/24/2022 10:40:18 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.178848681508256 on epoch=383
05/24/2022 10:40:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=384
05/24/2022 10:40:23 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=386
05/24/2022 10:40:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=388
05/24/2022 10:40:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=389
05/24/2022 10:40:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
05/24/2022 10:40:35 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.17022351797862 on epoch=391
05/24/2022 10:40:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=393
05/24/2022 10:40:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=394
05/24/2022 10:40:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=396
05/24/2022 10:40:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=398
05/24/2022 10:40:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=399
05/24/2022 10:40:51 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.24716650962444478 on epoch=399
05/24/2022 10:40:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=401
05/24/2022 10:40:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=403
05/24/2022 10:40:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=404
05/24/2022 10:41:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=406
05/24/2022 10:41:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=408
05/24/2022 10:41:07 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.18925926534997561 on epoch=408
05/24/2022 10:41:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=409
05/24/2022 10:41:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=411
05/24/2022 10:41:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=413
05/24/2022 10:41:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=414
05/24/2022 10:41:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=416
05/24/2022 10:41:24 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.19693877551020408 on epoch=416
05/24/2022 10:41:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=418
05/24/2022 10:41:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=419
05/24/2022 10:41:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=421
05/24/2022 10:41:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=423
05/24/2022 10:41:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=424
05/24/2022 10:41:40 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.15564841601485768 on epoch=424
05/24/2022 10:41:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=426
05/24/2022 10:41:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=428
05/24/2022 10:41:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=429
05/24/2022 10:41:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=431
05/24/2022 10:41:54 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=433
05/24/2022 10:41:56 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.18790079976520652 on epoch=433
05/24/2022 10:41:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=434
05/24/2022 10:42:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=436
05/24/2022 10:42:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=438
05/24/2022 10:42:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=439
05/24/2022 10:42:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=441
05/24/2022 10:42:13 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.18418176589013519 on epoch=441
05/24/2022 10:42:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=443
05/24/2022 10:42:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=444
05/24/2022 10:42:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=446
05/24/2022 10:42:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=448
05/24/2022 10:42:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=449
05/24/2022 10:42:29 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.21400445129258686 on epoch=449
05/24/2022 10:42:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=451
05/24/2022 10:42:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=453
05/24/2022 10:42:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=454
05/24/2022 10:42:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=456
05/24/2022 10:42:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 10:42:46 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.14460641065309815 on epoch=458
05/24/2022 10:42:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=459
05/24/2022 10:42:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=461
05/24/2022 10:42:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=463
05/24/2022 10:42:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=464
05/24/2022 10:42:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=466
05/24/2022 10:43:02 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.2130097801832597 on epoch=466
05/24/2022 10:43:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=468
05/24/2022 10:43:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=469
05/24/2022 10:43:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=471
05/24/2022 10:43:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=473
05/24/2022 10:43:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=474
05/24/2022 10:43:19 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.15692640692640691 on epoch=474
05/24/2022 10:43:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=476
05/24/2022 10:43:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=478
05/24/2022 10:43:27 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=479
05/24/2022 10:43:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=481
05/24/2022 10:43:32 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=483
05/24/2022 10:43:35 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.14134447927551377 on epoch=483
05/24/2022 10:43:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/24/2022 10:43:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=486
05/24/2022 10:43:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=488
05/24/2022 10:43:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=489
05/24/2022 10:43:49 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=491
05/24/2022 10:43:51 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.1319721532065632 on epoch=491
05/24/2022 10:43:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=493
05/24/2022 10:43:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=494
05/24/2022 10:44:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
05/24/2022 10:44:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=498
05/24/2022 10:44:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=499
05/24/2022 10:44:06 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:44:06 - INFO - __main__ - Printing 3 examples
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:44:06 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:44:06 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:44:06 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:44:06 - INFO - __main__ - Printing 3 examples
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:44:06 - INFO - __main__ - ['neutral']
05/24/2022 10:44:06 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:44:06 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:44:06 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:44:08 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.1306536951698242 on epoch=499
05/24/2022 10:44:08 - INFO - __main__ - save last model!
05/24/2022 10:44:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 10:44:08 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 10:44:08 - INFO - __main__ - Printing 3 examples
05/24/2022 10:44:08 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 10:44:08 - INFO - __main__ - ['contradiction']
05/24/2022 10:44:08 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 10:44:08 - INFO - __main__ - ['entailment']
05/24/2022 10:44:08 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 10:44:08 - INFO - __main__ - ['contradiction']
05/24/2022 10:44:08 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:44:09 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:44:10 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 10:44:23 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:44:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:44:24 - INFO - __main__ - Starting training!
05/24/2022 10:44:40 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_42_0.3_8_predictions.txt
05/24/2022 10:44:40 - INFO - __main__ - Classification-F1 on test data: 0.0331
05/24/2022 10:44:40 - INFO - __main__ - prefix=anli_32_42, lr=0.3, bsz=8, dev_performance=0.4819172334736586, test_performance=0.03306980512604209
05/24/2022 10:44:40 - INFO - __main__ - Running ... prefix=anli_32_42, lr=0.2, bsz=8 ...
05/24/2022 10:44:41 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:44:41 - INFO - __main__ - Printing 3 examples
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:44:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:44:41 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 10:44:41 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 10:44:41 - INFO - __main__ - Printing 3 examples
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: Stanisław Kiecal (September 14, 1886 – October 15, 1910), better known in the boxing world as Stanley Ketchel, was a Polish American professional boxer who became one of the greatest World Middleweight Champions in history. He was nicknamed "The Michigan Assassin." He was murdered at a ranch in Conway, Missouri, at the age of 24. [SEP] hypothesis: Kiecal had boxing matches as an amateur.
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: "22" is a song recorded by American singer-songwriter Taylor Swift for her fourth album, "Red" (2012). It was written by Swift along with Max Martin and Shellback. The song was released as the album's fourth single on March 12, 2013. The lyrics describe the joys of being 22 years old. [SEP] hypothesis: Taylor Swift is the first artist to write a song about the joys of being 22 years old.
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ -  [anli] premise: Robert Jack Duarte Wallace (born April 7, 1986 in Mexico City, Distrito Federal) is a Mexican actor and singer. He is known for his acting performance in the Mexican telenovela "Rebelde" as "Tomas Goycolea"" and as a member of the Mexican-Argentine pop band, "Eme 15". [SEP] hypothesis: Robert Jack Duarte Wallace has a brother.
05/24/2022 10:44:41 - INFO - __main__ - ['neutral']
05/24/2022 10:44:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 10:44:41 - INFO - __main__ - Tokenizing Output ...
05/24/2022 10:44:41 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 10:44:57 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 10:44:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 10:44:58 - INFO - __main__ - Starting training!
05/24/2022 10:45:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.71 on epoch=1
05/24/2022 10:45:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.61 on epoch=3
05/24/2022 10:45:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=4
05/24/2022 10:45:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=6
05/24/2022 10:45:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=8
05/24/2022 10:45:14 - INFO - __main__ - Global step 50 Train loss 0.59 Classification-F1 0.1679790026246719 on epoch=8
05/24/2022 10:45:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1679790026246719 on epoch=8, global_step=50
05/24/2022 10:45:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=9
05/24/2022 10:45:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=11
05/24/2022 10:45:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=13
05/24/2022 10:45:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
05/24/2022 10:45:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=16
05/24/2022 10:45:30 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 10:45:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=18
05/24/2022 10:45:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=19
05/24/2022 10:45:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=21
05/24/2022 10:45:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=23
05/24/2022 10:45:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=24
05/24/2022 10:45:46 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 10:45:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=26
05/24/2022 10:45:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/24/2022 10:45:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
05/24/2022 10:45:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=31
05/24/2022 10:45:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=33
05/24/2022 10:46:01 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 10:46:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=34
05/24/2022 10:46:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=36
05/24/2022 10:46:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=38
05/24/2022 10:46:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=39
05/24/2022 10:46:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
05/24/2022 10:46:17 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=41
05/24/2022 10:46:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=43
05/24/2022 10:46:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=44
05/24/2022 10:46:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=46
05/24/2022 10:46:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=48
05/24/2022 10:46:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=49
05/24/2022 10:46:33 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
05/24/2022 10:46:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=51
05/24/2022 10:46:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=53
05/24/2022 10:46:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=54
05/24/2022 10:46:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=56
05/24/2022 10:46:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=58
05/24/2022 10:46:49 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.2104787714543812 on epoch=58
05/24/2022 10:46:49 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.2104787714543812 on epoch=58, global_step=350
05/24/2022 10:46:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=59
05/24/2022 10:46:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=61
05/24/2022 10:46:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=63
05/24/2022 10:46:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=64
05/24/2022 10:47:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=66
05/24/2022 10:47:04 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=66
05/24/2022 10:47:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=68
05/24/2022 10:47:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=69
05/24/2022 10:47:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=71
05/24/2022 10:47:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=73
05/24/2022 10:47:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=74
05/24/2022 10:47:20 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.1881810228266921 on epoch=74
05/24/2022 10:47:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=76
05/24/2022 10:47:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=78
05/24/2022 10:47:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=79
05/24/2022 10:47:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=81
05/24/2022 10:47:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=83
05/24/2022 10:47:36 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.2087619047619048 on epoch=83
05/24/2022 10:47:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=84
05/24/2022 10:47:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=86
05/24/2022 10:47:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=88
05/24/2022 10:47:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=89
05/24/2022 10:47:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=91
05/24/2022 10:47:52 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
05/24/2022 10:47:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=93
05/24/2022 10:47:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=94
05/24/2022 10:48:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=96
05/24/2022 10:48:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=98
05/24/2022 10:48:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=99
05/24/2022 10:48:08 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.2085278555866791 on epoch=99
05/24/2022 10:48:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=101
05/24/2022 10:48:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.57 on epoch=103
05/24/2022 10:48:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=104
05/24/2022 10:48:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=106
05/24/2022 10:48:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=108
05/24/2022 10:48:23 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=108
05/24/2022 10:48:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=109
05/24/2022 10:48:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=111
05/24/2022 10:48:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=113
05/24/2022 10:48:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=114
05/24/2022 10:48:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=116
05/24/2022 10:48:39 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.2085278555866791 on epoch=116
05/24/2022 10:48:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=118
05/24/2022 10:48:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=119
05/24/2022 10:48:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=121
05/24/2022 10:48:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=123
05/24/2022 10:48:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=124
05/24/2022 10:48:55 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.24611708482676223 on epoch=124
05/24/2022 10:48:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2104787714543812 -> 0.24611708482676223 on epoch=124, global_step=750
05/24/2022 10:48:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=126
05/24/2022 10:49:00 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=128
05/24/2022 10:49:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=129
05/24/2022 10:49:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=131
05/24/2022 10:49:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=133
05/24/2022 10:49:11 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.2803418803418803 on epoch=133
05/24/2022 10:49:11 - INFO - __main__ - Saving model with best Classification-F1: 0.24611708482676223 -> 0.2803418803418803 on epoch=133, global_step=800
05/24/2022 10:49:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/24/2022 10:49:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=136
05/24/2022 10:49:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=138
05/24/2022 10:49:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=139
05/24/2022 10:49:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=141
05/24/2022 10:49:27 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.2085278555866791 on epoch=141
05/24/2022 10:49:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=143
05/24/2022 10:49:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=144
05/24/2022 10:49:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=146
05/24/2022 10:49:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=148
05/24/2022 10:49:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=149
05/24/2022 10:49:42 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.3551995931858632 on epoch=149
05/24/2022 10:49:42 - INFO - __main__ - Saving model with best Classification-F1: 0.2803418803418803 -> 0.3551995931858632 on epoch=149, global_step=900
05/24/2022 10:49:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=151
05/24/2022 10:49:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=153
05/24/2022 10:49:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=154
05/24/2022 10:49:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=156
05/24/2022 10:49:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=158
05/24/2022 10:49:58 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.3111111111111111 on epoch=158
05/24/2022 10:50:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=159
05/24/2022 10:50:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=161
05/24/2022 10:50:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=163
05/24/2022 10:50:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=164
05/24/2022 10:50:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=166
05/24/2022 10:50:14 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3395211191821361 on epoch=166
05/24/2022 10:50:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=168
05/24/2022 10:50:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=169
05/24/2022 10:50:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=171
05/24/2022 10:50:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=173
05/24/2022 10:50:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=174
05/24/2022 10:50:29 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.33319967923015237 on epoch=174
05/24/2022 10:50:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=176
05/24/2022 10:50:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=178
05/24/2022 10:50:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=179
05/24/2022 10:50:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=181
05/24/2022 10:50:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=183
05/24/2022 10:50:45 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.356803898935488 on epoch=183
05/24/2022 10:50:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3551995931858632 -> 0.356803898935488 on epoch=183, global_step=1100
05/24/2022 10:50:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=184
05/24/2022 10:50:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=186
05/24/2022 10:50:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=188
05/24/2022 10:50:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=189
05/24/2022 10:50:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.48 on epoch=191
05/24/2022 10:51:01 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.29743589743589743 on epoch=191
05/24/2022 10:51:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=193
05/24/2022 10:51:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=194
05/24/2022 10:51:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=196
05/24/2022 10:51:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=198
05/24/2022 10:51:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=199
05/24/2022 10:51:16 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.3126050420168067 on epoch=199
05/24/2022 10:51:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=201
05/24/2022 10:51:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=203
05/24/2022 10:51:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=204
05/24/2022 10:51:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=206
05/24/2022 10:51:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=208
05/24/2022 10:51:32 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.372993050563144 on epoch=208
05/24/2022 10:51:32 - INFO - __main__ - Saving model with best Classification-F1: 0.356803898935488 -> 0.372993050563144 on epoch=208, global_step=1250
05/24/2022 10:51:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=209
05/24/2022 10:51:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=211
05/24/2022 10:51:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=213
05/24/2022 10:51:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=214
05/24/2022 10:51:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=216
05/24/2022 10:51:48 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.38063063063063063 on epoch=216
05/24/2022 10:51:48 - INFO - __main__ - Saving model with best Classification-F1: 0.372993050563144 -> 0.38063063063063063 on epoch=216, global_step=1300
05/24/2022 10:51:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=218
05/24/2022 10:51:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=219
05/24/2022 10:51:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=221
05/24/2022 10:51:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=223
05/24/2022 10:52:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=224
05/24/2022 10:52:03 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.3800786369593709 on epoch=224
05/24/2022 10:52:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=226
05/24/2022 10:52:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=228
05/24/2022 10:52:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=229
05/24/2022 10:52:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=231
05/24/2022 10:52:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=233
05/24/2022 10:52:19 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.38743645606390703 on epoch=233
05/24/2022 10:52:19 - INFO - __main__ - Saving model with best Classification-F1: 0.38063063063063063 -> 0.38743645606390703 on epoch=233, global_step=1400
05/24/2022 10:52:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.36 on epoch=234
05/24/2022 10:52:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=236
05/24/2022 10:52:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=238
05/24/2022 10:52:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=239
05/24/2022 10:52:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=241
05/24/2022 10:52:34 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.3699224237981797 on epoch=241
05/24/2022 10:52:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=243
05/24/2022 10:52:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=244
05/24/2022 10:52:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=246
05/24/2022 10:52:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=248
05/24/2022 10:52:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=249
05/24/2022 10:52:50 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.3675830469644903 on epoch=249
05/24/2022 10:52:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=251
05/24/2022 10:52:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=253
05/24/2022 10:52:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=254
05/24/2022 10:53:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=256
05/24/2022 10:53:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=258
05/24/2022 10:53:05 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.3707070707070707 on epoch=258
05/24/2022 10:53:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=259
05/24/2022 10:53:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=261
05/24/2022 10:53:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=263
05/24/2022 10:53:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=264
05/24/2022 10:53:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=266
05/24/2022 10:53:21 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.38930307207434584 on epoch=266
05/24/2022 10:53:21 - INFO - __main__ - Saving model with best Classification-F1: 0.38743645606390703 -> 0.38930307207434584 on epoch=266, global_step=1600
05/24/2022 10:53:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=268
05/24/2022 10:53:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=269
05/24/2022 10:53:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=271
05/24/2022 10:53:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=273
05/24/2022 10:53:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=274
05/24/2022 10:53:37 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.4034412955465587 on epoch=274
05/24/2022 10:53:37 - INFO - __main__ - Saving model with best Classification-F1: 0.38930307207434584 -> 0.4034412955465587 on epoch=274, global_step=1650
05/24/2022 10:53:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=276
05/24/2022 10:53:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=278
05/24/2022 10:53:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=279
05/24/2022 10:53:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=281
05/24/2022 10:53:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=283
05/24/2022 10:53:52 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.40060250710259454 on epoch=283
05/24/2022 10:53:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=284
05/24/2022 10:53:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=286
05/24/2022 10:54:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=288
05/24/2022 10:54:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=289
05/24/2022 10:54:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=291
05/24/2022 10:54:08 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.39905591866376183 on epoch=291
05/24/2022 10:54:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=293
05/24/2022 10:54:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=294
05/24/2022 10:54:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=296
05/24/2022 10:54:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=298
05/24/2022 10:54:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=299
05/24/2022 10:54:24 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.3901308982951734 on epoch=299
05/24/2022 10:54:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=301
05/24/2022 10:54:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=303
05/24/2022 10:54:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=304
05/24/2022 10:54:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=306
05/24/2022 10:54:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=308
05/24/2022 10:54:40 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.4155336189234495 on epoch=308
05/24/2022 10:54:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4034412955465587 -> 0.4155336189234495 on epoch=308, global_step=1850
05/24/2022 10:54:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=309
05/24/2022 10:54:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=311
05/24/2022 10:54:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=313
05/24/2022 10:54:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=314
05/24/2022 10:54:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=316
05/24/2022 10:54:56 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.42103174603174603 on epoch=316
05/24/2022 10:54:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4155336189234495 -> 0.42103174603174603 on epoch=316, global_step=1900
05/24/2022 10:54:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.30 on epoch=318
05/24/2022 10:55:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=319
05/24/2022 10:55:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=321
05/24/2022 10:55:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=323
05/24/2022 10:55:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=324
05/24/2022 10:55:12 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.3822189883310427 on epoch=324
05/24/2022 10:55:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=326
05/24/2022 10:55:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=328
05/24/2022 10:55:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=329
05/24/2022 10:55:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=331
05/24/2022 10:55:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=333
05/24/2022 10:55:28 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.40280829949567565 on epoch=333
05/24/2022 10:55:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=334
05/24/2022 10:55:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=336
05/24/2022 10:55:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=338
05/24/2022 10:55:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=339
05/24/2022 10:55:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=341
05/24/2022 10:55:45 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.4248807983156066 on epoch=341
05/24/2022 10:55:45 - INFO - __main__ - Saving model with best Classification-F1: 0.42103174603174603 -> 0.4248807983156066 on epoch=341, global_step=2050
05/24/2022 10:55:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=343
05/24/2022 10:55:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=344
05/24/2022 10:55:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=346
05/24/2022 10:55:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=348
05/24/2022 10:55:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=349
05/24/2022 10:56:01 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.45643086905730784 on epoch=349
05/24/2022 10:56:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4248807983156066 -> 0.45643086905730784 on epoch=349, global_step=2100
05/24/2022 10:56:04 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=351
05/24/2022 10:56:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=353
05/24/2022 10:56:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=354
05/24/2022 10:56:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=356
05/24/2022 10:56:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.20 on epoch=358
05/24/2022 10:56:18 - INFO - __main__ - Global step 2150 Train loss 0.21 Classification-F1 0.4381567101050736 on epoch=358
05/24/2022 10:56:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=359
05/24/2022 10:56:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=361
05/24/2022 10:56:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=363
05/24/2022 10:56:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=364
05/24/2022 10:56:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.25 on epoch=366
05/24/2022 10:56:34 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.4243176178660049 on epoch=366
05/24/2022 10:56:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=368
05/24/2022 10:56:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=369
05/24/2022 10:56:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=371
05/24/2022 10:56:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=373
05/24/2022 10:56:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=374
05/24/2022 10:56:50 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.4574271087745443 on epoch=374
05/24/2022 10:56:51 - INFO - __main__ - Saving model with best Classification-F1: 0.45643086905730784 -> 0.4574271087745443 on epoch=374, global_step=2250
05/24/2022 10:56:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=376
05/24/2022 10:56:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=378
05/24/2022 10:56:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=379
05/24/2022 10:57:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=381
05/24/2022 10:57:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=383
05/24/2022 10:57:07 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.40198412698412705 on epoch=383
05/24/2022 10:57:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=384
05/24/2022 10:57:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=386
05/24/2022 10:57:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=388
05/24/2022 10:57:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=389
05/24/2022 10:57:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=391
05/24/2022 10:57:23 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.41819672131147545 on epoch=391
05/24/2022 10:57:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=393
05/24/2022 10:57:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=394
05/24/2022 10:57:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=396
05/24/2022 10:57:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=398
05/24/2022 10:57:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=399
05/24/2022 10:57:39 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.4927582705360483 on epoch=399
05/24/2022 10:57:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4574271087745443 -> 0.4927582705360483 on epoch=399, global_step=2400
05/24/2022 10:57:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=401
05/24/2022 10:57:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=403
05/24/2022 10:57:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=404
05/24/2022 10:57:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=406
05/24/2022 10:57:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=408
05/24/2022 10:57:56 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.4443929468957277 on epoch=408
05/24/2022 10:57:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=409
05/24/2022 10:58:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=411
05/24/2022 10:58:04 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=413
05/24/2022 10:58:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=414
05/24/2022 10:58:09 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=416
05/24/2022 10:58:12 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.33275342791471824 on epoch=416
05/24/2022 10:58:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.15 on epoch=418
05/24/2022 10:58:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=419
05/24/2022 10:58:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=421
05/24/2022 10:58:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=423
05/24/2022 10:58:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=424
05/24/2022 10:58:29 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.33828347578347573 on epoch=424
05/24/2022 10:58:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=426
05/24/2022 10:58:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=428
05/24/2022 10:58:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.20 on epoch=429
05/24/2022 10:58:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=431
05/24/2022 10:58:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=433
05/24/2022 10:58:45 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.21001602564102564 on epoch=433
05/24/2022 10:58:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=434
05/24/2022 10:58:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=436
05/24/2022 10:58:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=438
05/24/2022 10:58:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=439
05/24/2022 10:58:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=441
05/24/2022 10:59:02 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.339510184671475 on epoch=441
05/24/2022 10:59:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=443
05/24/2022 10:59:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=444
05/24/2022 10:59:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=446
05/24/2022 10:59:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=448
05/24/2022 10:59:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=449
05/24/2022 10:59:19 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.26977554179566565 on epoch=449
05/24/2022 10:59:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=451
05/24/2022 10:59:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=453
05/24/2022 10:59:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=454
05/24/2022 10:59:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=456
05/24/2022 10:59:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=458
05/24/2022 10:59:35 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.32765367491994063 on epoch=458
05/24/2022 10:59:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=459
05/24/2022 10:59:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=461
05/24/2022 10:59:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=463
05/24/2022 10:59:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=464
05/24/2022 10:59:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=466
05/24/2022 10:59:52 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.31229011194029854 on epoch=466
05/24/2022 10:59:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=468
05/24/2022 10:59:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=469
05/24/2022 11:00:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=471
05/24/2022 11:00:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=473
05/24/2022 11:00:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=474
05/24/2022 11:00:09 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.20282785208158347 on epoch=474
05/24/2022 11:00:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=476
05/24/2022 11:00:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=478
05/24/2022 11:00:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=479
05/24/2022 11:00:20 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=481
05/24/2022 11:00:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=483
05/24/2022 11:00:26 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.29201773138833 on epoch=483
05/24/2022 11:00:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=484
05/24/2022 11:00:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=486
05/24/2022 11:00:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
05/24/2022 11:00:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=489
05/24/2022 11:00:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=491
05/24/2022 11:00:43 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.3379850397381954 on epoch=491
05/24/2022 11:00:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=493
05/24/2022 11:00:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=494
05/24/2022 11:00:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=496
05/24/2022 11:00:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=498
05/24/2022 11:00:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=499
05/24/2022 11:00:58 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:00:58 - INFO - __main__ - Printing 3 examples
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:00:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:00:58 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:00:58 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:00:58 - INFO - __main__ - Printing 3 examples
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:00:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:00:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:00:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:00:58 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:00:59 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.2719758064516129 on epoch=499
05/24/2022 11:00:59 - INFO - __main__ - save last model!
05/24/2022 11:01:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 11:01:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 11:01:00 - INFO - __main__ - Printing 3 examples
05/24/2022 11:01:00 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 11:01:00 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:00 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 11:01:00 - INFO - __main__ - ['entailment']
05/24/2022 11:01:00 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 11:01:00 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:01:00 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:01:01 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 11:01:17 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:01:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:01:17 - INFO - __main__ - Starting training!
05/24/2022 11:01:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_42_0.2_8_predictions.txt
05/24/2022 11:01:35 - INFO - __main__ - Classification-F1 on test data: 0.0696
05/24/2022 11:01:35 - INFO - __main__ - prefix=anli_32_42, lr=0.2, bsz=8, dev_performance=0.4927582705360483, test_performance=0.06964688109027929
05/24/2022 11:01:36 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.5, bsz=8 ...
05/24/2022 11:01:36 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:01:36 - INFO - __main__ - Printing 3 examples
05/24/2022 11:01:36 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:01:36 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:36 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:01:36 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:36 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:01:36 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:36 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:01:36 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:01:37 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:01:37 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:01:37 - INFO - __main__ - Printing 3 examples
05/24/2022 11:01:37 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:01:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:37 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:01:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:37 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:01:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:01:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:01:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:01:37 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:01:52 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:01:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:01:53 - INFO - __main__ - Starting training!
05/24/2022 11:01:56 - INFO - __main__ - Step 10 Global step 10 Train loss 0.48 on epoch=1
05/24/2022 11:01:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=3
05/24/2022 11:02:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=4
05/24/2022 11:02:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=6
05/24/2022 11:02:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=8
05/24/2022 11:02:10 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 11:02:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 11:02:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=9
05/24/2022 11:02:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=11
05/24/2022 11:02:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=13
05/24/2022 11:02:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=14
05/24/2022 11:02:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=16
05/24/2022 11:02:26 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/24/2022 11:02:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=18
05/24/2022 11:02:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=19
05/24/2022 11:02:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=21
05/24/2022 11:02:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 11:02:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=24
05/24/2022 11:02:42 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 11:02:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=26
05/24/2022 11:02:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=28
05/24/2022 11:02:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=29
05/24/2022 11:02:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 11:02:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=33
05/24/2022 11:02:58 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 11:03:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=34
05/24/2022 11:03:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=36
05/24/2022 11:03:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=38
05/24/2022 11:03:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=39
05/24/2022 11:03:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=41
05/24/2022 11:03:14 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.23654078657278998 on epoch=41
05/24/2022 11:03:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.23654078657278998 on epoch=41, global_step=250
05/24/2022 11:03:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
05/24/2022 11:03:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
05/24/2022 11:03:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
05/24/2022 11:03:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=48
05/24/2022 11:03:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=49
05/24/2022 11:03:31 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.2761984392419175 on epoch=49
05/24/2022 11:03:31 - INFO - __main__ - Saving model with best Classification-F1: 0.23654078657278998 -> 0.2761984392419175 on epoch=49, global_step=300
05/24/2022 11:03:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=51
05/24/2022 11:03:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
05/24/2022 11:03:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/24/2022 11:03:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=56
05/24/2022 11:03:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=58
05/24/2022 11:03:46 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.25711820534943913 on epoch=58
05/24/2022 11:03:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=59
05/24/2022 11:03:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=61
05/24/2022 11:03:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=63
05/24/2022 11:03:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=64
05/24/2022 11:04:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=66
05/24/2022 11:04:02 - INFO - __main__ - Global step 400 Train loss 0.40 Classification-F1 0.3505554575912537 on epoch=66
05/24/2022 11:04:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2761984392419175 -> 0.3505554575912537 on epoch=66, global_step=400
05/24/2022 11:04:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=68
05/24/2022 11:04:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=69
05/24/2022 11:04:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=71
05/24/2022 11:04:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=73
05/24/2022 11:04:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=74
05/24/2022 11:04:18 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.35912698412698413 on epoch=74
05/24/2022 11:04:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3505554575912537 -> 0.35912698412698413 on epoch=74, global_step=450
05/24/2022 11:04:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
05/24/2022 11:04:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=78
05/24/2022 11:04:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.39 on epoch=79
05/24/2022 11:04:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=81
05/24/2022 11:04:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=83
05/24/2022 11:04:33 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.30877742946708464 on epoch=83
05/24/2022 11:04:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=84
05/24/2022 11:04:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=86
05/24/2022 11:04:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=88
05/24/2022 11:04:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=89
05/24/2022 11:04:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=91
05/24/2022 11:04:49 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.3744047559872352 on epoch=91
05/24/2022 11:04:49 - INFO - __main__ - Saving model with best Classification-F1: 0.35912698412698413 -> 0.3744047559872352 on epoch=91, global_step=550
05/24/2022 11:04:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=93
05/24/2022 11:04:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=94
05/24/2022 11:04:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=96
05/24/2022 11:05:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=98
05/24/2022 11:05:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=99
05/24/2022 11:05:05 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.3345080270738938 on epoch=99
05/24/2022 11:05:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=101
05/24/2022 11:05:10 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=103
05/24/2022 11:05:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=104
05/24/2022 11:05:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=106
05/24/2022 11:05:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=108
05/24/2022 11:05:21 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.3347763347763348 on epoch=108
05/24/2022 11:05:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=109
05/24/2022 11:05:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=111
05/24/2022 11:05:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=113
05/24/2022 11:05:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=114
05/24/2022 11:05:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=116
05/24/2022 11:05:36 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.3549965059399021 on epoch=116
05/24/2022 11:05:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=118
05/24/2022 11:05:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=119
05/24/2022 11:05:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=121
05/24/2022 11:05:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=123
05/24/2022 11:05:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=124
05/24/2022 11:05:52 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.3941885964912281 on epoch=124
05/24/2022 11:05:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3744047559872352 -> 0.3941885964912281 on epoch=124, global_step=750
05/24/2022 11:05:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=126
05/24/2022 11:05:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=128
05/24/2022 11:06:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=129
05/24/2022 11:06:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=131
05/24/2022 11:06:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=133
05/24/2022 11:06:08 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.3955555555555555 on epoch=133
05/24/2022 11:06:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3941885964912281 -> 0.3955555555555555 on epoch=133, global_step=800
05/24/2022 11:06:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=134
05/24/2022 11:06:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=136
05/24/2022 11:06:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=138
05/24/2022 11:06:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=139
05/24/2022 11:06:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=141
05/24/2022 11:06:23 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.44224149388623074 on epoch=141
05/24/2022 11:06:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3955555555555555 -> 0.44224149388623074 on epoch=141, global_step=850
05/24/2022 11:06:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=143
05/24/2022 11:06:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=144
05/24/2022 11:06:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=146
05/24/2022 11:06:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=148
05/24/2022 11:06:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=149
05/24/2022 11:06:39 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.4141147095542337 on epoch=149
05/24/2022 11:06:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=151
05/24/2022 11:06:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=153
05/24/2022 11:06:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=154
05/24/2022 11:06:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=156
05/24/2022 11:06:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=158
05/24/2022 11:06:55 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3283205186190261 on epoch=158
05/24/2022 11:06:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=159
05/24/2022 11:07:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=161
05/24/2022 11:07:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=163
05/24/2022 11:07:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=164
05/24/2022 11:07:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=166
05/24/2022 11:07:11 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.20458774053778517 on epoch=166
05/24/2022 11:07:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=168
05/24/2022 11:07:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=169
05/24/2022 11:07:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=171
05/24/2022 11:07:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=173
05/24/2022 11:07:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=174
05/24/2022 11:07:27 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.18255436741559536 on epoch=174
05/24/2022 11:07:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=176
05/24/2022 11:07:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=178
05/24/2022 11:07:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=179
05/24/2022 11:07:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=181
05/24/2022 11:07:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=183
05/24/2022 11:07:43 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.25656565656565655 on epoch=183
05/24/2022 11:07:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=184
05/24/2022 11:07:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=186
05/24/2022 11:07:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=188
05/24/2022 11:07:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=189
05/24/2022 11:07:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=191
05/24/2022 11:07:59 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.20880440299454386 on epoch=191
05/24/2022 11:08:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=193
05/24/2022 11:08:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=194
05/24/2022 11:08:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=196
05/24/2022 11:08:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=198
05/24/2022 11:08:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=199
05/24/2022 11:08:15 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.21250000000000002 on epoch=199
05/24/2022 11:08:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=201
05/24/2022 11:08:20 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=203
05/24/2022 11:08:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=204
05/24/2022 11:08:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=206
05/24/2022 11:08:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=208
05/24/2022 11:08:31 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.202632456737248 on epoch=208
05/24/2022 11:08:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=209
05/24/2022 11:08:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=211
05/24/2022 11:08:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=213
05/24/2022 11:08:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=214
05/24/2022 11:08:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=216
05/24/2022 11:08:47 - INFO - __main__ - Global step 1300 Train loss 0.09 Classification-F1 0.11552781484808673 on epoch=216
05/24/2022 11:08:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=218
05/24/2022 11:08:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=219
05/24/2022 11:08:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=221
05/24/2022 11:08:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=223
05/24/2022 11:09:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=224
05/24/2022 11:09:03 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.16000610710332458 on epoch=224
05/24/2022 11:09:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=226
05/24/2022 11:09:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=228
05/24/2022 11:09:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=229
05/24/2022 11:09:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=231
05/24/2022 11:09:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=233
05/24/2022 11:09:19 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.34930926916221033 on epoch=233
05/24/2022 11:09:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=234
05/24/2022 11:09:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=236
05/24/2022 11:09:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=238
05/24/2022 11:09:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=239
05/24/2022 11:09:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=241
05/24/2022 11:09:35 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.17742512964438747 on epoch=241
05/24/2022 11:09:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=243
05/24/2022 11:09:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=244
05/24/2022 11:09:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=246
05/24/2022 11:09:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=248
05/24/2022 11:09:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=249
05/24/2022 11:09:52 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.17390380365063912 on epoch=249
05/24/2022 11:09:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=251
05/24/2022 11:09:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=253
05/24/2022 11:10:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=254
05/24/2022 11:10:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=256
05/24/2022 11:10:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=258
05/24/2022 11:10:08 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.10464669738863286 on epoch=258
05/24/2022 11:10:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=259
05/24/2022 11:10:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=261
05/24/2022 11:10:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=263
05/24/2022 11:10:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=264
05/24/2022 11:10:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=266
05/24/2022 11:10:24 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.12462812462812461 on epoch=266
05/24/2022 11:10:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=268
05/24/2022 11:10:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=269
05/24/2022 11:10:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=271
05/24/2022 11:10:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=273
05/24/2022 11:10:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=274
05/24/2022 11:10:40 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.10377585377585379 on epoch=274
05/24/2022 11:10:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=276
05/24/2022 11:10:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=278
05/24/2022 11:10:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=279
05/24/2022 11:10:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=281
05/24/2022 11:10:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=283
05/24/2022 11:10:56 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.14827909949861168 on epoch=283
05/24/2022 11:10:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=284
05/24/2022 11:11:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=286
05/24/2022 11:11:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=288
05/24/2022 11:11:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=289
05/24/2022 11:11:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=291
05/24/2022 11:11:12 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.11453775038520801 on epoch=291
05/24/2022 11:11:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=293
05/24/2022 11:11:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=294
05/24/2022 11:11:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=296
05/24/2022 11:11:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=298
05/24/2022 11:11:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=299
05/24/2022 11:11:29 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.10551386283093601 on epoch=299
05/24/2022 11:11:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=301
05/24/2022 11:11:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=303
05/24/2022 11:11:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.03 on epoch=304
05/24/2022 11:11:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=306
05/24/2022 11:11:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=308
05/24/2022 11:11:45 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.11183456183456184 on epoch=308
05/24/2022 11:11:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=309
05/24/2022 11:11:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=311
05/24/2022 11:11:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=313
05/24/2022 11:11:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=314
05/24/2022 11:11:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=316
05/24/2022 11:12:01 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.1079830008998068 on epoch=316
05/24/2022 11:12:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=318
05/24/2022 11:12:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=319
05/24/2022 11:12:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=321
05/24/2022 11:12:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=323
05/24/2022 11:12:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=324
05/24/2022 11:12:17 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.08311434323513703 on epoch=324
05/24/2022 11:12:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=326
05/24/2022 11:12:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=328
05/24/2022 11:12:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=329
05/24/2022 11:12:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=331
05/24/2022 11:12:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=333
05/24/2022 11:12:33 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.06618939169114729 on epoch=333
05/24/2022 11:12:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=334
05/24/2022 11:12:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=336
05/24/2022 11:12:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=338
05/24/2022 11:12:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=339
05/24/2022 11:12:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=341
05/24/2022 11:12:49 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.10409953455066237 on epoch=341
05/24/2022 11:12:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=343
05/24/2022 11:12:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=344
05/24/2022 11:12:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=346
05/24/2022 11:13:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=348
05/24/2022 11:13:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=349
05/24/2022 11:13:06 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.11120510314058701 on epoch=349
05/24/2022 11:13:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=351
05/24/2022 11:13:11 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=353
05/24/2022 11:13:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=354
05/24/2022 11:13:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=356
05/24/2022 11:13:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=358
05/24/2022 11:13:22 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.09552040360990446 on epoch=358
05/24/2022 11:13:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=359
05/24/2022 11:13:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=361
05/24/2022 11:13:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=363
05/24/2022 11:13:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=364
05/24/2022 11:13:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=366
05/24/2022 11:13:38 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.09767563715627095 on epoch=366
05/24/2022 11:13:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=368
05/24/2022 11:13:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=369
05/24/2022 11:13:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=371
05/24/2022 11:13:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=373
05/24/2022 11:13:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=374
05/24/2022 11:13:54 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.09930879207474952 on epoch=374
05/24/2022 11:13:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=376
05/24/2022 11:14:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=378
05/24/2022 11:14:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=379
05/24/2022 11:14:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=381
05/24/2022 11:14:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=383
05/24/2022 11:14:10 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.07644178018037305 on epoch=383
05/24/2022 11:14:13 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=384
05/24/2022 11:14:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=386
05/24/2022 11:14:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=388
05/24/2022 11:14:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=389
05/24/2022 11:14:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=391
05/24/2022 11:14:27 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.06671858216971 on epoch=391
05/24/2022 11:14:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=393
05/24/2022 11:14:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=394
05/24/2022 11:14:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=396
05/24/2022 11:14:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=398
05/24/2022 11:14:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=399
05/24/2022 11:14:43 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.11061230208771192 on epoch=399
05/24/2022 11:14:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=401
05/24/2022 11:14:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=403
05/24/2022 11:14:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=404
05/24/2022 11:14:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=406
05/24/2022 11:14:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=408
05/24/2022 11:14:59 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.1005906238464378 on epoch=408
05/24/2022 11:15:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=409
05/24/2022 11:15:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=411
05/24/2022 11:15:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=413
05/24/2022 11:15:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=414
05/24/2022 11:15:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
05/24/2022 11:15:15 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.09444444444444443 on epoch=416
05/24/2022 11:15:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=418
05/24/2022 11:15:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=419
05/24/2022 11:15:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=421
05/24/2022 11:15:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=423
05/24/2022 11:15:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=424
05/24/2022 11:15:32 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.07936507936507936 on epoch=424
05/24/2022 11:15:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=426
05/24/2022 11:15:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=428
05/24/2022 11:15:40 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=429
05/24/2022 11:15:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=431
05/24/2022 11:15:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=433
05/24/2022 11:15:48 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.08767739613327849 on epoch=433
05/24/2022 11:15:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=434
05/24/2022 11:15:53 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=436
05/24/2022 11:15:56 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=438
05/24/2022 11:15:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
05/24/2022 11:16:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=441
05/24/2022 11:16:04 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.10914085914085914 on epoch=441
05/24/2022 11:16:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=443
05/24/2022 11:16:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=444
05/24/2022 11:16:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=446
05/24/2022 11:16:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=448
05/24/2022 11:16:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=449
05/24/2022 11:16:20 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.12978494623655912 on epoch=449
05/24/2022 11:16:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=451
05/24/2022 11:16:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=453
05/24/2022 11:16:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=454
05/24/2022 11:16:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=456
05/24/2022 11:16:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 11:16:36 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.12976827094474155 on epoch=458
05/24/2022 11:16:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=459
05/24/2022 11:16:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=461
05/24/2022 11:16:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=463
05/24/2022 11:16:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=464
05/24/2022 11:16:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=466
05/24/2022 11:16:52 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.15299145299145298 on epoch=466
05/24/2022 11:16:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
05/24/2022 11:16:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=469
05/24/2022 11:17:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=471
05/24/2022 11:17:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
05/24/2022 11:17:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=474
05/24/2022 11:17:09 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.12045739348370928 on epoch=474
05/24/2022 11:17:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=476
05/24/2022 11:17:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=478
05/24/2022 11:17:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/24/2022 11:17:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=481
05/24/2022 11:17:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=483
05/24/2022 11:17:25 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.13125489311447566 on epoch=483
05/24/2022 11:17:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=484
05/24/2022 11:17:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=486
05/24/2022 11:17:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=488
05/24/2022 11:17:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=489
05/24/2022 11:17:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=491
05/24/2022 11:17:41 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.08962280794608517 on epoch=491
05/24/2022 11:17:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=493
05/24/2022 11:17:47 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=494
05/24/2022 11:17:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=496
05/24/2022 11:17:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=498
05/24/2022 11:17:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=499
05/24/2022 11:17:56 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:17:56 - INFO - __main__ - Printing 3 examples
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:17:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:17:56 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:17:56 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:17:56 - INFO - __main__ - Printing 3 examples
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:17:56 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:56 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:17:56 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:17:57 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:17:58 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.11257877280265342 on epoch=499
05/24/2022 11:17:58 - INFO - __main__ - save last model!
05/24/2022 11:17:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 11:17:58 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 11:17:58 - INFO - __main__ - Printing 3 examples
05/24/2022 11:17:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 11:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 11:17:58 - INFO - __main__ - ['entailment']
05/24/2022 11:17:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 11:17:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:17:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:17:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:17:59 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 11:18:12 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:18:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:18:13 - INFO - __main__ - Starting training!
05/24/2022 11:18:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_87_0.5_8_predictions.txt
05/24/2022 11:18:29 - INFO - __main__ - Classification-F1 on test data: 0.0239
05/24/2022 11:18:29 - INFO - __main__ - prefix=anli_32_87, lr=0.5, bsz=8, dev_performance=0.44224149388623074, test_performance=0.023884379292411856
05/24/2022 11:18:29 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.4, bsz=8 ...
05/24/2022 11:18:30 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:18:30 - INFO - __main__ - Printing 3 examples
05/24/2022 11:18:30 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:18:30 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:30 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:18:30 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:30 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:18:30 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:30 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:18:30 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:18:31 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:18:31 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:18:31 - INFO - __main__ - Printing 3 examples
05/24/2022 11:18:31 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:18:31 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:31 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:18:31 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:31 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:18:31 - INFO - __main__ - ['contradiction']
05/24/2022 11:18:31 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:18:31 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:18:31 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:18:49 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:18:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:18:50 - INFO - __main__ - Starting training!
05/24/2022 11:18:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.51 on epoch=1
05/24/2022 11:18:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=3
05/24/2022 11:18:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=4
05/24/2022 11:19:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=6
05/24/2022 11:19:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=8
05/24/2022 11:19:07 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 11:19:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 11:19:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=9
05/24/2022 11:19:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=11
05/24/2022 11:19:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=13
05/24/2022 11:19:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=14
05/24/2022 11:19:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=16
05/24/2022 11:19:23 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.17980884109916365 on epoch=16
05/24/2022 11:19:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17980884109916365 on epoch=16, global_step=100
05/24/2022 11:19:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=18
05/24/2022 11:19:29 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=19
05/24/2022 11:19:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=21
05/24/2022 11:19:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=23
05/24/2022 11:19:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=24
05/24/2022 11:19:39 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16272965879265092 on epoch=24
05/24/2022 11:19:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=26
05/24/2022 11:19:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=28
05/24/2022 11:19:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=29
05/24/2022 11:19:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/24/2022 11:19:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=33
05/24/2022 11:19:55 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16272965879265092 on epoch=33
05/24/2022 11:19:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=34
05/24/2022 11:20:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=36
05/24/2022 11:20:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=38
05/24/2022 11:20:06 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=39
05/24/2022 11:20:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=41
05/24/2022 11:20:11 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.2432068872746839 on epoch=41
05/24/2022 11:20:11 - INFO - __main__ - Saving model with best Classification-F1: 0.17980884109916365 -> 0.2432068872746839 on epoch=41, global_step=250
05/24/2022 11:20:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
05/24/2022 11:20:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.39 on epoch=44
05/24/2022 11:20:19 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=46
05/24/2022 11:20:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=48
05/24/2022 11:20:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=49
05/24/2022 11:20:27 - INFO - __main__ - Global step 300 Train loss 0.43 Classification-F1 0.2005399618302844 on epoch=49
05/24/2022 11:20:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=51
05/24/2022 11:20:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=53
05/24/2022 11:20:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=54
05/24/2022 11:20:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=56
05/24/2022 11:20:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=58
05/24/2022 11:20:43 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.22815568270113726 on epoch=58
05/24/2022 11:20:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=59
05/24/2022 11:20:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=61
05/24/2022 11:20:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=63
05/24/2022 11:20:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=64
05/24/2022 11:20:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=66
05/24/2022 11:21:00 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.31601814580537985 on epoch=66
05/24/2022 11:21:00 - INFO - __main__ - Saving model with best Classification-F1: 0.2432068872746839 -> 0.31601814580537985 on epoch=66, global_step=400
05/24/2022 11:21:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=68
05/24/2022 11:21:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=69
05/24/2022 11:21:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=71
05/24/2022 11:21:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=73
05/24/2022 11:21:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=74
05/24/2022 11:21:16 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.27336300063572794 on epoch=74
05/24/2022 11:21:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=76
05/24/2022 11:21:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=78
05/24/2022 11:21:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=79
05/24/2022 11:21:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=81
05/24/2022 11:21:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=83
05/24/2022 11:21:32 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.303750768600123 on epoch=83
05/24/2022 11:21:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=84
05/24/2022 11:21:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=86
05/24/2022 11:21:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=88
05/24/2022 11:21:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=89
05/24/2022 11:21:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=91
05/24/2022 11:21:48 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.3551995931858632 on epoch=91
05/24/2022 11:21:48 - INFO - __main__ - Saving model with best Classification-F1: 0.31601814580537985 -> 0.3551995931858632 on epoch=91, global_step=550
05/24/2022 11:21:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=93
05/24/2022 11:21:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=94
05/24/2022 11:21:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=96
05/24/2022 11:21:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=98
05/24/2022 11:22:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=99
05/24/2022 11:22:04 - INFO - __main__ - Global step 600 Train loss 0.36 Classification-F1 0.3426731078904992 on epoch=99
05/24/2022 11:22:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=101
05/24/2022 11:22:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=103
05/24/2022 11:22:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=104
05/24/2022 11:22:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=106
05/24/2022 11:22:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=108
05/24/2022 11:22:20 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.34508348794063076 on epoch=108
05/24/2022 11:22:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=109
05/24/2022 11:22:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=111
05/24/2022 11:22:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=113
05/24/2022 11:22:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=114
05/24/2022 11:22:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=116
05/24/2022 11:22:36 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.3790123456790124 on epoch=116
05/24/2022 11:22:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3551995931858632 -> 0.3790123456790124 on epoch=116, global_step=700
05/24/2022 11:22:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=118
05/24/2022 11:22:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=119
05/24/2022 11:22:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=121
05/24/2022 11:22:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=123
05/24/2022 11:22:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=124
05/24/2022 11:22:52 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.429635501999542 on epoch=124
05/24/2022 11:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3790123456790124 -> 0.429635501999542 on epoch=124, global_step=750
05/24/2022 11:22:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=126
05/24/2022 11:22:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=128
05/24/2022 11:23:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=129
05/24/2022 11:23:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=131
05/24/2022 11:23:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=133
05/24/2022 11:23:08 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.38172685619966423 on epoch=133
05/24/2022 11:23:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=134
05/24/2022 11:23:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=136
05/24/2022 11:23:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=138
05/24/2022 11:23:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=139
05/24/2022 11:23:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=141
05/24/2022 11:23:25 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.39808184872990454 on epoch=141
05/24/2022 11:23:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=143
05/24/2022 11:23:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=144
05/24/2022 11:23:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=146
05/24/2022 11:23:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=148
05/24/2022 11:23:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=149
05/24/2022 11:23:41 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.3529089837501052 on epoch=149
05/24/2022 11:23:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=151
05/24/2022 11:23:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=153
05/24/2022 11:23:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=154
05/24/2022 11:23:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=156
05/24/2022 11:23:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=158
05/24/2022 11:23:57 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.38079794079794077 on epoch=158
05/24/2022 11:24:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=159
05/24/2022 11:24:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=161
05/24/2022 11:24:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=163
05/24/2022 11:24:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=164
05/24/2022 11:24:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=166
05/24/2022 11:24:14 - INFO - __main__ - Global step 1000 Train loss 0.25 Classification-F1 0.396106957634539 on epoch=166
05/24/2022 11:24:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=168
05/24/2022 11:24:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=169
05/24/2022 11:24:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=171
05/24/2022 11:24:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=173
05/24/2022 11:24:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=174
05/24/2022 11:24:30 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.43277191162011636 on epoch=174
05/24/2022 11:24:30 - INFO - __main__ - Saving model with best Classification-F1: 0.429635501999542 -> 0.43277191162011636 on epoch=174, global_step=1050
05/24/2022 11:24:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=176
05/24/2022 11:24:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=178
05/24/2022 11:24:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=179
05/24/2022 11:24:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=181
05/24/2022 11:24:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=183
05/24/2022 11:24:47 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.41528947243232955 on epoch=183
05/24/2022 11:24:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=184
05/24/2022 11:24:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=186
05/24/2022 11:24:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=188
05/24/2022 11:24:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=189
05/24/2022 11:25:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=191
05/24/2022 11:25:03 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.43188327962764045 on epoch=191
05/24/2022 11:25:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=193
05/24/2022 11:25:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=194
05/24/2022 11:25:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=196
05/24/2022 11:25:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=198
05/24/2022 11:25:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=199
05/24/2022 11:25:19 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.3353262961130353 on epoch=199
05/24/2022 11:25:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=201
05/24/2022 11:25:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=203
05/24/2022 11:25:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=204
05/24/2022 11:25:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=206
05/24/2022 11:25:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=208
05/24/2022 11:25:35 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.45692372316269675 on epoch=208
05/24/2022 11:25:35 - INFO - __main__ - Saving model with best Classification-F1: 0.43277191162011636 -> 0.45692372316269675 on epoch=208, global_step=1250
05/24/2022 11:25:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=209
05/24/2022 11:25:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=211
05/24/2022 11:25:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=213
05/24/2022 11:25:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=214
05/24/2022 11:25:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=216
05/24/2022 11:25:52 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.35377556005303934 on epoch=216
05/24/2022 11:25:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=218
05/24/2022 11:25:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=219
05/24/2022 11:26:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=221
05/24/2022 11:26:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=223
05/24/2022 11:26:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=224
05/24/2022 11:26:08 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.2203382193920461 on epoch=224
05/24/2022 11:26:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=226
05/24/2022 11:26:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=228
05/24/2022 11:26:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=229
05/24/2022 11:26:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=231
05/24/2022 11:26:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=233
05/24/2022 11:26:24 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.3439112050739958 on epoch=233
05/24/2022 11:26:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=234
05/24/2022 11:26:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=236
05/24/2022 11:26:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=238
05/24/2022 11:26:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=239
05/24/2022 11:26:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=241
05/24/2022 11:26:40 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.36095976852279377 on epoch=241
05/24/2022 11:26:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=243
05/24/2022 11:26:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=244
05/24/2022 11:26:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=246
05/24/2022 11:26:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=248
05/24/2022 11:26:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=249
05/24/2022 11:26:57 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.18869218500797447 on epoch=249
05/24/2022 11:26:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=251
05/24/2022 11:27:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=253
05/24/2022 11:27:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=254
05/24/2022 11:27:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=256
05/24/2022 11:27:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=258
05/24/2022 11:27:13 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.30527227158806103 on epoch=258
05/24/2022 11:27:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=259
05/24/2022 11:27:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=261
05/24/2022 11:27:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=263
05/24/2022 11:27:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=264
05/24/2022 11:27:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=266
05/24/2022 11:27:29 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.21672086556141443 on epoch=266
05/24/2022 11:27:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=268
05/24/2022 11:27:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=269
05/24/2022 11:27:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=271
05/24/2022 11:27:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=273
05/24/2022 11:27:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=274
05/24/2022 11:27:45 - INFO - __main__ - Global step 1650 Train loss 0.10 Classification-F1 0.4130472701901273 on epoch=274
05/24/2022 11:27:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=276
05/24/2022 11:27:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=278
05/24/2022 11:27:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=279
05/24/2022 11:27:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=281
05/24/2022 11:27:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=283
05/24/2022 11:28:01 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.3672027290448343 on epoch=283
05/24/2022 11:28:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=284
05/24/2022 11:28:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=286
05/24/2022 11:28:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=288
05/24/2022 11:28:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=289
05/24/2022 11:28:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=291
05/24/2022 11:28:18 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.22448384948384947 on epoch=291
05/24/2022 11:28:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=293
05/24/2022 11:28:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=294
05/24/2022 11:28:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=296
05/24/2022 11:28:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=298
05/24/2022 11:28:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=299
05/24/2022 11:28:34 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.4803090713956036 on epoch=299
05/24/2022 11:28:34 - INFO - __main__ - Saving model with best Classification-F1: 0.45692372316269675 -> 0.4803090713956036 on epoch=299, global_step=1800
05/24/2022 11:28:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=301
05/24/2022 11:28:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=303
05/24/2022 11:28:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=304
05/24/2022 11:28:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=306
05/24/2022 11:28:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=308
05/24/2022 11:28:50 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.1795142831774871 on epoch=308
05/24/2022 11:28:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=309
05/24/2022 11:28:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=311
05/24/2022 11:28:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=313
05/24/2022 11:29:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=314
05/24/2022 11:29:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=316
05/24/2022 11:29:06 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.1950634727415007 on epoch=316
05/24/2022 11:29:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=318
05/24/2022 11:29:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=319
05/24/2022 11:29:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=321
05/24/2022 11:29:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=323
05/24/2022 11:29:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=324
05/24/2022 11:29:23 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.17909140667761358 on epoch=324
05/24/2022 11:29:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=326
05/24/2022 11:29:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=328
05/24/2022 11:29:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=329
05/24/2022 11:29:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=331
05/24/2022 11:29:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=333
05/24/2022 11:29:39 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.22544217687074827 on epoch=333
05/24/2022 11:29:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=334
05/24/2022 11:29:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=336
05/24/2022 11:29:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=338
05/24/2022 11:29:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=339
05/24/2022 11:29:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=341
05/24/2022 11:29:55 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.2111963806657257 on epoch=341
05/24/2022 11:29:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=343
05/24/2022 11:30:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=344
05/24/2022 11:30:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=346
05/24/2022 11:30:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=348
05/24/2022 11:30:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=349
05/24/2022 11:30:11 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.2301779173207745 on epoch=349
05/24/2022 11:30:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=351
05/24/2022 11:30:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=353
05/24/2022 11:30:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=354
05/24/2022 11:30:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=356
05/24/2022 11:30:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=358
05/24/2022 11:30:28 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.19280147108441864 on epoch=358
05/24/2022 11:30:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=359
05/24/2022 11:30:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=361
05/24/2022 11:30:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=363
05/24/2022 11:30:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=364
05/24/2022 11:30:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=366
05/24/2022 11:30:44 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.19209299039005084 on epoch=366
05/24/2022 11:30:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=368
05/24/2022 11:30:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=369
05/24/2022 11:30:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=371
05/24/2022 11:30:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=373
05/24/2022 11:30:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=374
05/24/2022 11:31:00 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5440931563882384 on epoch=374
05/24/2022 11:31:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4803090713956036 -> 0.5440931563882384 on epoch=374, global_step=2250
05/24/2022 11:31:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=376
05/24/2022 11:31:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=378
05/24/2022 11:31:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=379
05/24/2022 11:31:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=381
05/24/2022 11:31:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=383
05/24/2022 11:31:16 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.12790910421545668 on epoch=383
05/24/2022 11:31:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=384
05/24/2022 11:31:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=386
05/24/2022 11:31:24 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=388
05/24/2022 11:31:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=389
05/24/2022 11:31:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=391
05/24/2022 11:31:32 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.26544844044844046 on epoch=391
05/24/2022 11:31:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=393
05/24/2022 11:31:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=394
05/24/2022 11:31:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=396
05/24/2022 11:31:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=398
05/24/2022 11:31:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=399
05/24/2022 11:31:49 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.2609148441880958 on epoch=399
05/24/2022 11:31:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=401
05/24/2022 11:31:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=403
05/24/2022 11:31:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=404
05/24/2022 11:31:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=406
05/24/2022 11:32:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=408
05/24/2022 11:32:05 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.25555555555555554 on epoch=408
05/24/2022 11:32:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=409
05/24/2022 11:32:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=411
05/24/2022 11:32:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=413
05/24/2022 11:32:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=414
05/24/2022 11:32:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=416
05/24/2022 11:32:21 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.3056781193490054 on epoch=416
05/24/2022 11:32:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=418
05/24/2022 11:32:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=419
05/24/2022 11:32:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=421
05/24/2022 11:32:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=423
05/24/2022 11:32:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=424
05/24/2022 11:32:38 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.29480943738656984 on epoch=424
05/24/2022 11:32:40 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=426
05/24/2022 11:32:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=428
05/24/2022 11:32:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=429
05/24/2022 11:32:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=431
05/24/2022 11:32:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=433
05/24/2022 11:32:54 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.48445211767446317 on epoch=433
05/24/2022 11:32:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=434
05/24/2022 11:32:59 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=436
05/24/2022 11:33:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=438
05/24/2022 11:33:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=439
05/24/2022 11:33:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=441
05/24/2022 11:33:10 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.3173611111111111 on epoch=441
05/24/2022 11:33:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=443
05/24/2022 11:33:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 11:33:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=446
05/24/2022 11:33:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=448
05/24/2022 11:33:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=449
05/24/2022 11:33:26 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.22154856637615256 on epoch=449
05/24/2022 11:33:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=451
05/24/2022 11:33:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=453
05/24/2022 11:33:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=454
05/24/2022 11:33:37 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=456
05/24/2022 11:33:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=458
05/24/2022 11:33:42 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.22477587373223043 on epoch=458
05/24/2022 11:33:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=459
05/24/2022 11:33:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=461
05/24/2022 11:33:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=463
05/24/2022 11:33:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=464
05/24/2022 11:33:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=466
05/24/2022 11:33:59 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.17648988897607312 on epoch=466
05/24/2022 11:34:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=468
05/24/2022 11:34:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=469
05/24/2022 11:34:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=471
05/24/2022 11:34:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=473
05/24/2022 11:34:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=474
05/24/2022 11:34:15 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.2204686640357268 on epoch=474
05/24/2022 11:34:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=476
05/24/2022 11:34:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=478
05/24/2022 11:34:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=479
05/24/2022 11:34:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=481
05/24/2022 11:34:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=483
05/24/2022 11:34:31 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.22327561327561327 on epoch=483
05/24/2022 11:34:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=484
05/24/2022 11:34:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=486
05/24/2022 11:34:39 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=488
05/24/2022 11:34:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=489
05/24/2022 11:34:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=491
05/24/2022 11:34:47 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.18626026848326988 on epoch=491
05/24/2022 11:34:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=493
05/24/2022 11:34:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=494
05/24/2022 11:34:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=496
05/24/2022 11:34:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=498
05/24/2022 11:35:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=499
05/24/2022 11:35:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:35:02 - INFO - __main__ - Printing 3 examples
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:35:02 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:35:02 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:35:02 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:35:02 - INFO - __main__ - Printing 3 examples
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:35:02 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:02 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:35:02 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:35:03 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:35:04 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.25428106134131695 on epoch=499
05/24/2022 11:35:04 - INFO - __main__ - save last model!
05/24/2022 11:35:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 11:35:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 11:35:04 - INFO - __main__ - Printing 3 examples
05/24/2022 11:35:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 11:35:04 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 11:35:04 - INFO - __main__ - ['entailment']
05/24/2022 11:35:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 11:35:04 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:04 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:35:04 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:35:05 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 11:35:18 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:35:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:35:19 - INFO - __main__ - Starting training!
05/24/2022 11:35:36 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_87_0.4_8_predictions.txt
05/24/2022 11:35:36 - INFO - __main__ - Classification-F1 on test data: 0.0271
05/24/2022 11:35:36 - INFO - __main__ - prefix=anli_32_87, lr=0.4, bsz=8, dev_performance=0.5440931563882384, test_performance=0.027105125699783076
05/24/2022 11:35:36 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.3, bsz=8 ...
05/24/2022 11:35:37 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:35:37 - INFO - __main__ - Printing 3 examples
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:35:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:35:37 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:35:37 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:35:37 - INFO - __main__ - Printing 3 examples
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:35:37 - INFO - __main__ - ['contradiction']
05/24/2022 11:35:37 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:35:37 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:35:37 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:35:56 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:35:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:35:57 - INFO - __main__ - Starting training!
05/24/2022 11:36:00 - INFO - __main__ - Step 10 Global step 10 Train loss 0.44 on epoch=1
05/24/2022 11:36:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=3
05/24/2022 11:36:05 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=4
05/24/2022 11:36:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=6
05/24/2022 11:36:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=8
05/24/2022 11:36:13 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 11:36:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 11:36:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=9
05/24/2022 11:36:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=11
05/24/2022 11:36:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=13
05/24/2022 11:36:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=14
05/24/2022 11:36:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=16
05/24/2022 11:36:29 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16272965879265092 on epoch=16
05/24/2022 11:36:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=18
05/24/2022 11:36:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=19
05/24/2022 11:36:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=21
05/24/2022 11:36:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=23
05/24/2022 11:36:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=24
05/24/2022 11:36:45 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 11:36:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=26
05/24/2022 11:36:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/24/2022 11:36:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=29
05/24/2022 11:36:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=31
05/24/2022 11:36:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=33
05/24/2022 11:37:01 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 11:37:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=34
05/24/2022 11:37:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=36
05/24/2022 11:37:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=38
05/24/2022 11:37:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=39
05/24/2022 11:37:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=41
05/24/2022 11:37:17 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.15873015873015875 on epoch=41
05/24/2022 11:37:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=43
05/24/2022 11:37:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=44
05/24/2022 11:37:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=46
05/24/2022 11:37:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=48
05/24/2022 11:37:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=49
05/24/2022 11:37:33 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.17904761904761904 on epoch=49
05/24/2022 11:37:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17904761904761904 on epoch=49, global_step=300
05/24/2022 11:37:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=51
05/24/2022 11:37:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=53
05/24/2022 11:37:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=54
05/24/2022 11:37:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=56
05/24/2022 11:37:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=58
05/24/2022 11:37:49 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.1836290071584189 on epoch=58
05/24/2022 11:37:49 - INFO - __main__ - Saving model with best Classification-F1: 0.17904761904761904 -> 0.1836290071584189 on epoch=58, global_step=350
05/24/2022 11:37:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=59
05/24/2022 11:37:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=61
05/24/2022 11:37:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=63
05/24/2022 11:38:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=64
05/24/2022 11:38:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=66
05/24/2022 11:38:05 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.34208259061200236 on epoch=66
05/24/2022 11:38:05 - INFO - __main__ - Saving model with best Classification-F1: 0.1836290071584189 -> 0.34208259061200236 on epoch=66, global_step=400
05/24/2022 11:38:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=68
05/24/2022 11:38:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=69
05/24/2022 11:38:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=71
05/24/2022 11:38:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/24/2022 11:38:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=74
05/24/2022 11:38:21 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.32650270083898403 on epoch=74
05/24/2022 11:38:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=76
05/24/2022 11:38:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=78
05/24/2022 11:38:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=79
05/24/2022 11:38:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=81
05/24/2022 11:38:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=83
05/24/2022 11:38:37 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.27777777777777773 on epoch=83
05/24/2022 11:38:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=84
05/24/2022 11:38:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=86
05/24/2022 11:38:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=88
05/24/2022 11:38:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=89
05/24/2022 11:38:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=91
05/24/2022 11:38:53 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.35019587804462615 on epoch=91
05/24/2022 11:38:53 - INFO - __main__ - Saving model with best Classification-F1: 0.34208259061200236 -> 0.35019587804462615 on epoch=91, global_step=550
05/24/2022 11:38:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=93
05/24/2022 11:38:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=94
05/24/2022 11:39:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=96
05/24/2022 11:39:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=98
05/24/2022 11:39:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=99
05/24/2022 11:39:09 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.33134920634920634 on epoch=99
05/24/2022 11:39:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=101
05/24/2022 11:39:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=103
05/24/2022 11:39:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=104
05/24/2022 11:39:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=106
05/24/2022 11:39:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=108
05/24/2022 11:39:25 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.32036613272311215 on epoch=108
05/24/2022 11:39:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=109
05/24/2022 11:39:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=111
05/24/2022 11:39:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=113
05/24/2022 11:39:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=114
05/24/2022 11:39:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=116
05/24/2022 11:39:40 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.32358674463937626 on epoch=116
05/24/2022 11:39:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=118
05/24/2022 11:39:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=119
05/24/2022 11:39:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=121
05/24/2022 11:39:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=123
05/24/2022 11:39:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=124
05/24/2022 11:39:56 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.3570509284794999 on epoch=124
05/24/2022 11:39:56 - INFO - __main__ - Saving model with best Classification-F1: 0.35019587804462615 -> 0.3570509284794999 on epoch=124, global_step=750
05/24/2022 11:39:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=126
05/24/2022 11:40:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=128
05/24/2022 11:40:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=129
05/24/2022 11:40:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=131
05/24/2022 11:40:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=133
05/24/2022 11:40:12 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.3558460421205519 on epoch=133
05/24/2022 11:40:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=134
05/24/2022 11:40:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=136
05/24/2022 11:40:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=138
05/24/2022 11:40:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=139
05/24/2022 11:40:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=141
05/24/2022 11:40:27 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.36676252096812845 on epoch=141
05/24/2022 11:40:27 - INFO - __main__ - Saving model with best Classification-F1: 0.3570509284794999 -> 0.36676252096812845 on epoch=141, global_step=850
05/24/2022 11:40:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=143
05/24/2022 11:40:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=144
05/24/2022 11:40:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=146
05/24/2022 11:40:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=148
05/24/2022 11:40:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=149
05/24/2022 11:40:43 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.41027429839860474 on epoch=149
05/24/2022 11:40:43 - INFO - __main__ - Saving model with best Classification-F1: 0.36676252096812845 -> 0.41027429839860474 on epoch=149, global_step=900
05/24/2022 11:40:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=151
05/24/2022 11:40:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=153
05/24/2022 11:40:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=154
05/24/2022 11:40:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=156
05/24/2022 11:40:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=158
05/24/2022 11:40:59 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.41656360657470537 on epoch=158
05/24/2022 11:40:59 - INFO - __main__ - Saving model with best Classification-F1: 0.41027429839860474 -> 0.41656360657470537 on epoch=158, global_step=950
05/24/2022 11:41:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=159
05/24/2022 11:41:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=161
05/24/2022 11:41:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=163
05/24/2022 11:41:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=164
05/24/2022 11:41:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=166
05/24/2022 11:41:15 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.42467354232060117 on epoch=166
05/24/2022 11:41:15 - INFO - __main__ - Saving model with best Classification-F1: 0.41656360657470537 -> 0.42467354232060117 on epoch=166, global_step=1000
05/24/2022 11:41:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=168
05/24/2022 11:41:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=169
05/24/2022 11:41:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=171
05/24/2022 11:41:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=173
05/24/2022 11:41:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=174
05/24/2022 11:41:31 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.47898799313893653 on epoch=174
05/24/2022 11:41:31 - INFO - __main__ - Saving model with best Classification-F1: 0.42467354232060117 -> 0.47898799313893653 on epoch=174, global_step=1050
05/24/2022 11:41:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=176
05/24/2022 11:41:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=178
05/24/2022 11:41:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=179
05/24/2022 11:41:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=181
05/24/2022 11:41:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=183
05/24/2022 11:41:47 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.38292165350988877 on epoch=183
05/24/2022 11:41:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=184
05/24/2022 11:41:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=186
05/24/2022 11:41:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=188
05/24/2022 11:41:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=189
05/24/2022 11:42:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=191
05/24/2022 11:42:03 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.4445576910632239 on epoch=191
05/24/2022 11:42:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=193
05/24/2022 11:42:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=194
05/24/2022 11:42:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=196
05/24/2022 11:42:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=198
05/24/2022 11:42:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=199
05/24/2022 11:42:19 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.40762400751301975 on epoch=199
05/24/2022 11:42:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=201
05/24/2022 11:42:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=203
05/24/2022 11:42:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=204
05/24/2022 11:42:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=206
05/24/2022 11:42:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=208
05/24/2022 11:42:35 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.38331211272387744 on epoch=208
05/24/2022 11:42:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=209
05/24/2022 11:42:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=211
05/24/2022 11:42:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=213
05/24/2022 11:42:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=214
05/24/2022 11:42:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=216
05/24/2022 11:42:51 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.3363486842105263 on epoch=216
05/24/2022 11:42:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=218
05/24/2022 11:42:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=219
05/24/2022 11:42:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=221
05/24/2022 11:43:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=223
05/24/2022 11:43:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=224
05/24/2022 11:43:07 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.26163793103448274 on epoch=224
05/24/2022 11:43:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=226
05/24/2022 11:43:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=228
05/24/2022 11:43:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=229
05/24/2022 11:43:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=231
05/24/2022 11:43:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=233
05/24/2022 11:43:23 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.38079794079794077 on epoch=233
05/24/2022 11:43:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=234
05/24/2022 11:43:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=236
05/24/2022 11:43:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=238
05/24/2022 11:43:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=239
05/24/2022 11:43:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=241
05/24/2022 11:43:39 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3451599671862182 on epoch=241
05/24/2022 11:43:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=243
05/24/2022 11:43:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=244
05/24/2022 11:43:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=246
05/24/2022 11:43:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=248
05/24/2022 11:43:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=249
05/24/2022 11:43:55 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.33396291208791207 on epoch=249
05/24/2022 11:43:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=251
05/24/2022 11:44:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=253
05/24/2022 11:44:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=254
05/24/2022 11:44:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=256
05/24/2022 11:44:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=258
05/24/2022 11:44:11 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.2405464675201517 on epoch=258
05/24/2022 11:44:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=259
05/24/2022 11:44:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=261
05/24/2022 11:44:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=263
05/24/2022 11:44:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=264
05/24/2022 11:44:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=266
05/24/2022 11:44:27 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.25511520737327187 on epoch=266
05/24/2022 11:44:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=268
05/24/2022 11:44:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=269
05/24/2022 11:44:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=271
05/24/2022 11:44:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=273
05/24/2022 11:44:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=274
05/24/2022 11:44:43 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.235477008672885 on epoch=274
05/24/2022 11:44:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=276
05/24/2022 11:44:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=278
05/24/2022 11:44:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=279
05/24/2022 11:44:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=281
05/24/2022 11:44:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=283
05/24/2022 11:45:00 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.25881415682447373 on epoch=283
05/24/2022 11:45:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=284
05/24/2022 11:45:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=286
05/24/2022 11:45:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=288
05/24/2022 11:45:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=289
05/24/2022 11:45:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=291
05/24/2022 11:45:16 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.3146230830153359 on epoch=291
05/24/2022 11:45:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=293
05/24/2022 11:45:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=294
05/24/2022 11:45:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=296
05/24/2022 11:45:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=298
05/24/2022 11:45:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=299
05/24/2022 11:45:32 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.2480089802670448 on epoch=299
05/24/2022 11:45:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=301
05/24/2022 11:45:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=303
05/24/2022 11:45:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=304
05/24/2022 11:45:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=306
05/24/2022 11:45:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=308
05/24/2022 11:45:48 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.35464049889948646 on epoch=308
05/24/2022 11:45:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=309
05/24/2022 11:45:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=311
05/24/2022 11:45:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=313
05/24/2022 11:45:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=314
05/24/2022 11:46:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=316
05/24/2022 11:46:04 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.28839826839826843 on epoch=316
05/24/2022 11:46:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=318
05/24/2022 11:46:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=319
05/24/2022 11:46:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=321
05/24/2022 11:46:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=323
05/24/2022 11:46:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=324
05/24/2022 11:46:20 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.2105014525704181 on epoch=324
05/24/2022 11:46:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=326
05/24/2022 11:46:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=328
05/24/2022 11:46:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=329
05/24/2022 11:46:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=331
05/24/2022 11:46:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=333
05/24/2022 11:46:37 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.21369768246281565 on epoch=333
05/24/2022 11:46:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=334
05/24/2022 11:46:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=336
05/24/2022 11:46:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=338
05/24/2022 11:46:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=339
05/24/2022 11:46:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=341
05/24/2022 11:46:53 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.24684601113172544 on epoch=341
05/24/2022 11:46:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=343
05/24/2022 11:46:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=344
05/24/2022 11:47:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=346
05/24/2022 11:47:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=348
05/24/2022 11:47:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=349
05/24/2022 11:47:09 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.2737667698658411 on epoch=349
05/24/2022 11:47:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=351
05/24/2022 11:47:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=353
05/24/2022 11:47:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=354
05/24/2022 11:47:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=356
05/24/2022 11:47:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=358
05/24/2022 11:47:26 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.1753703703703704 on epoch=358
05/24/2022 11:47:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=359
05/24/2022 11:47:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=361
05/24/2022 11:47:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=363
05/24/2022 11:47:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=364
05/24/2022 11:47:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=366
05/24/2022 11:47:42 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.2312260536398468 on epoch=366
05/24/2022 11:47:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=368
05/24/2022 11:47:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=369
05/24/2022 11:47:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=371
05/24/2022 11:47:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=373
05/24/2022 11:47:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=374
05/24/2022 11:47:58 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.199097919394733 on epoch=374
05/24/2022 11:48:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=376
05/24/2022 11:48:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=378
05/24/2022 11:48:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=379
05/24/2022 11:48:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=381
05/24/2022 11:48:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=383
05/24/2022 11:48:14 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.2640014242218134 on epoch=383
05/24/2022 11:48:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=384
05/24/2022 11:48:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=386
05/24/2022 11:48:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=388
05/24/2022 11:48:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=389
05/24/2022 11:48:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=391
05/24/2022 11:48:30 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.16348438795247305 on epoch=391
05/24/2022 11:48:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=393
05/24/2022 11:48:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=394
05/24/2022 11:48:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=396
05/24/2022 11:48:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=398
05/24/2022 11:48:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=399
05/24/2022 11:48:46 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.20425056199274352 on epoch=399
05/24/2022 11:48:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=401
05/24/2022 11:48:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=403
05/24/2022 11:48:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=404
05/24/2022 11:48:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=406
05/24/2022 11:49:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=408
05/24/2022 11:49:03 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.178125 on epoch=408
05/24/2022 11:49:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=409
05/24/2022 11:49:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=411
05/24/2022 11:49:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=413
05/24/2022 11:49:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=414
05/24/2022 11:49:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=416
05/24/2022 11:49:19 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.2098668280871671 on epoch=416
05/24/2022 11:49:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=418
05/24/2022 11:49:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=419
05/24/2022 11:49:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=421
05/24/2022 11:49:29 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=423
05/24/2022 11:49:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=424
05/24/2022 11:49:35 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.21938742878027448 on epoch=424
05/24/2022 11:49:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=426
05/24/2022 11:49:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=428
05/24/2022 11:49:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=429
05/24/2022 11:49:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=431
05/24/2022 11:49:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=433
05/24/2022 11:49:51 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.2364269992663243 on epoch=433
05/24/2022 11:49:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=434
05/24/2022 11:49:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=436
05/24/2022 11:49:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=438
05/24/2022 11:50:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=439
05/24/2022 11:50:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=441
05/24/2022 11:50:07 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.17606021597654298 on epoch=441
05/24/2022 11:50:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=443
05/24/2022 11:50:12 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=444
05/24/2022 11:50:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=446
05/24/2022 11:50:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=448
05/24/2022 11:50:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=449
05/24/2022 11:50:23 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.17295296167247387 on epoch=449
05/24/2022 11:50:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=451
05/24/2022 11:50:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=453
05/24/2022 11:50:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=454
05/24/2022 11:50:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=456
05/24/2022 11:50:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=458
05/24/2022 11:50:39 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.19884334340425308 on epoch=458
05/24/2022 11:50:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=459
05/24/2022 11:50:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=461
05/24/2022 11:50:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=463
05/24/2022 11:50:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=464
05/24/2022 11:50:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=466
05/24/2022 11:50:55 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.13693102297573928 on epoch=466
05/24/2022 11:50:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=468
05/24/2022 11:51:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=469
05/24/2022 11:51:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=471
05/24/2022 11:51:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=473
05/24/2022 11:51:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=474
05/24/2022 11:51:11 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.16303827751196173 on epoch=474
05/24/2022 11:51:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=476
05/24/2022 11:51:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=478
05/24/2022 11:51:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=479
05/24/2022 11:51:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=481
05/24/2022 11:51:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=483
05/24/2022 11:51:27 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.15090371022574411 on epoch=483
05/24/2022 11:51:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=484
05/24/2022 11:51:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=486
05/24/2022 11:51:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.07 on epoch=488
05/24/2022 11:51:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=489
05/24/2022 11:51:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=491
05/24/2022 11:51:44 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.13831091250446093 on epoch=491
05/24/2022 11:51:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=493
05/24/2022 11:51:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=494
05/24/2022 11:51:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=496
05/24/2022 11:51:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=498
05/24/2022 11:51:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=499
05/24/2022 11:51:58 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:51:58 - INFO - __main__ - Printing 3 examples
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:51:58 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:51:58 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:51:58 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:51:58 - INFO - __main__ - Printing 3 examples
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:51:58 - INFO - __main__ - ['contradiction']
05/24/2022 11:51:58 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:51:59 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:51:59 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:52:00 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.1707729104989379 on epoch=499
05/24/2022 11:52:00 - INFO - __main__ - save last model!
05/24/2022 11:52:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 11:52:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 11:52:00 - INFO - __main__ - Printing 3 examples
05/24/2022 11:52:00 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 11:52:00 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:00 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 11:52:00 - INFO - __main__ - ['entailment']
05/24/2022 11:52:00 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 11:52:00 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:00 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:52:00 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:52:01 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 11:52:14 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:52:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:52:15 - INFO - __main__ - Starting training!
05/24/2022 11:52:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_87_0.3_8_predictions.txt
05/24/2022 11:52:32 - INFO - __main__ - Classification-F1 on test data: 0.0278
05/24/2022 11:52:32 - INFO - __main__ - prefix=anli_32_87, lr=0.3, bsz=8, dev_performance=0.47898799313893653, test_performance=0.027793278117654543
05/24/2022 11:52:32 - INFO - __main__ - Running ... prefix=anli_32_87, lr=0.2, bsz=8 ...
05/24/2022 11:52:33 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:52:33 - INFO - __main__ - Printing 3 examples
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:52:33 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:52:33 - INFO - __main__ - Loaded 96 examples from train data
05/24/2022 11:52:33 - INFO - __main__ - Start tokenizing ... 96 instances
05/24/2022 11:52:33 - INFO - __main__ - Printing 3 examples
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: Philadelphia Free Press was a 1960s era underground newspaper published biweekly in Philadelphia, Pennsylvania from 1968 to 1972. Originally launched at Temple University in May 1968 as the monthly "Temple Free Press", it separated from Temple and became the "Philadelphia Free Press" in September 1968. [SEP] hypothesis: The Philadelphia Free Press was a 1960s era newspaper published biweekly from May 1968 to September 1968.
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: The 2000 KNVB Cup Final was a football match between NEC and Roda JC on 21 May 2000 at De Kuip, Rotterdam. It was the final match of the 1999–2000 KNVB Cup competition. Roda JC won 2–0 after goals from Bob Peeters and Eric van der Luer. It was their second KNVB Cup win. [SEP] hypothesis: The 1990 Star Champion Semi Final was a volley match between XXX and YYY on 24 April 1990 in Brussels. It was the semi final match. YYY lost 0–3. It was their first semi final game.
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ -  [anli] premise: Abraham Roqueñi Iglesias (born April 16, 1978) is a Spanish welterweight kickboxer. He was the K-1 MAX Spain 2004 tournament winner, and is a former ISKA, WAKO and WFCA world champion. He holds notable wins over Gago Drago, Luis Reis, Andy Souwer and Artur Kyshenko. [SEP] hypothesis: Abraham Roqueñi Iglesias (born April 14, 1998)
05/24/2022 11:52:33 - INFO - __main__ - ['contradiction']
05/24/2022 11:52:33 - INFO - __main__ - Tokenizing Input ...
05/24/2022 11:52:33 - INFO - __main__ - Tokenizing Output ...
05/24/2022 11:52:33 - INFO - __main__ - Loaded 96 examples from dev data
05/24/2022 11:52:49 - INFO - __main__ - load prompt embedding from ckpt
05/24/2022 11:52:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/24/2022 11:52:49 - INFO - __main__ - Starting training!
05/24/2022 11:52:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=1
05/24/2022 11:52:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=3
05/24/2022 11:52:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=4
05/24/2022 11:53:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=6
05/24/2022 11:53:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=8
05/24/2022 11:53:06 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/24/2022 11:53:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=8, global_step=50
05/24/2022 11:53:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=9
05/24/2022 11:53:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=11
05/24/2022 11:53:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=13
05/24/2022 11:53:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=14
05/24/2022 11:53:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=16
05/24/2022 11:53:22 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.15873015873015875 on epoch=16
05/24/2022 11:53:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=18
05/24/2022 11:53:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=19
05/24/2022 11:53:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=21
05/24/2022 11:53:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=23
05/24/2022 11:53:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=24
05/24/2022 11:53:39 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/24/2022 11:53:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.44 on epoch=26
05/24/2022 11:53:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=28
05/24/2022 11:53:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=29
05/24/2022 11:53:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=31
05/24/2022 11:53:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=33
05/24/2022 11:53:54 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/24/2022 11:53:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=34
05/24/2022 11:53:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=36
05/24/2022 11:54:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=38
05/24/2022 11:54:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=39
05/24/2022 11:54:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=41
05/24/2022 11:54:10 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16272965879265092 on epoch=41
05/24/2022 11:54:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=43
05/24/2022 11:54:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=44
05/24/2022 11:54:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=46
05/24/2022 11:54:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=48
05/24/2022 11:54:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=49
05/24/2022 11:54:25 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16272965879265092 on epoch=49
05/24/2022 11:54:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=51
05/24/2022 11:54:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=53
05/24/2022 11:54:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=54
05/24/2022 11:54:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=56
05/24/2022 11:54:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=58
05/24/2022 11:54:41 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.16272965879265092 on epoch=58
05/24/2022 11:54:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=59
05/24/2022 11:54:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=61
05/24/2022 11:54:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=63
05/24/2022 11:54:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=64
05/24/2022 11:54:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=66
05/24/2022 11:54:57 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16272965879265092 on epoch=66
05/24/2022 11:54:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=68
05/24/2022 11:55:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=69
05/24/2022 11:55:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=71
05/24/2022 11:55:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=73
05/24/2022 11:55:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=74
05/24/2022 11:55:13 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16272965879265092 on epoch=74
05/24/2022 11:55:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=76
05/24/2022 11:55:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=78
05/24/2022 11:55:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=79
05/24/2022 11:55:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=81
05/24/2022 11:55:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=83
05/24/2022 11:55:28 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16272965879265092 on epoch=83
05/24/2022 11:55:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=84
05/24/2022 11:55:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=86
05/24/2022 11:55:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=88
05/24/2022 11:55:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=89
05/24/2022 11:55:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=91
05/24/2022 11:55:44 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.19355766465343685 on epoch=91
05/24/2022 11:55:44 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19355766465343685 on epoch=91, global_step=550
05/24/2022 11:55:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=93
05/24/2022 11:55:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=94
05/24/2022 11:55:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=96
05/24/2022 11:55:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=98
05/24/2022 11:55:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=99
05/24/2022 11:56:00 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.1986376620522962 on epoch=99
05/24/2022 11:56:00 - INFO - __main__ - Saving model with best Classification-F1: 0.19355766465343685 -> 0.1986376620522962 on epoch=99, global_step=600
05/24/2022 11:56:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=101
05/24/2022 11:56:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=103
05/24/2022 11:56:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=104
05/24/2022 11:56:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=106
05/24/2022 11:56:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=108
05/24/2022 11:56:15 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.20370370370370372 on epoch=108
05/24/2022 11:56:15 - INFO - __main__ - Saving model with best Classification-F1: 0.1986376620522962 -> 0.20370370370370372 on epoch=108, global_step=650
05/24/2022 11:56:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=109
05/24/2022 11:56:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=111
05/24/2022 11:56:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=113
05/24/2022 11:56:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=114
05/24/2022 11:56:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=116
05/24/2022 11:56:31 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.21656600517687663 on epoch=116
05/24/2022 11:56:31 - INFO - __main__ - Saving model with best Classification-F1: 0.20370370370370372 -> 0.21656600517687663 on epoch=116, global_step=700
05/24/2022 11:56:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=118
05/24/2022 11:56:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=119
05/24/2022 11:56:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=121
05/24/2022 11:56:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=123
05/24/2022 11:56:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=124
05/24/2022 11:56:46 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.252567415358113 on epoch=124
05/24/2022 11:56:46 - INFO - __main__ - Saving model with best Classification-F1: 0.21656600517687663 -> 0.252567415358113 on epoch=124, global_step=750
05/24/2022 11:56:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=126
05/24/2022 11:56:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=128
05/24/2022 11:56:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=129
05/24/2022 11:56:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=131
05/24/2022 11:57:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=133
05/24/2022 11:57:02 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.2970760233918129 on epoch=133
05/24/2022 11:57:02 - INFO - __main__ - Saving model with best Classification-F1: 0.252567415358113 -> 0.2970760233918129 on epoch=133, global_step=800
05/24/2022 11:57:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=134
05/24/2022 11:57:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=136
05/24/2022 11:57:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=138
05/24/2022 11:57:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=139
05/24/2022 11:57:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=141
05/24/2022 11:57:18 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.30851063829787234 on epoch=141
05/24/2022 11:57:18 - INFO - __main__ - Saving model with best Classification-F1: 0.2970760233918129 -> 0.30851063829787234 on epoch=141, global_step=850
05/24/2022 11:57:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=143
05/24/2022 11:57:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=144
05/24/2022 11:57:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=146
05/24/2022 11:57:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=148
05/24/2022 11:57:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=149
05/24/2022 11:57:34 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.2970760233918129 on epoch=149
05/24/2022 11:57:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=151
05/24/2022 11:57:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=153
05/24/2022 11:57:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=154
05/24/2022 11:57:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=156
05/24/2022 11:57:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=158
05/24/2022 11:57:49 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.30705705705705705 on epoch=158
05/24/2022 11:57:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=159
05/24/2022 11:57:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=161
05/24/2022 11:57:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=163
05/24/2022 11:58:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=164
05/24/2022 11:58:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=166
05/24/2022 11:58:05 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.35332871012482664 on epoch=166
05/24/2022 11:58:05 - INFO - __main__ - Saving model with best Classification-F1: 0.30851063829787234 -> 0.35332871012482664 on epoch=166, global_step=1000
05/24/2022 11:58:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=168
05/24/2022 11:58:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=169
05/24/2022 11:58:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=171
05/24/2022 11:58:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=173
05/24/2022 11:58:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=174
05/24/2022 11:58:21 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.3193568336425479 on epoch=174
05/24/2022 11:58:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=176
05/24/2022 11:58:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=178
05/24/2022 11:58:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=179
05/24/2022 11:58:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=181
05/24/2022 11:58:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=183
05/24/2022 11:58:36 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3558460421205519 on epoch=183
05/24/2022 11:58:36 - INFO - __main__ - Saving model with best Classification-F1: 0.35332871012482664 -> 0.3558460421205519 on epoch=183, global_step=1100
05/24/2022 11:58:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=184
05/24/2022 11:58:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=186
05/24/2022 11:58:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=188
05/24/2022 11:58:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=189
05/24/2022 11:58:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=191
05/24/2022 11:58:52 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.39665607427615823 on epoch=191
05/24/2022 11:58:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3558460421205519 -> 0.39665607427615823 on epoch=191, global_step=1150
05/24/2022 11:58:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=193
05/24/2022 11:58:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=194
05/24/2022 11:59:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=196
05/24/2022 11:59:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=198
05/24/2022 11:59:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=199
05/24/2022 11:59:07 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3427741466957153 on epoch=199
05/24/2022 11:59:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=201
05/24/2022 11:59:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=203
05/24/2022 11:59:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=204
05/24/2022 11:59:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=206
05/24/2022 11:59:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=208
05/24/2022 11:59:23 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.35070336391437307 on epoch=208
05/24/2022 11:59:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=209
05/24/2022 11:59:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=211
05/24/2022 11:59:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=213
05/24/2022 11:59:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=214
05/24/2022 11:59:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=216
05/24/2022 11:59:38 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.36890634337915135 on epoch=216
05/24/2022 11:59:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=218
05/24/2022 11:59:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=219
05/24/2022 11:59:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=221
05/24/2022 11:59:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=223
05/24/2022 11:59:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=224
05/24/2022 11:59:54 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.380803787407561 on epoch=224
05/24/2022 11:59:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=226
05/24/2022 11:59:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=228
05/24/2022 12:00:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=229
05/24/2022 12:00:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=231
05/24/2022 12:00:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=233
05/24/2022 12:00:09 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.3943054725518655 on epoch=233
05/24/2022 12:00:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=234
05/24/2022 12:00:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=236
05/24/2022 12:00:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=238
05/24/2022 12:00:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=239
05/24/2022 12:00:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=241
05/24/2022 12:00:26 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.4117371936920809 on epoch=241
05/24/2022 12:00:26 - INFO - __main__ - Saving model with best Classification-F1: 0.39665607427615823 -> 0.4117371936920809 on epoch=241, global_step=1450
05/24/2022 12:00:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=243
05/24/2022 12:00:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=244
05/24/2022 12:00:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=246
05/24/2022 12:00:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=248
05/24/2022 12:00:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=249
05/24/2022 12:00:41 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.37815478392614904 on epoch=249
05/24/2022 12:00:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=251
05/24/2022 12:00:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=253
05/24/2022 12:00:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=254
05/24/2022 12:00:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=256
05/24/2022 12:00:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=258
05/24/2022 12:00:57 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.3840750741740841 on epoch=258
05/24/2022 12:01:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=259
05/24/2022 12:01:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=261
05/24/2022 12:01:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.31 on epoch=263
05/24/2022 12:01:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=264
05/24/2022 12:01:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=266
05/24/2022 12:01:13 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.43036194668623134 on epoch=266
05/24/2022 12:01:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4117371936920809 -> 0.43036194668623134 on epoch=266, global_step=1600
05/24/2022 12:01:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=268
05/24/2022 12:01:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=269
05/24/2022 12:01:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=271
05/24/2022 12:01:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=273
05/24/2022 12:01:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=274
05/24/2022 12:01:29 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.3679524807826695 on epoch=274
05/24/2022 12:01:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=276
05/24/2022 12:01:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=278
05/24/2022 12:01:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=279
05/24/2022 12:01:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=281
05/24/2022 12:01:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=283
05/24/2022 12:01:45 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3951935791099152 on epoch=283
05/24/2022 12:01:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=284
05/24/2022 12:01:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=286
05/24/2022 12:01:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=288
05/24/2022 12:01:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=289
05/24/2022 12:01:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=291
05/24/2022 12:02:00 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.379515455304929 on epoch=291
05/24/2022 12:02:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=293
05/24/2022 12:02:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=294
05/24/2022 12:02:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=296
05/24/2022 12:02:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=298
05/24/2022 12:02:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=299
05/24/2022 12:02:16 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.2653208177960653 on epoch=299
05/24/2022 12:02:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=301
05/24/2022 12:02:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=303
05/24/2022 12:02:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=304
05/24/2022 12:02:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=306
05/24/2022 12:02:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=308
05/24/2022 12:02:32 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.3048251416589701 on epoch=308
05/24/2022 12:02:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=309
05/24/2022 12:02:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=311
05/24/2022 12:02:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=313
05/24/2022 12:02:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=314
05/24/2022 12:02:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=316
05/24/2022 12:02:47 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.30688393134513825 on epoch=316
05/24/2022 12:02:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=318
05/24/2022 12:02:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=319
05/24/2022 12:02:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=321
05/24/2022 12:02:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=323
05/24/2022 12:03:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=324
05/24/2022 12:03:03 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.3523970824340999 on epoch=324
05/24/2022 12:03:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=326
05/24/2022 12:03:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=328
05/24/2022 12:03:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=329
05/24/2022 12:03:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=331
05/24/2022 12:03:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=333
05/24/2022 12:03:19 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.2561827646038172 on epoch=333
05/24/2022 12:03:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=334
05/24/2022 12:03:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=336
05/24/2022 12:03:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=338
05/24/2022 12:03:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=339
05/24/2022 12:03:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=341
05/24/2022 12:03:35 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.3189674908424909 on epoch=341
05/24/2022 12:03:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=343
05/24/2022 12:03:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=344
05/24/2022 12:03:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=346
05/24/2022 12:03:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=348
05/24/2022 12:03:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=349
05/24/2022 12:03:51 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.24448984681495975 on epoch=349
05/24/2022 12:03:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=351
05/24/2022 12:03:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=353
05/24/2022 12:03:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=354
05/24/2022 12:04:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=356
05/24/2022 12:04:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=358
05/24/2022 12:04:07 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.30589836660617054 on epoch=358
05/24/2022 12:04:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=359
05/24/2022 12:04:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.22 on epoch=361
05/24/2022 12:04:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=363
05/24/2022 12:04:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=364
05/24/2022 12:04:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=366
05/24/2022 12:04:23 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.24616255483749233 on epoch=366
05/24/2022 12:04:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.24 on epoch=368
05/24/2022 12:04:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=369
05/24/2022 12:04:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=371
05/24/2022 12:04:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=373
05/24/2022 12:04:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=374
05/24/2022 12:04:39 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.2535363974595062 on epoch=374
05/24/2022 12:04:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=376
05/24/2022 12:04:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.24 on epoch=378
05/24/2022 12:04:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=379
05/24/2022 12:04:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=381
05/24/2022 12:04:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=383
05/24/2022 12:04:55 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.23920211760257137 on epoch=383
05/24/2022 12:04:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=384
05/24/2022 12:05:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=386
05/24/2022 12:05:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=388
05/24/2022 12:05:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=389
05/24/2022 12:05:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=391
05/24/2022 12:05:11 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.2235483364720653 on epoch=391
05/24/2022 12:05:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=393
05/24/2022 12:05:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=394
05/24/2022 12:05:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=396
05/24/2022 12:05:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=398
05/24/2022 12:05:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=399
05/24/2022 12:05:27 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.27526664856688515 on epoch=399
05/24/2022 12:05:29 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=401
05/24/2022 12:05:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=403
05/24/2022 12:05:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.18 on epoch=404
05/24/2022 12:05:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=406
05/24/2022 12:05:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=408
05/24/2022 12:05:43 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.25417079679756005 on epoch=408
05/24/2022 12:05:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=409
05/24/2022 12:05:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=411
05/24/2022 12:05:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=413
05/24/2022 12:05:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=414
05/24/2022 12:05:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=416
05/24/2022 12:05:59 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.2558441558441558 on epoch=416
05/24/2022 12:06:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=418
05/24/2022 12:06:04 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=419
05/24/2022 12:06:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=421
05/24/2022 12:06:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=423
05/24/2022 12:06:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=424
05/24/2022 12:06:15 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.32444638694638694 on epoch=424
05/24/2022 12:06:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=426
05/24/2022 12:06:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=428
05/24/2022 12:06:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=429
05/24/2022 12:06:26 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=431
05/24/2022 12:06:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=433
05/24/2022 12:06:31 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.28270203359858537 on epoch=433
05/24/2022 12:06:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=434
05/24/2022 12:06:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=436
05/24/2022 12:06:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=438
05/24/2022 12:06:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=439
05/24/2022 12:06:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=441
05/24/2022 12:06:47 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.23913438807055828 on epoch=441
05/24/2022 12:06:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=443
05/24/2022 12:06:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=444
05/24/2022 12:06:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=446
05/24/2022 12:06:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=448
05/24/2022 12:07:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=449
05/24/2022 12:07:03 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.19799278886948166 on epoch=449
05/24/2022 12:07:06 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=451
05/24/2022 12:07:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=453
05/24/2022 12:07:11 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=454
05/24/2022 12:07:14 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=456
05/24/2022 12:07:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=458
05/24/2022 12:07:20 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.26215867675918136 on epoch=458
05/24/2022 12:07:22 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=459
05/24/2022 12:07:25 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=461
05/24/2022 12:07:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=463
05/24/2022 12:07:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=464
05/24/2022 12:07:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=466
05/24/2022 12:07:36 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.22022006012409853 on epoch=466
05/24/2022 12:07:38 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=468
05/24/2022 12:07:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=469
05/24/2022 12:07:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=471
05/24/2022 12:07:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=473
05/24/2022 12:07:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=474
05/24/2022 12:07:52 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.21241709653647756 on epoch=474
05/24/2022 12:07:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=476
05/24/2022 12:07:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=478
05/24/2022 12:08:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=479
05/24/2022 12:08:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=481
05/24/2022 12:08:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=483
05/24/2022 12:08:08 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.18853857110200634 on epoch=483
05/24/2022 12:08:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=484
05/24/2022 12:08:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=486
05/24/2022 12:08:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=488
05/24/2022 12:08:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=489
05/24/2022 12:08:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=491
05/24/2022 12:08:24 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.18520705400724294 on epoch=491
05/24/2022 12:08:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=493
05/24/2022 12:08:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=494
05/24/2022 12:08:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=496
05/24/2022 12:08:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=498
05/24/2022 12:08:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=499
05/24/2022 12:08:41 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.18835280687617512 on epoch=499
05/24/2022 12:08:41 - INFO - __main__ - save last model!
05/24/2022 12:08:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/24/2022 12:08:41 - INFO - __main__ - Start tokenizing ... 1000 instances
05/24/2022 12:08:41 - INFO - __main__ - Printing 3 examples
05/24/2022 12:08:41 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/24/2022 12:08:41 - INFO - __main__ - ['contradiction']
05/24/2022 12:08:41 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/24/2022 12:08:41 - INFO - __main__ - ['entailment']
05/24/2022 12:08:41 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/24/2022 12:08:41 - INFO - __main__ - ['contradiction']
05/24/2022 12:08:41 - INFO - __main__ - Tokenizing Input ...
05/24/2022 12:08:42 - INFO - __main__ - Tokenizing Output ...
05/24/2022 12:08:43 - INFO - __main__ - Loaded 1000 examples from test data
05/24/2022 12:09:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down32shot/singletask-anli/anli_32_87_0.2_8_predictions.txt
05/24/2022 12:09:13 - INFO - __main__ - Classification-F1 on test data: 0.0330
05/24/2022 12:09:13 - INFO - __main__ - prefix=anli_32_87, lr=0.2, bsz=8, dev_performance=0.43036194668623134, test_performance=0.033046707933019115
