05/18/2022 14:55:01 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
05/18/2022 14:55:01 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
05/18/2022 14:55:01 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
05/18/2022 14:55:01 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
05/18/2022 14:55:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/18/2022 14:55:01 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/18/2022 14:55:01 - INFO - __main__ - args.device: cuda:0
05/18/2022 14:55:01 - INFO - __main__ - args.device: cuda:1
05/18/2022 14:55:01 - INFO - __main__ - Using 2 gpus
05/18/2022 14:55:01 - INFO - __main__ - Using 2 gpus
05/18/2022 14:55:01 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/18/2022 14:55:01 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/18/2022 14:55:06 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
05/18/2022 14:55:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 14:55:07 - INFO - __main__ - Printing 3 examples
05/18/2022 14:55:07 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 14:55:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 14:55:07 - INFO - __main__ - Printing 3 examples
05/18/2022 14:55:07 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 14:55:07 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 14:55:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 14:55:07 - INFO - __main__ - Printing 3 examples
05/18/2022 14:55:07 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 14:55:07 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 14:55:07 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 14:55:07 - INFO - __main__ - Printing 3 examples
05/18/2022 14:55:07 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 14:55:07 - INFO - __main__ - ['others']
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Input ...
05/18/2022 14:55:07 - INFO - __main__ - Tokenizing Output ...
05/18/2022 14:55:07 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 14:55:07 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 14:55:13 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 14:55:13 - INFO - __main__ - task name: emo
05/18/2022 14:55:13 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 14:55:13 - INFO - __main__ - task name: emo
05/18/2022 14:55:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 14:55:13 - INFO - __main__ - Starting training!
05/18/2022 14:55:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 14:55:14 - INFO - __main__ - Starting training!
05/18/2022 14:55:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.18 on epoch=2
05/18/2022 14:55:17 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=4
05/18/2022 14:55:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=7
05/18/2022 14:55:19 - INFO - __main__ - Step 40 Global step 40 Train loss 1.46 on epoch=9
05/18/2022 14:55:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=12
05/18/2022 14:55:21 - INFO - __main__ - Global step 50 Train loss 2.68 Classification-F1 0.13167388167388167 on epoch=12
05/18/2022 14:55:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13167388167388167 on epoch=12, global_step=50
05/18/2022 14:55:22 - INFO - __main__ - Step 60 Global step 60 Train loss 1.23 on epoch=14
05/18/2022 14:55:23 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=17
05/18/2022 14:55:25 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=19
05/18/2022 14:55:26 - INFO - __main__ - Step 90 Global step 90 Train loss 2.30 on epoch=22
05/18/2022 14:55:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.38 on epoch=24
05/18/2022 14:55:28 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.16731898238747556 on epoch=24
05/18/2022 14:55:28 - INFO - __main__ - Saving model with best Classification-F1: 0.13167388167388167 -> 0.16731898238747556 on epoch=24, global_step=100
05/18/2022 14:55:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=27
05/18/2022 14:55:30 - INFO - __main__ - Step 120 Global step 120 Train loss 1.04 on epoch=29
05/18/2022 14:55:31 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=32
05/18/2022 14:55:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.10 on epoch=34
05/18/2022 14:55:34 - INFO - __main__ - Step 150 Global step 150 Train loss 1.01 on epoch=37
05/18/2022 14:55:34 - INFO - __main__ - Global step 150 Train loss 1.05 Classification-F1 0.20467032967032966 on epoch=37
05/18/2022 14:55:34 - INFO - __main__ - Saving model with best Classification-F1: 0.16731898238747556 -> 0.20467032967032966 on epoch=37, global_step=150
05/18/2022 14:55:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
05/18/2022 14:55:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
05/18/2022 14:55:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
05/18/2022 14:55:39 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=47
05/18/2022 14:55:40 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
05/18/2022 14:55:41 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.20462794918330307 on epoch=49
05/18/2022 14:55:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=52
05/18/2022 14:55:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/18/2022 14:55:44 - INFO - __main__ - Step 230 Global step 230 Train loss 1.02 on epoch=57
05/18/2022 14:55:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/18/2022 14:55:47 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
05/18/2022 14:55:47 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.1237183868762816 on epoch=62
05/18/2022 14:55:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/18/2022 14:55:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
05/18/2022 14:55:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=69
05/18/2022 14:55:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
05/18/2022 14:55:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/18/2022 14:55:54 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.23333333333333336 on epoch=74
05/18/2022 14:55:54 - INFO - __main__ - Saving model with best Classification-F1: 0.20467032967032966 -> 0.23333333333333336 on epoch=74, global_step=300
05/18/2022 14:55:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
05/18/2022 14:55:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.96 on epoch=79
05/18/2022 14:55:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=82
05/18/2022 14:55:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=84
05/18/2022 14:56:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=87
05/18/2022 14:56:00 - INFO - __main__ - Global step 350 Train loss 0.95 Classification-F1 0.35590909090909095 on epoch=87
05/18/2022 14:56:00 - INFO - __main__ - Saving model with best Classification-F1: 0.23333333333333336 -> 0.35590909090909095 on epoch=87, global_step=350
05/18/2022 14:56:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/18/2022 14:56:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/18/2022 14:56:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.04 on epoch=94
05/18/2022 14:56:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.97 on epoch=97
05/18/2022 14:56:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
05/18/2022 14:56:07 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.31721967963386727 on epoch=99
05/18/2022 14:56:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=102
05/18/2022 14:56:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.01 on epoch=104
05/18/2022 14:56:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
05/18/2022 14:56:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
05/18/2022 14:56:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
05/18/2022 14:56:14 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.20787037037037037 on epoch=112
05/18/2022 14:56:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.89 on epoch=114
05/18/2022 14:56:16 - INFO - __main__ - Step 470 Global step 470 Train loss 1.00 on epoch=117
05/18/2022 14:56:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
05/18/2022 14:56:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=122
05/18/2022 14:56:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.84 on epoch=124
05/18/2022 14:56:20 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.2714472309299895 on epoch=124
05/18/2022 14:56:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.81 on epoch=127
05/18/2022 14:56:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.98 on epoch=129
05/18/2022 14:56:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=132
05/18/2022 14:56:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=134
05/18/2022 14:56:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.81 on epoch=137
05/18/2022 14:56:27 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.4278902384165542 on epoch=137
05/18/2022 14:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.35590909090909095 -> 0.4278902384165542 on epoch=137, global_step=550
05/18/2022 14:56:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=139
05/18/2022 14:56:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=142
05/18/2022 14:56:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.80 on epoch=144
05/18/2022 14:56:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=147
05/18/2022 14:56:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.70 on epoch=149
05/18/2022 14:56:33 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.45240292041078306 on epoch=149
05/18/2022 14:56:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4278902384165542 -> 0.45240292041078306 on epoch=149, global_step=600
05/18/2022 14:56:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=152
05/18/2022 14:56:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
05/18/2022 14:56:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.85 on epoch=157
05/18/2022 14:56:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
05/18/2022 14:56:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=162
05/18/2022 14:56:40 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.37473684210526315 on epoch=162
05/18/2022 14:56:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.75 on epoch=164
05/18/2022 14:56:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=167
05/18/2022 14:56:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=169
05/18/2022 14:56:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.74 on epoch=172
05/18/2022 14:56:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.69 on epoch=174
05/18/2022 14:56:46 - INFO - __main__ - Global step 700 Train loss 0.74 Classification-F1 0.5099479392252636 on epoch=174
05/18/2022 14:56:47 - INFO - __main__ - Saving model with best Classification-F1: 0.45240292041078306 -> 0.5099479392252636 on epoch=174, global_step=700
05/18/2022 14:56:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
05/18/2022 14:56:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.68 on epoch=179
05/18/2022 14:56:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.73 on epoch=182
05/18/2022 14:56:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.72 on epoch=184
05/18/2022 14:56:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.79 on epoch=187
05/18/2022 14:56:53 - INFO - __main__ - Global step 750 Train loss 0.71 Classification-F1 0.4421961550993809 on epoch=187
05/18/2022 14:56:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.71 on epoch=189
05/18/2022 14:56:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=192
05/18/2022 14:56:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=194
05/18/2022 14:56:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=197
05/18/2022 14:56:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=199
05/18/2022 14:57:00 - INFO - __main__ - Global step 800 Train loss 0.68 Classification-F1 0.5292084726867335 on epoch=199
05/18/2022 14:57:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5099479392252636 -> 0.5292084726867335 on epoch=199, global_step=800
05/18/2022 14:57:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=202
05/18/2022 14:57:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=204
05/18/2022 14:57:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=207
05/18/2022 14:57:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.66 on epoch=209
05/18/2022 14:57:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=212
05/18/2022 14:57:06 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.5165201465201466 on epoch=212
05/18/2022 14:57:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=214
05/18/2022 14:57:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.65 on epoch=217
05/18/2022 14:57:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=219
05/18/2022 14:57:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.65 on epoch=222
05/18/2022 14:57:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=224
05/18/2022 14:57:13 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.6607378129117258 on epoch=224
05/18/2022 14:57:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5292084726867335 -> 0.6607378129117258 on epoch=224, global_step=900
05/18/2022 14:57:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=227
05/18/2022 14:57:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=229
05/18/2022 14:57:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=232
05/18/2022 14:57:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=234
05/18/2022 14:57:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=237
05/18/2022 14:57:20 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.5820289855072465 on epoch=237
05/18/2022 14:57:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=239
05/18/2022 14:57:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=242
05/18/2022 14:57:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=244
05/18/2022 14:57:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=247
05/18/2022 14:57:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=249
05/18/2022 14:57:26 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.5699570071423617 on epoch=249
05/18/2022 14:57:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=252
05/18/2022 14:57:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=254
05/18/2022 14:57:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=257
05/18/2022 14:57:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=259
05/18/2022 14:57:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=262
05/18/2022 14:57:33 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.6853264978264978 on epoch=262
05/18/2022 14:57:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6607378129117258 -> 0.6853264978264978 on epoch=262, global_step=1050
05/18/2022 14:57:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=264
05/18/2022 14:57:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=267
05/18/2022 14:57:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=269
05/18/2022 14:57:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=272
05/18/2022 14:57:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=274
05/18/2022 14:57:40 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.6228692451627496 on epoch=274
05/18/2022 14:57:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=277
05/18/2022 14:57:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=279
05/18/2022 14:57:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=282
05/18/2022 14:57:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=284
05/18/2022 14:57:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=287
05/18/2022 14:57:47 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.6309842056212291 on epoch=287
05/18/2022 14:57:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=289
05/18/2022 14:57:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=292
05/18/2022 14:57:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=294
05/18/2022 14:57:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=297
05/18/2022 14:57:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=299
05/18/2022 14:57:54 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.5927816736792894 on epoch=299
05/18/2022 14:57:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=302
05/18/2022 14:57:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=304
05/18/2022 14:57:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=307
05/18/2022 14:57:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=309
05/18/2022 14:58:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
05/18/2022 14:58:01 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7234546703296703 on epoch=312
05/18/2022 14:58:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6853264978264978 -> 0.7234546703296703 on epoch=312, global_step=1250
05/18/2022 14:58:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
05/18/2022 14:58:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=317
05/18/2022 14:58:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
05/18/2022 14:58:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=322
05/18/2022 14:58:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=324
05/18/2022 14:58:08 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.7526578998353192 on epoch=324
05/18/2022 14:58:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7234546703296703 -> 0.7526578998353192 on epoch=324, global_step=1300
05/18/2022 14:58:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=327
05/18/2022 14:58:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=329
05/18/2022 14:58:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=332
05/18/2022 14:58:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
05/18/2022 14:58:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=337
05/18/2022 14:58:15 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.7337591602297484 on epoch=337
05/18/2022 14:58:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=339
05/18/2022 14:58:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=342
05/18/2022 14:58:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=344
05/18/2022 14:58:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=347
05/18/2022 14:58:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=349
05/18/2022 14:58:22 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7202581636011263 on epoch=349
05/18/2022 14:58:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=352
05/18/2022 14:58:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=354
05/18/2022 14:58:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=357
05/18/2022 14:58:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=359
05/18/2022 14:58:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=362
05/18/2022 14:58:30 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7565488565488565 on epoch=362
05/18/2022 14:58:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7526578998353192 -> 0.7565488565488565 on epoch=362, global_step=1450
05/18/2022 14:58:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=364
05/18/2022 14:58:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=367
05/18/2022 14:58:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=369
05/18/2022 14:58:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=372
05/18/2022 14:58:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
05/18/2022 14:58:38 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.761908656677354 on epoch=374
05/18/2022 14:58:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7565488565488565 -> 0.761908656677354 on epoch=374, global_step=1500
05/18/2022 14:58:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=377
05/18/2022 14:58:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=379
05/18/2022 14:58:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=382
05/18/2022 14:58:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=384
05/18/2022 14:58:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=387
05/18/2022 14:58:47 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.6417748917748918 on epoch=387
05/18/2022 14:58:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=389
05/18/2022 14:58:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=392
05/18/2022 14:58:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
05/18/2022 14:58:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=397
05/18/2022 14:58:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=399
05/18/2022 14:58:56 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.7303729485524617 on epoch=399
05/18/2022 14:58:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=402
05/18/2022 14:58:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=404
05/18/2022 14:59:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=407
05/18/2022 14:59:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=409
05/18/2022 14:59:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
05/18/2022 14:59:04 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.708935384000799 on epoch=412
05/18/2022 14:59:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
05/18/2022 14:59:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=417
05/18/2022 14:59:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=419
05/18/2022 14:59:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=422
05/18/2022 14:59:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
05/18/2022 14:59:12 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.7247512528584635 on epoch=424
05/18/2022 14:59:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
05/18/2022 14:59:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=429
05/18/2022 14:59:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
05/18/2022 14:59:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/18/2022 14:59:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
05/18/2022 14:59:21 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.7121025660053761 on epoch=437
05/18/2022 14:59:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
05/18/2022 14:59:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=442
05/18/2022 14:59:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
05/18/2022 14:59:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
05/18/2022 14:59:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/18/2022 14:59:29 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7068360891437552 on epoch=449
05/18/2022 14:59:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=452
05/18/2022 14:59:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
05/18/2022 14:59:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
05/18/2022 14:59:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=459
05/18/2022 14:59:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=462
05/18/2022 14:59:38 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.69168457399224 on epoch=462
05/18/2022 14:59:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=464
05/18/2022 14:59:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=467
05/18/2022 14:59:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/18/2022 14:59:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=472
05/18/2022 14:59:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
05/18/2022 14:59:45 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.7065972222222222 on epoch=474
05/18/2022 14:59:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/18/2022 14:59:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
05/18/2022 14:59:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
05/18/2022 14:59:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=484
05/18/2022 14:59:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=487
05/18/2022 14:59:54 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7301963088408101 on epoch=487
05/18/2022 14:59:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
05/18/2022 14:59:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=492
05/18/2022 14:59:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/18/2022 15:00:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=497
05/18/2022 15:00:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
05/18/2022 15:00:02 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.682937834224599 on epoch=499
05/18/2022 15:00:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
05/18/2022 15:00:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/18/2022 15:00:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/18/2022 15:00:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
05/18/2022 15:00:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/18/2022 15:00:11 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.6904268292682927 on epoch=512
05/18/2022 15:00:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/18/2022 15:00:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/18/2022 15:00:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/18/2022 15:00:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/18/2022 15:00:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
05/18/2022 15:00:19 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7225906120023767 on epoch=524
05/18/2022 15:00:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
05/18/2022 15:00:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/18/2022 15:00:23 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/18/2022 15:00:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
05/18/2022 15:00:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=537
05/18/2022 15:00:27 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.6533492822966507 on epoch=537
05/18/2022 15:00:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
05/18/2022 15:00:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
05/18/2022 15:00:31 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/18/2022 15:00:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/18/2022 15:00:34 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/18/2022 15:00:35 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.6487903225806453 on epoch=549
05/18/2022 15:00:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
05/18/2022 15:00:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=554
05/18/2022 15:00:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
05/18/2022 15:00:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/18/2022 15:00:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/18/2022 15:00:44 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6660903714983412 on epoch=562
05/18/2022 15:00:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/18/2022 15:00:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/18/2022 15:00:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
05/18/2022 15:00:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/18/2022 15:00:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=574
05/18/2022 15:00:52 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.6954954954954955 on epoch=574
05/18/2022 15:00:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/18/2022 15:00:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=579
05/18/2022 15:00:56 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
05/18/2022 15:00:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
05/18/2022 15:00:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 15:01:00 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7238032934807128 on epoch=587
05/18/2022 15:01:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
05/18/2022 15:01:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/18/2022 15:01:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/18/2022 15:01:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/18/2022 15:01:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/18/2022 15:01:08 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6705008488964346 on epoch=599
05/18/2022 15:01:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
05/18/2022 15:01:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/18/2022 15:01:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=607
05/18/2022 15:01:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/18/2022 15:01:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/18/2022 15:01:16 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.6876793150370517 on epoch=612
05/18/2022 15:01:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
05/18/2022 15:01:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/18/2022 15:01:21 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
05/18/2022 15:01:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
05/18/2022 15:01:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
05/18/2022 15:01:25 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.6794153225806452 on epoch=624
05/18/2022 15:01:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/18/2022 15:01:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/18/2022 15:01:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 15:01:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/18/2022 15:01:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/18/2022 15:01:33 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6682319518716578 on epoch=637
05/18/2022 15:01:35 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 15:01:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
05/18/2022 15:01:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/18/2022 15:01:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 15:01:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=649
05/18/2022 15:01:42 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.673202614379085 on epoch=649
05/18/2022 15:01:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/18/2022 15:01:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
05/18/2022 15:01:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 15:01:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/18/2022 15:01:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/18/2022 15:01:50 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6879984051036683 on epoch=662
05/18/2022 15:01:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=664
05/18/2022 15:01:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=667
05/18/2022 15:01:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/18/2022 15:01:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/18/2022 15:01:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/18/2022 15:01:59 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6455653907904626 on epoch=674
05/18/2022 15:02:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/18/2022 15:02:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/18/2022 15:02:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
05/18/2022 15:02:05 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
05/18/2022 15:02:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/18/2022 15:02:08 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6894792486231314 on epoch=687
05/18/2022 15:02:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/18/2022 15:02:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=692
05/18/2022 15:02:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 15:02:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/18/2022 15:02:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/18/2022 15:02:16 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.638413789428815 on epoch=699
05/18/2022 15:02:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/18/2022 15:02:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
05/18/2022 15:02:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
05/18/2022 15:02:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
05/18/2022 15:02:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=712
05/18/2022 15:02:24 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.6827112004947892 on epoch=712
05/18/2022 15:02:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/18/2022 15:02:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
05/18/2022 15:02:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 15:02:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/18/2022 15:02:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/18/2022 15:02:32 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6219251336898396 on epoch=724
05/18/2022 15:02:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/18/2022 15:02:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/18/2022 15:02:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 15:02:39 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=734
05/18/2022 15:02:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
05/18/2022 15:02:41 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6671957671957672 on epoch=737
05/18/2022 15:02:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/18/2022 15:02:44 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=742
05/18/2022 15:02:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
05/18/2022 15:02:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/18/2022 15:02:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/18/2022 15:02:49 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6745391705069125 on epoch=749
05/18/2022 15:02:49 - INFO - __main__ - save last model!
05/18/2022 15:02:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 15:02:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 15:02:49 - INFO - __main__ - Printing 3 examples
05/18/2022 15:02:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 15:02:49 - INFO - __main__ - ['others']
05/18/2022 15:02:49 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 15:02:49 - INFO - __main__ - ['others']
05/18/2022 15:02:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 15:02:49 - INFO - __main__ - ['others']
05/18/2022 15:02:49 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:02:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:02:50 - INFO - __main__ - Printing 3 examples
05/18/2022 15:02:50 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:02:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:02:50 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:02:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:02:50 - INFO - __main__ - Printing 3 examples
05/18/2022 15:02:50 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:02:50 - INFO - __main__ - ['others']
05/18/2022 15:02:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:02:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:02:50 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:02:52 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:02:57 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:02:57 - INFO - __main__ - task name: emo
05/18/2022 15:02:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:02:57 - INFO - __main__ - Starting training!
05/18/2022 15:02:58 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 15:04:14 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.5_8_predictions.txt
05/18/2022 15:04:14 - INFO - __main__ - Classification-F1 on test data: 0.2531
05/18/2022 15:04:15 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.761908656677354, test_performance=0.2530649663645602
05/18/2022 15:04:15 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
05/18/2022 15:04:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:04:16 - INFO - __main__ - Printing 3 examples
05/18/2022 15:04:16 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:04:16 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:04:16 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:04:16 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:04:16 - INFO - __main__ - Printing 3 examples
05/18/2022 15:04:16 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:04:16 - INFO - __main__ - ['others']
05/18/2022 15:04:16 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:04:16 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:04:16 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:04:23 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:04:23 - INFO - __main__ - task name: emo
05/18/2022 15:04:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:04:23 - INFO - __main__ - Starting training!
05/18/2022 15:04:25 - INFO - __main__ - Step 10 Global step 10 Train loss 6.44 on epoch=2
05/18/2022 15:04:26 - INFO - __main__ - Step 20 Global step 20 Train loss 4.27 on epoch=4
05/18/2022 15:04:28 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=7
05/18/2022 15:04:30 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=9
05/18/2022 15:04:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.54 on epoch=12
05/18/2022 15:04:32 - INFO - __main__ - Global step 50 Train loss 3.24 Classification-F1 0.09493670886075949 on epoch=12
05/18/2022 15:04:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09493670886075949 on epoch=12, global_step=50
05/18/2022 15:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.38 on epoch=14
05/18/2022 15:04:35 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=17
05/18/2022 15:04:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=19
05/18/2022 15:04:38 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
05/18/2022 15:04:39 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
05/18/2022 15:04:40 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.1 on epoch=24
05/18/2022 15:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.09493670886075949 -> 0.1 on epoch=24, global_step=100
05/18/2022 15:04:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.24 on epoch=27
05/18/2022 15:04:43 - INFO - __main__ - Step 120 Global step 120 Train loss 1.24 on epoch=29
05/18/2022 15:04:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=32
05/18/2022 15:04:46 - INFO - __main__ - Step 140 Global step 140 Train loss 1.06 on epoch=34
05/18/2022 15:04:48 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
05/18/2022 15:04:48 - INFO - __main__ - Global step 150 Train loss 1.10 Classification-F1 0.12905551550108146 on epoch=37
05/18/2022 15:04:48 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.12905551550108146 on epoch=37, global_step=150
05/18/2022 15:04:50 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
05/18/2022 15:04:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.18 on epoch=42
05/18/2022 15:04:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=44
05/18/2022 15:04:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
05/18/2022 15:04:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.04 on epoch=49
05/18/2022 15:04:57 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.1900100944552599 on epoch=49
05/18/2022 15:04:57 - INFO - __main__ - Saving model with best Classification-F1: 0.12905551550108146 -> 0.1900100944552599 on epoch=49, global_step=200
05/18/2022 15:04:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
05/18/2022 15:05:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/18/2022 15:05:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.69 on epoch=57
05/18/2022 15:05:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.07 on epoch=59
05/18/2022 15:05:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
05/18/2022 15:05:05 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.17045454545454547 on epoch=62
05/18/2022 15:05:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=64
05/18/2022 15:05:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
05/18/2022 15:05:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.96 on epoch=69
05/18/2022 15:05:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
05/18/2022 15:05:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
05/18/2022 15:05:13 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.13034188034188032 on epoch=74
05/18/2022 15:05:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
05/18/2022 15:05:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=79
05/18/2022 15:05:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
05/18/2022 15:05:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/18/2022 15:05:20 - INFO - __main__ - Step 350 Global step 350 Train loss 1.03 on epoch=87
05/18/2022 15:05:20 - INFO - __main__ - Global step 350 Train loss 0.96 Classification-F1 0.254981884057971 on epoch=87
05/18/2022 15:05:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1900100944552599 -> 0.254981884057971 on epoch=87, global_step=350
05/18/2022 15:05:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=89
05/18/2022 15:05:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=92
05/18/2022 15:05:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=94
05/18/2022 15:05:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
05/18/2022 15:05:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=99
05/18/2022 15:05:29 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.15666666666666668 on epoch=99
05/18/2022 15:05:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
05/18/2022 15:05:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
05/18/2022 15:05:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=107
05/18/2022 15:05:34 - INFO - __main__ - Step 440 Global step 440 Train loss 2.93 on epoch=109
05/18/2022 15:05:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.91 on epoch=112
05/18/2022 15:05:36 - INFO - __main__ - Global step 450 Train loss 1.29 Classification-F1 0.22850678733031676 on epoch=112
05/18/2022 15:05:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/18/2022 15:05:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
05/18/2022 15:05:41 - INFO - __main__ - Step 480 Global step 480 Train loss 0.91 on epoch=119
05/18/2022 15:05:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
05/18/2022 15:05:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=124
05/18/2022 15:05:45 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.20689324116743468 on epoch=124
05/18/2022 15:05:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=127
05/18/2022 15:05:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
05/18/2022 15:05:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
05/18/2022 15:05:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
05/18/2022 15:05:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.92 on epoch=137
05/18/2022 15:05:53 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.17839889579020016 on epoch=137
05/18/2022 15:05:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
05/18/2022 15:05:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/18/2022 15:05:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=144
05/18/2022 15:05:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
05/18/2022 15:06:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=149
05/18/2022 15:06:02 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.22464925012094825 on epoch=149
05/18/2022 15:06:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/18/2022 15:06:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/18/2022 15:06:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.78 on epoch=157
05/18/2022 15:06:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.85 on epoch=159
05/18/2022 15:06:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.93 on epoch=162
05/18/2022 15:06:11 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.3562233589087809 on epoch=162
05/18/2022 15:06:11 - INFO - __main__ - Saving model with best Classification-F1: 0.254981884057971 -> 0.3562233589087809 on epoch=162, global_step=650
05/18/2022 15:06:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/18/2022 15:06:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
05/18/2022 15:06:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=169
05/18/2022 15:06:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.81 on epoch=172
05/18/2022 15:06:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
05/18/2022 15:06:20 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.2226190476190476 on epoch=174
05/18/2022 15:06:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=177
05/18/2022 15:06:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
05/18/2022 15:06:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.80 on epoch=182
05/18/2022 15:06:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
05/18/2022 15:06:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.88 on epoch=187
05/18/2022 15:06:28 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.3186975591151793 on epoch=187
05/18/2022 15:06:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.76 on epoch=189
05/18/2022 15:06:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
05/18/2022 15:06:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.84 on epoch=194
05/18/2022 15:06:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.90 on epoch=197
05/18/2022 15:06:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
05/18/2022 15:06:37 - INFO - __main__ - Global step 800 Train loss 0.83 Classification-F1 0.37909560723514213 on epoch=199
05/18/2022 15:06:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3562233589087809 -> 0.37909560723514213 on epoch=199, global_step=800
05/18/2022 15:06:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=202
05/18/2022 15:06:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.83 on epoch=204
05/18/2022 15:06:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=207
05/18/2022 15:06:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.72 on epoch=209
05/18/2022 15:06:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=212
05/18/2022 15:06:44 - INFO - __main__ - Global step 850 Train loss 0.78 Classification-F1 0.32999999999999996 on epoch=212
05/18/2022 15:06:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=214
05/18/2022 15:06:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
05/18/2022 15:06:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
05/18/2022 15:06:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
05/18/2022 15:06:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=224
05/18/2022 15:06:53 - INFO - __main__ - Global step 900 Train loss 0.75 Classification-F1 0.45565666217840134 on epoch=224
05/18/2022 15:06:53 - INFO - __main__ - Saving model with best Classification-F1: 0.37909560723514213 -> 0.45565666217840134 on epoch=224, global_step=900
05/18/2022 15:06:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=227
05/18/2022 15:06:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=229
05/18/2022 15:06:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=232
05/18/2022 15:06:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.70 on epoch=234
05/18/2022 15:07:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
05/18/2022 15:07:01 - INFO - __main__ - Global step 950 Train loss 0.76 Classification-F1 0.47884891688771003 on epoch=237
05/18/2022 15:07:01 - INFO - __main__ - Saving model with best Classification-F1: 0.45565666217840134 -> 0.47884891688771003 on epoch=237, global_step=950
05/18/2022 15:07:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
05/18/2022 15:07:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.69 on epoch=242
05/18/2022 15:07:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.73 on epoch=244
05/18/2022 15:07:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
05/18/2022 15:07:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.71 on epoch=249
05/18/2022 15:07:08 - INFO - __main__ - Global step 1000 Train loss 0.73 Classification-F1 0.31720301418439717 on epoch=249
05/18/2022 15:07:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.69 on epoch=252
05/18/2022 15:07:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=254
05/18/2022 15:07:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.63 on epoch=257
05/18/2022 15:07:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=259
05/18/2022 15:07:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.59 on epoch=262
05/18/2022 15:07:15 - INFO - __main__ - Global step 1050 Train loss 0.66 Classification-F1 0.40933006535947714 on epoch=262
05/18/2022 15:07:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=264
05/18/2022 15:07:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.73 on epoch=267
05/18/2022 15:07:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=269
05/18/2022 15:07:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.70 on epoch=272
05/18/2022 15:07:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=274
05/18/2022 15:07:23 - INFO - __main__ - Global step 1100 Train loss 0.70 Classification-F1 0.4119991702228544 on epoch=274
05/18/2022 15:07:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=277
05/18/2022 15:07:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.71 on epoch=279
05/18/2022 15:07:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=282
05/18/2022 15:07:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=284
05/18/2022 15:07:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=287
05/18/2022 15:07:32 - INFO - __main__ - Global step 1150 Train loss 0.68 Classification-F1 0.43418407019384286 on epoch=287
05/18/2022 15:07:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.62 on epoch=289
05/18/2022 15:07:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=292
05/18/2022 15:07:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.63 on epoch=294
05/18/2022 15:07:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=297
05/18/2022 15:07:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.68 on epoch=299
05/18/2022 15:07:41 - INFO - __main__ - Global step 1200 Train loss 0.65 Classification-F1 0.4762289068231842 on epoch=299
05/18/2022 15:07:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.59 on epoch=302
05/18/2022 15:07:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.55 on epoch=304
05/18/2022 15:07:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=307
05/18/2022 15:07:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=309
05/18/2022 15:07:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=312
05/18/2022 15:07:49 - INFO - __main__ - Global step 1250 Train loss 0.61 Classification-F1 0.4868693284936479 on epoch=312
05/18/2022 15:07:49 - INFO - __main__ - Saving model with best Classification-F1: 0.47884891688771003 -> 0.4868693284936479 on epoch=312, global_step=1250
05/18/2022 15:07:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=314
05/18/2022 15:07:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=317
05/18/2022 15:07:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=319
05/18/2022 15:07:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=322
05/18/2022 15:07:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=324
05/18/2022 15:07:57 - INFO - __main__ - Global step 1300 Train loss 0.57 Classification-F1 0.4975213434656159 on epoch=324
05/18/2022 15:07:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4868693284936479 -> 0.4975213434656159 on epoch=324, global_step=1300
05/18/2022 15:07:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.61 on epoch=327
05/18/2022 15:08:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=329
05/18/2022 15:08:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=332
05/18/2022 15:08:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=334
05/18/2022 15:08:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=337
05/18/2022 15:08:05 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.5557858807858808 on epoch=337
05/18/2022 15:08:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4975213434656159 -> 0.5557858807858808 on epoch=337, global_step=1350
05/18/2022 15:08:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.59 on epoch=339
05/18/2022 15:08:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.60 on epoch=342
05/18/2022 15:08:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=344
05/18/2022 15:08:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=347
05/18/2022 15:08:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=349
05/18/2022 15:08:14 - INFO - __main__ - Global step 1400 Train loss 0.52 Classification-F1 0.5193452380952381 on epoch=349
05/18/2022 15:08:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=352
05/18/2022 15:08:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=354
05/18/2022 15:08:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=357
05/18/2022 15:08:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=359
05/18/2022 15:08:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.56 on epoch=362
05/18/2022 15:08:22 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.5438405797101449 on epoch=362
05/18/2022 15:08:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=364
05/18/2022 15:08:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=367
05/18/2022 15:08:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=369
05/18/2022 15:08:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=372
05/18/2022 15:08:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=374
05/18/2022 15:08:30 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.6573461091753775 on epoch=374
05/18/2022 15:08:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5557858807858808 -> 0.6573461091753775 on epoch=374, global_step=1500
05/18/2022 15:08:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=377
05/18/2022 15:08:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=379
05/18/2022 15:08:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=382
05/18/2022 15:08:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=384
05/18/2022 15:08:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=387
05/18/2022 15:08:38 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.5182076879334729 on epoch=387
05/18/2022 15:08:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=389
05/18/2022 15:08:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=392
05/18/2022 15:08:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=394
05/18/2022 15:08:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=397
05/18/2022 15:08:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=399
05/18/2022 15:08:47 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.6053958944281526 on epoch=399
05/18/2022 15:08:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=402
05/18/2022 15:08:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=404
05/18/2022 15:08:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=407
05/18/2022 15:08:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=409
05/18/2022 15:08:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=412
05/18/2022 15:08:55 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.6244154788697174 on epoch=412
05/18/2022 15:08:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=414
05/18/2022 15:08:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=417
05/18/2022 15:09:00 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=419
05/18/2022 15:09:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=422
05/18/2022 15:09:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.04 on epoch=424
05/18/2022 15:09:04 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.5518833018833018 on epoch=424
05/18/2022 15:09:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.44 on epoch=427
05/18/2022 15:09:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.99 on epoch=429
05/18/2022 15:09:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.04 on epoch=432
05/18/2022 15:09:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.18 on epoch=434
05/18/2022 15:09:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.42 on epoch=437
05/18/2022 15:09:13 - INFO - __main__ - Global step 1750 Train loss 1.42 Classification-F1 0.6193001443001444 on epoch=437
05/18/2022 15:09:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=439
05/18/2022 15:09:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=442
05/18/2022 15:09:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=444
05/18/2022 15:09:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=447
05/18/2022 15:09:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=449
05/18/2022 15:09:21 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.649154334038055 on epoch=449
05/18/2022 15:09:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=452
05/18/2022 15:09:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=454
05/18/2022 15:09:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=457
05/18/2022 15:09:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=459
05/18/2022 15:09:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=462
05/18/2022 15:09:29 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.6545479440512302 on epoch=462
05/18/2022 15:09:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=464
05/18/2022 15:09:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=467
05/18/2022 15:09:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=469
05/18/2022 15:09:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=472
05/18/2022 15:09:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=474
05/18/2022 15:09:37 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.6397594457903013 on epoch=474
05/18/2022 15:09:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=477
05/18/2022 15:09:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=479
05/18/2022 15:09:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=482
05/18/2022 15:09:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=484
05/18/2022 15:09:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=487
05/18/2022 15:09:46 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.6471163245356795 on epoch=487
05/18/2022 15:09:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=489
05/18/2022 15:09:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=492
05/18/2022 15:09:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=494
05/18/2022 15:09:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=497
05/18/2022 15:09:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=499
05/18/2022 15:09:54 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.6744972769166317 on epoch=499
05/18/2022 15:09:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6573461091753775 -> 0.6744972769166317 on epoch=499, global_step=2000
05/18/2022 15:09:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=502
05/18/2022 15:09:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=504
05/18/2022 15:09:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=507
05/18/2022 15:10:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=509
05/18/2022 15:10:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=512
05/18/2022 15:10:02 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.6180351906158357 on epoch=512
05/18/2022 15:10:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=514
05/18/2022 15:10:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=517
05/18/2022 15:10:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=519
05/18/2022 15:10:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=522
05/18/2022 15:10:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=524
05/18/2022 15:10:11 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.7293948412698413 on epoch=524
05/18/2022 15:10:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6744972769166317 -> 0.7293948412698413 on epoch=524, global_step=2100
05/18/2022 15:10:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=527
05/18/2022 15:10:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=529
05/18/2022 15:10:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=532
05/18/2022 15:10:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=534
05/18/2022 15:10:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=537
05/18/2022 15:10:19 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.6038401253918495 on epoch=537
05/18/2022 15:10:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=539
05/18/2022 15:10:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=542
05/18/2022 15:10:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=544
05/18/2022 15:10:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=547
05/18/2022 15:10:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=549
05/18/2022 15:10:28 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.6256121939030485 on epoch=549
05/18/2022 15:10:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=552
05/18/2022 15:10:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=554
05/18/2022 15:10:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=557
05/18/2022 15:10:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=559
05/18/2022 15:10:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=562
05/18/2022 15:10:36 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.6831206741239924 on epoch=562
05/18/2022 15:10:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=564
05/18/2022 15:10:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=567
05/18/2022 15:10:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=569
05/18/2022 15:10:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=572
05/18/2022 15:10:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=574
05/18/2022 15:10:44 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.5966525665738559 on epoch=574
05/18/2022 15:10:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=577
05/18/2022 15:10:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=579
05/18/2022 15:10:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=582
05/18/2022 15:10:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=584
05/18/2022 15:10:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=587
05/18/2022 15:10:52 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7104489713185366 on epoch=587
05/18/2022 15:10:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=589
05/18/2022 15:10:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=592
05/18/2022 15:10:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=594
05/18/2022 15:10:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
05/18/2022 15:10:59 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=599
05/18/2022 15:11:00 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.6020658263305323 on epoch=599
05/18/2022 15:11:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=602
05/18/2022 15:11:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=604
05/18/2022 15:11:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=607
05/18/2022 15:11:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.30 on epoch=609
05/18/2022 15:11:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
05/18/2022 15:11:09 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.6704166666666667 on epoch=612
05/18/2022 15:11:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=614
05/18/2022 15:11:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=617
05/18/2022 15:11:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=619
05/18/2022 15:11:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=622
05/18/2022 15:11:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=624
05/18/2022 15:11:17 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.6969655797101451 on epoch=624
05/18/2022 15:11:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=627
05/18/2022 15:11:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=629
05/18/2022 15:11:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=632
05/18/2022 15:11:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=634
05/18/2022 15:11:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=637
05/18/2022 15:11:25 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.597962382445141 on epoch=637
05/18/2022 15:11:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=639
05/18/2022 15:11:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=642
05/18/2022 15:11:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=644
05/18/2022 15:11:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=647
05/18/2022 15:11:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=649
05/18/2022 15:11:34 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.6381465425644949 on epoch=649
05/18/2022 15:11:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=652
05/18/2022 15:11:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=654
05/18/2022 15:11:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=657
05/18/2022 15:11:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=659
05/18/2022 15:11:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=662
05/18/2022 15:11:42 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.6005291005291006 on epoch=662
05/18/2022 15:11:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=664
05/18/2022 15:11:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=667
05/18/2022 15:11:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=669
05/18/2022 15:11:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
05/18/2022 15:11:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=674
05/18/2022 15:11:51 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.711571330547065 on epoch=674
05/18/2022 15:11:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=677
05/18/2022 15:11:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=679
05/18/2022 15:11:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=682
05/18/2022 15:11:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=684
05/18/2022 15:11:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=687
05/18/2022 15:11:58 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.6750633267661132 on epoch=687
05/18/2022 15:12:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=689
05/18/2022 15:12:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=692
05/18/2022 15:12:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=694
05/18/2022 15:12:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=697
05/18/2022 15:12:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=699
05/18/2022 15:12:07 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.6688405797101449 on epoch=699
05/18/2022 15:12:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=702
05/18/2022 15:12:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=704
05/18/2022 15:12:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=707
05/18/2022 15:12:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=709
05/18/2022 15:12:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=712
05/18/2022 15:12:16 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.6119554204660588 on epoch=712
05/18/2022 15:12:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=714
05/18/2022 15:12:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=717
05/18/2022 15:12:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=719
05/18/2022 15:12:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=722
05/18/2022 15:12:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=724
05/18/2022 15:12:25 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.6885416666666666 on epoch=724
05/18/2022 15:12:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=727
05/18/2022 15:12:28 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=729
05/18/2022 15:12:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=732
05/18/2022 15:12:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=734
05/18/2022 15:12:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=737
05/18/2022 15:12:33 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.6618773946360154 on epoch=737
05/18/2022 15:12:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=739
05/18/2022 15:12:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=742
05/18/2022 15:12:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=744
05/18/2022 15:12:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=747
05/18/2022 15:12:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
05/18/2022 15:12:41 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.6327404479578393 on epoch=749
05/18/2022 15:12:41 - INFO - __main__ - save last model!
05/18/2022 15:12:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 15:12:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 15:12:41 - INFO - __main__ - Printing 3 examples
05/18/2022 15:12:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:12:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:12:41 - INFO - __main__ - Printing 3 examples
05/18/2022 15:12:41 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:12:41 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:12:41 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:12:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:12:41 - INFO - __main__ - Printing 3 examples
05/18/2022 15:12:41 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:12:41 - INFO - __main__ - ['others']
05/18/2022 15:12:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:12:41 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:12:41 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:12:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:12:47 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:12:47 - INFO - __main__ - task name: emo
05/18/2022 15:12:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:12:47 - INFO - __main__ - Starting training!
05/18/2022 15:12:49 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 15:14:07 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.4_8_predictions.txt
05/18/2022 15:14:07 - INFO - __main__ - Classification-F1 on test data: 0.2039
05/18/2022 15:14:07 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7293948412698413, test_performance=0.20390935460005227
05/18/2022 15:14:07 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
05/18/2022 15:14:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:14:08 - INFO - __main__ - Printing 3 examples
05/18/2022 15:14:08 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:14:08 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:14:08 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:14:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:14:08 - INFO - __main__ - Printing 3 examples
05/18/2022 15:14:08 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:14:08 - INFO - __main__ - ['others']
05/18/2022 15:14:08 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:14:09 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:14:09 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:14:15 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:14:15 - INFO - __main__ - task name: emo
05/18/2022 15:14:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:14:16 - INFO - __main__ - Starting training!
05/18/2022 15:14:17 - INFO - __main__ - Step 10 Global step 10 Train loss 7.74 on epoch=2
05/18/2022 15:14:19 - INFO - __main__ - Step 20 Global step 20 Train loss 8.06 on epoch=4
05/18/2022 15:14:20 - INFO - __main__ - Step 30 Global step 30 Train loss 8.00 on epoch=7
05/18/2022 15:14:22 - INFO - __main__ - Step 40 Global step 40 Train loss 8.08 on epoch=9
05/18/2022 15:14:23 - INFO - __main__ - Step 50 Global step 50 Train loss 8.17 on epoch=12
05/18/2022 15:14:48 - INFO - __main__ - Global step 50 Train loss 8.01 Classification-F1 0.0 on epoch=12
05/18/2022 15:14:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
05/18/2022 15:14:49 - INFO - __main__ - Step 60 Global step 60 Train loss 8.36 on epoch=14
05/18/2022 15:14:51 - INFO - __main__ - Step 70 Global step 70 Train loss 8.62 on epoch=17
05/18/2022 15:14:52 - INFO - __main__ - Step 80 Global step 80 Train loss 8.70 on epoch=19
05/18/2022 15:14:54 - INFO - __main__ - Step 90 Global step 90 Train loss 8.78 on epoch=22
05/18/2022 15:14:55 - INFO - __main__ - Step 100 Global step 100 Train loss 8.88 on epoch=24
05/18/2022 15:15:19 - INFO - __main__ - Global step 100 Train loss 8.67 Classification-F1 0.0 on epoch=24
05/18/2022 15:15:21 - INFO - __main__ - Step 110 Global step 110 Train loss 8.73 on epoch=27
05/18/2022 15:15:22 - INFO - __main__ - Step 120 Global step 120 Train loss 8.75 on epoch=29
05/18/2022 15:15:23 - INFO - __main__ - Step 130 Global step 130 Train loss 8.68 on epoch=32
05/18/2022 15:15:25 - INFO - __main__ - Step 140 Global step 140 Train loss 8.79 on epoch=34
05/18/2022 15:15:26 - INFO - __main__ - Step 150 Global step 150 Train loss 8.80 on epoch=37
05/18/2022 15:15:41 - INFO - __main__ - Global step 150 Train loss 8.75 Classification-F1 0.0 on epoch=37
05/18/2022 15:15:42 - INFO - __main__ - Step 160 Global step 160 Train loss 8.88 on epoch=39
05/18/2022 15:15:44 - INFO - __main__ - Step 170 Global step 170 Train loss 8.80 on epoch=42
05/18/2022 15:15:45 - INFO - __main__ - Step 180 Global step 180 Train loss 8.88 on epoch=44
05/18/2022 15:15:47 - INFO - __main__ - Step 190 Global step 190 Train loss 8.73 on epoch=47
05/18/2022 15:15:48 - INFO - __main__ - Step 200 Global step 200 Train loss 8.88 on epoch=49
05/18/2022 15:16:10 - INFO - __main__ - Global step 200 Train loss 8.83 Classification-F1 0.0 on epoch=49
05/18/2022 15:16:12 - INFO - __main__ - Step 210 Global step 210 Train loss 8.89 on epoch=52
05/18/2022 15:16:13 - INFO - __main__ - Step 220 Global step 220 Train loss 8.86 on epoch=54
05/18/2022 15:16:14 - INFO - __main__ - Step 230 Global step 230 Train loss 8.75 on epoch=57
05/18/2022 15:16:16 - INFO - __main__ - Step 240 Global step 240 Train loss 8.89 on epoch=59
05/18/2022 15:16:17 - INFO - __main__ - Step 250 Global step 250 Train loss 8.77 on epoch=62
05/18/2022 15:16:38 - INFO - __main__ - Global step 250 Train loss 8.83 Classification-F1 0.0 on epoch=62
05/18/2022 15:16:39 - INFO - __main__ - Step 260 Global step 260 Train loss 8.78 on epoch=64
05/18/2022 15:16:41 - INFO - __main__ - Step 270 Global step 270 Train loss 8.80 on epoch=67
05/18/2022 15:16:42 - INFO - __main__ - Step 280 Global step 280 Train loss 8.79 on epoch=69
05/18/2022 15:16:44 - INFO - __main__ - Step 290 Global step 290 Train loss 8.81 on epoch=72
05/18/2022 15:16:45 - INFO - __main__ - Step 300 Global step 300 Train loss 8.82 on epoch=74
05/18/2022 15:17:02 - INFO - __main__ - Global step 300 Train loss 8.80 Classification-F1 0.0 on epoch=74
05/18/2022 15:17:04 - INFO - __main__ - Step 310 Global step 310 Train loss 8.86 on epoch=77
05/18/2022 15:17:05 - INFO - __main__ - Step 320 Global step 320 Train loss 8.86 on epoch=79
05/18/2022 15:17:06 - INFO - __main__ - Step 330 Global step 330 Train loss 8.81 on epoch=82
05/18/2022 15:17:08 - INFO - __main__ - Step 340 Global step 340 Train loss 8.83 on epoch=84
05/18/2022 15:17:10 - INFO - __main__ - Step 350 Global step 350 Train loss 8.81 on epoch=87
05/18/2022 15:17:24 - INFO - __main__ - Global step 350 Train loss 8.83 Classification-F1 0.0 on epoch=87
05/18/2022 15:17:26 - INFO - __main__ - Step 360 Global step 360 Train loss 8.92 on epoch=89
05/18/2022 15:17:27 - INFO - __main__ - Step 370 Global step 370 Train loss 8.91 on epoch=92
05/18/2022 15:17:29 - INFO - __main__ - Step 380 Global step 380 Train loss 8.93 on epoch=94
05/18/2022 15:17:30 - INFO - __main__ - Step 390 Global step 390 Train loss 8.91 on epoch=97
05/18/2022 15:17:32 - INFO - __main__ - Step 400 Global step 400 Train loss 8.89 on epoch=99
05/18/2022 15:18:02 - INFO - __main__ - Global step 400 Train loss 8.91 Classification-F1 0.0 on epoch=99
05/18/2022 15:18:04 - INFO - __main__ - Step 410 Global step 410 Train loss 8.85 on epoch=102
05/18/2022 15:18:05 - INFO - __main__ - Step 420 Global step 420 Train loss 8.88 on epoch=104
05/18/2022 15:18:07 - INFO - __main__ - Step 430 Global step 430 Train loss 8.82 on epoch=107
05/18/2022 15:18:08 - INFO - __main__ - Step 440 Global step 440 Train loss 8.97 on epoch=109
05/18/2022 15:18:09 - INFO - __main__ - Step 450 Global step 450 Train loss 8.80 on epoch=112
05/18/2022 15:18:34 - INFO - __main__ - Global step 450 Train loss 8.86 Classification-F1 0.0 on epoch=112
05/18/2022 15:18:35 - INFO - __main__ - Step 460 Global step 460 Train loss 8.77 on epoch=114
05/18/2022 15:18:37 - INFO - __main__ - Step 470 Global step 470 Train loss 8.88 on epoch=117
05/18/2022 15:18:38 - INFO - __main__ - Step 480 Global step 480 Train loss 8.89 on epoch=119
05/18/2022 15:18:40 - INFO - __main__ - Step 490 Global step 490 Train loss 8.85 on epoch=122
05/18/2022 15:18:41 - INFO - __main__ - Step 500 Global step 500 Train loss 8.92 on epoch=124
05/18/2022 15:19:06 - INFO - __main__ - Global step 500 Train loss 8.86 Classification-F1 0.0 on epoch=124
05/18/2022 15:19:07 - INFO - __main__ - Step 510 Global step 510 Train loss 8.91 on epoch=127
05/18/2022 15:19:09 - INFO - __main__ - Step 520 Global step 520 Train loss 8.82 on epoch=129
05/18/2022 15:19:10 - INFO - __main__ - Step 530 Global step 530 Train loss 8.83 on epoch=132
05/18/2022 15:19:12 - INFO - __main__ - Step 540 Global step 540 Train loss 8.77 on epoch=134
05/18/2022 15:19:13 - INFO - __main__ - Step 550 Global step 550 Train loss 8.89 on epoch=137
05/18/2022 15:19:23 - INFO - __main__ - Global step 550 Train loss 8.85 Classification-F1 0.0 on epoch=137
05/18/2022 15:19:25 - INFO - __main__ - Step 560 Global step 560 Train loss 8.82 on epoch=139
05/18/2022 15:19:26 - INFO - __main__ - Step 570 Global step 570 Train loss 8.70 on epoch=142
05/18/2022 15:19:28 - INFO - __main__ - Step 580 Global step 580 Train loss 8.67 on epoch=144
05/18/2022 15:19:29 - INFO - __main__ - Step 590 Global step 590 Train loss 8.68 on epoch=147
05/18/2022 15:19:31 - INFO - __main__ - Step 600 Global step 600 Train loss 8.82 on epoch=149
05/18/2022 15:20:01 - INFO - __main__ - Global step 600 Train loss 8.74 Classification-F1 0.0 on epoch=149
05/18/2022 15:20:03 - INFO - __main__ - Step 610 Global step 610 Train loss 8.83 on epoch=152
05/18/2022 15:20:04 - INFO - __main__ - Step 620 Global step 620 Train loss 8.80 on epoch=154
05/18/2022 15:20:06 - INFO - __main__ - Step 630 Global step 630 Train loss 8.85 on epoch=157
05/18/2022 15:20:07 - INFO - __main__ - Step 640 Global step 640 Train loss 8.97 on epoch=159
05/18/2022 15:20:09 - INFO - __main__ - Step 650 Global step 650 Train loss 8.84 on epoch=162
05/18/2022 15:20:30 - INFO - __main__ - Global step 650 Train loss 8.86 Classification-F1 0.0 on epoch=162
05/18/2022 15:20:31 - INFO - __main__ - Step 660 Global step 660 Train loss 8.94 on epoch=164
05/18/2022 15:20:33 - INFO - __main__ - Step 670 Global step 670 Train loss 8.81 on epoch=167
05/18/2022 15:20:34 - INFO - __main__ - Step 680 Global step 680 Train loss 8.95 on epoch=169
05/18/2022 15:20:35 - INFO - __main__ - Step 690 Global step 690 Train loss 8.86 on epoch=172
05/18/2022 15:20:37 - INFO - __main__ - Step 700 Global step 700 Train loss 8.96 on epoch=174
05/18/2022 15:20:46 - INFO - __main__ - Global step 700 Train loss 8.90 Classification-F1 0.0 on epoch=174
05/18/2022 15:20:48 - INFO - __main__ - Step 710 Global step 710 Train loss 8.84 on epoch=177
05/18/2022 15:20:49 - INFO - __main__ - Step 720 Global step 720 Train loss 8.90 on epoch=179
05/18/2022 15:20:51 - INFO - __main__ - Step 730 Global step 730 Train loss 8.76 on epoch=182
05/18/2022 15:20:52 - INFO - __main__ - Step 740 Global step 740 Train loss 8.88 on epoch=184
05/18/2022 15:20:53 - INFO - __main__ - Step 750 Global step 750 Train loss 8.74 on epoch=187
05/18/2022 15:21:12 - INFO - __main__ - Global step 750 Train loss 8.83 Classification-F1 0.0 on epoch=187
05/18/2022 15:21:14 - INFO - __main__ - Step 760 Global step 760 Train loss 8.91 on epoch=189
05/18/2022 15:21:15 - INFO - __main__ - Step 770 Global step 770 Train loss 8.78 on epoch=192
05/18/2022 15:21:17 - INFO - __main__ - Step 780 Global step 780 Train loss 8.86 on epoch=194
05/18/2022 15:21:19 - INFO - __main__ - Step 790 Global step 790 Train loss 8.88 on epoch=197
05/18/2022 15:21:20 - INFO - __main__ - Step 800 Global step 800 Train loss 8.84 on epoch=199
05/18/2022 15:21:39 - INFO - __main__ - Global step 800 Train loss 8.85 Classification-F1 0.0 on epoch=199
05/18/2022 15:21:41 - INFO - __main__ - Step 810 Global step 810 Train loss 8.64 on epoch=202
05/18/2022 15:21:42 - INFO - __main__ - Step 820 Global step 820 Train loss 8.95 on epoch=204
05/18/2022 15:21:44 - INFO - __main__ - Step 830 Global step 830 Train loss 8.71 on epoch=207
05/18/2022 15:21:45 - INFO - __main__ - Step 840 Global step 840 Train loss 8.87 on epoch=209
05/18/2022 15:21:46 - INFO - __main__ - Step 850 Global step 850 Train loss 8.75 on epoch=212
05/18/2022 15:21:56 - INFO - __main__ - Global step 850 Train loss 8.79 Classification-F1 0.0 on epoch=212
05/18/2022 15:21:58 - INFO - __main__ - Step 860 Global step 860 Train loss 8.73 on epoch=214
05/18/2022 15:21:59 - INFO - __main__ - Step 870 Global step 870 Train loss 8.70 on epoch=217
05/18/2022 15:22:00 - INFO - __main__ - Step 880 Global step 880 Train loss 8.81 on epoch=219
05/18/2022 15:22:02 - INFO - __main__ - Step 890 Global step 890 Train loss 8.72 on epoch=222
05/18/2022 15:22:03 - INFO - __main__ - Step 900 Global step 900 Train loss 8.78 on epoch=224
05/18/2022 15:22:24 - INFO - __main__ - Global step 900 Train loss 8.75 Classification-F1 0.0 on epoch=224
05/18/2022 15:22:25 - INFO - __main__ - Step 910 Global step 910 Train loss 8.76 on epoch=227
05/18/2022 15:22:27 - INFO - __main__ - Step 920 Global step 920 Train loss 8.77 on epoch=229
05/18/2022 15:22:28 - INFO - __main__ - Step 930 Global step 930 Train loss 8.81 on epoch=232
05/18/2022 15:22:30 - INFO - __main__ - Step 940 Global step 940 Train loss 8.73 on epoch=234
05/18/2022 15:22:31 - INFO - __main__ - Step 950 Global step 950 Train loss 8.68 on epoch=237
05/18/2022 15:22:43 - INFO - __main__ - Global step 950 Train loss 8.75 Classification-F1 0.0 on epoch=237
05/18/2022 15:22:45 - INFO - __main__ - Step 960 Global step 960 Train loss 8.58 on epoch=239
05/18/2022 15:22:47 - INFO - __main__ - Step 970 Global step 970 Train loss 8.62 on epoch=242
05/18/2022 15:22:48 - INFO - __main__ - Step 980 Global step 980 Train loss 8.72 on epoch=244
05/18/2022 15:22:50 - INFO - __main__ - Step 990 Global step 990 Train loss 8.68 on epoch=247
05/18/2022 15:22:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 8.62 on epoch=249
05/18/2022 15:23:05 - INFO - __main__ - Global step 1000 Train loss 8.64 Classification-F1 0.0 on epoch=249
05/18/2022 15:23:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 8.61 on epoch=252
05/18/2022 15:23:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 8.76 on epoch=254
05/18/2022 15:23:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 8.66 on epoch=257
05/18/2022 15:23:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 8.73 on epoch=259
05/18/2022 15:23:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 8.50 on epoch=262
05/18/2022 15:23:32 - INFO - __main__ - Global step 1050 Train loss 8.65 Classification-F1 0.0 on epoch=262
05/18/2022 15:23:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 8.64 on epoch=264
05/18/2022 15:23:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 8.67 on epoch=267
05/18/2022 15:23:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 8.70 on epoch=269
05/18/2022 15:23:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 8.67 on epoch=272
05/18/2022 15:23:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 8.67 on epoch=274
05/18/2022 15:23:58 - INFO - __main__ - Global step 1100 Train loss 8.67 Classification-F1 0.0 on epoch=274
05/18/2022 15:24:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 8.61 on epoch=277
05/18/2022 15:24:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 8.75 on epoch=279
05/18/2022 15:24:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 8.72 on epoch=282
05/18/2022 15:24:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 8.73 on epoch=284
05/18/2022 15:24:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 8.71 on epoch=287
05/18/2022 15:24:13 - INFO - __main__ - Global step 1150 Train loss 8.70 Classification-F1 0.0 on epoch=287
05/18/2022 15:24:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 8.70 on epoch=289
05/18/2022 15:24:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 8.79 on epoch=292
05/18/2022 15:24:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 8.73 on epoch=294
05/18/2022 15:24:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 8.69 on epoch=297
05/18/2022 15:24:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 8.76 on epoch=299
05/18/2022 15:24:38 - INFO - __main__ - Global step 1200 Train loss 8.73 Classification-F1 0.0 on epoch=299
05/18/2022 15:24:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 8.73 on epoch=302
05/18/2022 15:24:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 8.70 on epoch=304
05/18/2022 15:24:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 8.73 on epoch=307
05/18/2022 15:24:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 8.64 on epoch=309
05/18/2022 15:24:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 8.69 on epoch=312
05/18/2022 15:25:02 - INFO - __main__ - Global step 1250 Train loss 8.70 Classification-F1 0.0 on epoch=312
05/18/2022 15:25:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 8.69 on epoch=314
05/18/2022 15:25:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 8.65 on epoch=317
05/18/2022 15:25:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 8.74 on epoch=319
05/18/2022 15:25:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 8.65 on epoch=322
05/18/2022 15:25:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 8.71 on epoch=324
05/18/2022 15:25:24 - INFO - __main__ - Global step 1300 Train loss 8.69 Classification-F1 0.0 on epoch=324
05/18/2022 15:25:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 8.80 on epoch=327
05/18/2022 15:25:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 8.74 on epoch=329
05/18/2022 15:25:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 8.61 on epoch=332
05/18/2022 15:25:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 8.61 on epoch=334
05/18/2022 15:25:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 8.64 on epoch=337
05/18/2022 15:25:46 - INFO - __main__ - Global step 1350 Train loss 8.68 Classification-F1 0.0 on epoch=337
05/18/2022 15:25:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 8.76 on epoch=339
05/18/2022 15:25:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 8.58 on epoch=342
05/18/2022 15:25:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 8.78 on epoch=344
05/18/2022 15:25:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 8.70 on epoch=347
05/18/2022 15:25:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 8.74 on epoch=349
05/18/2022 15:26:18 - INFO - __main__ - Global step 1400 Train loss 8.71 Classification-F1 0.0 on epoch=349
05/18/2022 15:26:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 8.65 on epoch=352
05/18/2022 15:26:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 8.65 on epoch=354
05/18/2022 15:26:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 8.64 on epoch=357
05/18/2022 15:26:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 8.75 on epoch=359
05/18/2022 15:26:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 8.64 on epoch=362
05/18/2022 15:26:34 - INFO - __main__ - Global step 1450 Train loss 8.67 Classification-F1 0.0 on epoch=362
05/18/2022 15:26:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 8.66 on epoch=364
05/18/2022 15:26:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 8.62 on epoch=367
05/18/2022 15:26:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 8.76 on epoch=369
05/18/2022 15:26:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 8.72 on epoch=372
05/18/2022 15:26:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 8.63 on epoch=374
05/18/2022 15:26:45 - INFO - __main__ - Global step 1500 Train loss 8.68 Classification-F1 0.0 on epoch=374
05/18/2022 15:26:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 8.69 on epoch=377
05/18/2022 15:26:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 8.69 on epoch=379
05/18/2022 15:26:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 8.65 on epoch=382
05/18/2022 15:26:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 8.58 on epoch=384
05/18/2022 15:26:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 8.66 on epoch=387
05/18/2022 15:27:07 - INFO - __main__ - Global step 1550 Train loss 8.65 Classification-F1 0.0 on epoch=387
05/18/2022 15:27:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 8.67 on epoch=389
05/18/2022 15:27:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 8.69 on epoch=392
05/18/2022 15:27:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 8.59 on epoch=394
05/18/2022 15:27:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 8.62 on epoch=397
05/18/2022 15:27:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 8.64 on epoch=399
05/18/2022 15:27:24 - INFO - __main__ - Global step 1600 Train loss 8.64 Classification-F1 0.0 on epoch=399
05/18/2022 15:27:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 8.63 on epoch=402
05/18/2022 15:27:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 8.61 on epoch=404
05/18/2022 15:27:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 8.55 on epoch=407
05/18/2022 15:27:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 8.62 on epoch=409
05/18/2022 15:27:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 8.58 on epoch=412
05/18/2022 15:27:42 - INFO - __main__ - Global step 1650 Train loss 8.60 Classification-F1 0.0 on epoch=412
05/18/2022 15:27:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 8.51 on epoch=414
05/18/2022 15:27:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 8.60 on epoch=417
05/18/2022 15:27:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 8.60 on epoch=419
05/18/2022 15:27:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 8.40 on epoch=422
05/18/2022 15:27:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 8.51 on epoch=424
05/18/2022 15:28:01 - INFO - __main__ - Global step 1700 Train loss 8.52 Classification-F1 0.0 on epoch=424
05/18/2022 15:28:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 8.67 on epoch=427
05/18/2022 15:28:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 8.54 on epoch=429
05/18/2022 15:28:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 8.47 on epoch=432
05/18/2022 15:28:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 8.58 on epoch=434
05/18/2022 15:28:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 8.54 on epoch=437
05/18/2022 15:28:20 - INFO - __main__ - Global step 1750 Train loss 8.56 Classification-F1 0.0 on epoch=437
05/18/2022 15:28:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 8.57 on epoch=439
05/18/2022 15:28:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 8.54 on epoch=442
05/18/2022 15:28:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 8.46 on epoch=444
05/18/2022 15:28:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 8.56 on epoch=447
05/18/2022 15:28:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 8.48 on epoch=449
05/18/2022 15:28:47 - INFO - __main__ - Global step 1800 Train loss 8.52 Classification-F1 0.0 on epoch=449
05/18/2022 15:28:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 8.54 on epoch=452
05/18/2022 15:28:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 8.42 on epoch=454
05/18/2022 15:28:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 8.43 on epoch=457
05/18/2022 15:28:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 8.42 on epoch=459
05/18/2022 15:28:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 8.40 on epoch=462
05/18/2022 15:29:04 - INFO - __main__ - Global step 1850 Train loss 8.44 Classification-F1 0.0 on epoch=462
05/18/2022 15:29:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 8.52 on epoch=464
05/18/2022 15:29:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 8.41 on epoch=467
05/18/2022 15:29:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 8.50 on epoch=469
05/18/2022 15:29:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 8.39 on epoch=472
05/18/2022 15:29:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 8.51 on epoch=474
05/18/2022 15:29:20 - INFO - __main__ - Global step 1900 Train loss 8.46 Classification-F1 0.0 on epoch=474
05/18/2022 15:29:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 8.49 on epoch=477
05/18/2022 15:29:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 8.54 on epoch=479
05/18/2022 15:29:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 8.38 on epoch=482
05/18/2022 15:29:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 8.56 on epoch=484
05/18/2022 15:29:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 8.48 on epoch=487
05/18/2022 15:29:36 - INFO - __main__ - Global step 1950 Train loss 8.49 Classification-F1 0.0 on epoch=487
05/18/2022 15:29:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 8.53 on epoch=489
05/18/2022 15:29:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 8.47 on epoch=492
05/18/2022 15:29:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 8.50 on epoch=494
05/18/2022 15:29:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 8.38 on epoch=497
05/18/2022 15:29:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 8.48 on epoch=499
05/18/2022 15:29:59 - INFO - __main__ - Global step 2000 Train loss 8.47 Classification-F1 0.0 on epoch=499
05/18/2022 15:30:00 - INFO - __main__ - Step 2010 Global step 2010 Train loss 8.43 on epoch=502
05/18/2022 15:30:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 8.50 on epoch=504
05/18/2022 15:30:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 8.39 on epoch=507
05/18/2022 15:30:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 8.37 on epoch=509
05/18/2022 15:30:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 8.49 on epoch=512
05/18/2022 15:30:21 - INFO - __main__ - Global step 2050 Train loss 8.44 Classification-F1 0.0 on epoch=512
05/18/2022 15:30:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 8.38 on epoch=514
05/18/2022 15:30:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 8.38 on epoch=517
05/18/2022 15:30:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 8.54 on epoch=519
05/18/2022 15:30:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 8.32 on epoch=522
05/18/2022 15:30:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 8.43 on epoch=524
05/18/2022 15:30:46 - INFO - __main__ - Global step 2100 Train loss 8.41 Classification-F1 0.0 on epoch=524
05/18/2022 15:30:48 - INFO - __main__ - Step 2110 Global step 2110 Train loss 8.34 on epoch=527
05/18/2022 15:30:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 8.47 on epoch=529
05/18/2022 15:30:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 8.34 on epoch=532
05/18/2022 15:30:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 8.48 on epoch=534
05/18/2022 15:30:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 8.40 on epoch=537
05/18/2022 15:31:06 - INFO - __main__ - Global step 2150 Train loss 8.40 Classification-F1 0.0 on epoch=537
05/18/2022 15:31:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 8.41 on epoch=539
05/18/2022 15:31:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 8.33 on epoch=542
05/18/2022 15:31:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 8.43 on epoch=544
05/18/2022 15:31:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 8.36 on epoch=547
05/18/2022 15:31:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 8.29 on epoch=549
05/18/2022 15:31:39 - INFO - __main__ - Global step 2200 Train loss 8.37 Classification-F1 0.0 on epoch=549
05/18/2022 15:31:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 8.35 on epoch=552
05/18/2022 15:31:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 8.40 on epoch=554
05/18/2022 15:31:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 8.43 on epoch=557
05/18/2022 15:31:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 8.42 on epoch=559
05/18/2022 15:31:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 8.45 on epoch=562
05/18/2022 15:31:52 - INFO - __main__ - Global step 2250 Train loss 8.41 Classification-F1 0.0 on epoch=562
05/18/2022 15:31:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 8.42 on epoch=564
05/18/2022 15:31:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 8.29 on epoch=567
05/18/2022 15:31:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 8.47 on epoch=569
05/18/2022 15:31:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 8.44 on epoch=572
05/18/2022 15:31:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 8.48 on epoch=574
05/18/2022 15:32:14 - INFO - __main__ - Global step 2300 Train loss 8.42 Classification-F1 0.0 on epoch=574
05/18/2022 15:32:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 8.34 on epoch=577
05/18/2022 15:32:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 8.32 on epoch=579
05/18/2022 15:32:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 8.24 on epoch=582
05/18/2022 15:32:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 8.28 on epoch=584
05/18/2022 15:32:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 8.28 on epoch=587
05/18/2022 15:32:30 - INFO - __main__ - Global step 2350 Train loss 8.29 Classification-F1 0.0 on epoch=587
05/18/2022 15:32:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 8.38 on epoch=589
05/18/2022 15:32:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 8.24 on epoch=592
05/18/2022 15:32:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 8.33 on epoch=594
05/18/2022 15:32:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 8.14 on epoch=597
05/18/2022 15:32:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 8.25 on epoch=599
05/18/2022 15:32:48 - INFO - __main__ - Global step 2400 Train loss 8.27 Classification-F1 0.0 on epoch=599
05/18/2022 15:32:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 8.17 on epoch=602
05/18/2022 15:32:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 8.28 on epoch=604
05/18/2022 15:32:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 8.18 on epoch=607
05/18/2022 15:32:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 8.30 on epoch=609
05/18/2022 15:32:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 8.17 on epoch=612
05/18/2022 15:33:12 - INFO - __main__ - Global step 2450 Train loss 8.22 Classification-F1 0.0 on epoch=612
05/18/2022 15:33:13 - INFO - __main__ - Step 2460 Global step 2460 Train loss 8.23 on epoch=614
05/18/2022 15:33:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 8.07 on epoch=617
05/18/2022 15:33:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 8.14 on epoch=619
05/18/2022 15:33:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 8.20 on epoch=622
05/18/2022 15:33:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 8.23 on epoch=624
05/18/2022 15:33:30 - INFO - __main__ - Global step 2500 Train loss 8.17 Classification-F1 0.0 on epoch=624
05/18/2022 15:33:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 8.14 on epoch=627
05/18/2022 15:33:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 8.05 on epoch=629
05/18/2022 15:33:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 8.25 on epoch=632
05/18/2022 15:33:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 8.14 on epoch=634
05/18/2022 15:33:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 7.88 on epoch=637
05/18/2022 15:33:48 - INFO - __main__ - Global step 2550 Train loss 8.09 Classification-F1 0.0 on epoch=637
05/18/2022 15:33:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 8.18 on epoch=639
05/18/2022 15:33:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 8.17 on epoch=642
05/18/2022 15:33:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 7.99 on epoch=644
05/18/2022 15:33:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 7.97 on epoch=647
05/18/2022 15:33:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 8.04 on epoch=649
05/18/2022 15:34:07 - INFO - __main__ - Global step 2600 Train loss 8.07 Classification-F1 0.0 on epoch=649
05/18/2022 15:34:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 8.13 on epoch=652
05/18/2022 15:34:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 8.01 on epoch=654
05/18/2022 15:34:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 7.93 on epoch=657
05/18/2022 15:34:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 7.91 on epoch=659
05/18/2022 15:34:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 7.85 on epoch=662
05/18/2022 15:34:25 - INFO - __main__ - Global step 2650 Train loss 7.96 Classification-F1 0.0 on epoch=662
05/18/2022 15:34:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 7.90 on epoch=664
05/18/2022 15:34:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 7.83 on epoch=667
05/18/2022 15:34:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 7.95 on epoch=669
05/18/2022 15:34:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 7.90 on epoch=672
05/18/2022 15:34:33 - INFO - __main__ - Step 2700 Global step 2700 Train loss 7.89 on epoch=674
05/18/2022 15:34:41 - INFO - __main__ - Global step 2700 Train loss 7.89 Classification-F1 0.0 on epoch=674
05/18/2022 15:34:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 7.77 on epoch=677
05/18/2022 15:34:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 7.96 on epoch=679
05/18/2022 15:34:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 7.89 on epoch=682
05/18/2022 15:34:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 7.78 on epoch=684
05/18/2022 15:34:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 7.92 on epoch=687
05/18/2022 15:34:57 - INFO - __main__ - Global step 2750 Train loss 7.87 Classification-F1 0.0 on epoch=687
05/18/2022 15:34:59 - INFO - __main__ - Step 2760 Global step 2760 Train loss 7.78 on epoch=689
05/18/2022 15:35:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 7.81 on epoch=692
05/18/2022 15:35:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 8.05 on epoch=694
05/18/2022 15:35:03 - INFO - __main__ - Step 2790 Global step 2790 Train loss 7.77 on epoch=697
05/18/2022 15:35:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 7.91 on epoch=699
05/18/2022 15:35:27 - INFO - __main__ - Global step 2800 Train loss 7.86 Classification-F1 0.0 on epoch=699
05/18/2022 15:35:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 7.88 on epoch=702
05/18/2022 15:35:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 7.83 on epoch=704
05/18/2022 15:35:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 7.87 on epoch=707
05/18/2022 15:35:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 7.86 on epoch=709
05/18/2022 15:35:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 7.81 on epoch=712
05/18/2022 15:35:39 - INFO - __main__ - Global step 2850 Train loss 7.85 Classification-F1 0.0 on epoch=712
05/18/2022 15:35:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 7.81 on epoch=714
05/18/2022 15:35:42 - INFO - __main__ - Step 2870 Global step 2870 Train loss 7.72 on epoch=717
05/18/2022 15:35:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 7.79 on epoch=719
05/18/2022 15:35:44 - INFO - __main__ - Step 2890 Global step 2890 Train loss 7.66 on epoch=722
05/18/2022 15:35:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 7.87 on epoch=724
05/18/2022 15:35:59 - INFO - __main__ - Global step 2900 Train loss 7.77 Classification-F1 0.0 on epoch=724
05/18/2022 15:36:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 7.56 on epoch=727
05/18/2022 15:36:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 7.70 on epoch=729
05/18/2022 15:36:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 7.65 on epoch=732
05/18/2022 15:36:05 - INFO - __main__ - Step 2940 Global step 2940 Train loss 7.66 on epoch=734
05/18/2022 15:36:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 7.44 on epoch=737
05/18/2022 15:36:13 - INFO - __main__ - Global step 2950 Train loss 7.60 Classification-F1 0.0 on epoch=737
05/18/2022 15:36:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 7.52 on epoch=739
05/18/2022 15:36:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 7.70 on epoch=742
05/18/2022 15:36:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 7.64 on epoch=744
05/18/2022 15:36:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 7.45 on epoch=747
05/18/2022 15:36:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 7.58 on epoch=749
05/18/2022 15:36:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:36:22 - INFO - __main__ - Printing 3 examples
05/18/2022 15:36:22 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:36:22 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:36:22 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:36:22 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:36:22 - INFO - __main__ - Printing 3 examples
05/18/2022 15:36:22 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:36:22 - INFO - __main__ - ['others']
05/18/2022 15:36:22 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:36:23 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:36:23 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:36:30 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:36:30 - INFO - __main__ - task name: emo
05/18/2022 15:36:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:36:30 - INFO - __main__ - Starting training!
05/18/2022 15:36:34 - INFO - __main__ - Global step 3000 Train loss 7.58 Classification-F1 0.0 on epoch=749
05/18/2022 15:36:34 - INFO - __main__ - save last model!
05/18/2022 15:36:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 15:36:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 15:36:34 - INFO - __main__ - Printing 3 examples
05/18/2022 15:36:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 15:36:34 - INFO - __main__ - ['others']
05/18/2022 15:36:34 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 15:36:34 - INFO - __main__ - ['others']
05/18/2022 15:36:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 15:36:34 - INFO - __main__ - ['others']
05/18/2022 15:36:34 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:36:36 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:36:42 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 15:48:01 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.3_8_predictions.txt
05/18/2022 15:48:02 - INFO - __main__ - Classification-F1 on test data: 0.0000
05/18/2022 15:48:02 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.0
05/18/2022 15:48:02 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
05/18/2022 15:48:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:48:03 - INFO - __main__ - Printing 3 examples
05/18/2022 15:48:03 - INFO - __main__ -  [emo] how cause yes am listening
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:48:03 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:48:03 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:48:03 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:48:03 - INFO - __main__ - Printing 3 examples
05/18/2022 15:48:03 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
05/18/2022 15:48:03 - INFO - __main__ - ['others']
05/18/2022 15:48:03 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:48:03 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:48:03 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:48:10 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:48:10 - INFO - __main__ - task name: emo
05/18/2022 15:48:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:48:10 - INFO - __main__ - Starting training!
05/18/2022 15:48:12 - INFO - __main__ - Step 10 Global step 10 Train loss 7.11 on epoch=2
05/18/2022 15:48:13 - INFO - __main__ - Step 20 Global step 20 Train loss 5.35 on epoch=4
05/18/2022 15:48:15 - INFO - __main__ - Step 30 Global step 30 Train loss 3.59 on epoch=7
05/18/2022 15:48:16 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=9
05/18/2022 15:48:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.97 on epoch=12
05/18/2022 15:48:19 - INFO - __main__ - Global step 50 Train loss 4.09 Classification-F1 0.16862745098039217 on epoch=12
05/18/2022 15:48:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16862745098039217 on epoch=12, global_step=50
05/18/2022 15:48:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=14
05/18/2022 15:48:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.71 on epoch=17
05/18/2022 15:48:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.42 on epoch=19
05/18/2022 15:48:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.43 on epoch=22
05/18/2022 15:48:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.32 on epoch=24
05/18/2022 15:48:27 - INFO - __main__ - Global step 100 Train loss 1.49 Classification-F1 0.25681818181818183 on epoch=24
05/18/2022 15:48:27 - INFO - __main__ - Saving model with best Classification-F1: 0.16862745098039217 -> 0.25681818181818183 on epoch=24, global_step=100
05/18/2022 15:48:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.26 on epoch=27
05/18/2022 15:48:30 - INFO - __main__ - Step 120 Global step 120 Train loss 1.20 on epoch=29
05/18/2022 15:48:32 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=32
05/18/2022 15:48:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=34
05/18/2022 15:48:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.03 on epoch=37
05/18/2022 15:48:36 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.2556818181818182 on epoch=37
05/18/2022 15:48:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.16 on epoch=39
05/18/2022 15:48:38 - INFO - __main__ - Step 170 Global step 170 Train loss 1.11 on epoch=42
05/18/2022 15:48:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.11 on epoch=44
05/18/2022 15:48:41 - INFO - __main__ - Step 190 Global step 190 Train loss 1.66 on epoch=47
05/18/2022 15:48:43 - INFO - __main__ - Step 200 Global step 200 Train loss 1.03 on epoch=49
05/18/2022 15:48:44 - INFO - __main__ - Global step 200 Train loss 1.21 Classification-F1 0.17764705882352944 on epoch=49
05/18/2022 15:48:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.02 on epoch=52
05/18/2022 15:48:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.04 on epoch=54
05/18/2022 15:48:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.04 on epoch=57
05/18/2022 15:48:50 - INFO - __main__ - Step 240 Global step 240 Train loss 1.10 on epoch=59
05/18/2022 15:48:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=62
05/18/2022 15:48:52 - INFO - __main__ - Global step 250 Train loss 1.02 Classification-F1 0.19927536231884058 on epoch=62
05/18/2022 15:48:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.07 on epoch=64
05/18/2022 15:48:55 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=67
05/18/2022 15:48:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.98 on epoch=69
05/18/2022 15:48:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=72
05/18/2022 15:48:59 - INFO - __main__ - Step 300 Global step 300 Train loss 1.18 on epoch=74
05/18/2022 15:49:01 - INFO - __main__ - Global step 300 Train loss 1.04 Classification-F1 0.22978926875917893 on epoch=74
05/18/2022 15:49:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/18/2022 15:49:04 - INFO - __main__ - Step 320 Global step 320 Train loss 1.15 on epoch=79
05/18/2022 15:49:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=82
05/18/2022 15:49:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/18/2022 15:49:08 - INFO - __main__ - Step 350 Global step 350 Train loss 1.00 on epoch=87
05/18/2022 15:49:09 - INFO - __main__ - Global step 350 Train loss 1.01 Classification-F1 0.1565276828434723 on epoch=87
05/18/2022 15:49:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
05/18/2022 15:49:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
05/18/2022 15:49:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.98 on epoch=94
05/18/2022 15:49:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.97 on epoch=97
05/18/2022 15:49:16 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
05/18/2022 15:49:17 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.3599154775462808 on epoch=99
05/18/2022 15:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.25681818181818183 -> 0.3599154775462808 on epoch=99, global_step=400
05/18/2022 15:49:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/18/2022 15:49:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/18/2022 15:49:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=107
05/18/2022 15:49:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
05/18/2022 15:49:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.97 on epoch=112
05/18/2022 15:49:25 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.22993546758958827 on epoch=112
05/18/2022 15:49:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.97 on epoch=114
05/18/2022 15:49:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
05/18/2022 15:49:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/18/2022 15:49:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.99 on epoch=122
05/18/2022 15:49:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.88 on epoch=124
05/18/2022 15:49:34 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.31889834881320955 on epoch=124
05/18/2022 15:49:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.93 on epoch=127
05/18/2022 15:49:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
05/18/2022 15:49:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.87 on epoch=132
05/18/2022 15:49:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
05/18/2022 15:49:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
05/18/2022 15:49:42 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.3182957393483709 on epoch=137
05/18/2022 15:49:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.86 on epoch=139
05/18/2022 15:49:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=142
05/18/2022 15:49:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/18/2022 15:49:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.88 on epoch=147
05/18/2022 15:49:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
05/18/2022 15:49:51 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.40730284208545076 on epoch=149
05/18/2022 15:49:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3599154775462808 -> 0.40730284208545076 on epoch=149, global_step=600
05/18/2022 15:49:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
05/18/2022 15:49:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.85 on epoch=154
05/18/2022 15:49:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
05/18/2022 15:49:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.95 on epoch=159
05/18/2022 15:49:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.88 on epoch=162
05/18/2022 15:50:00 - INFO - __main__ - Global step 650 Train loss 0.90 Classification-F1 0.39870006252651796 on epoch=162
05/18/2022 15:50:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=164
05/18/2022 15:50:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.96 on epoch=167
05/18/2022 15:50:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=169
05/18/2022 15:50:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.95 on epoch=172
05/18/2022 15:50:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
05/18/2022 15:50:08 - INFO - __main__ - Global step 700 Train loss 0.94 Classification-F1 0.29485380116959065 on epoch=174
05/18/2022 15:50:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=177
05/18/2022 15:50:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/18/2022 15:50:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/18/2022 15:50:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.90 on epoch=184
05/18/2022 15:50:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
05/18/2022 15:50:16 - INFO - __main__ - Global step 750 Train loss 0.89 Classification-F1 0.26525725232621783 on epoch=187
05/18/2022 15:50:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=189
05/18/2022 15:50:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.87 on epoch=192
05/18/2022 15:50:21 - INFO - __main__ - Step 780 Global step 780 Train loss 1.02 on epoch=194
05/18/2022 15:50:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.88 on epoch=197
05/18/2022 15:50:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.86 on epoch=199
05/18/2022 15:50:24 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.2969569362784046 on epoch=199
05/18/2022 15:50:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/18/2022 15:50:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=204
05/18/2022 15:50:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=207
05/18/2022 15:50:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.80 on epoch=209
05/18/2022 15:50:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
05/18/2022 15:50:33 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.32734704600194553 on epoch=212
05/18/2022 15:50:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=214
05/18/2022 15:50:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
05/18/2022 15:50:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.85 on epoch=219
05/18/2022 15:50:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.72 on epoch=222
05/18/2022 15:50:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.74 on epoch=224
05/18/2022 15:50:41 - INFO - __main__ - Global step 900 Train loss 0.82 Classification-F1 0.4368421052631579 on epoch=224
05/18/2022 15:50:41 - INFO - __main__ - Saving model with best Classification-F1: 0.40730284208545076 -> 0.4368421052631579 on epoch=224, global_step=900
05/18/2022 15:50:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=227
05/18/2022 15:50:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.69 on epoch=229
05/18/2022 15:50:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.72 on epoch=232
05/18/2022 15:50:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
05/18/2022 15:50:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.84 on epoch=237
05/18/2022 15:50:49 - INFO - __main__ - Global step 950 Train loss 0.76 Classification-F1 0.4957393483709273 on epoch=237
05/18/2022 15:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4368421052631579 -> 0.4957393483709273 on epoch=237, global_step=950
05/18/2022 15:50:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.79 on epoch=239
05/18/2022 15:50:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.75 on epoch=242
05/18/2022 15:50:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.75 on epoch=244
05/18/2022 15:50:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
05/18/2022 15:50:55 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.74 on epoch=249
05/18/2022 15:50:56 - INFO - __main__ - Global step 1000 Train loss 0.75 Classification-F1 0.42917592917592917 on epoch=249
05/18/2022 15:50:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.65 on epoch=252
05/18/2022 15:50:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=254
05/18/2022 15:51:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.75 on epoch=257
05/18/2022 15:51:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.77 on epoch=259
05/18/2022 15:51:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.77 on epoch=262
05/18/2022 15:51:05 - INFO - __main__ - Global step 1050 Train loss 0.74 Classification-F1 0.5277605032595959 on epoch=262
05/18/2022 15:51:05 - INFO - __main__ - Saving model with best Classification-F1: 0.4957393483709273 -> 0.5277605032595959 on epoch=262, global_step=1050
05/18/2022 15:51:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=264
05/18/2022 15:51:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.71 on epoch=267
05/18/2022 15:51:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=269
05/18/2022 15:51:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.65 on epoch=272
05/18/2022 15:51:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.62 on epoch=274
05/18/2022 15:51:13 - INFO - __main__ - Global step 1100 Train loss 0.67 Classification-F1 0.4886155913978495 on epoch=274
05/18/2022 15:51:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.76 on epoch=277
05/18/2022 15:51:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.67 on epoch=279
05/18/2022 15:51:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.63 on epoch=282
05/18/2022 15:51:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=284
05/18/2022 15:51:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=287
05/18/2022 15:51:21 - INFO - __main__ - Global step 1150 Train loss 0.66 Classification-F1 0.5288832694347011 on epoch=287
05/18/2022 15:51:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5277605032595959 -> 0.5288832694347011 on epoch=287, global_step=1150
05/18/2022 15:51:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=289
05/18/2022 15:51:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=292
05/18/2022 15:51:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.70 on epoch=294
05/18/2022 15:51:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=297
05/18/2022 15:51:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.66 on epoch=299
05/18/2022 15:51:30 - INFO - __main__ - Global step 1200 Train loss 0.70 Classification-F1 0.4613408521303257 on epoch=299
05/18/2022 15:51:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=302
05/18/2022 15:51:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=304
05/18/2022 15:51:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.59 on epoch=307
05/18/2022 15:51:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=309
05/18/2022 15:51:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.70 on epoch=312
05/18/2022 15:51:38 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.5095348837209303 on epoch=312
05/18/2022 15:51:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=314
05/18/2022 15:51:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=317
05/18/2022 15:51:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=319
05/18/2022 15:51:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=322
05/18/2022 15:51:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=324
05/18/2022 15:51:46 - INFO - __main__ - Global step 1300 Train loss 0.59 Classification-F1 0.49619453044375644 on epoch=324
05/18/2022 15:51:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.65 on epoch=327
05/18/2022 15:51:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=329
05/18/2022 15:51:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.63 on epoch=332
05/18/2022 15:51:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=334
05/18/2022 15:51:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=337
05/18/2022 15:51:55 - INFO - __main__ - Global step 1350 Train loss 0.57 Classification-F1 0.5477634892957474 on epoch=337
05/18/2022 15:51:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5288832694347011 -> 0.5477634892957474 on epoch=337, global_step=1350
05/18/2022 15:51:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=339
05/18/2022 15:51:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=342
05/18/2022 15:51:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=344
05/18/2022 15:52:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.62 on epoch=347
05/18/2022 15:52:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=349
05/18/2022 15:52:03 - INFO - __main__ - Global step 1400 Train loss 0.53 Classification-F1 0.5512076562373676 on epoch=349
05/18/2022 15:52:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5477634892957474 -> 0.5512076562373676 on epoch=349, global_step=1400
05/18/2022 15:52:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.56 on epoch=352
05/18/2022 15:52:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=354
05/18/2022 15:52:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=357
05/18/2022 15:52:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=359
05/18/2022 15:52:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=362
05/18/2022 15:52:12 - INFO - __main__ - Global step 1450 Train loss 0.52 Classification-F1 0.49241071428571426 on epoch=362
05/18/2022 15:52:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=364
05/18/2022 15:52:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=367
05/18/2022 15:52:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=369
05/18/2022 15:52:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=372
05/18/2022 15:52:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=374
05/18/2022 15:52:21 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.5289014377927053 on epoch=374
05/18/2022 15:52:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=377
05/18/2022 15:52:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=379
05/18/2022 15:52:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=382
05/18/2022 15:52:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=384
05/18/2022 15:52:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=387
05/18/2022 15:52:29 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.5286935286935287 on epoch=387
05/18/2022 15:52:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=389
05/18/2022 15:52:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=392
05/18/2022 15:52:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=394
05/18/2022 15:52:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=397
05/18/2022 15:52:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=399
05/18/2022 15:52:37 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.5513287582253099 on epoch=399
05/18/2022 15:52:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5512076562373676 -> 0.5513287582253099 on epoch=399, global_step=1600
05/18/2022 15:52:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
05/18/2022 15:52:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=404
05/18/2022 15:52:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=407
05/18/2022 15:52:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=409
05/18/2022 15:52:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=412
05/18/2022 15:52:45 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.5564039408866995 on epoch=412
05/18/2022 15:52:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5513287582253099 -> 0.5564039408866995 on epoch=412, global_step=1650
05/18/2022 15:52:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=414
05/18/2022 15:52:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=417
05/18/2022 15:52:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=419
05/18/2022 15:52:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=422
05/18/2022 15:52:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=424
05/18/2022 15:52:54 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.5642857142857143 on epoch=424
05/18/2022 15:52:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5564039408866995 -> 0.5642857142857143 on epoch=424, global_step=1700
05/18/2022 15:52:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=427
05/18/2022 15:52:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=429
05/18/2022 15:52:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=432
05/18/2022 15:52:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=434
05/18/2022 15:53:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=437
05/18/2022 15:53:02 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.5333520968195891 on epoch=437
05/18/2022 15:53:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=439
05/18/2022 15:53:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=442
05/18/2022 15:53:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=444
05/18/2022 15:53:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=447
05/18/2022 15:53:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=449
05/18/2022 15:53:10 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.5365141232575201 on epoch=449
05/18/2022 15:53:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=452
05/18/2022 15:53:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=454
05/18/2022 15:53:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=457
05/18/2022 15:53:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=459
05/18/2022 15:53:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=462
05/18/2022 15:53:19 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.5798275509650722 on epoch=462
05/18/2022 15:53:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5642857142857143 -> 0.5798275509650722 on epoch=462, global_step=1850
05/18/2022 15:53:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=464
05/18/2022 15:53:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=467
05/18/2022 15:53:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=469
05/18/2022 15:53:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=472
05/18/2022 15:53:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=474
05/18/2022 15:53:27 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.711154768899334 on epoch=474
05/18/2022 15:53:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5798275509650722 -> 0.711154768899334 on epoch=474, global_step=1900
05/18/2022 15:53:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=477
05/18/2022 15:53:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=479
05/18/2022 15:53:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=482
05/18/2022 15:53:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=484
05/18/2022 15:53:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=487
05/18/2022 15:53:36 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.6131160572337042 on epoch=487
05/18/2022 15:53:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=489
05/18/2022 15:53:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=492
05/18/2022 15:53:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=494
05/18/2022 15:53:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=497
05/18/2022 15:53:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=499
05/18/2022 15:53:44 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.6489826302729529 on epoch=499
05/18/2022 15:53:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=502
05/18/2022 15:53:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=504
05/18/2022 15:53:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=507
05/18/2022 15:53:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=509
05/18/2022 15:53:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=512
05/18/2022 15:53:51 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.6571637426900585 on epoch=512
05/18/2022 15:53:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=514
05/18/2022 15:53:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=517
05/18/2022 15:53:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=519
05/18/2022 15:53:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=522
05/18/2022 15:53:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=524
05/18/2022 15:54:00 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.6810065237651444 on epoch=524
05/18/2022 15:54:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=527
05/18/2022 15:54:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=529
05/18/2022 15:54:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=532
05/18/2022 15:54:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=534
05/18/2022 15:54:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=537
05/18/2022 15:54:08 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.6877552177858439 on epoch=537
05/18/2022 15:54:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=539
05/18/2022 15:54:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=542
05/18/2022 15:54:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=544
05/18/2022 15:54:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=547
05/18/2022 15:54:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=549
05/18/2022 15:54:16 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7176134781397939 on epoch=549
05/18/2022 15:54:16 - INFO - __main__ - Saving model with best Classification-F1: 0.711154768899334 -> 0.7176134781397939 on epoch=549, global_step=2200
05/18/2022 15:54:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=552
05/18/2022 15:54:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=554
05/18/2022 15:54:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=557
05/18/2022 15:54:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=559
05/18/2022 15:54:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=562
05/18/2022 15:54:24 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.6728534155597723 on epoch=562
05/18/2022 15:54:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=564
05/18/2022 15:54:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=567
05/18/2022 15:54:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=569
05/18/2022 15:54:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=572
05/18/2022 15:54:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=574
05/18/2022 15:54:32 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.68592060960482 on epoch=574
05/18/2022 15:54:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
05/18/2022 15:54:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=579
05/18/2022 15:54:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=582
05/18/2022 15:54:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=584
05/18/2022 15:54:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=587
05/18/2022 15:54:41 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.6957264957264956 on epoch=587
05/18/2022 15:54:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=589
05/18/2022 15:54:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
05/18/2022 15:54:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=594
05/18/2022 15:54:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
05/18/2022 15:54:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=599
05/18/2022 15:54:49 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.6990643764837313 on epoch=599
05/18/2022 15:54:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=602
05/18/2022 15:54:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=604
05/18/2022 15:54:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
05/18/2022 15:54:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
05/18/2022 15:54:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
05/18/2022 15:54:57 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.6823953073953073 on epoch=612
05/18/2022 15:54:59 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=614
05/18/2022 15:55:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
05/18/2022 15:55:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
05/18/2022 15:55:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
05/18/2022 15:55:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/18/2022 15:55:06 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7117117117117118 on epoch=624
05/18/2022 15:55:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=627
05/18/2022 15:55:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=629
05/18/2022 15:55:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=632
05/18/2022 15:55:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=634
05/18/2022 15:55:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=637
05/18/2022 15:55:14 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.6728707250446381 on epoch=637
05/18/2022 15:55:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=639
05/18/2022 15:55:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=642
05/18/2022 15:55:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=644
05/18/2022 15:55:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 15:55:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/18/2022 15:55:22 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.667251361912096 on epoch=649
05/18/2022 15:55:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
05/18/2022 15:55:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=654
05/18/2022 15:55:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=657
05/18/2022 15:55:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
05/18/2022 15:55:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=662
05/18/2022 15:55:31 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.7161709258483453 on epoch=662
05/18/2022 15:55:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=664
05/18/2022 15:55:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=667
05/18/2022 15:55:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/18/2022 15:55:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
05/18/2022 15:55:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/18/2022 15:55:39 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6646198149147333 on epoch=674
05/18/2022 15:55:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=677
05/18/2022 15:55:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/18/2022 15:55:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=682
05/18/2022 15:55:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=684
05/18/2022 15:55:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/18/2022 15:55:47 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.6503923519009726 on epoch=687
05/18/2022 15:55:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
05/18/2022 15:55:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=692
05/18/2022 15:55:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/18/2022 15:55:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
05/18/2022 15:55:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/18/2022 15:55:56 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.6825593761077633 on epoch=699
05/18/2022 15:55:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/18/2022 15:55:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=704
05/18/2022 15:56:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=707
05/18/2022 15:56:02 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=709
05/18/2022 15:56:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
05/18/2022 15:56:04 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.6763578715191618 on epoch=712
05/18/2022 15:56:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=714
05/18/2022 15:56:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/18/2022 15:56:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 15:56:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/18/2022 15:56:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/18/2022 15:56:13 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6606959706959707 on epoch=724
05/18/2022 15:56:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=727
05/18/2022 15:56:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
05/18/2022 15:56:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/18/2022 15:56:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=734
05/18/2022 15:56:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=737
05/18/2022 15:56:21 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7098684210526316 on epoch=737
05/18/2022 15:56:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/18/2022 15:56:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
05/18/2022 15:56:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=744
05/18/2022 15:56:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
05/18/2022 15:56:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
05/18/2022 15:56:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:56:30 - INFO - __main__ - Printing 3 examples
05/18/2022 15:56:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:56:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:56:30 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:56:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:56:30 - INFO - __main__ - Printing 3 examples
05/18/2022 15:56:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:56:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:56:30 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:56:30 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.6968641361468187 on epoch=749
05/18/2022 15:56:30 - INFO - __main__ - save last model!
05/18/2022 15:56:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 15:56:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 15:56:30 - INFO - __main__ - Printing 3 examples
05/18/2022 15:56:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 15:56:30 - INFO - __main__ - ['others']
05/18/2022 15:56:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:56:33 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:56:37 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:56:37 - INFO - __main__ - task name: emo
05/18/2022 15:56:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:56:37 - INFO - __main__ - Starting training!
05/18/2022 15:56:39 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 15:58:01 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.2_8_predictions.txt
05/18/2022 15:58:01 - INFO - __main__ - Classification-F1 on test data: 0.3346
05/18/2022 15:58:01 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7176134781397939, test_performance=0.33456786551417467
05/18/2022 15:58:01 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
05/18/2022 15:58:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:58:02 - INFO - __main__ - Printing 3 examples
05/18/2022 15:58:02 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:58:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:58:02 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 15:58:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 15:58:02 - INFO - __main__ - Printing 3 examples
05/18/2022 15:58:02 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 15:58:02 - INFO - __main__ - ['others']
05/18/2022 15:58:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 15:58:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 15:58:02 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 15:58:08 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 15:58:08 - INFO - __main__ - task name: emo
05/18/2022 15:58:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 15:58:09 - INFO - __main__ - Starting training!
05/18/2022 15:58:10 - INFO - __main__ - Step 10 Global step 10 Train loss 5.61 on epoch=2
05/18/2022 15:58:12 - INFO - __main__ - Step 20 Global step 20 Train loss 2.75 on epoch=4
05/18/2022 15:58:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=7
05/18/2022 15:58:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.28 on epoch=9
05/18/2022 15:58:16 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=12
05/18/2022 15:58:18 - INFO - __main__ - Global step 50 Train loss 2.55 Classification-F1 0.1 on epoch=12
05/18/2022 15:58:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 15:58:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=14
05/18/2022 15:58:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.45 on epoch=17
05/18/2022 15:58:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=19
05/18/2022 15:58:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
05/18/2022 15:58:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=24
05/18/2022 15:58:27 - INFO - __main__ - Global step 100 Train loss 1.09 Classification-F1 0.10273972602739727 on epoch=24
05/18/2022 15:58:27 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10273972602739727 on epoch=24, global_step=100
05/18/2022 15:58:29 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/18/2022 15:58:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=29
05/18/2022 15:58:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=32
05/18/2022 15:58:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
05/18/2022 15:58:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=37
05/18/2022 15:58:36 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.10389610389610389 on epoch=37
05/18/2022 15:58:36 - INFO - __main__ - Saving model with best Classification-F1: 0.10273972602739727 -> 0.10389610389610389 on epoch=37, global_step=150
05/18/2022 15:58:37 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=39
05/18/2022 15:58:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
05/18/2022 15:58:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=44
05/18/2022 15:58:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
05/18/2022 15:58:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
05/18/2022 15:58:44 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.1 on epoch=49
05/18/2022 15:58:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
05/18/2022 15:58:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
05/18/2022 15:58:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
05/18/2022 15:58:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=59
05/18/2022 15:58:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=62
05/18/2022 15:58:53 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.2341430499325236 on epoch=62
05/18/2022 15:58:53 - INFO - __main__ - Saving model with best Classification-F1: 0.10389610389610389 -> 0.2341430499325236 on epoch=62, global_step=250
05/18/2022 15:58:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
05/18/2022 15:58:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=67
05/18/2022 15:58:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
05/18/2022 15:58:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
05/18/2022 15:59:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
05/18/2022 15:59:02 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.1622222222222222 on epoch=74
05/18/2022 15:59:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
05/18/2022 15:59:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
05/18/2022 15:59:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
05/18/2022 15:59:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=84
05/18/2022 15:59:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/18/2022 15:59:11 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.24116959064327484 on epoch=87
05/18/2022 15:59:11 - INFO - __main__ - Saving model with best Classification-F1: 0.2341430499325236 -> 0.24116959064327484 on epoch=87, global_step=350
05/18/2022 15:59:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=89
05/18/2022 15:59:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.76 on epoch=92
05/18/2022 15:59:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=94
05/18/2022 15:59:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.80 on epoch=97
05/18/2022 15:59:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
05/18/2022 15:59:19 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.5567877140627351 on epoch=99
05/18/2022 15:59:19 - INFO - __main__ - Saving model with best Classification-F1: 0.24116959064327484 -> 0.5567877140627351 on epoch=99, global_step=400
05/18/2022 15:59:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=102
05/18/2022 15:59:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=104
05/18/2022 15:59:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=107
05/18/2022 15:59:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
05/18/2022 15:59:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=112
05/18/2022 15:59:26 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.43861870989530566 on epoch=112
05/18/2022 15:59:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.65 on epoch=114
05/18/2022 15:59:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=117
05/18/2022 15:59:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.69 on epoch=119
05/18/2022 15:59:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=122
05/18/2022 15:59:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=124
05/18/2022 15:59:34 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3578259605729084 on epoch=124
05/18/2022 15:59:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
05/18/2022 15:59:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=129
05/18/2022 15:59:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=132
05/18/2022 15:59:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
05/18/2022 15:59:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=137
05/18/2022 15:59:42 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.47337178188242024 on epoch=137
05/18/2022 15:59:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=139
05/18/2022 15:59:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=142
05/18/2022 15:59:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
05/18/2022 15:59:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=147
05/18/2022 15:59:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=149
05/18/2022 15:59:50 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.40147058823529413 on epoch=149
05/18/2022 15:59:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=152
05/18/2022 15:59:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=154
05/18/2022 15:59:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=157
05/18/2022 15:59:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=159
05/18/2022 15:59:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=162
05/18/2022 15:59:59 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.5366430020283975 on epoch=162
05/18/2022 16:00:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=164
05/18/2022 16:00:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=167
05/18/2022 16:00:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=169
05/18/2022 16:00:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=172
05/18/2022 16:00:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=174
05/18/2022 16:00:07 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.5989372469635628 on epoch=174
05/18/2022 16:00:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5567877140627351 -> 0.5989372469635628 on epoch=174, global_step=700
05/18/2022 16:00:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
05/18/2022 16:00:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=179
05/18/2022 16:00:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=182
05/18/2022 16:00:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=184
05/18/2022 16:00:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=187
05/18/2022 16:00:15 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.589676529978254 on epoch=187
05/18/2022 16:00:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=189
05/18/2022 16:00:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=192
05/18/2022 16:00:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=194
05/18/2022 16:00:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=197
05/18/2022 16:00:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=199
05/18/2022 16:00:24 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.5045333923042901 on epoch=199
05/18/2022 16:00:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=202
05/18/2022 16:00:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
05/18/2022 16:00:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
05/18/2022 16:00:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=209
05/18/2022 16:00:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
05/18/2022 16:00:32 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.5160670746877642 on epoch=212
05/18/2022 16:00:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=214
05/18/2022 16:00:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=217
05/18/2022 16:00:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=219
05/18/2022 16:00:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
05/18/2022 16:00:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=224
05/18/2022 16:00:41 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.5922532129428681 on epoch=224
05/18/2022 16:00:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=227
05/18/2022 16:00:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=229
05/18/2022 16:00:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=232
05/18/2022 16:00:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
05/18/2022 16:00:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=237
05/18/2022 16:00:49 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.6075980392156862 on epoch=237
05/18/2022 16:00:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5989372469635628 -> 0.6075980392156862 on epoch=237, global_step=950
05/18/2022 16:00:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
05/18/2022 16:00:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
05/18/2022 16:00:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
05/18/2022 16:00:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=247
05/18/2022 16:00:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
05/18/2022 16:00:58 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5919104459150226 on epoch=249
05/18/2022 16:00:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
05/18/2022 16:01:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
05/18/2022 16:01:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
05/18/2022 16:01:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
05/18/2022 16:01:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=262
05/18/2022 16:01:06 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.6313095238095238 on epoch=262
05/18/2022 16:01:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6075980392156862 -> 0.6313095238095238 on epoch=262, global_step=1050
05/18/2022 16:01:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
05/18/2022 16:01:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
05/18/2022 16:01:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=269
05/18/2022 16:01:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
05/18/2022 16:01:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
05/18/2022 16:01:15 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.5923983386458032 on epoch=274
05/18/2022 16:01:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
05/18/2022 16:01:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=279
05/18/2022 16:01:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/18/2022 16:01:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
05/18/2022 16:01:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
05/18/2022 16:01:23 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6302443517813536 on epoch=287
05/18/2022 16:01:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
05/18/2022 16:01:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/18/2022 16:01:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
05/18/2022 16:01:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
05/18/2022 16:01:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
05/18/2022 16:01:32 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.5980653815580286 on epoch=299
05/18/2022 16:01:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
05/18/2022 16:01:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
05/18/2022 16:01:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/18/2022 16:01:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
05/18/2022 16:01:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/18/2022 16:01:40 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5952750206782466 on epoch=312
05/18/2022 16:01:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
05/18/2022 16:01:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
05/18/2022 16:01:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
05/18/2022 16:01:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=322
05/18/2022 16:01:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/18/2022 16:01:49 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6009523809523809 on epoch=324
05/18/2022 16:01:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
05/18/2022 16:01:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
05/18/2022 16:01:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
05/18/2022 16:01:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
05/18/2022 16:01:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/18/2022 16:01:58 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6402646815550042 on epoch=337
05/18/2022 16:01:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6313095238095238 -> 0.6402646815550042 on epoch=337, global_step=1350
05/18/2022 16:01:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
05/18/2022 16:02:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=342
05/18/2022 16:02:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
05/18/2022 16:02:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
05/18/2022 16:02:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/18/2022 16:02:06 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6174768518518519 on epoch=349
05/18/2022 16:02:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
05/18/2022 16:02:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
05/18/2022 16:02:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/18/2022 16:02:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=359
05/18/2022 16:02:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
05/18/2022 16:02:14 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.599946524064171 on epoch=362
05/18/2022 16:02:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
05/18/2022 16:02:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
05/18/2022 16:02:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
05/18/2022 16:02:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/18/2022 16:02:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
05/18/2022 16:02:22 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.621875 on epoch=374
05/18/2022 16:02:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/18/2022 16:02:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
05/18/2022 16:02:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
05/18/2022 16:02:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/18/2022 16:02:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
05/18/2022 16:02:30 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.658655462184874 on epoch=387
05/18/2022 16:02:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6402646815550042 -> 0.658655462184874 on epoch=387, global_step=1550
05/18/2022 16:02:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
05/18/2022 16:02:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/18/2022 16:02:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/18/2022 16:02:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/18/2022 16:02:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
05/18/2022 16:02:38 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6660287081339712 on epoch=399
05/18/2022 16:02:38 - INFO - __main__ - Saving model with best Classification-F1: 0.658655462184874 -> 0.6660287081339712 on epoch=399, global_step=1600
05/18/2022 16:02:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
05/18/2022 16:02:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/18/2022 16:02:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/18/2022 16:02:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
05/18/2022 16:02:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/18/2022 16:02:46 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6261904761904762 on epoch=412
05/18/2022 16:02:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
05/18/2022 16:02:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/18/2022 16:02:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/18/2022 16:02:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/18/2022 16:02:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
05/18/2022 16:02:55 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.601984126984127 on epoch=424
05/18/2022 16:02:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=427
05/18/2022 16:02:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/18/2022 16:02:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=432
05/18/2022 16:03:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/18/2022 16:03:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
05/18/2022 16:03:03 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.705448717948718 on epoch=437
05/18/2022 16:03:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6660287081339712 -> 0.705448717948718 on epoch=437, global_step=1750
05/18/2022 16:03:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
05/18/2022 16:03:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
05/18/2022 16:03:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
05/18/2022 16:03:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/18/2022 16:03:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
05/18/2022 16:03:12 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6735455090718248 on epoch=449
05/18/2022 16:03:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
05/18/2022 16:03:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
05/18/2022 16:03:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/18/2022 16:03:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/18/2022 16:03:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
05/18/2022 16:03:20 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6916852210969858 on epoch=462
05/18/2022 16:03:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
05/18/2022 16:03:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/18/2022 16:03:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/18/2022 16:03:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
05/18/2022 16:03:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
05/18/2022 16:03:29 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6704433497536946 on epoch=474
05/18/2022 16:03:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/18/2022 16:03:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/18/2022 16:03:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/18/2022 16:03:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/18/2022 16:03:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
05/18/2022 16:03:37 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6560442654192654 on epoch=487
05/18/2022 16:03:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
05/18/2022 16:03:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/18/2022 16:03:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=494
05/18/2022 16:03:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
05/18/2022 16:03:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
05/18/2022 16:03:45 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.694352146263911 on epoch=499
05/18/2022 16:03:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/18/2022 16:03:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/18/2022 16:03:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
05/18/2022 16:03:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/18/2022 16:03:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/18/2022 16:03:53 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6788868857180052 on epoch=512
05/18/2022 16:03:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
05/18/2022 16:03:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
05/18/2022 16:03:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/18/2022 16:03:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
05/18/2022 16:04:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
05/18/2022 16:04:01 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6507103409246141 on epoch=524
05/18/2022 16:04:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/18/2022 16:04:04 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/18/2022 16:04:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
05/18/2022 16:04:08 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/18/2022 16:04:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/18/2022 16:04:11 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6292393880629175 on epoch=537
05/18/2022 16:04:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/18/2022 16:04:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
05/18/2022 16:04:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/18/2022 16:04:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/18/2022 16:04:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
05/18/2022 16:04:18 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6801782682512734 on epoch=549
05/18/2022 16:04:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/18/2022 16:04:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/18/2022 16:04:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
05/18/2022 16:04:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/18/2022 16:04:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/18/2022 16:04:26 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6262288786482334 on epoch=562
05/18/2022 16:04:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/18/2022 16:04:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/18/2022 16:04:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/18/2022 16:04:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
05/18/2022 16:04:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/18/2022 16:04:35 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6236559139784945 on epoch=574
05/18/2022 16:04:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
05/18/2022 16:04:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/18/2022 16:04:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
05/18/2022 16:04:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
05/18/2022 16:04:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/18/2022 16:04:43 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.60625 on epoch=587
05/18/2022 16:04:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/18/2022 16:04:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/18/2022 16:04:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/18/2022 16:04:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/18/2022 16:04:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/18/2022 16:04:51 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.623214708435256 on epoch=599
05/18/2022 16:04:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
05/18/2022 16:04:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
05/18/2022 16:04:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/18/2022 16:04:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
05/18/2022 16:04:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/18/2022 16:04:59 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6059139784946236 on epoch=612
05/18/2022 16:05:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/18/2022 16:05:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/18/2022 16:05:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/18/2022 16:05:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/18/2022 16:05:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/18/2022 16:05:07 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.650775694893342 on epoch=624
05/18/2022 16:05:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/18/2022 16:05:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/18/2022 16:05:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/18/2022 16:05:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
05/18/2022 16:05:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/18/2022 16:05:15 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.635844263263618 on epoch=637
05/18/2022 16:05:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 16:05:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/18/2022 16:05:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
05/18/2022 16:05:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/18/2022 16:05:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
05/18/2022 16:05:24 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6483754690651242 on epoch=649
05/18/2022 16:05:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/18/2022 16:05:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/18/2022 16:05:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/18/2022 16:05:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 16:05:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/18/2022 16:05:32 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6293153815580286 on epoch=662
05/18/2022 16:05:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/18/2022 16:05:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/18/2022 16:05:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/18/2022 16:05:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/18/2022 16:05:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/18/2022 16:05:41 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6349217571644041 on epoch=674
05/18/2022 16:05:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/18/2022 16:05:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/18/2022 16:05:46 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
05/18/2022 16:05:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
05/18/2022 16:05:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/18/2022 16:05:50 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5988727198196352 on epoch=687
05/18/2022 16:05:51 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/18/2022 16:05:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/18/2022 16:05:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/18/2022 16:05:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/18/2022 16:05:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/18/2022 16:05:57 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.5824655299844534 on epoch=699
05/18/2022 16:05:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/18/2022 16:06:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/18/2022 16:06:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/18/2022 16:06:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/18/2022 16:06:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/18/2022 16:06:06 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5932351994851994 on epoch=712
05/18/2022 16:06:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/18/2022 16:06:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/18/2022 16:06:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/18/2022 16:06:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/18/2022 16:06:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/18/2022 16:06:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6105090311986865 on epoch=724
05/18/2022 16:06:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/18/2022 16:06:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 16:06:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 16:06:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/18/2022 16:06:20 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 16:06:21 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5960365853658537 on epoch=737
05/18/2022 16:06:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/18/2022 16:06:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/18/2022 16:06:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/18/2022 16:06:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/18/2022 16:06:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/18/2022 16:06:30 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6295001364127017 on epoch=749
05/18/2022 16:06:30 - INFO - __main__ - save last model!
05/18/2022 16:06:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:06:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:06:30 - INFO - __main__ - Printing 3 examples
05/18/2022 16:06:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:06:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:06:30 - INFO - __main__ - Printing 3 examples
05/18/2022 16:06:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:06:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:06:30 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:06:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:06:30 - INFO - __main__ - Printing 3 examples
05/18/2022 16:06:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:06:30 - INFO - __main__ - ['others']
05/18/2022 16:06:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:06:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:06:30 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:06:32 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:06:37 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:06:37 - INFO - __main__ - task name: emo
05/18/2022 16:06:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:06:37 - INFO - __main__ - Starting training!
05/18/2022 16:06:39 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:07:59 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.5_8_predictions.txt
05/18/2022 16:07:59 - INFO - __main__ - Classification-F1 on test data: 0.2904
05/18/2022 16:07:59 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.705448717948718, test_performance=0.29042110643879665
05/18/2022 16:07:59 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
05/18/2022 16:08:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:08:00 - INFO - __main__ - Printing 3 examples
05/18/2022 16:08:00 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:08:00 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:08:00 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:08:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:08:00 - INFO - __main__ - Printing 3 examples
05/18/2022 16:08:00 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:08:00 - INFO - __main__ - ['others']
05/18/2022 16:08:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:08:00 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:08:00 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:08:07 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:08:07 - INFO - __main__ - task name: emo
05/18/2022 16:08:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:08:07 - INFO - __main__ - Starting training!
05/18/2022 16:08:09 - INFO - __main__ - Step 10 Global step 10 Train loss 6.54 on epoch=2
05/18/2022 16:08:10 - INFO - __main__ - Step 20 Global step 20 Train loss 4.10 on epoch=4
05/18/2022 16:08:12 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
05/18/2022 16:08:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=9
05/18/2022 16:08:15 - INFO - __main__ - Step 50 Global step 50 Train loss 1.54 on epoch=12
05/18/2022 16:08:16 - INFO - __main__ - Global step 50 Train loss 3.29 Classification-F1 0.2684178743961353 on epoch=12
05/18/2022 16:08:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2684178743961353 on epoch=12, global_step=50
05/18/2022 16:08:18 - INFO - __main__ - Step 60 Global step 60 Train loss 1.31 on epoch=14
05/18/2022 16:08:19 - INFO - __main__ - Step 70 Global step 70 Train loss 1.32 on epoch=17
05/18/2022 16:08:21 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=19
05/18/2022 16:08:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
05/18/2022 16:08:24 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
05/18/2022 16:08:25 - INFO - __main__ - Global step 100 Train loss 1.22 Classification-F1 0.17154886148007592 on epoch=24
05/18/2022 16:08:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
05/18/2022 16:08:28 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=29
05/18/2022 16:08:29 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
05/18/2022 16:08:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
05/18/2022 16:08:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
05/18/2022 16:08:33 - INFO - __main__ - Global step 150 Train loss 1.06 Classification-F1 0.30303030303030304 on epoch=37
05/18/2022 16:08:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2684178743961353 -> 0.30303030303030304 on epoch=37, global_step=150
05/18/2022 16:08:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/18/2022 16:08:36 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/18/2022 16:08:37 - INFO - __main__ - Step 180 Global step 180 Train loss 1.07 on epoch=44
05/18/2022 16:08:39 - INFO - __main__ - Step 190 Global step 190 Train loss 1.06 on epoch=47
05/18/2022 16:08:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
05/18/2022 16:08:41 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.3472222222222222 on epoch=49
05/18/2022 16:08:41 - INFO - __main__ - Saving model with best Classification-F1: 0.30303030303030304 -> 0.3472222222222222 on epoch=49, global_step=200
05/18/2022 16:08:43 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
05/18/2022 16:08:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
05/18/2022 16:08:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
05/18/2022 16:08:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/18/2022 16:08:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
05/18/2022 16:08:51 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.3617162407484988 on epoch=62
05/18/2022 16:08:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3472222222222222 -> 0.3617162407484988 on epoch=62, global_step=250
05/18/2022 16:08:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=64
05/18/2022 16:08:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
05/18/2022 16:08:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=69
05/18/2022 16:08:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
05/18/2022 16:08:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
05/18/2022 16:08:59 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.46434676434676436 on epoch=74
05/18/2022 16:08:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3617162407484988 -> 0.46434676434676436 on epoch=74, global_step=300
05/18/2022 16:09:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=77
05/18/2022 16:09:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=79
05/18/2022 16:09:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=82
05/18/2022 16:09:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=84
05/18/2022 16:09:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
05/18/2022 16:09:07 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.6576178451178452 on epoch=87
05/18/2022 16:09:07 - INFO - __main__ - Saving model with best Classification-F1: 0.46434676434676436 -> 0.6576178451178452 on epoch=87, global_step=350
05/18/2022 16:09:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
05/18/2022 16:09:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=92
05/18/2022 16:09:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=94
05/18/2022 16:09:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=97
05/18/2022 16:09:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=99
05/18/2022 16:09:15 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.45570026046884393 on epoch=99
05/18/2022 16:09:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=102
05/18/2022 16:09:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=104
05/18/2022 16:09:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.63 on epoch=107
05/18/2022 16:09:21 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
05/18/2022 16:09:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=112
05/18/2022 16:09:23 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.5725490196078431 on epoch=112
05/18/2022 16:09:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=114
05/18/2022 16:09:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=117
05/18/2022 16:09:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=119
05/18/2022 16:09:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
05/18/2022 16:09:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.69 on epoch=124
05/18/2022 16:09:32 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.5094322344322344 on epoch=124
05/18/2022 16:09:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=127
05/18/2022 16:09:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=129
05/18/2022 16:09:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=132
05/18/2022 16:09:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=134
05/18/2022 16:09:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=137
05/18/2022 16:09:40 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.5462902752376436 on epoch=137
05/18/2022 16:09:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=139
05/18/2022 16:09:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=142
05/18/2022 16:09:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=144
05/18/2022 16:09:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=147
05/18/2022 16:09:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=149
05/18/2022 16:09:48 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.442621809635279 on epoch=149
05/18/2022 16:09:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=152
05/18/2022 16:09:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=154
05/18/2022 16:09:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=157
05/18/2022 16:09:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=159
05/18/2022 16:09:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
05/18/2022 16:09:56 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.5686937933067964 on epoch=162
05/18/2022 16:09:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=164
05/18/2022 16:09:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=167
05/18/2022 16:10:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=169
05/18/2022 16:10:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=172
05/18/2022 16:10:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=174
05/18/2022 16:10:04 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.6699560358069813 on epoch=174
05/18/2022 16:10:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6576178451178452 -> 0.6699560358069813 on epoch=174, global_step=700
05/18/2022 16:10:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=177
05/18/2022 16:10:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=179
05/18/2022 16:10:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=182
05/18/2022 16:10:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=184
05/18/2022 16:10:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=187
05/18/2022 16:10:13 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.652076692746064 on epoch=187
05/18/2022 16:10:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=189
05/18/2022 16:10:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
05/18/2022 16:10:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=194
05/18/2022 16:10:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=197
05/18/2022 16:10:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
05/18/2022 16:10:21 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.6501758205612161 on epoch=199
05/18/2022 16:10:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
05/18/2022 16:10:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=204
05/18/2022 16:10:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
05/18/2022 16:10:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=209
05/18/2022 16:10:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
05/18/2022 16:10:29 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.6468504795117698 on epoch=212
05/18/2022 16:10:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
05/18/2022 16:10:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
05/18/2022 16:10:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
05/18/2022 16:10:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
05/18/2022 16:10:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
05/18/2022 16:10:38 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.5912949239562142 on epoch=224
05/18/2022 16:10:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
05/18/2022 16:10:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
05/18/2022 16:10:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=232
05/18/2022 16:10:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
05/18/2022 16:10:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=237
05/18/2022 16:10:46 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.6216764418377322 on epoch=237
05/18/2022 16:10:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
05/18/2022 16:10:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
05/18/2022 16:10:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
05/18/2022 16:10:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
05/18/2022 16:10:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
05/18/2022 16:10:54 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.5541827541827542 on epoch=249
05/18/2022 16:10:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=252
05/18/2022 16:10:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/18/2022 16:10:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
05/18/2022 16:11:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/18/2022 16:11:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
05/18/2022 16:11:03 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.6175516795865633 on epoch=262
05/18/2022 16:11:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=264
05/18/2022 16:11:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
05/18/2022 16:11:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=269
05/18/2022 16:11:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
05/18/2022 16:11:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=274
05/18/2022 16:11:11 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.5755170755170755 on epoch=274
05/18/2022 16:11:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
05/18/2022 16:11:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/18/2022 16:11:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
05/18/2022 16:11:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
05/18/2022 16:11:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
05/18/2022 16:11:19 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5880821998469058 on epoch=287
05/18/2022 16:11:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
05/18/2022 16:11:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
05/18/2022 16:11:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
05/18/2022 16:11:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
05/18/2022 16:11:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
05/18/2022 16:11:28 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6215142596712396 on epoch=299
05/18/2022 16:11:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
05/18/2022 16:11:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/18/2022 16:11:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
05/18/2022 16:11:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
05/18/2022 16:11:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
05/18/2022 16:11:37 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.618034732633683 on epoch=312
05/18/2022 16:11:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
05/18/2022 16:11:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
05/18/2022 16:11:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
05/18/2022 16:11:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
05/18/2022 16:11:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
05/18/2022 16:11:45 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.5586424934251021 on epoch=324
05/18/2022 16:11:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/18/2022 16:11:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
05/18/2022 16:11:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
05/18/2022 16:11:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
05/18/2022 16:11:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
05/18/2022 16:11:55 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5685808814841073 on epoch=337
05/18/2022 16:11:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
05/18/2022 16:11:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
05/18/2022 16:11:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
05/18/2022 16:12:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
05/18/2022 16:12:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
05/18/2022 16:12:03 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6462041156840934 on epoch=349
05/18/2022 16:12:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
05/18/2022 16:12:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/18/2022 16:12:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/18/2022 16:12:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
05/18/2022 16:12:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
05/18/2022 16:12:11 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6545925881095038 on epoch=362
05/18/2022 16:12:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
05/18/2022 16:12:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
05/18/2022 16:12:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
05/18/2022 16:12:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/18/2022 16:12:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
05/18/2022 16:12:18 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6176009428161509 on epoch=374
05/18/2022 16:12:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
05/18/2022 16:12:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
05/18/2022 16:12:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
05/18/2022 16:12:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
05/18/2022 16:12:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/18/2022 16:12:28 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6135101769947982 on epoch=387
05/18/2022 16:12:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
05/18/2022 16:12:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
05/18/2022 16:12:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
05/18/2022 16:12:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/18/2022 16:12:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
05/18/2022 16:12:36 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6738095238095239 on epoch=399
05/18/2022 16:12:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6699560358069813 -> 0.6738095238095239 on epoch=399, global_step=1600
05/18/2022 16:12:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
05/18/2022 16:12:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/18/2022 16:12:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/18/2022 16:12:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/18/2022 16:12:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
05/18/2022 16:12:45 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6526091026091027 on epoch=412
05/18/2022 16:12:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
05/18/2022 16:12:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
05/18/2022 16:12:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
05/18/2022 16:12:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
05/18/2022 16:12:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/18/2022 16:12:53 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6212788563701342 on epoch=424
05/18/2022 16:12:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
05/18/2022 16:12:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/18/2022 16:12:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/18/2022 16:12:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/18/2022 16:13:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/18/2022 16:13:01 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6187140804597702 on epoch=437
05/18/2022 16:13:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
05/18/2022 16:13:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/18/2022 16:13:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/18/2022 16:13:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
05/18/2022 16:13:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/18/2022 16:13:10 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.581246687864335 on epoch=449
05/18/2022 16:13:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/18/2022 16:13:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
05/18/2022 16:13:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/18/2022 16:13:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
05/18/2022 16:13:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/18/2022 16:13:18 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6200583230616601 on epoch=462
05/18/2022 16:13:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/18/2022 16:13:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
05/18/2022 16:13:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/18/2022 16:13:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
05/18/2022 16:13:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/18/2022 16:13:26 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6053921568627452 on epoch=474
05/18/2022 16:13:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/18/2022 16:13:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/18/2022 16:13:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/18/2022 16:13:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/18/2022 16:13:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
05/18/2022 16:13:34 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6239622865275142 on epoch=487
05/18/2022 16:13:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/18/2022 16:13:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/18/2022 16:13:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
05/18/2022 16:13:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/18/2022 16:13:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/18/2022 16:13:43 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5208680208680209 on epoch=499
05/18/2022 16:13:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/18/2022 16:13:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/18/2022 16:13:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/18/2022 16:13:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
05/18/2022 16:13:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
05/18/2022 16:13:51 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.5853601055806938 on epoch=512
05/18/2022 16:13:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/18/2022 16:13:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/18/2022 16:13:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
05/18/2022 16:13:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
05/18/2022 16:13:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
05/18/2022 16:14:00 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6126155712362609 on epoch=524
05/18/2022 16:14:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/18/2022 16:14:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/18/2022 16:14:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
05/18/2022 16:14:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
05/18/2022 16:14:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/18/2022 16:14:08 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6000865800865801 on epoch=537
05/18/2022 16:14:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/18/2022 16:14:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
05/18/2022 16:14:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/18/2022 16:14:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/18/2022 16:14:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/18/2022 16:14:16 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6654463780418789 on epoch=549
05/18/2022 16:14:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=552
05/18/2022 16:14:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
05/18/2022 16:14:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/18/2022 16:14:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
05/18/2022 16:14:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/18/2022 16:14:25 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5600788288288288 on epoch=562
05/18/2022 16:14:26 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
05/18/2022 16:14:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
05/18/2022 16:14:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/18/2022 16:14:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/18/2022 16:14:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/18/2022 16:14:33 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6070869741922373 on epoch=574
05/18/2022 16:14:35 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
05/18/2022 16:14:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
05/18/2022 16:14:38 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/18/2022 16:14:40 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/18/2022 16:14:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
05/18/2022 16:14:42 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6330362620685202 on epoch=587
05/18/2022 16:14:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/18/2022 16:14:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=592
05/18/2022 16:14:47 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/18/2022 16:14:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/18/2022 16:14:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
05/18/2022 16:14:50 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.5996642246642246 on epoch=599
05/18/2022 16:14:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/18/2022 16:14:53 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/18/2022 16:14:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
05/18/2022 16:14:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/18/2022 16:14:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/18/2022 16:14:59 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6713424299631197 on epoch=612
05/18/2022 16:15:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
05/18/2022 16:15:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
05/18/2022 16:15:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/18/2022 16:15:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
05/18/2022 16:15:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
05/18/2022 16:15:07 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.622481684981685 on epoch=624
05/18/2022 16:15:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=627
05/18/2022 16:15:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/18/2022 16:15:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
05/18/2022 16:15:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
05/18/2022 16:15:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/18/2022 16:15:15 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.6427849927849928 on epoch=637
05/18/2022 16:15:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 16:15:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 16:15:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/18/2022 16:15:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 16:15:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
05/18/2022 16:15:24 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6294642857142857 on epoch=649
05/18/2022 16:15:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/18/2022 16:15:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/18/2022 16:15:28 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/18/2022 16:15:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 16:15:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/18/2022 16:15:32 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6718864468864468 on epoch=662
05/18/2022 16:15:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/18/2022 16:15:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
05/18/2022 16:15:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/18/2022 16:15:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/18/2022 16:15:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/18/2022 16:15:40 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6011738640814728 on epoch=674
05/18/2022 16:15:42 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/18/2022 16:15:43 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
05/18/2022 16:15:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/18/2022 16:15:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=684
05/18/2022 16:15:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/18/2022 16:15:48 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6503643020483628 on epoch=687
05/18/2022 16:15:50 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=689
05/18/2022 16:15:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/18/2022 16:15:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/18/2022 16:15:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/18/2022 16:15:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/18/2022 16:15:56 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6291208876643146 on epoch=699
05/18/2022 16:15:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/18/2022 16:16:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/18/2022 16:16:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/18/2022 16:16:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/18/2022 16:16:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/18/2022 16:16:05 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6236415130568356 on epoch=712
05/18/2022 16:16:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/18/2022 16:16:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/18/2022 16:16:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 16:16:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
05/18/2022 16:16:12 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 16:16:13 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6273477480374032 on epoch=724
05/18/2022 16:16:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 16:16:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 16:16:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 16:16:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/18/2022 16:16:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 16:16:21 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6413616625310173 on epoch=737
05/18/2022 16:16:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/18/2022 16:16:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/18/2022 16:16:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/18/2022 16:16:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/18/2022 16:16:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/18/2022 16:16:29 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.600219298245614 on epoch=749
05/18/2022 16:16:29 - INFO - __main__ - save last model!
05/18/2022 16:16:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:16:29 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:16:29 - INFO - __main__ - Printing 3 examples
05/18/2022 16:16:29 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:16:29 - INFO - __main__ - ['others']
05/18/2022 16:16:29 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:16:29 - INFO - __main__ - ['others']
05/18/2022 16:16:29 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:16:29 - INFO - __main__ - ['others']
05/18/2022 16:16:29 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:16:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:16:30 - INFO - __main__ - Printing 3 examples
05/18/2022 16:16:30 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:16:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:16:30 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:16:30 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:16:30 - INFO - __main__ - Printing 3 examples
05/18/2022 16:16:30 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:16:30 - INFO - __main__ - ['others']
05/18/2022 16:16:30 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:16:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:16:30 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:16:32 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:16:37 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:16:37 - INFO - __main__ - task name: emo
05/18/2022 16:16:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:16:37 - INFO - __main__ - Starting training!
05/18/2022 16:16:38 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:17:56 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.4_8_predictions.txt
05/18/2022 16:17:56 - INFO - __main__ - Classification-F1 on test data: 0.2017
05/18/2022 16:17:56 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.6738095238095239, test_performance=0.201668123716036
05/18/2022 16:17:56 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
05/18/2022 16:17:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:17:57 - INFO - __main__ - Printing 3 examples
05/18/2022 16:17:57 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:17:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:17:57 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:17:57 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:17:57 - INFO - __main__ - Printing 3 examples
05/18/2022 16:17:57 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:17:57 - INFO - __main__ - ['others']
05/18/2022 16:17:57 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:17:57 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:17:57 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:18:04 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:18:04 - INFO - __main__ - task name: emo
05/18/2022 16:18:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:18:05 - INFO - __main__ - Starting training!
05/18/2022 16:18:07 - INFO - __main__ - Step 10 Global step 10 Train loss 6.69 on epoch=2
05/18/2022 16:18:08 - INFO - __main__ - Step 20 Global step 20 Train loss 3.95 on epoch=4
05/18/2022 16:18:09 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
05/18/2022 16:18:11 - INFO - __main__ - Step 40 Global step 40 Train loss 2.16 on epoch=9
05/18/2022 16:18:12 - INFO - __main__ - Step 50 Global step 50 Train loss 2.10 on epoch=12
05/18/2022 16:18:13 - INFO - __main__ - Global step 50 Train loss 3.51 Classification-F1 0.10144927536231885 on epoch=12
05/18/2022 16:18:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10144927536231885 on epoch=12, global_step=50
05/18/2022 16:18:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.67 on epoch=14
05/18/2022 16:18:16 - INFO - __main__ - Step 70 Global step 70 Train loss 1.59 on epoch=17
05/18/2022 16:18:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.20 on epoch=19
05/18/2022 16:18:18 - INFO - __main__ - Step 90 Global step 90 Train loss 1.44 on epoch=22
05/18/2022 16:18:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=24
05/18/2022 16:18:21 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.12213225371120107 on epoch=24
05/18/2022 16:18:21 - INFO - __main__ - Saving model with best Classification-F1: 0.10144927536231885 -> 0.12213225371120107 on epoch=24, global_step=100
05/18/2022 16:18:22 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
05/18/2022 16:18:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.22 on epoch=29
05/18/2022 16:18:25 - INFO - __main__ - Step 130 Global step 130 Train loss 1.09 on epoch=32
05/18/2022 16:18:26 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=34
05/18/2022 16:18:28 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
05/18/2022 16:18:29 - INFO - __main__ - Global step 150 Train loss 1.09 Classification-F1 0.40138352638352637 on epoch=37
05/18/2022 16:18:29 - INFO - __main__ - Saving model with best Classification-F1: 0.12213225371120107 -> 0.40138352638352637 on epoch=37, global_step=150
05/18/2022 16:18:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
05/18/2022 16:18:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
05/18/2022 16:18:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
05/18/2022 16:18:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=47
05/18/2022 16:18:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
05/18/2022 16:18:37 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.12151702786377708 on epoch=49
05/18/2022 16:18:39 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
05/18/2022 16:18:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
05/18/2022 16:18:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
05/18/2022 16:18:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
05/18/2022 16:18:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
05/18/2022 16:18:46 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.16984126984126985 on epoch=62
05/18/2022 16:18:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.99 on epoch=64
05/18/2022 16:18:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
05/18/2022 16:18:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
05/18/2022 16:18:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
05/18/2022 16:18:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
05/18/2022 16:18:54 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.2569444444444445 on epoch=74
05/18/2022 16:18:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.96 on epoch=77
05/18/2022 16:18:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.88 on epoch=79
05/18/2022 16:18:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=82
05/18/2022 16:19:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
05/18/2022 16:19:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
05/18/2022 16:19:02 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.39856150793650796 on epoch=87
05/18/2022 16:19:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
05/18/2022 16:19:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
05/18/2022 16:19:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
05/18/2022 16:19:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
05/18/2022 16:19:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=99
05/18/2022 16:19:10 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3163396886801142 on epoch=99
05/18/2022 16:19:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
05/18/2022 16:19:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=104
05/18/2022 16:19:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.88 on epoch=107
05/18/2022 16:19:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.90 on epoch=109
05/18/2022 16:19:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
05/18/2022 16:19:18 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.4258231594524602 on epoch=112
05/18/2022 16:19:18 - INFO - __main__ - Saving model with best Classification-F1: 0.40138352638352637 -> 0.4258231594524602 on epoch=112, global_step=450
05/18/2022 16:19:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=114
05/18/2022 16:19:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
05/18/2022 16:19:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=119
05/18/2022 16:19:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=122
05/18/2022 16:19:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=124
05/18/2022 16:19:28 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.4842929905408684 on epoch=124
05/18/2022 16:19:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4258231594524602 -> 0.4842929905408684 on epoch=124, global_step=500
05/18/2022 16:19:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.83 on epoch=127
05/18/2022 16:19:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=129
05/18/2022 16:19:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=132
05/18/2022 16:19:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
05/18/2022 16:19:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.73 on epoch=137
05/18/2022 16:19:36 - INFO - __main__ - Global step 550 Train loss 0.76 Classification-F1 0.5536096296965862 on epoch=137
05/18/2022 16:19:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4842929905408684 -> 0.5536096296965862 on epoch=137, global_step=550
05/18/2022 16:19:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
05/18/2022 16:19:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=142
05/18/2022 16:19:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=144
05/18/2022 16:19:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=147
05/18/2022 16:19:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
05/18/2022 16:19:45 - INFO - __main__ - Global step 600 Train loss 0.70 Classification-F1 0.3006657608695652 on epoch=149
05/18/2022 16:19:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
05/18/2022 16:19:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=154
05/18/2022 16:19:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=157
05/18/2022 16:19:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.64 on epoch=159
05/18/2022 16:19:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=162
05/18/2022 16:19:53 - INFO - __main__ - Global step 650 Train loss 0.68 Classification-F1 0.6143790849673203 on epoch=162
05/18/2022 16:19:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5536096296965862 -> 0.6143790849673203 on epoch=162, global_step=650
05/18/2022 16:19:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
05/18/2022 16:19:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=167
05/18/2022 16:19:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=169
05/18/2022 16:19:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=172
05/18/2022 16:20:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=174
05/18/2022 16:20:01 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.5917131414113646 on epoch=174
05/18/2022 16:20:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=177
05/18/2022 16:20:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=179
05/18/2022 16:20:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=182
05/18/2022 16:20:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=184
05/18/2022 16:20:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=187
05/18/2022 16:20:10 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.7233090185676393 on epoch=187
05/18/2022 16:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6143790849673203 -> 0.7233090185676393 on epoch=187, global_step=750
05/18/2022 16:20:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=189
05/18/2022 16:20:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=192
05/18/2022 16:20:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=194
05/18/2022 16:20:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=197
05/18/2022 16:20:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=199
05/18/2022 16:20:18 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.7262679848886745 on epoch=199
05/18/2022 16:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7233090185676393 -> 0.7262679848886745 on epoch=199, global_step=800
05/18/2022 16:20:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=202
05/18/2022 16:20:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=204
05/18/2022 16:20:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=207
05/18/2022 16:20:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=209
05/18/2022 16:20:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=212
05/18/2022 16:20:27 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.6486973345525977 on epoch=212
05/18/2022 16:20:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=214
05/18/2022 16:20:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
05/18/2022 16:20:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=219
05/18/2022 16:20:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=222
05/18/2022 16:20:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=224
05/18/2022 16:20:35 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.7582815551565552 on epoch=224
05/18/2022 16:20:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7262679848886745 -> 0.7582815551565552 on epoch=224, global_step=900
05/18/2022 16:20:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=227
05/18/2022 16:20:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
05/18/2022 16:20:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=232
05/18/2022 16:20:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=234
05/18/2022 16:20:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=237
05/18/2022 16:20:43 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.7763573232323232 on epoch=237
05/18/2022 16:20:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7582815551565552 -> 0.7763573232323232 on epoch=237, global_step=950
05/18/2022 16:20:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=239
05/18/2022 16:20:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
05/18/2022 16:20:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=244
05/18/2022 16:20:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=247
05/18/2022 16:20:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=249
05/18/2022 16:20:51 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.7375717656789764 on epoch=249
05/18/2022 16:20:53 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=252
05/18/2022 16:20:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=254
05/18/2022 16:20:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
05/18/2022 16:20:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=259
05/18/2022 16:20:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=262
05/18/2022 16:21:00 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.6953441295546559 on epoch=262
05/18/2022 16:21:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=264
05/18/2022 16:21:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=267
05/18/2022 16:21:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=269
05/18/2022 16:21:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=272
05/18/2022 16:21:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=274
05/18/2022 16:21:08 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.6715547965547966 on epoch=274
05/18/2022 16:21:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
05/18/2022 16:21:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=279
05/18/2022 16:21:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=282
05/18/2022 16:21:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=284
05/18/2022 16:21:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
05/18/2022 16:21:17 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.6791583416583418 on epoch=287
05/18/2022 16:21:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=289
05/18/2022 16:21:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=292
05/18/2022 16:21:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
05/18/2022 16:21:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
05/18/2022 16:21:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=299
05/18/2022 16:21:25 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.6876050420168067 on epoch=299
05/18/2022 16:21:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=302
05/18/2022 16:21:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
05/18/2022 16:21:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
05/18/2022 16:21:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
05/18/2022 16:21:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
05/18/2022 16:21:33 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.7076100370218016 on epoch=312
05/18/2022 16:21:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
05/18/2022 16:21:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=317
05/18/2022 16:21:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=319
05/18/2022 16:21:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
05/18/2022 16:21:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=324
05/18/2022 16:21:41 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.6588209088209087 on epoch=324
05/18/2022 16:21:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=327
05/18/2022 16:21:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
05/18/2022 16:21:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
05/18/2022 16:21:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
05/18/2022 16:21:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
05/18/2022 16:21:50 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.6964937363834423 on epoch=337
05/18/2022 16:21:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
05/18/2022 16:21:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
05/18/2022 16:21:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
05/18/2022 16:21:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=347
05/18/2022 16:21:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
05/18/2022 16:21:59 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.676174696864352 on epoch=349
05/18/2022 16:22:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
05/18/2022 16:22:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
05/18/2022 16:22:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
05/18/2022 16:22:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/18/2022 16:22:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=362
05/18/2022 16:22:07 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.6543306346929015 on epoch=362
05/18/2022 16:22:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=364
05/18/2022 16:22:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
05/18/2022 16:22:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
05/18/2022 16:22:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=372
05/18/2022 16:22:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/18/2022 16:22:15 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.5870138633296528 on epoch=374
05/18/2022 16:22:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/18/2022 16:22:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=379
05/18/2022 16:22:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
05/18/2022 16:22:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
05/18/2022 16:22:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
05/18/2022 16:22:23 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.6777027027027027 on epoch=387
05/18/2022 16:22:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
05/18/2022 16:22:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
05/18/2022 16:22:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
05/18/2022 16:22:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=397
05/18/2022 16:22:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/18/2022 16:22:32 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.6803440385070623 on epoch=399
05/18/2022 16:22:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=402
05/18/2022 16:22:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
05/18/2022 16:22:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
05/18/2022 16:22:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
05/18/2022 16:22:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
05/18/2022 16:22:40 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.6088924588924589 on epoch=412
05/18/2022 16:22:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/18/2022 16:22:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
05/18/2022 16:22:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/18/2022 16:22:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
05/18/2022 16:22:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=424
05/18/2022 16:22:49 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.5172422949265054 on epoch=424
05/18/2022 16:22:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
05/18/2022 16:22:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/18/2022 16:22:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
05/18/2022 16:22:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=434
05/18/2022 16:22:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
05/18/2022 16:22:57 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.65 on epoch=437
05/18/2022 16:22:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
05/18/2022 16:23:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
05/18/2022 16:23:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=444
05/18/2022 16:23:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
05/18/2022 16:23:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
05/18/2022 16:23:07 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6506274046319813 on epoch=449
05/18/2022 16:23:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
05/18/2022 16:23:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=454
05/18/2022 16:23:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/18/2022 16:23:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/18/2022 16:23:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/18/2022 16:23:15 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.5858337065233616 on epoch=462
05/18/2022 16:23:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
05/18/2022 16:23:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/18/2022 16:23:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/18/2022 16:23:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/18/2022 16:23:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
05/18/2022 16:23:23 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6542471042471043 on epoch=474
05/18/2022 16:23:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
05/18/2022 16:23:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/18/2022 16:23:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/18/2022 16:23:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/18/2022 16:23:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/18/2022 16:23:31 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6509575569358178 on epoch=487
05/18/2022 16:23:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/18/2022 16:23:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
05/18/2022 16:23:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/18/2022 16:23:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/18/2022 16:23:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/18/2022 16:23:39 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6411844089862666 on epoch=499
05/18/2022 16:23:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=502
05/18/2022 16:23:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
05/18/2022 16:23:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/18/2022 16:23:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=509
05/18/2022 16:23:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
05/18/2022 16:23:48 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.644855193539404 on epoch=512
05/18/2022 16:23:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
05/18/2022 16:23:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/18/2022 16:23:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
05/18/2022 16:23:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
05/18/2022 16:23:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/18/2022 16:23:55 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6541353383458647 on epoch=524
05/18/2022 16:23:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
05/18/2022 16:23:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
05/18/2022 16:24:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
05/18/2022 16:24:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
05/18/2022 16:24:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
05/18/2022 16:24:04 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6657738095238096 on epoch=537
05/18/2022 16:24:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/18/2022 16:24:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
05/18/2022 16:24:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
05/18/2022 16:24:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
05/18/2022 16:24:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/18/2022 16:24:13 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6670250896057347 on epoch=549
05/18/2022 16:24:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/18/2022 16:24:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/18/2022 16:24:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/18/2022 16:24:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/18/2022 16:24:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/18/2022 16:24:22 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5983147235564428 on epoch=562
05/18/2022 16:24:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/18/2022 16:24:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/18/2022 16:24:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/18/2022 16:24:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/18/2022 16:24:29 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/18/2022 16:24:30 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.5745576108479334 on epoch=574
05/18/2022 16:24:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/18/2022 16:24:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=579
05/18/2022 16:24:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/18/2022 16:24:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/18/2022 16:24:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 16:24:38 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6473494098494099 on epoch=587
05/18/2022 16:24:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
05/18/2022 16:24:41 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/18/2022 16:24:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/18/2022 16:24:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/18/2022 16:24:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/18/2022 16:24:46 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.633822623571935 on epoch=599
05/18/2022 16:24:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/18/2022 16:24:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/18/2022 16:24:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
05/18/2022 16:24:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/18/2022 16:24:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/18/2022 16:24:54 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6444163135339606 on epoch=612
05/18/2022 16:24:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
05/18/2022 16:24:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
05/18/2022 16:24:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/18/2022 16:25:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
05/18/2022 16:25:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/18/2022 16:25:03 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.686740890688259 on epoch=624
05/18/2022 16:25:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/18/2022 16:25:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/18/2022 16:25:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/18/2022 16:25:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/18/2022 16:25:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/18/2022 16:25:12 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6939881057528116 on epoch=637
05/18/2022 16:25:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=639
05/18/2022 16:25:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/18/2022 16:25:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/18/2022 16:25:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 16:25:19 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
05/18/2022 16:25:20 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6602893215796442 on epoch=649
05/18/2022 16:25:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/18/2022 16:25:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/18/2022 16:25:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/18/2022 16:25:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/18/2022 16:25:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/18/2022 16:25:29 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6758824320210036 on epoch=662
05/18/2022 16:25:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/18/2022 16:25:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/18/2022 16:25:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/18/2022 16:25:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/18/2022 16:25:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/18/2022 16:25:37 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6631001681404908 on epoch=674
05/18/2022 16:25:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/18/2022 16:25:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/18/2022 16:25:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/18/2022 16:25:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
05/18/2022 16:25:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/18/2022 16:25:46 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6073432012633464 on epoch=687
05/18/2022 16:25:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/18/2022 16:25:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/18/2022 16:25:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 16:25:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/18/2022 16:25:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/18/2022 16:25:54 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6546980304913956 on epoch=699
05/18/2022 16:25:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=702
05/18/2022 16:25:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/18/2022 16:25:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/18/2022 16:26:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/18/2022 16:26:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/18/2022 16:26:03 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.681759715380405 on epoch=712
05/18/2022 16:26:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/18/2022 16:26:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
05/18/2022 16:26:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 16:26:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/18/2022 16:26:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/18/2022 16:26:11 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5630812324929971 on epoch=724
05/18/2022 16:26:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 16:26:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
05/18/2022 16:26:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 16:26:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/18/2022 16:26:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
05/18/2022 16:26:19 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.540960020371785 on epoch=737
05/18/2022 16:26:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/18/2022 16:26:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/18/2022 16:26:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
05/18/2022 16:26:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
05/18/2022 16:26:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/18/2022 16:26:28 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6816293183940243 on epoch=749
05/18/2022 16:26:28 - INFO - __main__ - save last model!
05/18/2022 16:26:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:26:28 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:26:28 - INFO - __main__ - Printing 3 examples
05/18/2022 16:26:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:26:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:26:28 - INFO - __main__ - Printing 3 examples
05/18/2022 16:26:28 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:26:28 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:26:28 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:26:28 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:26:28 - INFO - __main__ - Printing 3 examples
05/18/2022 16:26:28 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:26:28 - INFO - __main__ - ['others']
05/18/2022 16:26:28 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:26:28 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:26:28 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:26:30 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:26:35 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:26:35 - INFO - __main__ - task name: emo
05/18/2022 16:26:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:26:35 - INFO - __main__ - Starting training!
05/18/2022 16:26:36 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:27:54 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.3_8_predictions.txt
05/18/2022 16:27:54 - INFO - __main__ - Classification-F1 on test data: 0.2495
05/18/2022 16:27:54 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7763573232323232, test_performance=0.24947396221510335
05/18/2022 16:27:54 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
05/18/2022 16:27:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:27:55 - INFO - __main__ - Printing 3 examples
05/18/2022 16:27:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:27:55 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:27:55 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:27:55 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:27:55 - INFO - __main__ - Printing 3 examples
05/18/2022 16:27:55 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ -  [emo] i did ask now you did tell ms
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ -  [emo] buddy how you tell me your contact no
05/18/2022 16:27:55 - INFO - __main__ - ['others']
05/18/2022 16:27:55 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:27:55 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:27:55 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:28:01 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:28:01 - INFO - __main__ - task name: emo
05/18/2022 16:28:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:28:02 - INFO - __main__ - Starting training!
05/18/2022 16:28:03 - INFO - __main__ - Step 10 Global step 10 Train loss 7.49 on epoch=2
05/18/2022 16:28:05 - INFO - __main__ - Step 20 Global step 20 Train loss 5.06 on epoch=4
05/18/2022 16:28:06 - INFO - __main__ - Step 30 Global step 30 Train loss 3.32 on epoch=7
05/18/2022 16:28:08 - INFO - __main__ - Step 40 Global step 40 Train loss 2.30 on epoch=9
05/18/2022 16:28:09 - INFO - __main__ - Step 50 Global step 50 Train loss 1.99 on epoch=12
05/18/2022 16:28:10 - INFO - __main__ - Global step 50 Train loss 4.03 Classification-F1 0.18881544156530936 on epoch=12
05/18/2022 16:28:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18881544156530936 on epoch=12, global_step=50
05/18/2022 16:28:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.65 on epoch=14
05/18/2022 16:28:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.49 on epoch=17
05/18/2022 16:28:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.49 on epoch=19
05/18/2022 16:28:16 - INFO - __main__ - Step 90 Global step 90 Train loss 1.62 on epoch=22
05/18/2022 16:28:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.33 on epoch=24
05/18/2022 16:28:18 - INFO - __main__ - Global step 100 Train loss 1.52 Classification-F1 0.24002382370458603 on epoch=24
05/18/2022 16:28:18 - INFO - __main__ - Saving model with best Classification-F1: 0.18881544156530936 -> 0.24002382370458603 on epoch=24, global_step=100
05/18/2022 16:28:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.28 on epoch=27
05/18/2022 16:28:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
05/18/2022 16:28:22 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
05/18/2022 16:28:24 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
05/18/2022 16:28:25 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
05/18/2022 16:28:26 - INFO - __main__ - Global step 150 Train loss 1.12 Classification-F1 0.1307631160572337 on epoch=37
05/18/2022 16:28:28 - INFO - __main__ - Step 160 Global step 160 Train loss 1.23 on epoch=39
05/18/2022 16:28:30 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
05/18/2022 16:28:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.02 on epoch=44
05/18/2022 16:28:32 - INFO - __main__ - Step 190 Global step 190 Train loss 1.13 on epoch=47
05/18/2022 16:28:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
05/18/2022 16:28:35 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.253734827264239 on epoch=49
05/18/2022 16:28:35 - INFO - __main__ - Saving model with best Classification-F1: 0.24002382370458603 -> 0.253734827264239 on epoch=49, global_step=200
05/18/2022 16:28:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
05/18/2022 16:28:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.03 on epoch=54
05/18/2022 16:28:39 - INFO - __main__ - Step 230 Global step 230 Train loss 1.10 on epoch=57
05/18/2022 16:28:41 - INFO - __main__ - Step 240 Global step 240 Train loss 1.01 on epoch=59
05/18/2022 16:28:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.98 on epoch=62
05/18/2022 16:28:43 - INFO - __main__ - Global step 250 Train loss 1.03 Classification-F1 0.10126582278481013 on epoch=62
05/18/2022 16:28:45 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
05/18/2022 16:28:46 - INFO - __main__ - Step 270 Global step 270 Train loss 1.05 on epoch=67
05/18/2022 16:28:48 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=69
05/18/2022 16:28:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
05/18/2022 16:28:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.95 on epoch=74
05/18/2022 16:28:52 - INFO - __main__ - Global step 300 Train loss 1.01 Classification-F1 0.2907925407925408 on epoch=74
05/18/2022 16:28:52 - INFO - __main__ - Saving model with best Classification-F1: 0.253734827264239 -> 0.2907925407925408 on epoch=74, global_step=300
05/18/2022 16:28:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/18/2022 16:28:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=79
05/18/2022 16:28:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
05/18/2022 16:28:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
05/18/2022 16:28:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=87
05/18/2022 16:29:00 - INFO - __main__ - Global step 350 Train loss 0.95 Classification-F1 0.23853640951694305 on epoch=87
05/18/2022 16:29:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
05/18/2022 16:29:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
05/18/2022 16:29:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=94
05/18/2022 16:29:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=97
05/18/2022 16:29:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
05/18/2022 16:29:09 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.28420920607048794 on epoch=99
05/18/2022 16:29:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=102
05/18/2022 16:29:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
05/18/2022 16:29:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
05/18/2022 16:29:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
05/18/2022 16:29:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=112
05/18/2022 16:29:17 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.2880911517925248 on epoch=112
05/18/2022 16:29:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
05/18/2022 16:29:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=117
05/18/2022 16:29:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
05/18/2022 16:29:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=122
05/18/2022 16:29:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=124
05/18/2022 16:29:25 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.42064818809318383 on epoch=124
05/18/2022 16:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.2907925407925408 -> 0.42064818809318383 on epoch=124, global_step=500
05/18/2022 16:29:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
05/18/2022 16:29:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
05/18/2022 16:29:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
05/18/2022 16:29:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.74 on epoch=134
05/18/2022 16:29:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=137
05/18/2022 16:29:34 - INFO - __main__ - Global step 550 Train loss 0.75 Classification-F1 0.42822479928635143 on epoch=137
05/18/2022 16:29:34 - INFO - __main__ - Saving model with best Classification-F1: 0.42064818809318383 -> 0.42822479928635143 on epoch=137, global_step=550
05/18/2022 16:29:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
05/18/2022 16:29:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=142
05/18/2022 16:29:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=144
05/18/2022 16:29:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.71 on epoch=147
05/18/2022 16:29:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.69 on epoch=149
05/18/2022 16:29:42 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.5597436879077727 on epoch=149
05/18/2022 16:29:42 - INFO - __main__ - Saving model with best Classification-F1: 0.42822479928635143 -> 0.5597436879077727 on epoch=149, global_step=600
05/18/2022 16:29:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=152
05/18/2022 16:29:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.66 on epoch=154
05/18/2022 16:29:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=157
05/18/2022 16:29:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=159
05/18/2022 16:29:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=162
05/18/2022 16:29:51 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.2673397199705232 on epoch=162
05/18/2022 16:29:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.69 on epoch=164
05/18/2022 16:29:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.69 on epoch=167
05/18/2022 16:29:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=169
05/18/2022 16:29:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
05/18/2022 16:29:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=174
05/18/2022 16:30:00 - INFO - __main__ - Global step 700 Train loss 0.65 Classification-F1 0.628027378027378 on epoch=174
05/18/2022 16:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5597436879077727 -> 0.628027378027378 on epoch=174, global_step=700
05/18/2022 16:30:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
05/18/2022 16:30:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=179
05/18/2022 16:30:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=182
05/18/2022 16:30:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.65 on epoch=184
05/18/2022 16:30:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=187
05/18/2022 16:30:07 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.507422969187675 on epoch=187
05/18/2022 16:30:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=189
05/18/2022 16:30:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=192
05/18/2022 16:30:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=194
05/18/2022 16:30:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=197
05/18/2022 16:30:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=199
05/18/2022 16:30:15 - INFO - __main__ - Global step 800 Train loss 0.57 Classification-F1 0.5013631022326674 on epoch=199
05/18/2022 16:30:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=202
05/18/2022 16:30:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=204
05/18/2022 16:30:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=207
05/18/2022 16:30:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=209
05/18/2022 16:30:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=212
05/18/2022 16:30:24 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.5204678362573099 on epoch=212
05/18/2022 16:30:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.52 on epoch=214
05/18/2022 16:30:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
05/18/2022 16:30:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=219
05/18/2022 16:30:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.63 on epoch=222
05/18/2022 16:30:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=224
05/18/2022 16:30:32 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.5591525591525591 on epoch=224
05/18/2022 16:30:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=227
05/18/2022 16:30:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=229
05/18/2022 16:30:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=232
05/18/2022 16:30:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
05/18/2022 16:30:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=237
05/18/2022 16:30:40 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.5092585196033472 on epoch=237
05/18/2022 16:30:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=239
05/18/2022 16:30:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=242
05/18/2022 16:30:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=244
05/18/2022 16:30:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=247
05/18/2022 16:30:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=249
05/18/2022 16:30:49 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.5330746187363834 on epoch=249
05/18/2022 16:30:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=252
05/18/2022 16:30:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
05/18/2022 16:30:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=257
05/18/2022 16:30:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=259
05/18/2022 16:30:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=262
05/18/2022 16:30:58 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.4796171171171171 on epoch=262
05/18/2022 16:30:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=264
05/18/2022 16:31:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=267
05/18/2022 16:31:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=269
05/18/2022 16:31:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=272
05/18/2022 16:31:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=274
05/18/2022 16:31:06 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.5478367777062233 on epoch=274
05/18/2022 16:31:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=277
05/18/2022 16:31:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=279
05/18/2022 16:31:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=282
05/18/2022 16:31:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=284
05/18/2022 16:31:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=287
05/18/2022 16:31:15 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.5986768018018017 on epoch=287
05/18/2022 16:31:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=289
05/18/2022 16:31:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/18/2022 16:31:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=294
05/18/2022 16:31:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=297
05/18/2022 16:31:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=299
05/18/2022 16:31:23 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.5719725276680545 on epoch=299
05/18/2022 16:31:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=302
05/18/2022 16:31:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=304
05/18/2022 16:31:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=307
05/18/2022 16:31:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=309
05/18/2022 16:31:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=312
05/18/2022 16:31:32 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.5070529320822905 on epoch=312
05/18/2022 16:31:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=314
05/18/2022 16:31:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=317
05/18/2022 16:31:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=319
05/18/2022 16:31:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=322
05/18/2022 16:31:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=324
05/18/2022 16:31:40 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.49637834931952585 on epoch=324
05/18/2022 16:31:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=327
05/18/2022 16:31:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=329
05/18/2022 16:31:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=332
05/18/2022 16:31:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
05/18/2022 16:31:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=337
05/18/2022 16:31:48 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.5351178622231254 on epoch=337
05/18/2022 16:31:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=339
05/18/2022 16:31:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=342
05/18/2022 16:31:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=344
05/18/2022 16:31:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=347
05/18/2022 16:31:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=349
05/18/2022 16:31:56 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.5510984326841104 on epoch=349
05/18/2022 16:31:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=352
05/18/2022 16:31:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=354
05/18/2022 16:32:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=357
05/18/2022 16:32:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=359
05/18/2022 16:32:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=362
05/18/2022 16:32:05 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.5308557431728164 on epoch=362
05/18/2022 16:32:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=364
05/18/2022 16:32:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=367
05/18/2022 16:32:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=369
05/18/2022 16:32:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=372
05/18/2022 16:32:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=374
05/18/2022 16:32:13 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.5554737253847376 on epoch=374
05/18/2022 16:32:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=377
05/18/2022 16:32:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=379
05/18/2022 16:32:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=382
05/18/2022 16:32:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
05/18/2022 16:32:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=387
05/18/2022 16:32:21 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.5773707773707774 on epoch=387
05/18/2022 16:32:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=389
05/18/2022 16:32:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=392
05/18/2022 16:32:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=394
05/18/2022 16:32:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=397
05/18/2022 16:32:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=399
05/18/2022 16:32:30 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.5628369331817609 on epoch=399
05/18/2022 16:32:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=402
05/18/2022 16:32:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=404
05/18/2022 16:32:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=407
05/18/2022 16:32:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=409
05/18/2022 16:32:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=412
05/18/2022 16:32:38 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.5397368421052633 on epoch=412
05/18/2022 16:32:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=414
05/18/2022 16:32:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=417
05/18/2022 16:32:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=419
05/18/2022 16:32:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=422
05/18/2022 16:32:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=424
05/18/2022 16:32:46 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.610042735042735 on epoch=424
05/18/2022 16:32:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=427
05/18/2022 16:32:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=429
05/18/2022 16:32:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=432
05/18/2022 16:32:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
05/18/2022 16:32:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=437
05/18/2022 16:32:54 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.6061947856065504 on epoch=437
05/18/2022 16:32:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
05/18/2022 16:32:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=442
05/18/2022 16:32:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=444
05/18/2022 16:32:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
05/18/2022 16:33:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=449
05/18/2022 16:33:01 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.6196613190730837 on epoch=449
05/18/2022 16:33:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=452
05/18/2022 16:33:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=454
05/18/2022 16:33:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=457
05/18/2022 16:33:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/18/2022 16:33:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=462
05/18/2022 16:33:10 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.6136012469503931 on epoch=462
05/18/2022 16:33:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
05/18/2022 16:33:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=467
05/18/2022 16:33:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=469
05/18/2022 16:33:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
05/18/2022 16:33:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=474
05/18/2022 16:33:18 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.5865237651444548 on epoch=474
05/18/2022 16:33:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=477
05/18/2022 16:33:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
05/18/2022 16:33:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
05/18/2022 16:33:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/18/2022 16:33:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
05/18/2022 16:33:27 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.5886580086580087 on epoch=487
05/18/2022 16:33:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/18/2022 16:33:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=492
05/18/2022 16:33:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=494
05/18/2022 16:33:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=497
05/18/2022 16:33:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
05/18/2022 16:33:35 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.560923098822106 on epoch=499
05/18/2022 16:33:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
05/18/2022 16:33:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
05/18/2022 16:33:40 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=507
05/18/2022 16:33:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=509
05/18/2022 16:33:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
05/18/2022 16:33:44 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.5791211884961884 on epoch=512
05/18/2022 16:33:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=514
05/18/2022 16:33:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
05/18/2022 16:33:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=519
05/18/2022 16:33:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=522
05/18/2022 16:33:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
05/18/2022 16:33:52 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.5721834471834472 on epoch=524
05/18/2022 16:33:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=527
05/18/2022 16:33:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=529
05/18/2022 16:33:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
05/18/2022 16:33:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=534
05/18/2022 16:34:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
05/18/2022 16:34:01 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.5746403170087381 on epoch=537
05/18/2022 16:34:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
05/18/2022 16:34:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=542
05/18/2022 16:34:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/18/2022 16:34:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
05/18/2022 16:34:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
05/18/2022 16:34:09 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.5468986568986569 on epoch=549
05/18/2022 16:34:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
05/18/2022 16:34:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=554
05/18/2022 16:34:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
05/18/2022 16:34:15 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=559
05/18/2022 16:34:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
05/18/2022 16:34:17 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5394951332451332 on epoch=562
05/18/2022 16:34:19 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/18/2022 16:34:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
05/18/2022 16:34:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
05/18/2022 16:34:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
05/18/2022 16:34:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/18/2022 16:34:26 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.5604964878671775 on epoch=574
05/18/2022 16:34:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=577
05/18/2022 16:34:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
05/18/2022 16:34:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
05/18/2022 16:34:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=584
05/18/2022 16:34:33 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=587
05/18/2022 16:34:34 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.560540239487608 on epoch=587
05/18/2022 16:34:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
05/18/2022 16:34:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=592
05/18/2022 16:34:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/18/2022 16:34:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
05/18/2022 16:34:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/18/2022 16:34:42 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.5743181818181818 on epoch=599
05/18/2022 16:34:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
05/18/2022 16:34:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/18/2022 16:34:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/18/2022 16:34:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/18/2022 16:34:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/18/2022 16:34:51 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.5877345884605413 on epoch=612
05/18/2022 16:34:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/18/2022 16:34:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=617
05/18/2022 16:34:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
05/18/2022 16:34:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
05/18/2022 16:34:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
05/18/2022 16:34:59 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.5628363896165753 on epoch=624
05/18/2022 16:35:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/18/2022 16:35:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
05/18/2022 16:35:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/18/2022 16:35:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=634
05/18/2022 16:35:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
05/18/2022 16:35:08 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.5397386397386397 on epoch=637
05/18/2022 16:35:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/18/2022 16:35:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 16:35:12 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=644
05/18/2022 16:35:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 16:35:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=649
05/18/2022 16:35:16 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.5789760348583879 on epoch=649
05/18/2022 16:35:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/18/2022 16:35:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
05/18/2022 16:35:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 16:35:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
05/18/2022 16:35:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/18/2022 16:35:25 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5035170361247947 on epoch=662
05/18/2022 16:35:26 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/18/2022 16:35:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=667
05/18/2022 16:35:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
05/18/2022 16:35:31 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/18/2022 16:35:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/18/2022 16:35:33 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5579911524500907 on epoch=674
05/18/2022 16:35:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/18/2022 16:35:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
05/18/2022 16:35:38 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=682
05/18/2022 16:35:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/18/2022 16:35:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/18/2022 16:35:42 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.5570210286561789 on epoch=687
05/18/2022 16:35:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=689
05/18/2022 16:35:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
05/18/2022 16:35:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 16:35:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/18/2022 16:35:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/18/2022 16:35:50 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.5284085491816304 on epoch=699
05/18/2022 16:35:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=702
05/18/2022 16:35:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/18/2022 16:35:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=707
05/18/2022 16:35:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/18/2022 16:35:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/18/2022 16:35:58 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.5392308897243108 on epoch=712
05/18/2022 16:36:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
05/18/2022 16:36:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
05/18/2022 16:36:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=719
05/18/2022 16:36:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
05/18/2022 16:36:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
05/18/2022 16:36:07 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.577970177970178 on epoch=724
05/18/2022 16:36:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/18/2022 16:36:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 16:36:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/18/2022 16:36:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/18/2022 16:36:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
05/18/2022 16:36:15 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5347985347985348 on epoch=737
05/18/2022 16:36:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=739
05/18/2022 16:36:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/18/2022 16:36:20 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/18/2022 16:36:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/18/2022 16:36:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/18/2022 16:36:23 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.5908119658119658 on epoch=749
05/18/2022 16:36:23 - INFO - __main__ - save last model!
05/18/2022 16:36:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:36:23 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:36:23 - INFO - __main__ - Printing 3 examples
05/18/2022 16:36:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:36:23 - INFO - __main__ - ['others']
05/18/2022 16:36:23 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:36:23 - INFO - __main__ - ['others']
05/18/2022 16:36:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:36:23 - INFO - __main__ - ['others']
05/18/2022 16:36:23 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:36:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:36:24 - INFO - __main__ - Printing 3 examples
05/18/2022 16:36:24 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:36:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:36:24 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:36:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:36:24 - INFO - __main__ - Printing 3 examples
05/18/2022 16:36:24 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:36:24 - INFO - __main__ - ['sad']
05/18/2022 16:36:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:36:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:36:24 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:36:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:36:31 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:36:31 - INFO - __main__ - task name: emo
05/18/2022 16:36:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:36:31 - INFO - __main__ - Starting training!
05/18/2022 16:36:32 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:37:49 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.2_8_predictions.txt
05/18/2022 16:37:49 - INFO - __main__ - Classification-F1 on test data: 0.3483
05/18/2022 16:37:49 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.628027378027378, test_performance=0.3482672792591976
05/18/2022 16:37:49 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
05/18/2022 16:37:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:37:50 - INFO - __main__ - Printing 3 examples
05/18/2022 16:37:50 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:37:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:37:50 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:37:50 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:37:50 - INFO - __main__ - Printing 3 examples
05/18/2022 16:37:50 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:37:50 - INFO - __main__ - ['sad']
05/18/2022 16:37:50 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:37:50 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:37:51 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:37:57 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:37:57 - INFO - __main__ - task name: emo
05/18/2022 16:37:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:37:58 - INFO - __main__ - Starting training!
05/18/2022 16:37:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.32 on epoch=2
05/18/2022 16:38:00 - INFO - __main__ - Step 20 Global step 20 Train loss 3.73 on epoch=4
05/18/2022 16:38:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.45 on epoch=7
05/18/2022 16:38:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.99 on epoch=9
05/18/2022 16:38:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.45 on epoch=12
05/18/2022 16:38:06 - INFO - __main__ - Global step 50 Train loss 3.39 Classification-F1 0.13067758749069247 on epoch=12
05/18/2022 16:38:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
05/18/2022 16:38:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.67 on epoch=14
05/18/2022 16:38:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.78 on epoch=17
05/18/2022 16:38:11 - INFO - __main__ - Step 80 Global step 80 Train loss 1.36 on epoch=19
05/18/2022 16:38:12 - INFO - __main__ - Step 90 Global step 90 Train loss 1.34 on epoch=22
05/18/2022 16:38:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.25 on epoch=24
05/18/2022 16:38:15 - INFO - __main__ - Global step 100 Train loss 1.48 Classification-F1 0.16132939662351425 on epoch=24
05/18/2022 16:38:15 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.16132939662351425 on epoch=24, global_step=100
05/18/2022 16:38:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.31 on epoch=27
05/18/2022 16:38:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
05/18/2022 16:38:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.20 on epoch=32
05/18/2022 16:38:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
05/18/2022 16:38:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
05/18/2022 16:38:23 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.1794380308904873 on epoch=37
05/18/2022 16:38:23 - INFO - __main__ - Saving model with best Classification-F1: 0.16132939662351425 -> 0.1794380308904873 on epoch=37, global_step=150
05/18/2022 16:38:25 - INFO - __main__ - Step 160 Global step 160 Train loss 1.10 on epoch=39
05/18/2022 16:38:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.13 on epoch=42
05/18/2022 16:38:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.06 on epoch=44
05/18/2022 16:38:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
05/18/2022 16:38:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.08 on epoch=49
05/18/2022 16:38:31 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.10256410256410256 on epoch=49
05/18/2022 16:38:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
05/18/2022 16:38:35 - INFO - __main__ - Step 220 Global step 220 Train loss 1.13 on epoch=54
05/18/2022 16:38:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.97 on epoch=57
05/18/2022 16:38:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
05/18/2022 16:38:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
05/18/2022 16:38:40 - INFO - __main__ - Global step 250 Train loss 0.98 Classification-F1 0.24016531713900136 on epoch=62
05/18/2022 16:38:40 - INFO - __main__ - Saving model with best Classification-F1: 0.1794380308904873 -> 0.24016531713900136 on epoch=62, global_step=250
05/18/2022 16:38:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.93 on epoch=64
05/18/2022 16:38:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.06 on epoch=67
05/18/2022 16:38:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
05/18/2022 16:38:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
05/18/2022 16:38:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
05/18/2022 16:38:48 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.23973727422003283 on epoch=74
05/18/2022 16:38:49 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
05/18/2022 16:38:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
05/18/2022 16:38:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
05/18/2022 16:38:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.99 on epoch=84
05/18/2022 16:38:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
05/18/2022 16:38:56 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.3074074074074074 on epoch=87
05/18/2022 16:38:56 - INFO - __main__ - Saving model with best Classification-F1: 0.24016531713900136 -> 0.3074074074074074 on epoch=87, global_step=350
05/18/2022 16:38:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
05/18/2022 16:38:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
05/18/2022 16:39:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
05/18/2022 16:39:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.88 on epoch=97
05/18/2022 16:39:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
05/18/2022 16:39:04 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.4158653846153846 on epoch=99
05/18/2022 16:39:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3074074074074074 -> 0.4158653846153846 on epoch=99, global_step=400
05/18/2022 16:39:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=102
05/18/2022 16:39:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=104
05/18/2022 16:39:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
05/18/2022 16:39:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=109
05/18/2022 16:39:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=112
05/18/2022 16:39:12 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.4131224525961368 on epoch=112
05/18/2022 16:39:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
05/18/2022 16:39:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=117
05/18/2022 16:39:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=119
05/18/2022 16:39:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
05/18/2022 16:39:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=124
05/18/2022 16:39:20 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.4901573787409701 on epoch=124
05/18/2022 16:39:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4158653846153846 -> 0.4901573787409701 on epoch=124, global_step=500
05/18/2022 16:39:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=127
05/18/2022 16:39:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=129
05/18/2022 16:39:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.60 on epoch=132
05/18/2022 16:39:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
05/18/2022 16:39:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=137
05/18/2022 16:39:29 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.4578508727676319 on epoch=137
05/18/2022 16:39:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=139
05/18/2022 16:39:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=142
05/18/2022 16:39:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=144
05/18/2022 16:39:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=147
05/18/2022 16:39:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=149
05/18/2022 16:39:38 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.5666699893673578 on epoch=149
05/18/2022 16:39:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4901573787409701 -> 0.5666699893673578 on epoch=149, global_step=600
05/18/2022 16:39:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=152
05/18/2022 16:39:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=154
05/18/2022 16:39:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=157
05/18/2022 16:39:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=159
05/18/2022 16:39:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
05/18/2022 16:39:46 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.655924419140697 on epoch=162
05/18/2022 16:39:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5666699893673578 -> 0.655924419140697 on epoch=162, global_step=650
05/18/2022 16:39:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=164
05/18/2022 16:39:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=167
05/18/2022 16:39:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=169
05/18/2022 16:39:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=172
05/18/2022 16:39:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=174
05/18/2022 16:39:54 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.5489911906791702 on epoch=174
05/18/2022 16:39:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=177
05/18/2022 16:39:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=179
05/18/2022 16:39:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=182
05/18/2022 16:40:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=184
05/18/2022 16:40:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=187
05/18/2022 16:40:03 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6084459148975278 on epoch=187
05/18/2022 16:40:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=189
05/18/2022 16:40:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
05/18/2022 16:40:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
05/18/2022 16:40:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=197
05/18/2022 16:40:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/18/2022 16:40:12 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.5004047356988534 on epoch=199
05/18/2022 16:40:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=202
05/18/2022 16:40:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=204
05/18/2022 16:40:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
05/18/2022 16:40:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
05/18/2022 16:40:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
05/18/2022 16:40:20 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.6189301777682588 on epoch=212
05/18/2022 16:40:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
05/18/2022 16:40:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=217
05/18/2022 16:40:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
05/18/2022 16:40:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
05/18/2022 16:40:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
05/18/2022 16:40:28 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.7488530668677729 on epoch=224
05/18/2022 16:40:28 - INFO - __main__ - Saving model with best Classification-F1: 0.655924419140697 -> 0.7488530668677729 on epoch=224, global_step=900
05/18/2022 16:40:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
05/18/2022 16:40:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
05/18/2022 16:40:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
05/18/2022 16:40:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
05/18/2022 16:40:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
05/18/2022 16:40:36 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.6670518680445151 on epoch=237
05/18/2022 16:40:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
05/18/2022 16:40:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
05/18/2022 16:40:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
05/18/2022 16:40:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=247
05/18/2022 16:40:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
05/18/2022 16:40:45 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6785633995103149 on epoch=249
05/18/2022 16:40:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
05/18/2022 16:40:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
05/18/2022 16:40:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
05/18/2022 16:40:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=259
05/18/2022 16:40:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
05/18/2022 16:40:54 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7047754585705249 on epoch=262
05/18/2022 16:40:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
05/18/2022 16:40:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
05/18/2022 16:40:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=269
05/18/2022 16:40:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
05/18/2022 16:41:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
05/18/2022 16:41:02 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.6591662709309768 on epoch=274
05/18/2022 16:41:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
05/18/2022 16:41:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
05/18/2022 16:41:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
05/18/2022 16:41:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=284
05/18/2022 16:41:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
05/18/2022 16:41:10 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6998268398268399 on epoch=287
05/18/2022 16:41:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
05/18/2022 16:41:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
05/18/2022 16:41:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
05/18/2022 16:41:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
05/18/2022 16:41:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
05/18/2022 16:41:19 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6405445111277821 on epoch=299
05/18/2022 16:41:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
05/18/2022 16:41:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
05/18/2022 16:41:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
05/18/2022 16:41:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
05/18/2022 16:41:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
05/18/2022 16:41:27 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.668123012718601 on epoch=312
05/18/2022 16:41:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
05/18/2022 16:41:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
05/18/2022 16:41:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
05/18/2022 16:41:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
05/18/2022 16:41:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
05/18/2022 16:41:36 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.649469696969697 on epoch=324
05/18/2022 16:41:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
05/18/2022 16:41:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
05/18/2022 16:41:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
05/18/2022 16:41:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
05/18/2022 16:41:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
05/18/2022 16:41:44 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6777381990285216 on epoch=337
05/18/2022 16:41:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
05/18/2022 16:41:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/18/2022 16:41:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
05/18/2022 16:41:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
05/18/2022 16:41:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
05/18/2022 16:41:53 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.5801806171470806 on epoch=349
05/18/2022 16:41:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=352
05/18/2022 16:41:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
05/18/2022 16:41:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
05/18/2022 16:41:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
05/18/2022 16:42:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
05/18/2022 16:42:01 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.6714285714285714 on epoch=362
05/18/2022 16:42:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
05/18/2022 16:42:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/18/2022 16:42:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
05/18/2022 16:42:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
05/18/2022 16:42:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
05/18/2022 16:42:09 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.64582225016524 on epoch=374
05/18/2022 16:42:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
05/18/2022 16:42:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
05/18/2022 16:42:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
05/18/2022 16:42:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
05/18/2022 16:42:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
05/18/2022 16:42:18 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6904115853658537 on epoch=387
05/18/2022 16:42:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/18/2022 16:42:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
05/18/2022 16:42:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
05/18/2022 16:42:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
05/18/2022 16:42:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
05/18/2022 16:42:26 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7023172905525847 on epoch=399
05/18/2022 16:42:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
05/18/2022 16:42:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
05/18/2022 16:42:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/18/2022 16:42:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/18/2022 16:42:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
05/18/2022 16:42:34 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.632901575777576 on epoch=412
05/18/2022 16:42:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
05/18/2022 16:42:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
05/18/2022 16:42:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
05/18/2022 16:42:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
05/18/2022 16:42:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/18/2022 16:42:42 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6569711538461538 on epoch=424
05/18/2022 16:42:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
05/18/2022 16:42:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
05/18/2022 16:42:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
05/18/2022 16:42:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=434
05/18/2022 16:42:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/18/2022 16:42:51 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6337903225806452 on epoch=437
05/18/2022 16:42:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
05/18/2022 16:42:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
05/18/2022 16:42:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
05/18/2022 16:42:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
05/18/2022 16:42:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/18/2022 16:42:59 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6971093954891162 on epoch=449
05/18/2022 16:43:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/18/2022 16:43:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/18/2022 16:43:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
05/18/2022 16:43:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
05/18/2022 16:43:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/18/2022 16:43:07 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6368627149877151 on epoch=462
05/18/2022 16:43:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
05/18/2022 16:43:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
05/18/2022 16:43:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
05/18/2022 16:43:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
05/18/2022 16:43:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/18/2022 16:43:16 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.656075119236884 on epoch=474
05/18/2022 16:43:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
05/18/2022 16:43:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
05/18/2022 16:43:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
05/18/2022 16:43:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
05/18/2022 16:43:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
05/18/2022 16:43:24 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6361512460233297 on epoch=487
05/18/2022 16:43:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/18/2022 16:43:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/18/2022 16:43:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/18/2022 16:43:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
05/18/2022 16:43:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
05/18/2022 16:43:32 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7034208027151575 on epoch=499
05/18/2022 16:43:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/18/2022 16:43:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
05/18/2022 16:43:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/18/2022 16:43:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
05/18/2022 16:43:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/18/2022 16:43:40 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6416197799449863 on epoch=512
05/18/2022 16:43:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/18/2022 16:43:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
05/18/2022 16:43:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/18/2022 16:43:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
05/18/2022 16:43:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/18/2022 16:43:49 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6922619047619047 on epoch=524
05/18/2022 16:43:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
05/18/2022 16:43:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
05/18/2022 16:43:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
05/18/2022 16:43:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
05/18/2022 16:43:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
05/18/2022 16:43:57 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6465544871794872 on epoch=537
05/18/2022 16:43:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/18/2022 16:43:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=542
05/18/2022 16:44:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
05/18/2022 16:44:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/18/2022 16:44:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/18/2022 16:44:05 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6206358182533438 on epoch=549
05/18/2022 16:44:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
05/18/2022 16:44:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
05/18/2022 16:44:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/18/2022 16:44:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
05/18/2022 16:44:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/18/2022 16:44:13 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6621794871794872 on epoch=562
05/18/2022 16:44:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/18/2022 16:44:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/18/2022 16:44:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/18/2022 16:44:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
05/18/2022 16:44:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/18/2022 16:44:21 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6065668202764977 on epoch=574
05/18/2022 16:44:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
05/18/2022 16:44:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/18/2022 16:44:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
05/18/2022 16:44:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
05/18/2022 16:44:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
05/18/2022 16:44:30 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6086209052263065 on epoch=587
05/18/2022 16:44:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
05/18/2022 16:44:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/18/2022 16:44:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
05/18/2022 16:44:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/18/2022 16:44:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/18/2022 16:44:38 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6925526024363233 on epoch=599
05/18/2022 16:44:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/18/2022 16:44:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/18/2022 16:44:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/18/2022 16:44:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
05/18/2022 16:44:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/18/2022 16:44:47 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5791981239990849 on epoch=612
05/18/2022 16:44:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/18/2022 16:44:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
05/18/2022 16:44:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
05/18/2022 16:44:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/18/2022 16:44:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/18/2022 16:44:55 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.657490694789082 on epoch=624
05/18/2022 16:44:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/18/2022 16:44:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
05/18/2022 16:44:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 16:45:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/18/2022 16:45:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
05/18/2022 16:45:02 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6779569892473118 on epoch=637
05/18/2022 16:45:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 16:45:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/18/2022 16:45:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/18/2022 16:45:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/18/2022 16:45:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/18/2022 16:45:10 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6850559350559351 on epoch=649
05/18/2022 16:45:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/18/2022 16:45:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/18/2022 16:45:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 16:45:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
05/18/2022 16:45:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
05/18/2022 16:45:19 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6986895457483693 on epoch=662
05/18/2022 16:45:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/18/2022 16:45:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/18/2022 16:45:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/18/2022 16:45:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/18/2022 16:45:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/18/2022 16:45:27 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6685782967032967 on epoch=674
05/18/2022 16:45:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
05/18/2022 16:45:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/18/2022 16:45:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/18/2022 16:45:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/18/2022 16:45:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/18/2022 16:45:35 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6473921469497809 on epoch=687
05/18/2022 16:45:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/18/2022 16:45:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
05/18/2022 16:45:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
05/18/2022 16:45:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
05/18/2022 16:45:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/18/2022 16:45:43 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6752747252747252 on epoch=699
05/18/2022 16:45:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/18/2022 16:45:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/18/2022 16:45:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/18/2022 16:45:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
05/18/2022 16:45:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/18/2022 16:45:52 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6515407464752585 on epoch=712
05/18/2022 16:45:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/18/2022 16:45:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/18/2022 16:45:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/18/2022 16:45:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
05/18/2022 16:45:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 16:46:01 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6896756978653531 on epoch=724
05/18/2022 16:46:02 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 16:46:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 16:46:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
05/18/2022 16:46:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/18/2022 16:46:09 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 16:46:10 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.653372668997669 on epoch=737
05/18/2022 16:46:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/18/2022 16:46:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/18/2022 16:46:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/18/2022 16:46:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/18/2022 16:46:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/18/2022 16:46:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6190015479876161 on epoch=749
05/18/2022 16:46:18 - INFO - __main__ - save last model!
05/18/2022 16:46:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:46:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:46:18 - INFO - __main__ - Printing 3 examples
05/18/2022 16:46:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:46:18 - INFO - __main__ - ['others']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:46:18 - INFO - __main__ - ['others']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:46:18 - INFO - __main__ - ['others']
05/18/2022 16:46:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:46:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:46:18 - INFO - __main__ - Printing 3 examples
05/18/2022 16:46:18 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:46:18 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:46:18 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:46:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:46:18 - INFO - __main__ - Printing 3 examples
05/18/2022 16:46:18 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:46:18 - INFO - __main__ - ['sad']
05/18/2022 16:46:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:46:18 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:46:18 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:46:20 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:46:24 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:46:24 - INFO - __main__ - task name: emo
05/18/2022 16:46:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:46:24 - INFO - __main__ - Starting training!
05/18/2022 16:46:27 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:47:42 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.5_8_predictions.txt
05/18/2022 16:47:42 - INFO - __main__ - Classification-F1 on test data: 0.3970
05/18/2022 16:47:42 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7488530668677729, test_performance=0.39696243311422963
05/18/2022 16:47:42 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
05/18/2022 16:47:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:47:43 - INFO - __main__ - Printing 3 examples
05/18/2022 16:47:43 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:47:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:47:43 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:47:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:47:43 - INFO - __main__ - Printing 3 examples
05/18/2022 16:47:43 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:47:43 - INFO - __main__ - ['sad']
05/18/2022 16:47:43 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:47:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:47:43 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:47:50 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:47:50 - INFO - __main__ - task name: emo
05/18/2022 16:47:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:47:50 - INFO - __main__ - Starting training!
05/18/2022 16:47:52 - INFO - __main__ - Step 10 Global step 10 Train loss 6.31 on epoch=2
05/18/2022 16:47:54 - INFO - __main__ - Step 20 Global step 20 Train loss 3.79 on epoch=4
05/18/2022 16:47:55 - INFO - __main__ - Step 30 Global step 30 Train loss 3.02 on epoch=7
05/18/2022 16:47:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=9
05/18/2022 16:47:58 - INFO - __main__ - Step 50 Global step 50 Train loss 2.56 on epoch=12
05/18/2022 16:47:59 - INFO - __main__ - Global step 50 Train loss 3.55 Classification-F1 0.15842490842490842 on epoch=12
05/18/2022 16:47:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15842490842490842 on epoch=12, global_step=50
05/18/2022 16:48:01 - INFO - __main__ - Step 60 Global step 60 Train loss 1.48 on epoch=14
05/18/2022 16:48:02 - INFO - __main__ - Step 70 Global step 70 Train loss 1.36 on epoch=17
05/18/2022 16:48:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.28 on epoch=19
05/18/2022 16:48:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.18 on epoch=22
05/18/2022 16:48:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.34 on epoch=24
05/18/2022 16:48:07 - INFO - __main__ - Global step 100 Train loss 1.33 Classification-F1 0.20237348858038512 on epoch=24
05/18/2022 16:48:07 - INFO - __main__ - Saving model with best Classification-F1: 0.15842490842490842 -> 0.20237348858038512 on epoch=24, global_step=100
05/18/2022 16:48:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.26 on epoch=27
05/18/2022 16:48:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.22 on epoch=29
05/18/2022 16:48:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.19 on epoch=32
05/18/2022 16:48:13 - INFO - __main__ - Step 140 Global step 140 Train loss 1.24 on epoch=34
05/18/2022 16:48:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
05/18/2022 16:48:15 - INFO - __main__ - Global step 150 Train loss 1.21 Classification-F1 0.12351351351351353 on epoch=37
05/18/2022 16:48:16 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
05/18/2022 16:48:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=42
05/18/2022 16:48:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
05/18/2022 16:48:20 - INFO - __main__ - Step 190 Global step 190 Train loss 1.10 on epoch=47
05/18/2022 16:48:22 - INFO - __main__ - Step 200 Global step 200 Train loss 1.08 on epoch=49
05/18/2022 16:48:23 - INFO - __main__ - Global step 200 Train loss 1.06 Classification-F1 0.12779973649538867 on epoch=49
05/18/2022 16:48:24 - INFO - __main__ - Step 210 Global step 210 Train loss 1.01 on epoch=52
05/18/2022 16:48:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
05/18/2022 16:48:27 - INFO - __main__ - Step 230 Global step 230 Train loss 1.09 on epoch=57
05/18/2022 16:48:28 - INFO - __main__ - Step 240 Global step 240 Train loss 1.00 on epoch=59
05/18/2022 16:48:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
05/18/2022 16:48:31 - INFO - __main__ - Global step 250 Train loss 1.00 Classification-F1 0.1 on epoch=62
05/18/2022 16:48:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.10 on epoch=64
05/18/2022 16:48:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.99 on epoch=67
05/18/2022 16:48:35 - INFO - __main__ - Step 280 Global step 280 Train loss 1.12 on epoch=69
05/18/2022 16:48:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.05 on epoch=72
05/18/2022 16:48:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
05/18/2022 16:48:39 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.23881673881673882 on epoch=74
05/18/2022 16:48:39 - INFO - __main__ - Saving model with best Classification-F1: 0.20237348858038512 -> 0.23881673881673882 on epoch=74, global_step=300
05/18/2022 16:48:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
05/18/2022 16:48:42 - INFO - __main__ - Step 320 Global step 320 Train loss 1.02 on epoch=79
05/18/2022 16:48:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.99 on epoch=82
05/18/2022 16:48:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.97 on epoch=84
05/18/2022 16:48:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=87
05/18/2022 16:48:47 - INFO - __main__ - Global step 350 Train loss 0.98 Classification-F1 0.16749134092033646 on epoch=87
05/18/2022 16:48:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
05/18/2022 16:48:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.96 on epoch=92
05/18/2022 16:48:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
05/18/2022 16:48:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=97
05/18/2022 16:48:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.04 on epoch=99
05/18/2022 16:48:55 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.2163760896637609 on epoch=99
05/18/2022 16:48:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=102
05/18/2022 16:48:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.97 on epoch=104
05/18/2022 16:49:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.87 on epoch=107
05/18/2022 16:49:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=109
05/18/2022 16:49:03 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=112
05/18/2022 16:49:03 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.1 on epoch=112
05/18/2022 16:49:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.93 on epoch=114
05/18/2022 16:49:06 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=117
05/18/2022 16:49:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
05/18/2022 16:49:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=122
05/18/2022 16:49:10 - INFO - __main__ - Step 500 Global step 500 Train loss 1.03 on epoch=124
05/18/2022 16:49:11 - INFO - __main__ - Global step 500 Train loss 0.95 Classification-F1 0.32504054253280257 on epoch=124
05/18/2022 16:49:11 - INFO - __main__ - Saving model with best Classification-F1: 0.23881673881673882 -> 0.32504054253280257 on epoch=124, global_step=500
05/18/2022 16:49:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.01 on epoch=127
05/18/2022 16:49:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
05/18/2022 16:49:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.94 on epoch=132
05/18/2022 16:49:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
05/18/2022 16:49:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=137
05/18/2022 16:49:19 - INFO - __main__ - Global step 550 Train loss 0.95 Classification-F1 0.20606060606060606 on epoch=137
05/18/2022 16:49:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
05/18/2022 16:49:22 - INFO - __main__ - Step 570 Global step 570 Train loss 1.05 on epoch=142
05/18/2022 16:49:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
05/18/2022 16:49:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.94 on epoch=147
05/18/2022 16:49:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
05/18/2022 16:49:28 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.2507869249394673 on epoch=149
05/18/2022 16:49:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/18/2022 16:49:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 16:49:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=157
05/18/2022 16:49:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
05/18/2022 16:49:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.91 on epoch=162
05/18/2022 16:49:35 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.1581196581196581 on epoch=162
05/18/2022 16:49:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/18/2022 16:49:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
05/18/2022 16:49:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
05/18/2022 16:49:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
05/18/2022 16:49:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
05/18/2022 16:49:43 - INFO - __main__ - Global step 700 Train loss 0.87 Classification-F1 0.2183447749809306 on epoch=174
05/18/2022 16:49:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
05/18/2022 16:49:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/18/2022 16:49:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.89 on epoch=182
05/18/2022 16:49:49 - INFO - __main__ - Step 740 Global step 740 Train loss 0.74 on epoch=184
05/18/2022 16:49:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
05/18/2022 16:49:51 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.25298141644717587 on epoch=187
05/18/2022 16:49:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
05/18/2022 16:49:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.76 on epoch=192
05/18/2022 16:49:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=194
05/18/2022 16:49:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.74 on epoch=197
05/18/2022 16:49:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.68 on epoch=199
05/18/2022 16:49:59 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.4523892773892775 on epoch=199
05/18/2022 16:49:59 - INFO - __main__ - Saving model with best Classification-F1: 0.32504054253280257 -> 0.4523892773892775 on epoch=199, global_step=800
05/18/2022 16:50:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=202
05/18/2022 16:50:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=204
05/18/2022 16:50:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.75 on epoch=207
05/18/2022 16:50:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=209
05/18/2022 16:50:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=212
05/18/2022 16:50:07 - INFO - __main__ - Global step 850 Train loss 0.66 Classification-F1 0.3693671630094044 on epoch=212
05/18/2022 16:50:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.65 on epoch=214
05/18/2022 16:50:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=217
05/18/2022 16:50:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.57 on epoch=219
05/18/2022 16:50:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
05/18/2022 16:50:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=224
05/18/2022 16:50:16 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.4577269577269577 on epoch=224
05/18/2022 16:50:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4523892773892775 -> 0.4577269577269577 on epoch=224, global_step=900
05/18/2022 16:50:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=227
05/18/2022 16:50:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=229
05/18/2022 16:50:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.57 on epoch=232
05/18/2022 16:50:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
05/18/2022 16:50:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=237
05/18/2022 16:50:24 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.5815445665445665 on epoch=237
05/18/2022 16:50:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4577269577269577 -> 0.5815445665445665 on epoch=237, global_step=950
05/18/2022 16:50:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=239
05/18/2022 16:50:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=242
05/18/2022 16:50:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=244
05/18/2022 16:50:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=247
05/18/2022 16:50:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=249
05/18/2022 16:50:31 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.571078431372549 on epoch=249
05/18/2022 16:50:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=252
05/18/2022 16:50:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=254
05/18/2022 16:50:36 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=257
05/18/2022 16:50:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=259
05/18/2022 16:50:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=262
05/18/2022 16:50:39 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6380412670735252 on epoch=262
05/18/2022 16:50:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5815445665445665 -> 0.6380412670735252 on epoch=262, global_step=1050
05/18/2022 16:50:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=264
05/18/2022 16:50:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=267
05/18/2022 16:50:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=269
05/18/2022 16:50:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=272
05/18/2022 16:50:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
05/18/2022 16:50:48 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.6388556618819776 on epoch=274
05/18/2022 16:50:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6380412670735252 -> 0.6388556618819776 on epoch=274, global_step=1100
05/18/2022 16:50:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=277
05/18/2022 16:50:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
05/18/2022 16:50:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=282
05/18/2022 16:50:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=284
05/18/2022 16:50:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=287
05/18/2022 16:50:57 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.6070660522273426 on epoch=287
05/18/2022 16:50:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
05/18/2022 16:51:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=292
05/18/2022 16:51:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
05/18/2022 16:51:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=297
05/18/2022 16:51:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
05/18/2022 16:51:05 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.575595238095238 on epoch=299
05/18/2022 16:51:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
05/18/2022 16:51:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
05/18/2022 16:51:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
05/18/2022 16:51:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=309
05/18/2022 16:51:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
05/18/2022 16:51:13 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.6286043221527092 on epoch=312
05/18/2022 16:51:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
05/18/2022 16:51:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
05/18/2022 16:51:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=319
05/18/2022 16:51:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=322
05/18/2022 16:51:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=324
05/18/2022 16:51:21 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.624421347486363 on epoch=324
05/18/2022 16:51:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
05/18/2022 16:51:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
05/18/2022 16:51:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=332
05/18/2022 16:51:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
05/18/2022 16:51:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=337
05/18/2022 16:51:29 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.6638405948889821 on epoch=337
05/18/2022 16:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6388556618819776 -> 0.6638405948889821 on epoch=337, global_step=1350
05/18/2022 16:51:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/18/2022 16:51:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
05/18/2022 16:51:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=344
05/18/2022 16:51:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=347
05/18/2022 16:51:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=349
05/18/2022 16:51:38 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.6286043221527092 on epoch=349
05/18/2022 16:51:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=352
05/18/2022 16:51:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
05/18/2022 16:51:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
05/18/2022 16:51:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=359
05/18/2022 16:51:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
05/18/2022 16:51:46 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.685530462184874 on epoch=362
05/18/2022 16:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6638405948889821 -> 0.685530462184874 on epoch=362, global_step=1450
05/18/2022 16:51:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=364
05/18/2022 16:51:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=367
05/18/2022 16:51:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=369
05/18/2022 16:51:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
05/18/2022 16:51:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
05/18/2022 16:51:55 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.6556878306878307 on epoch=374
05/18/2022 16:51:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
05/18/2022 16:51:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=379
05/18/2022 16:52:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=382
05/18/2022 16:52:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=384
05/18/2022 16:52:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=387
05/18/2022 16:52:04 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.7045038468668892 on epoch=387
05/18/2022 16:52:04 - INFO - __main__ - Saving model with best Classification-F1: 0.685530462184874 -> 0.7045038468668892 on epoch=387, global_step=1550
05/18/2022 16:52:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
05/18/2022 16:52:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
05/18/2022 16:52:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
05/18/2022 16:52:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
05/18/2022 16:52:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
05/18/2022 16:52:12 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6388888888888888 on epoch=399
05/18/2022 16:52:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=402
05/18/2022 16:52:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
05/18/2022 16:52:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
05/18/2022 16:52:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/18/2022 16:52:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
05/18/2022 16:52:20 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6836038961038962 on epoch=412
05/18/2022 16:52:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/18/2022 16:52:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
05/18/2022 16:52:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/18/2022 16:52:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
05/18/2022 16:52:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
05/18/2022 16:52:29 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.6391402714932127 on epoch=424
05/18/2022 16:52:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
05/18/2022 16:52:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
05/18/2022 16:52:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
05/18/2022 16:52:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
05/18/2022 16:52:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
05/18/2022 16:52:38 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7131597774244833 on epoch=437
05/18/2022 16:52:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7045038468668892 -> 0.7131597774244833 on epoch=437, global_step=1750
05/18/2022 16:52:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/18/2022 16:52:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
05/18/2022 16:52:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
05/18/2022 16:52:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=447
05/18/2022 16:52:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
05/18/2022 16:52:47 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7322850875981806 on epoch=449
05/18/2022 16:52:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7131597774244833 -> 0.7322850875981806 on epoch=449, global_step=1800
05/18/2022 16:52:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
05/18/2022 16:52:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
05/18/2022 16:52:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
05/18/2022 16:52:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
05/18/2022 16:52:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/18/2022 16:52:55 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7176030668677729 on epoch=462
05/18/2022 16:52:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/18/2022 16:52:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
05/18/2022 16:52:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/18/2022 16:53:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/18/2022 16:53:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/18/2022 16:53:03 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7011467086834734 on epoch=474
05/18/2022 16:53:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=477
05/18/2022 16:53:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
05/18/2022 16:53:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/18/2022 16:53:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
05/18/2022 16:53:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/18/2022 16:53:11 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7458593114665222 on epoch=487
05/18/2022 16:53:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7322850875981806 -> 0.7458593114665222 on epoch=487, global_step=1950
05/18/2022 16:53:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
05/18/2022 16:53:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/18/2022 16:53:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/18/2022 16:53:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
05/18/2022 16:53:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/18/2022 16:53:20 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7157426188866352 on epoch=499
05/18/2022 16:53:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
05/18/2022 16:53:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/18/2022 16:53:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
05/18/2022 16:53:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/18/2022 16:53:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
05/18/2022 16:53:27 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.747259118701244 on epoch=512
05/18/2022 16:53:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7458593114665222 -> 0.747259118701244 on epoch=512, global_step=2050
05/18/2022 16:53:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
05/18/2022 16:53:30 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/18/2022 16:53:31 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
05/18/2022 16:53:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
05/18/2022 16:53:34 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/18/2022 16:53:35 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6876427494074553 on epoch=524
05/18/2022 16:53:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
05/18/2022 16:53:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
05/18/2022 16:53:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
05/18/2022 16:53:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=534
05/18/2022 16:53:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
05/18/2022 16:53:44 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7160292355525621 on epoch=537
05/18/2022 16:53:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
05/18/2022 16:53:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
05/18/2022 16:53:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/18/2022 16:53:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
05/18/2022 16:53:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/18/2022 16:53:51 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7021618324749255 on epoch=549
05/18/2022 16:53:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
05/18/2022 16:53:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/18/2022 16:53:56 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/18/2022 16:53:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/18/2022 16:53:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
05/18/2022 16:54:00 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6686487854251013 on epoch=562
05/18/2022 16:54:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
05/18/2022 16:54:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/18/2022 16:54:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/18/2022 16:54:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
05/18/2022 16:54:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
05/18/2022 16:54:09 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7288515406162466 on epoch=574
05/18/2022 16:54:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
05/18/2022 16:54:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/18/2022 16:54:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/18/2022 16:54:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
05/18/2022 16:54:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/18/2022 16:54:16 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7023535851122058 on epoch=587
05/18/2022 16:54:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=589
05/18/2022 16:54:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/18/2022 16:54:21 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/18/2022 16:54:22 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/18/2022 16:54:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
05/18/2022 16:54:24 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.7164713064713065 on epoch=599
05/18/2022 16:54:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/18/2022 16:54:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/18/2022 16:54:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/18/2022 16:54:30 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/18/2022 16:54:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
05/18/2022 16:54:32 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7264160401002507 on epoch=612
05/18/2022 16:54:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/18/2022 16:54:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/18/2022 16:54:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/18/2022 16:54:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/18/2022 16:54:40 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/18/2022 16:54:41 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6993571821158027 on epoch=624
05/18/2022 16:54:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
05/18/2022 16:54:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/18/2022 16:54:45 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 16:54:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/18/2022 16:54:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
05/18/2022 16:54:49 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6808565531475749 on epoch=637
05/18/2022 16:54:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
05/18/2022 16:54:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
05/18/2022 16:54:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/18/2022 16:54:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/18/2022 16:54:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/18/2022 16:54:58 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6836843336843337 on epoch=649
05/18/2022 16:54:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/18/2022 16:55:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
05/18/2022 16:55:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/18/2022 16:55:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 16:55:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/18/2022 16:55:07 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6543136598580147 on epoch=662
05/18/2022 16:55:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/18/2022 16:55:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/18/2022 16:55:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
05/18/2022 16:55:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
05/18/2022 16:55:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
05/18/2022 16:55:15 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7273873963847 on epoch=674
05/18/2022 16:55:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/18/2022 16:55:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/18/2022 16:55:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/18/2022 16:55:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/18/2022 16:55:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
05/18/2022 16:55:23 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6584576305628937 on epoch=687
05/18/2022 16:55:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/18/2022 16:55:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
05/18/2022 16:55:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/18/2022 16:55:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
05/18/2022 16:55:31 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
05/18/2022 16:55:32 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6726093956644241 on epoch=699
05/18/2022 16:55:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
05/18/2022 16:55:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/18/2022 16:55:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/18/2022 16:55:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
05/18/2022 16:55:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
05/18/2022 16:55:40 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.670185291858679 on epoch=712
05/18/2022 16:55:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/18/2022 16:55:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/18/2022 16:55:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/18/2022 16:55:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/18/2022 16:55:47 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 16:55:48 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6536931818181818 on epoch=724
05/18/2022 16:55:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/18/2022 16:55:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
05/18/2022 16:55:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 16:55:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
05/18/2022 16:55:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 16:55:57 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6513888888888889 on epoch=737
05/18/2022 16:55:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
05/18/2022 16:56:00 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/18/2022 16:56:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/18/2022 16:56:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
05/18/2022 16:56:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/18/2022 16:56:05 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6336024024451483 on epoch=749
05/18/2022 16:56:05 - INFO - __main__ - save last model!
05/18/2022 16:56:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 16:56:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 16:56:05 - INFO - __main__ - Printing 3 examples
05/18/2022 16:56:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 16:56:05 - INFO - __main__ - ['others']
05/18/2022 16:56:05 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 16:56:05 - INFO - __main__ - ['others']
05/18/2022 16:56:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 16:56:05 - INFO - __main__ - ['others']
05/18/2022 16:56:05 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:56:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:56:06 - INFO - __main__ - Printing 3 examples
05/18/2022 16:56:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:56:06 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:56:06 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:56:06 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:56:06 - INFO - __main__ - Printing 3 examples
05/18/2022 16:56:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:56:06 - INFO - __main__ - ['sad']
05/18/2022 16:56:06 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:56:06 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:56:06 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:56:08 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:56:12 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:56:12 - INFO - __main__ - task name: emo
05/18/2022 16:56:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:56:13 - INFO - __main__ - Starting training!
05/18/2022 16:56:15 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 16:57:32 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.4_8_predictions.txt
05/18/2022 16:57:32 - INFO - __main__ - Classification-F1 on test data: 0.3691
05/18/2022 16:57:32 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.747259118701244, test_performance=0.3691317128312531
05/18/2022 16:57:32 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
05/18/2022 16:57:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:57:33 - INFO - __main__ - Printing 3 examples
05/18/2022 16:57:33 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:57:33 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:57:33 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 16:57:33 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 16:57:33 - INFO - __main__ - Printing 3 examples
05/18/2022 16:57:33 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 16:57:33 - INFO - __main__ - ['sad']
05/18/2022 16:57:33 - INFO - __main__ - Tokenizing Input ...
05/18/2022 16:57:33 - INFO - __main__ - Tokenizing Output ...
05/18/2022 16:57:33 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 16:57:40 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 16:57:40 - INFO - __main__ - task name: emo
05/18/2022 16:57:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 16:57:40 - INFO - __main__ - Starting training!
05/18/2022 16:57:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.49 on epoch=2
05/18/2022 16:57:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.62 on epoch=4
05/18/2022 16:57:45 - INFO - __main__ - Step 30 Global step 30 Train loss 2.70 on epoch=7
05/18/2022 16:57:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.80 on epoch=9
05/18/2022 16:57:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.55 on epoch=12
05/18/2022 16:57:49 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.10897435897435896 on epoch=12
05/18/2022 16:57:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10897435897435896 on epoch=12, global_step=50
05/18/2022 16:57:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.64 on epoch=14
05/18/2022 16:57:52 - INFO - __main__ - Step 70 Global step 70 Train loss 1.47 on epoch=17
05/18/2022 16:57:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=19
05/18/2022 16:57:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=22
05/18/2022 16:57:56 - INFO - __main__ - Step 100 Global step 100 Train loss 1.18 on epoch=24
05/18/2022 16:57:57 - INFO - __main__ - Global step 100 Train loss 1.34 Classification-F1 0.22807017543859648 on epoch=24
05/18/2022 16:57:57 - INFO - __main__ - Saving model with best Classification-F1: 0.10897435897435896 -> 0.22807017543859648 on epoch=24, global_step=100
05/18/2022 16:57:58 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=27
05/18/2022 16:58:00 - INFO - __main__ - Step 120 Global step 120 Train loss 1.12 on epoch=29
05/18/2022 16:58:01 - INFO - __main__ - Step 130 Global step 130 Train loss 1.15 on epoch=32
05/18/2022 16:58:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.11 on epoch=34
05/18/2022 16:58:04 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
05/18/2022 16:58:06 - INFO - __main__ - Global step 150 Train loss 1.13 Classification-F1 0.11805555555555555 on epoch=37
05/18/2022 16:58:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
05/18/2022 16:58:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.21 on epoch=42
05/18/2022 16:58:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.12 on epoch=44
05/18/2022 16:58:11 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=47
05/18/2022 16:58:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
05/18/2022 16:58:14 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.130952380952381 on epoch=49
05/18/2022 16:58:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
05/18/2022 16:58:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
05/18/2022 16:58:18 - INFO - __main__ - Step 230 Global step 230 Train loss 1.09 on epoch=57
05/18/2022 16:58:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=59
05/18/2022 16:58:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
05/18/2022 16:58:22 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.13034188034188032 on epoch=62
05/18/2022 16:58:23 - INFO - __main__ - Step 260 Global step 260 Train loss 1.22 on epoch=64
05/18/2022 16:58:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
05/18/2022 16:58:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.99 on epoch=69
05/18/2022 16:58:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
05/18/2022 16:58:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.99 on epoch=74
05/18/2022 16:58:30 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.24221003134796243 on epoch=74
05/18/2022 16:58:30 - INFO - __main__ - Saving model with best Classification-F1: 0.22807017543859648 -> 0.24221003134796243 on epoch=74, global_step=300
05/18/2022 16:58:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
05/18/2022 16:58:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=79
05/18/2022 16:58:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.98 on epoch=82
05/18/2022 16:58:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
05/18/2022 16:58:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=87
05/18/2022 16:58:39 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.11732186732186733 on epoch=87
05/18/2022 16:58:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.99 on epoch=89
05/18/2022 16:58:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=92
05/18/2022 16:58:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
05/18/2022 16:58:45 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
05/18/2022 16:58:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.92 on epoch=99
05/18/2022 16:58:48 - INFO - __main__ - Global step 400 Train loss 0.94 Classification-F1 0.29630376344086023 on epoch=99
05/18/2022 16:58:48 - INFO - __main__ - Saving model with best Classification-F1: 0.24221003134796243 -> 0.29630376344086023 on epoch=99, global_step=400
05/18/2022 16:58:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=102
05/18/2022 16:58:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/18/2022 16:58:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
05/18/2022 16:58:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
05/18/2022 16:58:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
05/18/2022 16:58:56 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.23214285714285715 on epoch=112
05/18/2022 16:58:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
05/18/2022 16:58:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.84 on epoch=117
05/18/2022 16:59:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=119
05/18/2022 16:59:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
05/18/2022 16:59:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
05/18/2022 16:59:04 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.2705821415211074 on epoch=124
05/18/2022 16:59:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
05/18/2022 16:59:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
05/18/2022 16:59:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.73 on epoch=132
05/18/2022 16:59:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=134
05/18/2022 16:59:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=137
05/18/2022 16:59:11 - INFO - __main__ - Global step 550 Train loss 0.80 Classification-F1 0.4477747909199522 on epoch=137
05/18/2022 16:59:12 - INFO - __main__ - Saving model with best Classification-F1: 0.29630376344086023 -> 0.4477747909199522 on epoch=137, global_step=550
05/18/2022 16:59:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=139
05/18/2022 16:59:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=142
05/18/2022 16:59:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=144
05/18/2022 16:59:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=147
05/18/2022 16:59:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=149
05/18/2022 16:59:20 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.3820409982174688 on epoch=149
05/18/2022 16:59:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
05/18/2022 16:59:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=154
05/18/2022 16:59:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=157
05/18/2022 16:59:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=159
05/18/2022 16:59:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
05/18/2022 16:59:28 - INFO - __main__ - Global step 650 Train loss 0.71 Classification-F1 0.37881231671554255 on epoch=162
05/18/2022 16:59:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
05/18/2022 16:59:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
05/18/2022 16:59:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=169
05/18/2022 16:59:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
05/18/2022 16:59:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=174
05/18/2022 16:59:38 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.4722222222222222 on epoch=174
05/18/2022 16:59:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4477747909199522 -> 0.4722222222222222 on epoch=174, global_step=700
05/18/2022 16:59:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.65 on epoch=177
05/18/2022 16:59:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.60 on epoch=179
05/18/2022 16:59:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=182
05/18/2022 16:59:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=184
05/18/2022 16:59:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=187
05/18/2022 16:59:46 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.47088392740566654 on epoch=187
05/18/2022 16:59:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=189
05/18/2022 16:59:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=192
05/18/2022 16:59:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=194
05/18/2022 16:59:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.60 on epoch=197
05/18/2022 16:59:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=199
05/18/2022 16:59:55 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.5361111111111111 on epoch=199
05/18/2022 16:59:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4722222222222222 -> 0.5361111111111111 on epoch=199, global_step=800
05/18/2022 16:59:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=202
05/18/2022 16:59:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=204
05/18/2022 17:00:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=207
05/18/2022 17:00:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=209
05/18/2022 17:00:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=212
05/18/2022 17:00:03 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.5037748982651531 on epoch=212
05/18/2022 17:00:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=214
05/18/2022 17:00:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
05/18/2022 17:00:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=219
05/18/2022 17:00:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=222
05/18/2022 17:00:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=224
05/18/2022 17:00:11 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.5589893852277754 on epoch=224
05/18/2022 17:00:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5361111111111111 -> 0.5589893852277754 on epoch=224, global_step=900
05/18/2022 17:00:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=227
05/18/2022 17:00:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=229
05/18/2022 17:00:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=232
05/18/2022 17:00:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=234
05/18/2022 17:00:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=237
05/18/2022 17:00:19 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.49413484692122467 on epoch=237
05/18/2022 17:00:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=239
05/18/2022 17:00:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=242
05/18/2022 17:00:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=244
05/18/2022 17:00:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=247
05/18/2022 17:00:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=249
05/18/2022 17:00:27 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.6201862495699113 on epoch=249
05/18/2022 17:00:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5589893852277754 -> 0.6201862495699113 on epoch=249, global_step=1000
05/18/2022 17:00:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=252
05/18/2022 17:00:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
05/18/2022 17:00:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=257
05/18/2022 17:00:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=259
05/18/2022 17:00:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=262
05/18/2022 17:00:35 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.6018687070444785 on epoch=262
05/18/2022 17:00:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=264
05/18/2022 17:00:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=267
05/18/2022 17:00:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=269
05/18/2022 17:00:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=272
05/18/2022 17:00:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=274
05/18/2022 17:00:44 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.6257352941176471 on epoch=274
05/18/2022 17:00:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6201862495699113 -> 0.6257352941176471 on epoch=274, global_step=1100
05/18/2022 17:00:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=277
05/18/2022 17:00:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=279
05/18/2022 17:00:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=282
05/18/2022 17:00:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=284
05/18/2022 17:00:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=287
05/18/2022 17:00:53 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.5287258521395607 on epoch=287
05/18/2022 17:00:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
05/18/2022 17:00:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
05/18/2022 17:00:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
05/18/2022 17:00:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=297
05/18/2022 17:01:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=299
05/18/2022 17:01:01 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.6077922077922079 on epoch=299
05/18/2022 17:01:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=302
05/18/2022 17:01:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=304
05/18/2022 17:01:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=307
05/18/2022 17:01:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=309
05/18/2022 17:01:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=312
05/18/2022 17:01:09 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.6606636500754148 on epoch=312
05/18/2022 17:01:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6257352941176471 -> 0.6606636500754148 on epoch=312, global_step=1250
05/18/2022 17:01:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
05/18/2022 17:01:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=317
05/18/2022 17:01:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=319
05/18/2022 17:01:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=322
05/18/2022 17:01:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=324
05/18/2022 17:01:17 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.6024393754656913 on epoch=324
05/18/2022 17:01:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=327
05/18/2022 17:01:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
05/18/2022 17:01:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
05/18/2022 17:01:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=334
05/18/2022 17:01:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
05/18/2022 17:01:25 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.621600765079026 on epoch=337
05/18/2022 17:01:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=339
05/18/2022 17:01:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=342
05/18/2022 17:01:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
05/18/2022 17:01:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
05/18/2022 17:01:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
05/18/2022 17:01:33 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.6760308028134854 on epoch=349
05/18/2022 17:01:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6606636500754148 -> 0.6760308028134854 on epoch=349, global_step=1400
05/18/2022 17:01:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
05/18/2022 17:01:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=354
05/18/2022 17:01:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=357
05/18/2022 17:01:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=359
05/18/2022 17:01:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
05/18/2022 17:01:41 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.6808442982456141 on epoch=362
05/18/2022 17:01:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6760308028134854 -> 0.6808442982456141 on epoch=362, global_step=1450
05/18/2022 17:01:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/18/2022 17:01:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=367
05/18/2022 17:01:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/18/2022 17:01:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
05/18/2022 17:01:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=374
05/18/2022 17:01:50 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.6626195945271148 on epoch=374
05/18/2022 17:01:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=377
05/18/2022 17:01:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=379
05/18/2022 17:01:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/18/2022 17:01:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
05/18/2022 17:01:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
05/18/2022 17:01:58 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.6192754613807245 on epoch=387
05/18/2022 17:02:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
05/18/2022 17:02:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
05/18/2022 17:02:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=394
05/18/2022 17:02:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
05/18/2022 17:02:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=399
05/18/2022 17:02:07 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.6332070707070707 on epoch=399
05/18/2022 17:02:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=402
05/18/2022 17:02:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=404
05/18/2022 17:02:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=407
05/18/2022 17:02:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
05/18/2022 17:02:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
05/18/2022 17:02:15 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7239583333333334 on epoch=412
05/18/2022 17:02:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6808442982456141 -> 0.7239583333333334 on epoch=412, global_step=1650
05/18/2022 17:02:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=414
05/18/2022 17:02:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=417
05/18/2022 17:02:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=419
05/18/2022 17:02:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
05/18/2022 17:02:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/18/2022 17:02:23 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7318014705882353 on epoch=424
05/18/2022 17:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7239583333333334 -> 0.7318014705882353 on epoch=424, global_step=1700
05/18/2022 17:02:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
05/18/2022 17:02:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=429
05/18/2022 17:02:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=432
05/18/2022 17:02:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
05/18/2022 17:02:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/18/2022 17:02:32 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.694246385422856 on epoch=437
05/18/2022 17:02:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/18/2022 17:02:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
05/18/2022 17:02:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
05/18/2022 17:02:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
05/18/2022 17:02:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
05/18/2022 17:02:40 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.6594957983193277 on epoch=449
05/18/2022 17:02:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
05/18/2022 17:02:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=454
05/18/2022 17:02:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=457
05/18/2022 17:02:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
05/18/2022 17:02:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
05/18/2022 17:02:49 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7286676286676287 on epoch=462
05/18/2022 17:02:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/18/2022 17:02:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
05/18/2022 17:02:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/18/2022 17:02:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
05/18/2022 17:02:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/18/2022 17:02:57 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7088960059548295 on epoch=474
05/18/2022 17:02:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
05/18/2022 17:03:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/18/2022 17:03:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=482
05/18/2022 17:03:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
05/18/2022 17:03:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/18/2022 17:03:06 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6976699770817418 on epoch=487
05/18/2022 17:03:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
05/18/2022 17:03:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
05/18/2022 17:03:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/18/2022 17:03:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/18/2022 17:03:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
05/18/2022 17:03:15 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.6902871621621621 on epoch=499
05/18/2022 17:03:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
05/18/2022 17:03:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
05/18/2022 17:03:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/18/2022 17:03:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/18/2022 17:03:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/18/2022 17:03:22 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6755443548387097 on epoch=512
05/18/2022 17:03:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
05/18/2022 17:03:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
05/18/2022 17:03:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/18/2022 17:03:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
05/18/2022 17:03:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/18/2022 17:03:31 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6760014478764479 on epoch=524
05/18/2022 17:03:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
05/18/2022 17:03:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
05/18/2022 17:03:36 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=532
05/18/2022 17:03:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
05/18/2022 17:03:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
05/18/2022 17:03:39 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.6771962074303406 on epoch=537
05/18/2022 17:03:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/18/2022 17:03:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
05/18/2022 17:03:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/18/2022 17:03:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
05/18/2022 17:03:47 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
05/18/2022 17:03:48 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6948832417582417 on epoch=549
05/18/2022 17:03:50 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
05/18/2022 17:03:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/18/2022 17:03:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=557
05/18/2022 17:03:54 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/18/2022 17:03:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=562
05/18/2022 17:03:57 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.6560096153846153 on epoch=562
05/18/2022 17:03:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/18/2022 17:04:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/18/2022 17:04:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
05/18/2022 17:04:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
05/18/2022 17:04:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/18/2022 17:04:05 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.689236111111111 on epoch=574
05/18/2022 17:04:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/18/2022 17:04:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/18/2022 17:04:09 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/18/2022 17:04:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/18/2022 17:04:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 17:04:14 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7292541389315582 on epoch=587
05/18/2022 17:04:15 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
05/18/2022 17:04:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
05/18/2022 17:04:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
05/18/2022 17:04:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
05/18/2022 17:04:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/18/2022 17:04:21 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6559403153153154 on epoch=599
05/18/2022 17:04:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
05/18/2022 17:04:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
05/18/2022 17:04:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/18/2022 17:04:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/18/2022 17:04:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
05/18/2022 17:04:29 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6647427788319629 on epoch=612
05/18/2022 17:04:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/18/2022 17:04:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/18/2022 17:04:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/18/2022 17:04:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/18/2022 17:04:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/18/2022 17:04:38 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6760657299356989 on epoch=624
05/18/2022 17:04:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/18/2022 17:04:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/18/2022 17:04:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
05/18/2022 17:04:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=634
05/18/2022 17:04:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
05/18/2022 17:04:47 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6567669172932331 on epoch=637
05/18/2022 17:04:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=639
05/18/2022 17:04:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 17:04:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/18/2022 17:04:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
05/18/2022 17:04:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
05/18/2022 17:04:54 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.6526892821010468 on epoch=649
05/18/2022 17:04:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
05/18/2022 17:04:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
05/18/2022 17:04:58 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/18/2022 17:05:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
05/18/2022 17:05:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
05/18/2022 17:05:02 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.69998460591133 on epoch=662
05/18/2022 17:05:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/18/2022 17:05:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/18/2022 17:05:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
05/18/2022 17:05:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/18/2022 17:05:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
05/18/2022 17:05:11 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.661313954862342 on epoch=674
05/18/2022 17:05:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/18/2022 17:05:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
05/18/2022 17:05:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
05/18/2022 17:05:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
05/18/2022 17:05:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
05/18/2022 17:05:19 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6563628740970072 on epoch=687
05/18/2022 17:05:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/18/2022 17:05:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
05/18/2022 17:05:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 17:05:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=697
05/18/2022 17:05:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
05/18/2022 17:05:26 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.6636029411764706 on epoch=699
05/18/2022 17:05:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/18/2022 17:05:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/18/2022 17:05:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/18/2022 17:05:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
05/18/2022 17:05:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/18/2022 17:05:34 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6692502645112078 on epoch=712
05/18/2022 17:05:36 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/18/2022 17:05:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/18/2022 17:05:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=719
05/18/2022 17:05:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
05/18/2022 17:05:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
05/18/2022 17:05:43 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6528875713658322 on epoch=724
05/18/2022 17:05:45 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/18/2022 17:05:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 17:05:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/18/2022 17:05:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/18/2022 17:05:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 17:05:51 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6601033375226923 on epoch=737
05/18/2022 17:05:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/18/2022 17:05:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/18/2022 17:05:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/18/2022 17:05:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
05/18/2022 17:05:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/18/2022 17:06:00 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6161054102230572 on epoch=749
05/18/2022 17:06:00 - INFO - __main__ - save last model!
05/18/2022 17:06:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:06:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:06:00 - INFO - __main__ - Printing 3 examples
05/18/2022 17:06:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:06:00 - INFO - __main__ - ['others']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:06:00 - INFO - __main__ - ['others']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:06:00 - INFO - __main__ - ['others']
05/18/2022 17:06:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:06:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:06:00 - INFO - __main__ - Printing 3 examples
05/18/2022 17:06:00 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:06:00 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:06:00 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:06:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:06:00 - INFO - __main__ - Printing 3 examples
05/18/2022 17:06:00 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 17:06:00 - INFO - __main__ - ['sad']
05/18/2022 17:06:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:06:00 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:06:00 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:06:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:06:06 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:06:06 - INFO - __main__ - task name: emo
05/18/2022 17:06:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:06:07 - INFO - __main__ - Starting training!
05/18/2022 17:06:08 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:07:25 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.3_8_predictions.txt
05/18/2022 17:07:25 - INFO - __main__ - Classification-F1 on test data: 0.2811
05/18/2022 17:07:25 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.7318014705882353, test_performance=0.28111361443384264
05/18/2022 17:07:25 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
05/18/2022 17:07:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:07:26 - INFO - __main__ - Printing 3 examples
05/18/2022 17:07:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:07:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:07:26 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:07:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:07:26 - INFO - __main__ - Printing 3 examples
05/18/2022 17:07:26 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
05/18/2022 17:07:26 - INFO - __main__ - ['sad']
05/18/2022 17:07:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:07:27 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:07:27 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:07:33 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:07:33 - INFO - __main__ - task name: emo
05/18/2022 17:07:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:07:34 - INFO - __main__ - Starting training!
05/18/2022 17:07:36 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=2
05/18/2022 17:07:37 - INFO - __main__ - Step 20 Global step 20 Train loss 5.46 on epoch=4
05/18/2022 17:07:39 - INFO - __main__ - Step 30 Global step 30 Train loss 4.14 on epoch=7
05/18/2022 17:07:40 - INFO - __main__ - Step 40 Global step 40 Train loss 3.02 on epoch=9
05/18/2022 17:07:42 - INFO - __main__ - Step 50 Global step 50 Train loss 2.57 on epoch=12
05/18/2022 17:07:42 - INFO - __main__ - Global step 50 Train loss 4.50 Classification-F1 0.1 on epoch=12
05/18/2022 17:07:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 17:07:44 - INFO - __main__ - Step 60 Global step 60 Train loss 2.03 on epoch=14
05/18/2022 17:07:45 - INFO - __main__ - Step 70 Global step 70 Train loss 1.83 on epoch=17
05/18/2022 17:07:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.55 on epoch=19
05/18/2022 17:07:49 - INFO - __main__ - Step 90 Global step 90 Train loss 1.78 on epoch=22
05/18/2022 17:07:50 - INFO - __main__ - Step 100 Global step 100 Train loss 1.36 on epoch=24
05/18/2022 17:07:51 - INFO - __main__ - Global step 100 Train loss 1.71 Classification-F1 0.14560439560439564 on epoch=24
05/18/2022 17:07:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14560439560439564 on epoch=24, global_step=100
05/18/2022 17:07:52 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=27
05/18/2022 17:07:54 - INFO - __main__ - Step 120 Global step 120 Train loss 1.43 on epoch=29
05/18/2022 17:07:55 - INFO - __main__ - Step 130 Global step 130 Train loss 1.39 on epoch=32
05/18/2022 17:07:57 - INFO - __main__ - Step 140 Global step 140 Train loss 1.08 on epoch=34
05/18/2022 17:07:58 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
05/18/2022 17:07:59 - INFO - __main__ - Global step 150 Train loss 1.27 Classification-F1 0.038461538461538464 on epoch=37
05/18/2022 17:08:00 - INFO - __main__ - Step 160 Global step 160 Train loss 1.19 on epoch=39
05/18/2022 17:08:02 - INFO - __main__ - Step 170 Global step 170 Train loss 1.24 on epoch=42
05/18/2022 17:08:03 - INFO - __main__ - Step 180 Global step 180 Train loss 1.16 on epoch=44
05/18/2022 17:08:05 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=47
05/18/2022 17:08:06 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/18/2022 17:08:07 - INFO - __main__ - Global step 200 Train loss 1.16 Classification-F1 0.12945998071359693 on epoch=49
05/18/2022 17:08:09 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
05/18/2022 17:08:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
05/18/2022 17:08:12 - INFO - __main__ - Step 230 Global step 230 Train loss 1.10 on epoch=57
05/18/2022 17:08:14 - INFO - __main__ - Step 240 Global step 240 Train loss 1.21 on epoch=59
05/18/2022 17:08:15 - INFO - __main__ - Step 250 Global step 250 Train loss 1.04 on epoch=62
05/18/2022 17:08:16 - INFO - __main__ - Global step 250 Train loss 1.11 Classification-F1 0.22731356693620847 on epoch=62
05/18/2022 17:08:16 - INFO - __main__ - Saving model with best Classification-F1: 0.14560439560439564 -> 0.22731356693620847 on epoch=62, global_step=250
05/18/2022 17:08:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
05/18/2022 17:08:19 - INFO - __main__ - Step 270 Global step 270 Train loss 1.05 on epoch=67
05/18/2022 17:08:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
05/18/2022 17:08:22 - INFO - __main__ - Step 290 Global step 290 Train loss 1.06 on epoch=72
05/18/2022 17:08:24 - INFO - __main__ - Step 300 Global step 300 Train loss 1.01 on epoch=74
05/18/2022 17:08:25 - INFO - __main__ - Global step 300 Train loss 1.02 Classification-F1 0.23541666666666666 on epoch=74
05/18/2022 17:08:25 - INFO - __main__ - Saving model with best Classification-F1: 0.22731356693620847 -> 0.23541666666666666 on epoch=74, global_step=300
05/18/2022 17:08:26 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
05/18/2022 17:08:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
05/18/2022 17:08:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
05/18/2022 17:08:30 - INFO - __main__ - Step 340 Global step 340 Train loss 1.07 on epoch=84
05/18/2022 17:08:32 - INFO - __main__ - Step 350 Global step 350 Train loss 1.04 on epoch=87
05/18/2022 17:08:33 - INFO - __main__ - Global step 350 Train loss 1.01 Classification-F1 0.2869298245614035 on epoch=87
05/18/2022 17:08:33 - INFO - __main__ - Saving model with best Classification-F1: 0.23541666666666666 -> 0.2869298245614035 on epoch=87, global_step=350
05/18/2022 17:08:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.93 on epoch=89
05/18/2022 17:08:36 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=92
05/18/2022 17:08:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
05/18/2022 17:08:38 - INFO - __main__ - Step 390 Global step 390 Train loss 1.02 on epoch=97
05/18/2022 17:08:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=99
05/18/2022 17:08:41 - INFO - __main__ - Global step 400 Train loss 0.95 Classification-F1 0.23494219419516535 on epoch=99
05/18/2022 17:08:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.97 on epoch=102
05/18/2022 17:08:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
05/18/2022 17:08:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=107
05/18/2022 17:08:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.98 on epoch=109
05/18/2022 17:08:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=112
05/18/2022 17:08:49 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.21666666666666667 on epoch=112
05/18/2022 17:08:51 - INFO - __main__ - Step 460 Global step 460 Train loss 1.00 on epoch=114
05/18/2022 17:08:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=117
05/18/2022 17:08:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.96 on epoch=119
05/18/2022 17:08:55 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=122
05/18/2022 17:08:57 - INFO - __main__ - Step 500 Global step 500 Train loss 1.04 on epoch=124
05/18/2022 17:08:58 - INFO - __main__ - Global step 500 Train loss 1.01 Classification-F1 0.15675057208237989 on epoch=124
05/18/2022 17:08:59 - INFO - __main__ - Step 510 Global step 510 Train loss 1.01 on epoch=127
05/18/2022 17:09:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.99 on epoch=129
05/18/2022 17:09:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=132
05/18/2022 17:09:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
05/18/2022 17:09:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
05/18/2022 17:09:06 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.26363636363636367 on epoch=137
05/18/2022 17:09:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.96 on epoch=139
05/18/2022 17:09:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
05/18/2022 17:09:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.82 on epoch=144
05/18/2022 17:09:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.89 on epoch=147
05/18/2022 17:09:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
05/18/2022 17:09:14 - INFO - __main__ - Global step 600 Train loss 0.89 Classification-F1 0.24678304787000435 on epoch=149
05/18/2022 17:09:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
05/18/2022 17:09:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.94 on epoch=154
05/18/2022 17:09:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=157
05/18/2022 17:09:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.89 on epoch=159
05/18/2022 17:09:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=162
05/18/2022 17:09:23 - INFO - __main__ - Global step 650 Train loss 0.92 Classification-F1 0.3792610837438424 on epoch=162
05/18/2022 17:09:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2869298245614035 -> 0.3792610837438424 on epoch=162, global_step=650
05/18/2022 17:09:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
05/18/2022 17:09:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.90 on epoch=167
05/18/2022 17:09:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/18/2022 17:09:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=172
05/18/2022 17:09:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.82 on epoch=174
05/18/2022 17:09:31 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3309502122254237 on epoch=174
05/18/2022 17:09:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.76 on epoch=177
05/18/2022 17:09:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=179
05/18/2022 17:09:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.89 on epoch=182
05/18/2022 17:09:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.84 on epoch=184
05/18/2022 17:09:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
05/18/2022 17:09:39 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.3787447603154737 on epoch=187
05/18/2022 17:09:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.76 on epoch=189
05/18/2022 17:09:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.79 on epoch=192
05/18/2022 17:09:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=194
05/18/2022 17:09:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=197
05/18/2022 17:09:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.67 on epoch=199
05/18/2022 17:09:48 - INFO - __main__ - Global step 800 Train loss 0.77 Classification-F1 0.27893374741200827 on epoch=199
05/18/2022 17:09:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
05/18/2022 17:09:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.92 on epoch=204
05/18/2022 17:09:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=207
05/18/2022 17:09:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
05/18/2022 17:09:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=212
05/18/2022 17:09:56 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.36192185007974476 on epoch=212
05/18/2022 17:09:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.70 on epoch=214
05/18/2022 17:09:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=217
05/18/2022 17:10:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
05/18/2022 17:10:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
05/18/2022 17:10:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=224
05/18/2022 17:10:04 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.46370851370851374 on epoch=224
05/18/2022 17:10:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3792610837438424 -> 0.46370851370851374 on epoch=224, global_step=900
05/18/2022 17:10:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.66 on epoch=227
05/18/2022 17:10:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
05/18/2022 17:10:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=232
05/18/2022 17:10:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=234
05/18/2022 17:10:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.74 on epoch=237
05/18/2022 17:10:12 - INFO - __main__ - Global step 950 Train loss 0.71 Classification-F1 0.4506237006237006 on epoch=237
05/18/2022 17:10:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=239
05/18/2022 17:10:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.74 on epoch=242
05/18/2022 17:10:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.68 on epoch=244
05/18/2022 17:10:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
05/18/2022 17:10:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.57 on epoch=249
05/18/2022 17:10:20 - INFO - __main__ - Global step 1000 Train loss 0.69 Classification-F1 0.5276679841897234 on epoch=249
05/18/2022 17:10:20 - INFO - __main__ - Saving model with best Classification-F1: 0.46370851370851374 -> 0.5276679841897234 on epoch=249, global_step=1000
05/18/2022 17:10:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=252
05/18/2022 17:10:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.57 on epoch=254
05/18/2022 17:10:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=257
05/18/2022 17:10:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=259
05/18/2022 17:10:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.67 on epoch=262
05/18/2022 17:10:28 - INFO - __main__ - Global step 1050 Train loss 0.59 Classification-F1 0.5618390405361229 on epoch=262
05/18/2022 17:10:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5276679841897234 -> 0.5618390405361229 on epoch=262, global_step=1050
05/18/2022 17:10:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=264
05/18/2022 17:10:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=267
05/18/2022 17:10:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=269
05/18/2022 17:10:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=272
05/18/2022 17:10:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.57 on epoch=274
05/18/2022 17:10:36 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.6030923315244395 on epoch=274
05/18/2022 17:10:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5618390405361229 -> 0.6030923315244395 on epoch=274, global_step=1100
05/18/2022 17:10:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=277
05/18/2022 17:10:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=279
05/18/2022 17:10:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.67 on epoch=282
05/18/2022 17:10:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=284
05/18/2022 17:10:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.62 on epoch=287
05/18/2022 17:10:45 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.5199916937249868 on epoch=287
05/18/2022 17:10:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=289
05/18/2022 17:10:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.73 on epoch=292
05/18/2022 17:10:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=294
05/18/2022 17:10:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=297
05/18/2022 17:10:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=299
05/18/2022 17:10:53 - INFO - __main__ - Global step 1200 Train loss 0.57 Classification-F1 0.5130638052305402 on epoch=299
05/18/2022 17:10:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=302
05/18/2022 17:10:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=304
05/18/2022 17:10:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.61 on epoch=307
05/18/2022 17:10:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=309
05/18/2022 17:11:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.63 on epoch=312
05/18/2022 17:11:01 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.5557865251363704 on epoch=312
05/18/2022 17:11:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=314
05/18/2022 17:11:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=317
05/18/2022 17:11:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=319
05/18/2022 17:11:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=322
05/18/2022 17:11:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=324
05/18/2022 17:11:10 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.5515512265512266 on epoch=324
05/18/2022 17:11:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=327
05/18/2022 17:11:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=329
05/18/2022 17:11:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=332
05/18/2022 17:11:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=334
05/18/2022 17:11:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=337
05/18/2022 17:11:18 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.6035714285714286 on epoch=337
05/18/2022 17:11:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6030923315244395 -> 0.6035714285714286 on epoch=337, global_step=1350
05/18/2022 17:11:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=339
05/18/2022 17:11:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=342
05/18/2022 17:11:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=344
05/18/2022 17:11:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=347
05/18/2022 17:11:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=349
05/18/2022 17:11:26 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.5599028303117256 on epoch=349
05/18/2022 17:11:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=352
05/18/2022 17:11:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=354
05/18/2022 17:11:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
05/18/2022 17:11:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=359
05/18/2022 17:11:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=362
05/18/2022 17:11:35 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.6783771929824561 on epoch=362
05/18/2022 17:11:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6035714285714286 -> 0.6783771929824561 on epoch=362, global_step=1450
05/18/2022 17:11:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=364
05/18/2022 17:11:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=367
05/18/2022 17:11:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=369
05/18/2022 17:11:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=372
05/18/2022 17:11:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=374
05/18/2022 17:11:43 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.6264666516279419 on epoch=374
05/18/2022 17:11:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=377
05/18/2022 17:11:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=379
05/18/2022 17:11:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=382
05/18/2022 17:11:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=384
05/18/2022 17:11:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=387
05/18/2022 17:11:52 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.5499923092698934 on epoch=387
05/18/2022 17:11:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=389
05/18/2022 17:11:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=392
05/18/2022 17:11:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=394
05/18/2022 17:11:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=397
05/18/2022 17:11:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=399
05/18/2022 17:12:00 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.6374418773612321 on epoch=399
05/18/2022 17:12:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
05/18/2022 17:12:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=404
05/18/2022 17:12:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=407
05/18/2022 17:12:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=409
05/18/2022 17:12:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=412
05/18/2022 17:12:09 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.6865543394777266 on epoch=412
05/18/2022 17:12:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6783771929824561 -> 0.6865543394777266 on epoch=412, global_step=1650
05/18/2022 17:12:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=414
05/18/2022 17:12:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=417
05/18/2022 17:12:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=419
05/18/2022 17:12:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=422
05/18/2022 17:12:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=424
05/18/2022 17:12:17 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.6807706093189965 on epoch=424
05/18/2022 17:12:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=427
05/18/2022 17:12:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=429
05/18/2022 17:12:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=432
05/18/2022 17:12:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=434
05/18/2022 17:12:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=437
05/18/2022 17:12:26 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.69869379748412 on epoch=437
05/18/2022 17:12:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6865543394777266 -> 0.69869379748412 on epoch=437, global_step=1750
05/18/2022 17:12:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=439
05/18/2022 17:12:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=442
05/18/2022 17:12:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=444
05/18/2022 17:12:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=447
05/18/2022 17:12:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=449
05/18/2022 17:12:34 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7323967086834734 on epoch=449
05/18/2022 17:12:34 - INFO - __main__ - Saving model with best Classification-F1: 0.69869379748412 -> 0.7323967086834734 on epoch=449, global_step=1800
05/18/2022 17:12:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=452
05/18/2022 17:12:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=454
05/18/2022 17:12:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=457
05/18/2022 17:12:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=459
05/18/2022 17:12:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=462
05/18/2022 17:12:43 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.699378667927055 on epoch=462
05/18/2022 17:12:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
05/18/2022 17:12:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=467
05/18/2022 17:12:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=469
05/18/2022 17:12:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=472
05/18/2022 17:12:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=474
05/18/2022 17:12:51 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.6544069729553601 on epoch=474
05/18/2022 17:12:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=477
05/18/2022 17:12:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=479
05/18/2022 17:12:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=482
05/18/2022 17:12:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=484
05/18/2022 17:12:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=487
05/18/2022 17:13:00 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7362745098039216 on epoch=487
05/18/2022 17:13:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7323967086834734 -> 0.7362745098039216 on epoch=487, global_step=1950
05/18/2022 17:13:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=489
05/18/2022 17:13:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=492
05/18/2022 17:13:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=494
05/18/2022 17:13:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=497
05/18/2022 17:13:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
05/18/2022 17:13:08 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.7323967086834734 on epoch=499
05/18/2022 17:13:10 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=502
05/18/2022 17:13:11 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=504
05/18/2022 17:13:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=507
05/18/2022 17:13:14 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
05/18/2022 17:13:16 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=512
05/18/2022 17:13:17 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.701461038961039 on epoch=512
05/18/2022 17:13:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=514
05/18/2022 17:13:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
05/18/2022 17:13:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=519
05/18/2022 17:13:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/18/2022 17:13:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=524
05/18/2022 17:13:26 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.6421919243342856 on epoch=524
05/18/2022 17:13:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=527
05/18/2022 17:13:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
05/18/2022 17:13:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=532
05/18/2022 17:13:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/18/2022 17:13:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
05/18/2022 17:13:34 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.7521124519763358 on epoch=537
05/18/2022 17:13:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7362745098039216 -> 0.7521124519763358 on epoch=537, global_step=2150
05/18/2022 17:13:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
05/18/2022 17:13:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
05/18/2022 17:13:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=544
05/18/2022 17:13:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/18/2022 17:13:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=549
05/18/2022 17:13:42 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.7338848039215686 on epoch=549
05/18/2022 17:13:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=552
05/18/2022 17:13:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/18/2022 17:13:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/18/2022 17:13:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=559
05/18/2022 17:13:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=562
05/18/2022 17:13:51 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.7001002506265666 on epoch=562
05/18/2022 17:13:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=564
05/18/2022 17:13:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=567
05/18/2022 17:13:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=569
05/18/2022 17:13:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
05/18/2022 17:13:58 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=574
05/18/2022 17:14:00 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.7347069597069598 on epoch=574
05/18/2022 17:14:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=577
05/18/2022 17:14:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=579
05/18/2022 17:14:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=582
05/18/2022 17:14:06 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/18/2022 17:14:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
05/18/2022 17:14:09 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.7166666666666667 on epoch=587
05/18/2022 17:14:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
05/18/2022 17:14:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=592
05/18/2022 17:14:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=594
05/18/2022 17:14:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
05/18/2022 17:14:17 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
05/18/2022 17:14:18 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.6688218390804598 on epoch=599
05/18/2022 17:14:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=602
05/18/2022 17:14:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
05/18/2022 17:14:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
05/18/2022 17:14:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/18/2022 17:14:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=612
05/18/2022 17:14:26 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7632962862564381 on epoch=612
05/18/2022 17:14:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7521124519763358 -> 0.7632962862564381 on epoch=612, global_step=2450
05/18/2022 17:14:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=614
05/18/2022 17:14:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
05/18/2022 17:14:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/18/2022 17:14:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
05/18/2022 17:14:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=624
05/18/2022 17:14:35 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.6891403718989926 on epoch=624
05/18/2022 17:14:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/18/2022 17:14:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=629
05/18/2022 17:14:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
05/18/2022 17:14:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/18/2022 17:14:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
05/18/2022 17:14:44 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.7520289097875305 on epoch=637
05/18/2022 17:14:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
05/18/2022 17:14:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=642
05/18/2022 17:14:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
05/18/2022 17:14:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=647
05/18/2022 17:14:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/18/2022 17:14:52 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.733629477678421 on epoch=649
05/18/2022 17:14:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/18/2022 17:14:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
05/18/2022 17:14:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=657
05/18/2022 17:14:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/18/2022 17:15:00 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
05/18/2022 17:15:01 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.6739794967381174 on epoch=662
05/18/2022 17:15:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=664
05/18/2022 17:15:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
05/18/2022 17:15:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
05/18/2022 17:15:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=672
05/18/2022 17:15:08 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/18/2022 17:15:09 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7500960061443933 on epoch=674
05/18/2022 17:15:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
05/18/2022 17:15:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=679
05/18/2022 17:15:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/18/2022 17:15:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
05/18/2022 17:15:16 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=687
05/18/2022 17:15:18 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7027346592287169 on epoch=687
05/18/2022 17:15:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
05/18/2022 17:15:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
05/18/2022 17:15:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=694
05/18/2022 17:15:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
05/18/2022 17:15:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
05/18/2022 17:15:27 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7328951514435387 on epoch=699
05/18/2022 17:15:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=702
05/18/2022 17:15:30 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
05/18/2022 17:15:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
05/18/2022 17:15:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
05/18/2022 17:15:34 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=712
05/18/2022 17:15:35 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7473169191919192 on epoch=712
05/18/2022 17:15:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/18/2022 17:15:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
05/18/2022 17:15:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 17:15:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=722
05/18/2022 17:15:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
05/18/2022 17:15:44 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7178811354457906 on epoch=724
05/18/2022 17:15:46 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/18/2022 17:15:47 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=729
05/18/2022 17:15:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=732
05/18/2022 17:15:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=734
05/18/2022 17:15:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/18/2022 17:15:53 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7317879253363125 on epoch=737
05/18/2022 17:15:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/18/2022 17:15:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
05/18/2022 17:15:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
05/18/2022 17:15:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
05/18/2022 17:16:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/18/2022 17:16:01 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7636849606613806 on epoch=749
05/18/2022 17:16:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7632962862564381 -> 0.7636849606613806 on epoch=749, global_step=3000
05/18/2022 17:16:01 - INFO - __main__ - save last model!
05/18/2022 17:16:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:16:01 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:16:01 - INFO - __main__ - Printing 3 examples
05/18/2022 17:16:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:16:01 - INFO - __main__ - ['others']
05/18/2022 17:16:01 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:16:01 - INFO - __main__ - ['others']
05/18/2022 17:16:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:16:01 - INFO - __main__ - ['others']
05/18/2022 17:16:01 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:16:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:16:02 - INFO - __main__ - Printing 3 examples
05/18/2022 17:16:02 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:16:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:16:02 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:16:02 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:16:02 - INFO - __main__ - Printing 3 examples
05/18/2022 17:16:02 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:16:02 - INFO - __main__ - ['happy']
05/18/2022 17:16:02 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:16:02 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:16:02 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:16:04 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:16:08 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:16:08 - INFO - __main__ - task name: emo
05/18/2022 17:16:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:16:09 - INFO - __main__ - Starting training!
05/18/2022 17:16:10 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:17:25 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.2_8_predictions.txt
05/18/2022 17:17:25 - INFO - __main__ - Classification-F1 on test data: 0.4638
05/18/2022 17:17:25 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7636849606613806, test_performance=0.4638090184615515
05/18/2022 17:17:25 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
05/18/2022 17:17:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:17:26 - INFO - __main__ - Printing 3 examples
05/18/2022 17:17:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:17:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:17:26 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:17:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:17:26 - INFO - __main__ - Printing 3 examples
05/18/2022 17:17:26 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:17:26 - INFO - __main__ - ['happy']
05/18/2022 17:17:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:17:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:17:26 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:17:32 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:17:32 - INFO - __main__ - task name: emo
05/18/2022 17:17:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:17:32 - INFO - __main__ - Starting training!
05/18/2022 17:17:34 - INFO - __main__ - Step 10 Global step 10 Train loss 5.88 on epoch=2
05/18/2022 17:17:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=4
05/18/2022 17:17:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.93 on epoch=7
05/18/2022 17:17:38 - INFO - __main__ - Step 40 Global step 40 Train loss 1.47 on epoch=9
05/18/2022 17:17:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
05/18/2022 17:17:40 - INFO - __main__ - Global step 50 Train loss 2.66 Classification-F1 0.13067758749069247 on epoch=12
05/18/2022 17:17:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
05/18/2022 17:17:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=14
05/18/2022 17:17:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=17
05/18/2022 17:17:44 - INFO - __main__ - Step 80 Global step 80 Train loss 1.18 on epoch=19
05/18/2022 17:17:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.33 on epoch=22
05/18/2022 17:17:47 - INFO - __main__ - Step 100 Global step 100 Train loss 1.68 on epoch=24
05/18/2022 17:17:48 - INFO - __main__ - Global step 100 Train loss 1.33 Classification-F1 0.16138763197586728 on epoch=24
05/18/2022 17:17:48 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.16138763197586728 on epoch=24, global_step=100
05/18/2022 17:17:49 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
05/18/2022 17:17:51 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
05/18/2022 17:17:52 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
05/18/2022 17:17:54 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
05/18/2022 17:17:56 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
05/18/2022 17:17:56 - INFO - __main__ - Global step 150 Train loss 1.07 Classification-F1 0.21377995642701525 on epoch=37
05/18/2022 17:17:56 - INFO - __main__ - Saving model with best Classification-F1: 0.16138763197586728 -> 0.21377995642701525 on epoch=37, global_step=150
05/18/2022 17:17:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
05/18/2022 17:17:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
05/18/2022 17:18:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=44
05/18/2022 17:18:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=47
05/18/2022 17:18:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
05/18/2022 17:18:04 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.1 on epoch=49
05/18/2022 17:18:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=52
05/18/2022 17:18:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
05/18/2022 17:18:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
05/18/2022 17:18:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
05/18/2022 17:18:12 - INFO - __main__ - Step 250 Global step 250 Train loss 1.00 on epoch=62
05/18/2022 17:18:13 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.21584440227703988 on epoch=62
05/18/2022 17:18:13 - INFO - __main__ - Saving model with best Classification-F1: 0.21377995642701525 -> 0.21584440227703988 on epoch=62, global_step=250
05/18/2022 17:18:14 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
05/18/2022 17:18:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.97 on epoch=67
05/18/2022 17:18:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.02 on epoch=69
05/18/2022 17:18:19 - INFO - __main__ - Step 290 Global step 290 Train loss 1.00 on epoch=72
05/18/2022 17:18:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
05/18/2022 17:18:22 - INFO - __main__ - Global step 300 Train loss 0.99 Classification-F1 0.34285516093229745 on epoch=74
05/18/2022 17:18:22 - INFO - __main__ - Saving model with best Classification-F1: 0.21584440227703988 -> 0.34285516093229745 on epoch=74, global_step=300
05/18/2022 17:18:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
05/18/2022 17:18:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
05/18/2022 17:18:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
05/18/2022 17:18:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=84
05/18/2022 17:18:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
05/18/2022 17:18:30 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.18284347231715653 on epoch=87
05/18/2022 17:18:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.93 on epoch=89
05/18/2022 17:18:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
05/18/2022 17:18:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/18/2022 17:18:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
05/18/2022 17:18:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.99 on epoch=99
05/18/2022 17:18:39 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.13067758749069247 on epoch=99
05/18/2022 17:18:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.98 on epoch=102
05/18/2022 17:18:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=104
05/18/2022 17:18:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
05/18/2022 17:18:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
05/18/2022 17:18:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.91 on epoch=112
05/18/2022 17:18:47 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.23866366366366365 on epoch=112
05/18/2022 17:18:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=114
05/18/2022 17:18:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.88 on epoch=117
05/18/2022 17:18:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
05/18/2022 17:18:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=122
05/18/2022 17:18:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
05/18/2022 17:18:55 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.1888634241575418 on epoch=124
05/18/2022 17:18:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
05/18/2022 17:18:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.89 on epoch=129
05/18/2022 17:19:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=132
05/18/2022 17:19:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=134
05/18/2022 17:19:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
05/18/2022 17:19:04 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.4373265066929862 on epoch=137
05/18/2022 17:19:04 - INFO - __main__ - Saving model with best Classification-F1: 0.34285516093229745 -> 0.4373265066929862 on epoch=137, global_step=550
05/18/2022 17:19:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=139
05/18/2022 17:19:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.82 on epoch=142
05/18/2022 17:19:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
05/18/2022 17:19:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=147
05/18/2022 17:19:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=149
05/18/2022 17:19:12 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.5844961240310077 on epoch=149
05/18/2022 17:19:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4373265066929862 -> 0.5844961240310077 on epoch=149, global_step=600
05/18/2022 17:19:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
05/18/2022 17:19:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
05/18/2022 17:19:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.66 on epoch=157
05/18/2022 17:19:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.71 on epoch=159
05/18/2022 17:19:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
05/18/2022 17:19:21 - INFO - __main__ - Global step 650 Train loss 0.73 Classification-F1 0.39413937511763597 on epoch=162
05/18/2022 17:19:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.78 on epoch=164
05/18/2022 17:19:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=167
05/18/2022 17:19:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
05/18/2022 17:19:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=172
05/18/2022 17:19:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=174
05/18/2022 17:19:28 - INFO - __main__ - Global step 700 Train loss 0.73 Classification-F1 0.4137254901960784 on epoch=174
05/18/2022 17:19:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=177
05/18/2022 17:19:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.69 on epoch=179
05/18/2022 17:19:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=182
05/18/2022 17:19:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.67 on epoch=184
05/18/2022 17:19:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.65 on epoch=187
05/18/2022 17:19:36 - INFO - __main__ - Global step 750 Train loss 0.68 Classification-F1 0.4680930930930931 on epoch=187
05/18/2022 17:19:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=189
05/18/2022 17:19:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=192
05/18/2022 17:19:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=194
05/18/2022 17:19:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=197
05/18/2022 17:19:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=199
05/18/2022 17:19:44 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.5797397047397048 on epoch=199
05/18/2022 17:19:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=202
05/18/2022 17:19:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=204
05/18/2022 17:19:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=207
05/18/2022 17:19:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=209
05/18/2022 17:19:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=212
05/18/2022 17:19:52 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.5085125448028674 on epoch=212
05/18/2022 17:19:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=214
05/18/2022 17:19:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=217
05/18/2022 17:19:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=219
05/18/2022 17:19:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
05/18/2022 17:20:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=224
05/18/2022 17:20:00 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.6032509157509158 on epoch=224
05/18/2022 17:20:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5844961240310077 -> 0.6032509157509158 on epoch=224, global_step=900
05/18/2022 17:20:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=227
05/18/2022 17:20:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=229
05/18/2022 17:20:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=232
05/18/2022 17:20:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=234
05/18/2022 17:20:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=237
05/18/2022 17:20:09 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.6228413163897035 on epoch=237
05/18/2022 17:20:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6032509157509158 -> 0.6228413163897035 on epoch=237, global_step=950
05/18/2022 17:20:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=239
05/18/2022 17:20:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=242
05/18/2022 17:20:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=244
05/18/2022 17:20:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=247
05/18/2022 17:20:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=249
05/18/2022 17:20:18 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.6896296935196365 on epoch=249
05/18/2022 17:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6228413163897035 -> 0.6896296935196365 on epoch=249, global_step=1000
05/18/2022 17:20:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
05/18/2022 17:20:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=254
05/18/2022 17:20:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=257
05/18/2022 17:20:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.56 on epoch=259
05/18/2022 17:20:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.23 on epoch=262
05/18/2022 17:20:27 - INFO - __main__ - Global step 1050 Train loss 0.55 Classification-F1 0.221460282226098 on epoch=262
05/18/2022 17:20:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.61 on epoch=264
05/18/2022 17:20:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=267
05/18/2022 17:20:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=269
05/18/2022 17:20:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=272
05/18/2022 17:20:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=274
05/18/2022 17:20:36 - INFO - __main__ - Global step 1100 Train loss 0.72 Classification-F1 0.607843137254902 on epoch=274
05/18/2022 17:20:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=277
05/18/2022 17:20:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=279
05/18/2022 17:20:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=282
05/18/2022 17:20:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=284
05/18/2022 17:20:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=287
05/18/2022 17:20:44 - INFO - __main__ - Global step 1150 Train loss 0.55 Classification-F1 0.6142746522454386 on epoch=287
05/18/2022 17:20:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=289
05/18/2022 17:20:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=292
05/18/2022 17:20:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=294
05/18/2022 17:20:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=297
05/18/2022 17:20:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=299
05/18/2022 17:20:52 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.67440424295263 on epoch=299
05/18/2022 17:20:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=302
05/18/2022 17:20:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=304
05/18/2022 17:20:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=307
05/18/2022 17:20:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=309
05/18/2022 17:20:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=312
05/18/2022 17:21:00 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.6802041605172535 on epoch=312
05/18/2022 17:21:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=314
05/18/2022 17:21:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=317
05/18/2022 17:21:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=319
05/18/2022 17:21:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=322
05/18/2022 17:21:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=324
05/18/2022 17:21:08 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.645170985060691 on epoch=324
05/18/2022 17:21:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=327
05/18/2022 17:21:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=329
05/18/2022 17:21:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=332
05/18/2022 17:21:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=334
05/18/2022 17:21:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=337
05/18/2022 17:21:16 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.6603708133971291 on epoch=337
05/18/2022 17:21:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
05/18/2022 17:21:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=342
05/18/2022 17:21:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
05/18/2022 17:21:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=347
05/18/2022 17:21:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=349
05/18/2022 17:21:24 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.6406508478081059 on epoch=349
05/18/2022 17:21:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
05/18/2022 17:21:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=354
05/18/2022 17:21:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=357
05/18/2022 17:21:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=359
05/18/2022 17:21:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=362
05/18/2022 17:21:32 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.6510501864817118 on epoch=362
05/18/2022 17:21:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=364
05/18/2022 17:21:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=367
05/18/2022 17:21:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
05/18/2022 17:21:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=372
05/18/2022 17:21:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=374
05/18/2022 17:21:40 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.6459401709401709 on epoch=374
05/18/2022 17:21:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
05/18/2022 17:21:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
05/18/2022 17:21:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=382
05/18/2022 17:21:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=384
05/18/2022 17:21:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
05/18/2022 17:21:49 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.612196164269335 on epoch=387
05/18/2022 17:21:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=389
05/18/2022 17:21:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=392
05/18/2022 17:21:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=394
05/18/2022 17:21:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=397
05/18/2022 17:21:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=399
05/18/2022 17:21:57 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.6196476964769648 on epoch=399
05/18/2022 17:21:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=402
05/18/2022 17:22:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=404
05/18/2022 17:22:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
05/18/2022 17:22:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
05/18/2022 17:22:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
05/18/2022 17:22:06 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.6568181818181819 on epoch=412
05/18/2022 17:22:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
05/18/2022 17:22:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/18/2022 17:22:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
05/18/2022 17:22:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
05/18/2022 17:22:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=424
05/18/2022 17:22:15 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.6423915130568356 on epoch=424
05/18/2022 17:22:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
05/18/2022 17:22:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=429
05/18/2022 17:22:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=432
05/18/2022 17:22:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
05/18/2022 17:22:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=437
05/18/2022 17:22:23 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.5861190452411864 on epoch=437
05/18/2022 17:22:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
05/18/2022 17:22:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
05/18/2022 17:22:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=444
05/18/2022 17:22:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=447
05/18/2022 17:22:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
05/18/2022 17:22:32 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.6572622779519331 on epoch=449
05/18/2022 17:22:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=452
05/18/2022 17:22:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=454
05/18/2022 17:22:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=457
05/18/2022 17:22:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=459
05/18/2022 17:22:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=462
05/18/2022 17:22:40 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.6172875615763547 on epoch=462
05/18/2022 17:22:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
05/18/2022 17:22:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/18/2022 17:22:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
05/18/2022 17:22:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
05/18/2022 17:22:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
05/18/2022 17:22:48 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.4875210138368033 on epoch=474
05/18/2022 17:22:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
05/18/2022 17:22:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=479
05/18/2022 17:22:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/18/2022 17:22:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
05/18/2022 17:22:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
05/18/2022 17:22:56 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.5786477036477036 on epoch=487
05/18/2022 17:22:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/18/2022 17:22:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
05/18/2022 17:23:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
05/18/2022 17:23:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
05/18/2022 17:23:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=499
05/18/2022 17:23:05 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.6410539215686275 on epoch=499
05/18/2022 17:23:06 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=502
05/18/2022 17:23:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
05/18/2022 17:23:09 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
05/18/2022 17:23:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
05/18/2022 17:23:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/18/2022 17:23:14 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6252324612725058 on epoch=512
05/18/2022 17:23:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
05/18/2022 17:23:17 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
05/18/2022 17:23:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/18/2022 17:23:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
05/18/2022 17:23:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/18/2022 17:23:22 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6364743837914569 on epoch=524
05/18/2022 17:23:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=527
05/18/2022 17:23:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
05/18/2022 17:23:26 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=532
05/18/2022 17:23:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
05/18/2022 17:23:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
05/18/2022 17:23:31 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.5630779130779131 on epoch=537
05/18/2022 17:23:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=539
05/18/2022 17:23:34 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/18/2022 17:23:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=544
05/18/2022 17:23:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
05/18/2022 17:23:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/18/2022 17:23:39 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.5444970016542597 on epoch=549
05/18/2022 17:23:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/18/2022 17:23:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/18/2022 17:23:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
05/18/2022 17:23:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
05/18/2022 17:23:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
05/18/2022 17:23:48 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.64609059490525 on epoch=562
05/18/2022 17:23:49 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
05/18/2022 17:23:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=567
05/18/2022 17:23:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
05/18/2022 17:23:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/18/2022 17:23:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/18/2022 17:23:56 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.5898119122257053 on epoch=574
05/18/2022 17:23:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/18/2022 17:23:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
05/18/2022 17:24:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=582
05/18/2022 17:24:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/18/2022 17:24:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=587
05/18/2022 17:24:05 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.6216201423097976 on epoch=587
05/18/2022 17:24:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/18/2022 17:24:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/18/2022 17:24:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/18/2022 17:24:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/18/2022 17:24:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
05/18/2022 17:24:13 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6882927951239146 on epoch=599
05/18/2022 17:24:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/18/2022 17:24:16 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=604
05/18/2022 17:24:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=607
05/18/2022 17:24:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
05/18/2022 17:24:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=612
05/18/2022 17:24:21 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.6523809523809524 on epoch=612
05/18/2022 17:24:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
05/18/2022 17:24:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=617
05/18/2022 17:24:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=619
05/18/2022 17:24:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
05/18/2022 17:24:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=624
05/18/2022 17:24:29 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6707987229677997 on epoch=624
05/18/2022 17:24:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
05/18/2022 17:24:32 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
05/18/2022 17:24:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/18/2022 17:24:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=634
05/18/2022 17:24:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=637
05/18/2022 17:24:37 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7080517202221056 on epoch=637
05/18/2022 17:24:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6896296935196365 -> 0.7080517202221056 on epoch=637, global_step=2550
05/18/2022 17:24:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
05/18/2022 17:24:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 17:24:42 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/18/2022 17:24:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
05/18/2022 17:24:45 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=649
05/18/2022 17:24:46 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.6416359307612747 on epoch=649
05/18/2022 17:24:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/18/2022 17:24:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/18/2022 17:24:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
05/18/2022 17:24:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/18/2022 17:24:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/18/2022 17:24:54 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.4630124777183601 on epoch=662
05/18/2022 17:24:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
05/18/2022 17:24:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/18/2022 17:24:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/18/2022 17:25:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=672
05/18/2022 17:25:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/18/2022 17:25:02 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6892857142857143 on epoch=674
05/18/2022 17:25:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/18/2022 17:25:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=679
05/18/2022 17:25:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
05/18/2022 17:25:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
05/18/2022 17:25:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/18/2022 17:25:11 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6595238095238094 on epoch=687
05/18/2022 17:25:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=689
05/18/2022 17:25:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=692
05/18/2022 17:25:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
05/18/2022 17:25:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/18/2022 17:25:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=699
05/18/2022 17:25:20 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.6901109257199357 on epoch=699
05/18/2022 17:25:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/18/2022 17:25:23 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
05/18/2022 17:25:25 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/18/2022 17:25:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
05/18/2022 17:25:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/18/2022 17:25:29 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6156331498116488 on epoch=712
05/18/2022 17:25:30 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=714
05/18/2022 17:25:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/18/2022 17:25:33 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
05/18/2022 17:25:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
05/18/2022 17:25:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/18/2022 17:25:37 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.640180265654649 on epoch=724
05/18/2022 17:25:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 17:25:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
05/18/2022 17:25:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/18/2022 17:25:43 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/18/2022 17:25:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/18/2022 17:25:45 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7237067983438219 on epoch=737
05/18/2022 17:25:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7080517202221056 -> 0.7237067983438219 on epoch=737, global_step=2950
05/18/2022 17:25:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/18/2022 17:25:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/18/2022 17:25:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/18/2022 17:25:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/18/2022 17:25:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
05/18/2022 17:25:53 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6538961038961039 on epoch=749
05/18/2022 17:25:53 - INFO - __main__ - save last model!
05/18/2022 17:25:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:25:53 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:25:53 - INFO - __main__ - Printing 3 examples
05/18/2022 17:25:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:25:53 - INFO - __main__ - ['others']
05/18/2022 17:25:53 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:25:53 - INFO - __main__ - ['others']
05/18/2022 17:25:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:25:53 - INFO - __main__ - ['others']
05/18/2022 17:25:53 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:25:53 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:25:53 - INFO - __main__ - Printing 3 examples
05/18/2022 17:25:53 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:25:53 - INFO - __main__ - ['happy']
05/18/2022 17:25:53 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:25:53 - INFO - __main__ - ['happy']
05/18/2022 17:25:53 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:25:53 - INFO - __main__ - ['happy']
05/18/2022 17:25:53 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:25:53 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:25:54 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:25:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:25:54 - INFO - __main__ - Printing 3 examples
05/18/2022 17:25:54 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:25:54 - INFO - __main__ - ['happy']
05/18/2022 17:25:54 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:25:54 - INFO - __main__ - ['happy']
05/18/2022 17:25:54 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:25:54 - INFO - __main__ - ['happy']
05/18/2022 17:25:54 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:25:54 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:25:54 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:25:56 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:26:00 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:26:00 - INFO - __main__ - task name: emo
05/18/2022 17:26:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:26:00 - INFO - __main__ - Starting training!
05/18/2022 17:26:01 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:27:17 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.5_8_predictions.txt
05/18/2022 17:27:17 - INFO - __main__ - Classification-F1 on test data: 0.2699
05/18/2022 17:27:17 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7237067983438219, test_performance=0.26986174430673965
05/18/2022 17:27:17 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
05/18/2022 17:27:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:27:18 - INFO - __main__ - Printing 3 examples
05/18/2022 17:27:18 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:27:18 - INFO - __main__ - ['happy']
05/18/2022 17:27:18 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:27:18 - INFO - __main__ - ['happy']
05/18/2022 17:27:18 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:27:18 - INFO - __main__ - ['happy']
05/18/2022 17:27:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:27:19 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:27:19 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:27:19 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:27:19 - INFO - __main__ - Printing 3 examples
05/18/2022 17:27:19 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:27:19 - INFO - __main__ - ['happy']
05/18/2022 17:27:19 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:27:19 - INFO - __main__ - ['happy']
05/18/2022 17:27:19 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:27:19 - INFO - __main__ - ['happy']
05/18/2022 17:27:19 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:27:19 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:27:19 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:27:25 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:27:25 - INFO - __main__ - task name: emo
05/18/2022 17:27:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:27:26 - INFO - __main__ - Starting training!
05/18/2022 17:27:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.15 on epoch=2
05/18/2022 17:27:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.40 on epoch=4
05/18/2022 17:27:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.57 on epoch=7
05/18/2022 17:27:32 - INFO - __main__ - Step 40 Global step 40 Train loss 2.31 on epoch=9
05/18/2022 17:27:33 - INFO - __main__ - Step 50 Global step 50 Train loss 1.67 on epoch=12
05/18/2022 17:27:34 - INFO - __main__ - Global step 50 Train loss 3.22 Classification-F1 0.1 on epoch=12
05/18/2022 17:27:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 17:27:36 - INFO - __main__ - Step 60 Global step 60 Train loss 1.54 on epoch=14
05/18/2022 17:27:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.37 on epoch=17
05/18/2022 17:27:38 - INFO - __main__ - Step 80 Global step 80 Train loss 1.37 on epoch=19
05/18/2022 17:27:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=22
05/18/2022 17:27:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.12 on epoch=24
05/18/2022 17:27:42 - INFO - __main__ - Global step 100 Train loss 1.35 Classification-F1 0.1 on epoch=24
05/18/2022 17:27:44 - INFO - __main__ - Step 110 Global step 110 Train loss 1.15 on epoch=27
05/18/2022 17:27:45 - INFO - __main__ - Step 120 Global step 120 Train loss 1.10 on epoch=29
05/18/2022 17:27:47 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=32
05/18/2022 17:27:48 - INFO - __main__ - Step 140 Global step 140 Train loss 1.16 on epoch=34
05/18/2022 17:27:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.12 on epoch=37
05/18/2022 17:27:51 - INFO - __main__ - Global step 150 Train loss 1.11 Classification-F1 0.14583333333333334 on epoch=37
05/18/2022 17:27:51 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14583333333333334 on epoch=37, global_step=150
05/18/2022 17:27:52 - INFO - __main__ - Step 160 Global step 160 Train loss 1.08 on epoch=39
05/18/2022 17:27:54 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
05/18/2022 17:27:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
05/18/2022 17:27:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
05/18/2022 17:27:59 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
05/18/2022 17:28:00 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.1 on epoch=49
05/18/2022 17:28:01 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
05/18/2022 17:28:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
05/18/2022 17:28:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.99 on epoch=57
05/18/2022 17:28:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
05/18/2022 17:28:06 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
05/18/2022 17:28:07 - INFO - __main__ - Global step 250 Train loss 1.03 Classification-F1 0.2095890410958904 on epoch=62
05/18/2022 17:28:08 - INFO - __main__ - Saving model with best Classification-F1: 0.14583333333333334 -> 0.2095890410958904 on epoch=62, global_step=250
05/18/2022 17:28:09 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
05/18/2022 17:28:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
05/18/2022 17:28:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
05/18/2022 17:28:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/18/2022 17:28:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
05/18/2022 17:28:16 - INFO - __main__ - Global step 300 Train loss 0.94 Classification-F1 0.1081081081081081 on epoch=74
05/18/2022 17:28:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
05/18/2022 17:28:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
05/18/2022 17:28:20 - INFO - __main__ - Step 330 Global step 330 Train loss 1.00 on epoch=82
05/18/2022 17:28:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
05/18/2022 17:28:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
05/18/2022 17:28:25 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.20526315789473687 on epoch=87
05/18/2022 17:28:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
05/18/2022 17:28:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/18/2022 17:28:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.85 on epoch=94
05/18/2022 17:28:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 17:28:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
05/18/2022 17:28:33 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.4794313369630974 on epoch=99
05/18/2022 17:28:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2095890410958904 -> 0.4794313369630974 on epoch=99, global_step=400
05/18/2022 17:28:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.92 on epoch=102
05/18/2022 17:28:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=104
05/18/2022 17:28:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.88 on epoch=107
05/18/2022 17:28:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=109
05/18/2022 17:28:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=112
05/18/2022 17:28:42 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.40527950310559 on epoch=112
05/18/2022 17:28:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.82 on epoch=114
05/18/2022 17:28:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.92 on epoch=117
05/18/2022 17:28:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
05/18/2022 17:28:47 - INFO - __main__ - Step 490 Global step 490 Train loss 1.65 on epoch=122
05/18/2022 17:28:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
05/18/2022 17:28:50 - INFO - __main__ - Global step 500 Train loss 1.03 Classification-F1 0.28399015732775323 on epoch=124
05/18/2022 17:28:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.71 on epoch=127
05/18/2022 17:28:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.72 on epoch=129
05/18/2022 17:28:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
05/18/2022 17:28:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
05/18/2022 17:28:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=137
05/18/2022 17:28:58 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.5017009719839909 on epoch=137
05/18/2022 17:28:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4794313369630974 -> 0.5017009719839909 on epoch=137, global_step=550
05/18/2022 17:28:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
05/18/2022 17:29:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=142
05/18/2022 17:29:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.80 on epoch=144
05/18/2022 17:29:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
05/18/2022 17:29:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
05/18/2022 17:29:06 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.2627363184079602 on epoch=149
05/18/2022 17:29:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.73 on epoch=152
05/18/2022 17:29:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.66 on epoch=154
05/18/2022 17:29:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.72 on epoch=157
05/18/2022 17:29:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.64 on epoch=159
05/18/2022 17:29:13 - INFO - __main__ - Step 650 Global step 650 Train loss 0.73 on epoch=162
05/18/2022 17:29:14 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.4641267537493952 on epoch=162
05/18/2022 17:29:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.75 on epoch=164
05/18/2022 17:29:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
05/18/2022 17:29:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=169
05/18/2022 17:29:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
05/18/2022 17:29:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=174
05/18/2022 17:29:22 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.2639355742296918 on epoch=174
05/18/2022 17:29:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=177
05/18/2022 17:29:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.72 on epoch=179
05/18/2022 17:29:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=182
05/18/2022 17:29:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=184
05/18/2022 17:29:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.64 on epoch=187
05/18/2022 17:29:30 - INFO - __main__ - Global step 750 Train loss 0.67 Classification-F1 0.5330759551590025 on epoch=187
05/18/2022 17:29:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5017009719839909 -> 0.5330759551590025 on epoch=187, global_step=750
05/18/2022 17:29:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=189
05/18/2022 17:29:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.66 on epoch=192
05/18/2022 17:29:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.69 on epoch=194
05/18/2022 17:29:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.63 on epoch=197
05/18/2022 17:29:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.66 on epoch=199
05/18/2022 17:29:38 - INFO - __main__ - Global step 800 Train loss 0.65 Classification-F1 0.4752585377585377 on epoch=199
05/18/2022 17:29:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=202
05/18/2022 17:29:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=204
05/18/2022 17:29:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=207
05/18/2022 17:29:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.69 on epoch=209
05/18/2022 17:29:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=212
05/18/2022 17:29:46 - INFO - __main__ - Global step 850 Train loss 0.63 Classification-F1 0.5604427254772877 on epoch=212
05/18/2022 17:29:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5330759551590025 -> 0.5604427254772877 on epoch=212, global_step=850
05/18/2022 17:29:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=214
05/18/2022 17:29:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=217
05/18/2022 17:29:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=219
05/18/2022 17:29:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=222
05/18/2022 17:29:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=224
05/18/2022 17:29:55 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.42327264617630356 on epoch=224
05/18/2022 17:29:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=227
05/18/2022 17:29:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=229
05/18/2022 17:29:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.60 on epoch=232
05/18/2022 17:30:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
05/18/2022 17:30:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=237
05/18/2022 17:30:03 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.4974381496120627 on epoch=237
05/18/2022 17:30:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=239
05/18/2022 17:30:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=242
05/18/2022 17:30:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=244
05/18/2022 17:30:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=247
05/18/2022 17:30:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=249
05/18/2022 17:30:12 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.6365309448770238 on epoch=249
05/18/2022 17:30:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5604427254772877 -> 0.6365309448770238 on epoch=249, global_step=1000
05/18/2022 17:30:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
05/18/2022 17:30:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=254
05/18/2022 17:30:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=257
05/18/2022 17:30:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=259
05/18/2022 17:30:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=262
05/18/2022 17:30:20 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.41901470848839273 on epoch=262
05/18/2022 17:30:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=264
05/18/2022 17:30:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=267
05/18/2022 17:30:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=269
05/18/2022 17:30:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=272
05/18/2022 17:30:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=274
05/18/2022 17:30:28 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.46111111111111114 on epoch=274
05/18/2022 17:30:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=277
05/18/2022 17:30:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=279
05/18/2022 17:30:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=282
05/18/2022 17:30:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=284
05/18/2022 17:30:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=287
05/18/2022 17:30:36 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.4435854001071393 on epoch=287
05/18/2022 17:30:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=289
05/18/2022 17:30:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=292
05/18/2022 17:30:40 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=294
05/18/2022 17:30:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=297
05/18/2022 17:30:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=299
05/18/2022 17:30:44 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.616042291042291 on epoch=299
05/18/2022 17:30:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=302
05/18/2022 17:30:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=304
05/18/2022 17:30:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=307
05/18/2022 17:30:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=309
05/18/2022 17:30:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
05/18/2022 17:30:52 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.5413913428619311 on epoch=312
05/18/2022 17:30:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=314
05/18/2022 17:30:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=317
05/18/2022 17:30:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=319
05/18/2022 17:30:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=322
05/18/2022 17:30:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=324
05/18/2022 17:31:00 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.5764695893158412 on epoch=324
05/18/2022 17:31:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=327
05/18/2022 17:31:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
05/18/2022 17:31:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
05/18/2022 17:31:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
05/18/2022 17:31:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=337
05/18/2022 17:31:08 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.6021241830065359 on epoch=337
05/18/2022 17:31:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
05/18/2022 17:31:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=342
05/18/2022 17:31:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
05/18/2022 17:31:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=347
05/18/2022 17:31:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
05/18/2022 17:31:16 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.5562306148513045 on epoch=349
05/18/2022 17:31:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
05/18/2022 17:31:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=354
05/18/2022 17:31:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
05/18/2022 17:31:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
05/18/2022 17:31:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
05/18/2022 17:31:25 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.5646223146223146 on epoch=362
05/18/2022 17:31:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
05/18/2022 17:31:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
05/18/2022 17:31:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
05/18/2022 17:31:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
05/18/2022 17:31:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
05/18/2022 17:31:33 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.50899898464147 on epoch=374
05/18/2022 17:31:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=377
05/18/2022 17:31:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
05/18/2022 17:31:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/18/2022 17:31:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
05/18/2022 17:31:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=387
05/18/2022 17:31:40 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.601419878296146 on epoch=387
05/18/2022 17:31:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=389
05/18/2022 17:31:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
05/18/2022 17:31:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
05/18/2022 17:31:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
05/18/2022 17:31:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
05/18/2022 17:31:48 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.5275375375375375 on epoch=399
05/18/2022 17:31:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
05/18/2022 17:31:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
05/18/2022 17:31:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
05/18/2022 17:31:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
05/18/2022 17:31:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
05/18/2022 17:31:56 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.5587044534412955 on epoch=412
05/18/2022 17:31:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
05/18/2022 17:31:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
05/18/2022 17:32:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
05/18/2022 17:32:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
05/18/2022 17:32:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
05/18/2022 17:32:04 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5832246039142591 on epoch=424
05/18/2022 17:32:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
05/18/2022 17:32:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
05/18/2022 17:32:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=432
05/18/2022 17:32:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
05/18/2022 17:32:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
05/18/2022 17:32:12 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.6069325394228882 on epoch=437
05/18/2022 17:32:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
05/18/2022 17:32:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
05/18/2022 17:32:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
05/18/2022 17:32:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=447
05/18/2022 17:32:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
05/18/2022 17:32:20 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.5274020694752403 on epoch=449
05/18/2022 17:32:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
05/18/2022 17:32:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
05/18/2022 17:32:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
05/18/2022 17:32:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
05/18/2022 17:32:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
05/18/2022 17:32:29 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6933458378691645 on epoch=462
05/18/2022 17:32:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6365309448770238 -> 0.6933458378691645 on epoch=462, global_step=1850
05/18/2022 17:32:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
05/18/2022 17:32:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
05/18/2022 17:32:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
05/18/2022 17:32:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/18/2022 17:32:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
05/18/2022 17:32:37 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5956439393939393 on epoch=474
05/18/2022 17:32:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
05/18/2022 17:32:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
05/18/2022 17:32:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
05/18/2022 17:32:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/18/2022 17:32:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
05/18/2022 17:32:45 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6418708066781089 on epoch=487
05/18/2022 17:32:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
05/18/2022 17:32:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
05/18/2022 17:32:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=494
05/18/2022 17:32:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
05/18/2022 17:32:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
05/18/2022 17:32:53 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.5826539589442815 on epoch=499
05/18/2022 17:32:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/18/2022 17:32:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
05/18/2022 17:32:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
05/18/2022 17:33:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
05/18/2022 17:33:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/18/2022 17:33:02 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.5641403764581124 on epoch=512
05/18/2022 17:33:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=514
05/18/2022 17:33:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
05/18/2022 17:33:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
05/18/2022 17:33:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=522
05/18/2022 17:33:10 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
05/18/2022 17:33:11 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.5477281534985736 on epoch=524
05/18/2022 17:33:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/18/2022 17:33:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
05/18/2022 17:33:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
05/18/2022 17:33:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
05/18/2022 17:33:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=537
05/18/2022 17:33:19 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.5680295073768442 on epoch=537
05/18/2022 17:33:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
05/18/2022 17:33:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
05/18/2022 17:33:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
05/18/2022 17:33:25 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
05/18/2022 17:33:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
05/18/2022 17:33:28 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6071560993696701 on epoch=549
05/18/2022 17:33:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
05/18/2022 17:33:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
05/18/2022 17:33:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
05/18/2022 17:33:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
05/18/2022 17:33:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
05/18/2022 17:33:36 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5871194271194271 on epoch=562
05/18/2022 17:33:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
05/18/2022 17:33:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/18/2022 17:33:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
05/18/2022 17:33:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=572
05/18/2022 17:33:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
05/18/2022 17:33:43 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.5926501035196687 on epoch=574
05/18/2022 17:33:45 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
05/18/2022 17:33:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
05/18/2022 17:33:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
05/18/2022 17:33:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/18/2022 17:33:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/18/2022 17:33:52 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6363899613899614 on epoch=587
05/18/2022 17:33:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
05/18/2022 17:33:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
05/18/2022 17:33:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
05/18/2022 17:33:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
05/18/2022 17:34:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
05/18/2022 17:34:01 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5343465891520811 on epoch=599
05/18/2022 17:34:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
05/18/2022 17:34:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
05/18/2022 17:34:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
05/18/2022 17:34:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
05/18/2022 17:34:08 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
05/18/2022 17:34:09 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.5893635231870527 on epoch=612
05/18/2022 17:34:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/18/2022 17:34:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
05/18/2022 17:34:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/18/2022 17:34:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
05/18/2022 17:34:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
05/18/2022 17:34:18 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.522070772070772 on epoch=624
05/18/2022 17:34:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/18/2022 17:34:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
05/18/2022 17:34:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 17:34:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
05/18/2022 17:34:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
05/18/2022 17:34:25 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.560014619883041 on epoch=637
05/18/2022 17:34:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
05/18/2022 17:34:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
05/18/2022 17:34:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/18/2022 17:34:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
05/18/2022 17:34:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/18/2022 17:34:34 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.516798418972332 on epoch=649
05/18/2022 17:34:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
05/18/2022 17:34:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/18/2022 17:34:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
05/18/2022 17:34:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 17:34:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/18/2022 17:34:43 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5222793335696561 on epoch=662
05/18/2022 17:34:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/18/2022 17:34:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
05/18/2022 17:34:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
05/18/2022 17:34:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/18/2022 17:34:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
05/18/2022 17:34:51 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5317721662549248 on epoch=674
05/18/2022 17:34:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
05/18/2022 17:34:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/18/2022 17:34:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/18/2022 17:34:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/18/2022 17:34:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
05/18/2022 17:34:59 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5839482746503619 on epoch=687
05/18/2022 17:35:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
05/18/2022 17:35:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
05/18/2022 17:35:04 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 17:35:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
05/18/2022 17:35:07 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
05/18/2022 17:35:08 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.5738075213264449 on epoch=699
05/18/2022 17:35:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
05/18/2022 17:35:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/18/2022 17:35:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
05/18/2022 17:35:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/18/2022 17:35:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/18/2022 17:35:16 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.56036866359447 on epoch=712
05/18/2022 17:35:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
05/18/2022 17:35:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
05/18/2022 17:35:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/18/2022 17:35:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/18/2022 17:35:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 17:35:24 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.5744397759103641 on epoch=724
05/18/2022 17:35:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 17:35:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/18/2022 17:35:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/18/2022 17:35:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/18/2022 17:35:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/18/2022 17:35:32 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5767067223963775 on epoch=737
05/18/2022 17:35:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/18/2022 17:35:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/18/2022 17:35:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/18/2022 17:35:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
05/18/2022 17:35:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
05/18/2022 17:35:41 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5267319023569024 on epoch=749
05/18/2022 17:35:41 - INFO - __main__ - save last model!
05/18/2022 17:35:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:35:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:35:41 - INFO - __main__ - Printing 3 examples
05/18/2022 17:35:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:35:41 - INFO - __main__ - ['others']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:35:41 - INFO - __main__ - ['others']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:35:41 - INFO - __main__ - ['others']
05/18/2022 17:35:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:35:41 - INFO - __main__ - Printing 3 examples
05/18/2022 17:35:41 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:35:41 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:35:41 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:35:41 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:35:41 - INFO - __main__ - Printing 3 examples
05/18/2022 17:35:41 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:35:41 - INFO - __main__ - ['happy']
05/18/2022 17:35:41 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:35:41 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:35:41 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:35:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:35:47 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:35:47 - INFO - __main__ - task name: emo
05/18/2022 17:35:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:35:47 - INFO - __main__ - Starting training!
05/18/2022 17:35:49 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:37:10 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.4_8_predictions.txt
05/18/2022 17:37:10 - INFO - __main__ - Classification-F1 on test data: 0.3329
05/18/2022 17:37:10 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.6933458378691645, test_performance=0.33290717128840874
05/18/2022 17:37:10 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
05/18/2022 17:37:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:37:11 - INFO - __main__ - Printing 3 examples
05/18/2022 17:37:11 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:37:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:37:11 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:37:11 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:37:11 - INFO - __main__ - Printing 3 examples
05/18/2022 17:37:11 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:37:11 - INFO - __main__ - ['happy']
05/18/2022 17:37:11 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:37:11 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:37:11 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:37:18 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:37:18 - INFO - __main__ - task name: emo
05/18/2022 17:37:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:37:19 - INFO - __main__ - Starting training!
05/18/2022 17:37:22 - INFO - __main__ - Step 10 Global step 10 Train loss 7.03 on epoch=2
05/18/2022 17:37:23 - INFO - __main__ - Step 20 Global step 20 Train loss 5.18 on epoch=4
05/18/2022 17:37:25 - INFO - __main__ - Step 30 Global step 30 Train loss 3.46 on epoch=7
05/18/2022 17:37:26 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
05/18/2022 17:37:28 - INFO - __main__ - Step 50 Global step 50 Train loss 1.93 on epoch=12
05/18/2022 17:37:29 - INFO - __main__ - Global step 50 Train loss 4.03 Classification-F1 0.2157754010695187 on epoch=12
05/18/2022 17:37:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2157754010695187 on epoch=12, global_step=50
05/18/2022 17:37:30 - INFO - __main__ - Step 60 Global step 60 Train loss 1.64 on epoch=14
05/18/2022 17:37:32 - INFO - __main__ - Step 70 Global step 70 Train loss 1.57 on epoch=17
05/18/2022 17:37:33 - INFO - __main__ - Step 80 Global step 80 Train loss 1.32 on epoch=19
05/18/2022 17:37:35 - INFO - __main__ - Step 90 Global step 90 Train loss 1.37 on epoch=22
05/18/2022 17:37:36 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=24
05/18/2022 17:37:37 - INFO - __main__ - Global step 100 Train loss 1.40 Classification-F1 0.13067758749069247 on epoch=24
05/18/2022 17:37:39 - INFO - __main__ - Step 110 Global step 110 Train loss 1.15 on epoch=27
05/18/2022 17:37:40 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=29
05/18/2022 17:37:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.10 on epoch=32
05/18/2022 17:37:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=34
05/18/2022 17:37:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.07 on epoch=37
05/18/2022 17:37:46 - INFO - __main__ - Global step 150 Train loss 1.14 Classification-F1 0.26883780332056195 on epoch=37
05/18/2022 17:37:46 - INFO - __main__ - Saving model with best Classification-F1: 0.2157754010695187 -> 0.26883780332056195 on epoch=37, global_step=150
05/18/2022 17:37:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
05/18/2022 17:37:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
05/18/2022 17:37:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=44
05/18/2022 17:37:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
05/18/2022 17:37:52 - INFO - __main__ - Step 200 Global step 200 Train loss 1.09 on epoch=49
05/18/2022 17:37:53 - INFO - __main__ - Global step 200 Train loss 1.01 Classification-F1 0.0974025974025974 on epoch=49
05/18/2022 17:37:55 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=52
05/18/2022 17:37:56 - INFO - __main__ - Step 220 Global step 220 Train loss 1.02 on epoch=54
05/18/2022 17:37:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
05/18/2022 17:37:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
05/18/2022 17:38:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
05/18/2022 17:38:03 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.19722222222222224 on epoch=62
05/18/2022 17:38:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=64
05/18/2022 17:38:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
05/18/2022 17:38:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
05/18/2022 17:38:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
05/18/2022 17:38:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/18/2022 17:38:11 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.2629541137583943 on epoch=74
05/18/2022 17:38:12 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
05/18/2022 17:38:14 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=79
05/18/2022 17:38:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=82
05/18/2022 17:38:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=84
05/18/2022 17:38:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
05/18/2022 17:38:19 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.14095238095238094 on epoch=87
05/18/2022 17:38:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
05/18/2022 17:38:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
05/18/2022 17:38:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.75 on epoch=94
05/18/2022 17:38:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=97
05/18/2022 17:38:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=99
05/18/2022 17:38:27 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.32608225108225114 on epoch=99
05/18/2022 17:38:27 - INFO - __main__ - Saving model with best Classification-F1: 0.26883780332056195 -> 0.32608225108225114 on epoch=99, global_step=400
05/18/2022 17:38:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=102
05/18/2022 17:38:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=104
05/18/2022 17:38:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
05/18/2022 17:38:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=109
05/18/2022 17:38:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
05/18/2022 17:38:35 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.31043956043956045 on epoch=112
05/18/2022 17:38:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=114
05/18/2022 17:38:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=117
05/18/2022 17:38:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.68 on epoch=119
05/18/2022 17:38:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=122
05/18/2022 17:38:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=124
05/18/2022 17:38:43 - INFO - __main__ - Global step 500 Train loss 0.71 Classification-F1 0.26627959927140255 on epoch=124
05/18/2022 17:38:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=127
05/18/2022 17:38:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=129
05/18/2022 17:38:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=132
05/18/2022 17:38:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=134
05/18/2022 17:38:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=137
05/18/2022 17:38:52 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.30159467963386727 on epoch=137
05/18/2022 17:38:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
05/18/2022 17:38:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=142
05/18/2022 17:38:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=144
05/18/2022 17:38:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.70 on epoch=147
05/18/2022 17:38:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
05/18/2022 17:39:00 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.4426691729323309 on epoch=149
05/18/2022 17:39:00 - INFO - __main__ - Saving model with best Classification-F1: 0.32608225108225114 -> 0.4426691729323309 on epoch=149, global_step=600
05/18/2022 17:39:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=152
05/18/2022 17:39:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=154
05/18/2022 17:39:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=157
05/18/2022 17:39:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=159
05/18/2022 17:39:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=162
05/18/2022 17:39:08 - INFO - __main__ - Global step 650 Train loss 0.62 Classification-F1 0.3836320926838168 on epoch=162
05/18/2022 17:39:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.72 on epoch=164
05/18/2022 17:39:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
05/18/2022 17:39:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=169
05/18/2022 17:39:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.70 on epoch=172
05/18/2022 17:39:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.73 on epoch=174
05/18/2022 17:39:16 - INFO - __main__ - Global step 700 Train loss 0.72 Classification-F1 0.38217787114845936 on epoch=174
05/18/2022 17:39:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
05/18/2022 17:39:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=179
05/18/2022 17:39:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.78 on epoch=182
05/18/2022 17:39:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
05/18/2022 17:39:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.66 on epoch=187
05/18/2022 17:39:24 - INFO - __main__ - Global step 750 Train loss 0.69 Classification-F1 0.5050770308123249 on epoch=187
05/18/2022 17:39:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4426691729323309 -> 0.5050770308123249 on epoch=187, global_step=750
05/18/2022 17:39:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=189
05/18/2022 17:39:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.63 on epoch=192
05/18/2022 17:39:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=194
05/18/2022 17:39:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=197
05/18/2022 17:39:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.67 on epoch=199
05/18/2022 17:39:32 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.42704342273307794 on epoch=199
05/18/2022 17:39:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=202
05/18/2022 17:39:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=204
05/18/2022 17:39:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.72 on epoch=207
05/18/2022 17:39:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=209
05/18/2022 17:39:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=212
05/18/2022 17:39:40 - INFO - __main__ - Global step 850 Train loss 0.61 Classification-F1 0.43984171508715686 on epoch=212
05/18/2022 17:39:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.62 on epoch=214
05/18/2022 17:39:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=217
05/18/2022 17:39:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=219
05/18/2022 17:39:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.61 on epoch=222
05/18/2022 17:39:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=224
05/18/2022 17:39:49 - INFO - __main__ - Global step 900 Train loss 0.59 Classification-F1 0.4646551724137931 on epoch=224
05/18/2022 17:39:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.62 on epoch=227
05/18/2022 17:39:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.67 on epoch=229
05/18/2022 17:39:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=232
05/18/2022 17:39:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=234
05/18/2022 17:39:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=237
05/18/2022 17:39:57 - INFO - __main__ - Global step 950 Train loss 0.61 Classification-F1 0.4912903225806451 on epoch=237
05/18/2022 17:39:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=239
05/18/2022 17:40:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=242
05/18/2022 17:40:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=244
05/18/2022 17:40:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=247
05/18/2022 17:40:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=249
05/18/2022 17:40:05 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.45020604395604397 on epoch=249
05/18/2022 17:40:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=252
05/18/2022 17:40:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=254
05/18/2022 17:40:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=257
05/18/2022 17:40:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.54 on epoch=259
05/18/2022 17:40:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=262
05/18/2022 17:40:12 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.44637422832797147 on epoch=262
05/18/2022 17:40:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=264
05/18/2022 17:40:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=267
05/18/2022 17:40:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
05/18/2022 17:40:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=272
05/18/2022 17:40:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=274
05/18/2022 17:40:21 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.46806491985950177 on epoch=274
05/18/2022 17:40:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=277
05/18/2022 17:40:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=279
05/18/2022 17:40:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=282
05/18/2022 17:40:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.54 on epoch=284
05/18/2022 17:40:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=287
05/18/2022 17:40:29 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.507010582010582 on epoch=287
05/18/2022 17:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5050770308123249 -> 0.507010582010582 on epoch=287, global_step=1150
05/18/2022 17:40:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=289
05/18/2022 17:40:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=292
05/18/2022 17:40:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=294
05/18/2022 17:40:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=297
05/18/2022 17:40:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=299
05/18/2022 17:40:38 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.511578947368421 on epoch=299
05/18/2022 17:40:38 - INFO - __main__ - Saving model with best Classification-F1: 0.507010582010582 -> 0.511578947368421 on epoch=299, global_step=1200
05/18/2022 17:40:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=302
05/18/2022 17:40:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=304
05/18/2022 17:40:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=307
05/18/2022 17:40:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=309
05/18/2022 17:40:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=312
05/18/2022 17:40:46 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.5916717980295567 on epoch=312
05/18/2022 17:40:46 - INFO - __main__ - Saving model with best Classification-F1: 0.511578947368421 -> 0.5916717980295567 on epoch=312, global_step=1250
05/18/2022 17:40:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=314
05/18/2022 17:40:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=317
05/18/2022 17:40:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=319
05/18/2022 17:40:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=322
05/18/2022 17:40:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=324
05/18/2022 17:40:55 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.5942930911680911 on epoch=324
05/18/2022 17:40:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5916717980295567 -> 0.5942930911680911 on epoch=324, global_step=1300
05/18/2022 17:40:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=327
05/18/2022 17:40:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=329
05/18/2022 17:40:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=332
05/18/2022 17:41:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=334
05/18/2022 17:41:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=337
05/18/2022 17:41:03 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.47944518716577544 on epoch=337
05/18/2022 17:41:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=339
05/18/2022 17:41:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=342
05/18/2022 17:41:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=344
05/18/2022 17:41:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=347
05/18/2022 17:41:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=349
05/18/2022 17:41:11 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.6077233250620346 on epoch=349
05/18/2022 17:41:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5942930911680911 -> 0.6077233250620346 on epoch=349, global_step=1400
05/18/2022 17:41:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=352
05/18/2022 17:41:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=354
05/18/2022 17:41:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=357
05/18/2022 17:41:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=359
05/18/2022 17:41:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=362
05/18/2022 17:41:20 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.5862103174603175 on epoch=362
05/18/2022 17:41:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=364
05/18/2022 17:41:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=367
05/18/2022 17:41:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=369
05/18/2022 17:41:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=372
05/18/2022 17:41:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=374
05/18/2022 17:41:28 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.5623559907834101 on epoch=374
05/18/2022 17:41:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
05/18/2022 17:41:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
05/18/2022 17:41:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=382
05/18/2022 17:41:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=384
05/18/2022 17:41:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=387
05/18/2022 17:41:37 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.6122329059829059 on epoch=387
05/18/2022 17:41:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6077233250620346 -> 0.6122329059829059 on epoch=387, global_step=1550
05/18/2022 17:41:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
05/18/2022 17:41:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=392
05/18/2022 17:41:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=394
05/18/2022 17:41:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=397
05/18/2022 17:41:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=399
05/18/2022 17:41:46 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6313725490196078 on epoch=399
05/18/2022 17:41:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6122329059829059 -> 0.6313725490196078 on epoch=399, global_step=1600
05/18/2022 17:41:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=402
05/18/2022 17:41:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=404
05/18/2022 17:41:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=407
05/18/2022 17:41:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=409
05/18/2022 17:41:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=412
05/18/2022 17:41:54 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.5241403807899971 on epoch=412
05/18/2022 17:41:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=414
05/18/2022 17:41:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=417
05/18/2022 17:41:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
05/18/2022 17:42:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=422
05/18/2022 17:42:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=424
05/18/2022 17:42:03 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7028179458657767 on epoch=424
05/18/2022 17:42:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6313725490196078 -> 0.7028179458657767 on epoch=424, global_step=1700
05/18/2022 17:42:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=427
05/18/2022 17:42:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=429
05/18/2022 17:42:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=432
05/18/2022 17:42:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=434
05/18/2022 17:42:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
05/18/2022 17:42:11 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.6095947864240547 on epoch=437
05/18/2022 17:42:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
05/18/2022 17:42:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=442
05/18/2022 17:42:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/18/2022 17:42:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
05/18/2022 17:42:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=449
05/18/2022 17:42:19 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.6877314814814814 on epoch=449
05/18/2022 17:42:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=452
05/18/2022 17:42:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
05/18/2022 17:42:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=457
05/18/2022 17:42:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
05/18/2022 17:42:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
05/18/2022 17:42:27 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.6477990034866096 on epoch=462
05/18/2022 17:42:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
05/18/2022 17:42:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=467
05/18/2022 17:42:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=469
05/18/2022 17:42:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
05/18/2022 17:42:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=474
05/18/2022 17:42:35 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.6000663423264042 on epoch=474
05/18/2022 17:42:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=477
05/18/2022 17:42:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
05/18/2022 17:42:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
05/18/2022 17:42:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
05/18/2022 17:42:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
05/18/2022 17:42:44 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6462971954858365 on epoch=487
05/18/2022 17:42:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=489
05/18/2022 17:42:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=492
05/18/2022 17:42:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=494
05/18/2022 17:42:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=497
05/18/2022 17:42:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
05/18/2022 17:42:53 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.6352495543672014 on epoch=499
05/18/2022 17:42:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=502
05/18/2022 17:42:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=504
05/18/2022 17:42:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=507
05/18/2022 17:42:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
05/18/2022 17:43:00 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
05/18/2022 17:43:01 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.6705521767813203 on epoch=512
05/18/2022 17:43:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=514
05/18/2022 17:43:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=517
05/18/2022 17:43:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
05/18/2022 17:43:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
05/18/2022 17:43:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=524
05/18/2022 17:43:10 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.5895348837209302 on epoch=524
05/18/2022 17:43:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=527
05/18/2022 17:43:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=529
05/18/2022 17:43:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
05/18/2022 17:43:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
05/18/2022 17:43:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/18/2022 17:43:19 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6251838235294118 on epoch=537
05/18/2022 17:43:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/18/2022 17:43:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=542
05/18/2022 17:43:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=544
05/18/2022 17:43:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
05/18/2022 17:43:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
05/18/2022 17:43:28 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.6366007834757835 on epoch=549
05/18/2022 17:43:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=552
05/18/2022 17:43:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
05/18/2022 17:43:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
05/18/2022 17:43:34 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/18/2022 17:43:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
05/18/2022 17:43:36 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6464433416046318 on epoch=562
05/18/2022 17:43:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=564
05/18/2022 17:43:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=567
05/18/2022 17:43:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=569
05/18/2022 17:43:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/18/2022 17:43:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/18/2022 17:43:44 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.6706257104964002 on epoch=574
05/18/2022 17:43:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=577
05/18/2022 17:43:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/18/2022 17:43:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
05/18/2022 17:43:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
05/18/2022 17:43:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 17:43:53 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.6315826330532214 on epoch=587
05/18/2022 17:43:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
05/18/2022 17:43:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
05/18/2022 17:43:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
05/18/2022 17:43:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
05/18/2022 17:44:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=599
05/18/2022 17:44:01 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6652802535155476 on epoch=599
05/18/2022 17:44:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/18/2022 17:44:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=604
05/18/2022 17:44:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
05/18/2022 17:44:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=609
05/18/2022 17:44:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
05/18/2022 17:44:09 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.6581696998722861 on epoch=612
05/18/2022 17:44:11 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
05/18/2022 17:44:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=617
05/18/2022 17:44:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=619
05/18/2022 17:44:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/18/2022 17:44:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/18/2022 17:44:17 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6137065875664318 on epoch=624
05/18/2022 17:44:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
05/18/2022 17:44:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
05/18/2022 17:44:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 17:44:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/18/2022 17:44:25 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/18/2022 17:44:26 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.5661910669975186 on epoch=637
05/18/2022 17:44:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
05/18/2022 17:44:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 17:44:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
05/18/2022 17:44:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 17:44:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=649
05/18/2022 17:44:34 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.5488886242756211 on epoch=649
05/18/2022 17:44:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
05/18/2022 17:44:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/18/2022 17:44:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 17:44:40 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
05/18/2022 17:44:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=662
05/18/2022 17:44:42 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.599193470491645 on epoch=662
05/18/2022 17:44:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
05/18/2022 17:44:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=667
05/18/2022 17:44:47 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
05/18/2022 17:44:49 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
05/18/2022 17:44:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
05/18/2022 17:44:51 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6266789516789517 on epoch=674
05/18/2022 17:44:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=677
05/18/2022 17:44:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=679
05/18/2022 17:44:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/18/2022 17:44:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
05/18/2022 17:44:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
05/18/2022 17:44:59 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7149976565861453 on epoch=687
05/18/2022 17:44:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7028179458657767 -> 0.7149976565861453 on epoch=687, global_step=2750
05/18/2022 17:45:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
05/18/2022 17:45:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
05/18/2022 17:45:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
05/18/2022 17:45:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
05/18/2022 17:45:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
05/18/2022 17:45:08 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.6738815284178187 on epoch=699
05/18/2022 17:45:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
05/18/2022 17:45:11 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=704
05/18/2022 17:45:12 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=707
05/18/2022 17:45:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=709
05/18/2022 17:45:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=712
05/18/2022 17:45:15 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6754734848484848 on epoch=712
05/18/2022 17:45:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
05/18/2022 17:45:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
05/18/2022 17:45:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=719
05/18/2022 17:45:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/18/2022 17:45:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 17:45:24 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.6746700879765396 on epoch=724
05/18/2022 17:45:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
05/18/2022 17:45:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=729
05/18/2022 17:45:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=732
05/18/2022 17:45:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=734
05/18/2022 17:45:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/18/2022 17:45:31 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.662194011617186 on epoch=737
05/18/2022 17:45:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=739
05/18/2022 17:45:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/18/2022 17:45:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
05/18/2022 17:45:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
05/18/2022 17:45:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/18/2022 17:45:40 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.6388028740642755 on epoch=749
05/18/2022 17:45:40 - INFO - __main__ - save last model!
05/18/2022 17:45:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:45:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:45:40 - INFO - __main__ - Printing 3 examples
05/18/2022 17:45:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:45:40 - INFO - __main__ - ['others']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:45:40 - INFO - __main__ - ['others']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:45:40 - INFO - __main__ - ['others']
05/18/2022 17:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:45:40 - INFO - __main__ - Printing 3 examples
05/18/2022 17:45:40 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:45:40 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:45:40 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:45:40 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:45:40 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:45:40 - INFO - __main__ - Printing 3 examples
05/18/2022 17:45:40 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:45:40 - INFO - __main__ - ['happy']
05/18/2022 17:45:40 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:45:40 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:45:40 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:45:42 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:45:46 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:45:46 - INFO - __main__ - task name: emo
05/18/2022 17:45:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:45:46 - INFO - __main__ - Starting training!
05/18/2022 17:45:49 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:47:07 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.3_8_predictions.txt
05/18/2022 17:47:07 - INFO - __main__ - Classification-F1 on test data: 0.4317
05/18/2022 17:47:07 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.7149976565861453, test_performance=0.43165340428220306
05/18/2022 17:47:07 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
05/18/2022 17:47:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:47:08 - INFO - __main__ - Printing 3 examples
05/18/2022 17:47:08 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:47:08 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:47:08 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:47:08 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:47:08 - INFO - __main__ - Printing 3 examples
05/18/2022 17:47:08 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
05/18/2022 17:47:08 - INFO - __main__ - ['happy']
05/18/2022 17:47:08 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:47:08 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:47:08 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:47:14 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:47:14 - INFO - __main__ - task name: emo
05/18/2022 17:47:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:47:15 - INFO - __main__ - Starting training!
05/18/2022 17:47:17 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=2
05/18/2022 17:47:18 - INFO - __main__ - Step 20 Global step 20 Train loss 5.75 on epoch=4
05/18/2022 17:47:20 - INFO - __main__ - Step 30 Global step 30 Train loss 4.16 on epoch=7
05/18/2022 17:47:21 - INFO - __main__ - Step 40 Global step 40 Train loss 3.01 on epoch=9
05/18/2022 17:47:23 - INFO - __main__ - Step 50 Global step 50 Train loss 2.49 on epoch=12
05/18/2022 17:47:24 - INFO - __main__ - Global step 50 Train loss 4.55 Classification-F1 0.23040411286465806 on epoch=12
05/18/2022 17:47:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23040411286465806 on epoch=12, global_step=50
05/18/2022 17:47:25 - INFO - __main__ - Step 60 Global step 60 Train loss 2.17 on epoch=14
05/18/2022 17:47:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.76 on epoch=17
05/18/2022 17:47:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.79 on epoch=19
05/18/2022 17:47:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.47 on epoch=22
05/18/2022 17:47:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.45 on epoch=24
05/18/2022 17:47:32 - INFO - __main__ - Global step 100 Train loss 1.73 Classification-F1 0.13197586726998492 on epoch=24
05/18/2022 17:47:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.50 on epoch=27
05/18/2022 17:47:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.48 on epoch=29
05/18/2022 17:47:36 - INFO - __main__ - Step 130 Global step 130 Train loss 1.43 on epoch=32
05/18/2022 17:47:38 - INFO - __main__ - Step 140 Global step 140 Train loss 1.23 on epoch=34
05/18/2022 17:47:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.38 on epoch=37
05/18/2022 17:47:40 - INFO - __main__ - Global step 150 Train loss 1.41 Classification-F1 0.125 on epoch=37
05/18/2022 17:47:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.37 on epoch=39
05/18/2022 17:47:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.26 on epoch=42
05/18/2022 17:47:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.22 on epoch=44
05/18/2022 17:47:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=47
05/18/2022 17:47:48 - INFO - __main__ - Step 200 Global step 200 Train loss 1.24 on epoch=49
05/18/2022 17:47:49 - INFO - __main__ - Global step 200 Train loss 1.24 Classification-F1 0.13415384615384615 on epoch=49
05/18/2022 17:47:51 - INFO - __main__ - Step 210 Global step 210 Train loss 1.67 on epoch=52
05/18/2022 17:47:53 - INFO - __main__ - Step 220 Global step 220 Train loss 1.29 on epoch=54
05/18/2022 17:47:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.17 on epoch=57
05/18/2022 17:47:56 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
05/18/2022 17:47:57 - INFO - __main__ - Step 250 Global step 250 Train loss 1.21 on epoch=62
05/18/2022 17:47:59 - INFO - __main__ - Global step 250 Train loss 1.29 Classification-F1 0.3362085769980506 on epoch=62
05/18/2022 17:47:59 - INFO - __main__ - Saving model with best Classification-F1: 0.23040411286465806 -> 0.3362085769980506 on epoch=62, global_step=250
05/18/2022 17:48:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.35 on epoch=64
05/18/2022 17:48:02 - INFO - __main__ - Step 270 Global step 270 Train loss 1.15 on epoch=67
05/18/2022 17:48:03 - INFO - __main__ - Step 280 Global step 280 Train loss 1.14 on epoch=69
05/18/2022 17:48:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.11 on epoch=72
05/18/2022 17:48:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
05/18/2022 17:48:07 - INFO - __main__ - Global step 300 Train loss 1.14 Classification-F1 0.1581196581196581 on epoch=74
05/18/2022 17:48:08 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
05/18/2022 17:48:10 - INFO - __main__ - Step 320 Global step 320 Train loss 1.05 on epoch=79
05/18/2022 17:48:11 - INFO - __main__ - Step 330 Global step 330 Train loss 1.05 on epoch=82
05/18/2022 17:48:13 - INFO - __main__ - Step 340 Global step 340 Train loss 1.08 on epoch=84
05/18/2022 17:48:14 - INFO - __main__ - Step 350 Global step 350 Train loss 1.07 on epoch=87
05/18/2022 17:48:15 - INFO - __main__ - Global step 350 Train loss 1.05 Classification-F1 0.1 on epoch=87
05/18/2022 17:48:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.12 on epoch=89
05/18/2022 17:48:18 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=92
05/18/2022 17:48:20 - INFO - __main__ - Step 380 Global step 380 Train loss 1.05 on epoch=94
05/18/2022 17:48:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
05/18/2022 17:48:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
05/18/2022 17:48:23 - INFO - __main__ - Global step 400 Train loss 1.02 Classification-F1 0.1 on epoch=99
05/18/2022 17:48:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=102
05/18/2022 17:48:26 - INFO - __main__ - Step 420 Global step 420 Train loss 1.03 on epoch=104
05/18/2022 17:48:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
05/18/2022 17:48:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.04 on epoch=109
05/18/2022 17:48:31 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=112
05/18/2022 17:48:32 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.1 on epoch=112
05/18/2022 17:48:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.98 on epoch=114
05/18/2022 17:48:35 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=117
05/18/2022 17:48:37 - INFO - __main__ - Step 480 Global step 480 Train loss 1.07 on epoch=119
05/18/2022 17:48:38 - INFO - __main__ - Step 490 Global step 490 Train loss 1.00 on epoch=122
05/18/2022 17:48:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
05/18/2022 17:48:40 - INFO - __main__ - Global step 500 Train loss 0.98 Classification-F1 0.1 on epoch=124
05/18/2022 17:48:42 - INFO - __main__ - Step 510 Global step 510 Train loss 1.00 on epoch=127
05/18/2022 17:48:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.88 on epoch=129
05/18/2022 17:48:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
05/18/2022 17:48:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.86 on epoch=134
05/18/2022 17:48:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
05/18/2022 17:48:49 - INFO - __main__ - Global step 550 Train loss 0.89 Classification-F1 0.1860730593607306 on epoch=137
05/18/2022 17:48:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.92 on epoch=139
05/18/2022 17:48:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.89 on epoch=142
05/18/2022 17:48:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.95 on epoch=144
05/18/2022 17:48:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.93 on epoch=147
05/18/2022 17:48:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.85 on epoch=149
05/18/2022 17:48:56 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.18026315789473685 on epoch=149
05/18/2022 17:48:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.86 on epoch=152
05/18/2022 17:48:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
05/18/2022 17:49:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.85 on epoch=157
05/18/2022 17:49:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=159
05/18/2022 17:49:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.79 on epoch=162
05/18/2022 17:49:05 - INFO - __main__ - Global step 650 Train loss 0.85 Classification-F1 0.30801619433198385 on epoch=162
05/18/2022 17:49:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=164
05/18/2022 17:49:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/18/2022 17:49:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.84 on epoch=169
05/18/2022 17:49:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.78 on epoch=172
05/18/2022 17:49:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.71 on epoch=174
05/18/2022 17:49:14 - INFO - __main__ - Global step 700 Train loss 0.81 Classification-F1 0.25869565217391305 on epoch=174
05/18/2022 17:49:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=177
05/18/2022 17:49:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
05/18/2022 17:49:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=182
05/18/2022 17:49:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.87 on epoch=184
05/18/2022 17:49:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=187
05/18/2022 17:49:22 - INFO - __main__ - Global step 750 Train loss 0.81 Classification-F1 0.37473769168684423 on epoch=187
05/18/2022 17:49:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3362085769980506 -> 0.37473769168684423 on epoch=187, global_step=750
05/18/2022 17:49:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.80 on epoch=189
05/18/2022 17:49:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=192
05/18/2022 17:49:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=194
05/18/2022 17:49:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.72 on epoch=197
05/18/2022 17:49:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=199
05/18/2022 17:49:31 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.44639730639730635 on epoch=199
05/18/2022 17:49:31 - INFO - __main__ - Saving model with best Classification-F1: 0.37473769168684423 -> 0.44639730639730635 on epoch=199, global_step=800
05/18/2022 17:49:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.78 on epoch=202
05/18/2022 17:49:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.70 on epoch=204
05/18/2022 17:49:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.68 on epoch=207
05/18/2022 17:49:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/18/2022 17:49:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.63 on epoch=212
05/18/2022 17:49:39 - INFO - __main__ - Global step 850 Train loss 0.71 Classification-F1 0.4748452696728559 on epoch=212
05/18/2022 17:49:39 - INFO - __main__ - Saving model with best Classification-F1: 0.44639730639730635 -> 0.4748452696728559 on epoch=212, global_step=850
05/18/2022 17:49:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=214
05/18/2022 17:49:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=217
05/18/2022 17:49:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.76 on epoch=219
05/18/2022 17:49:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=222
05/18/2022 17:49:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.72 on epoch=224
05/18/2022 17:49:47 - INFO - __main__ - Global step 900 Train loss 0.68 Classification-F1 0.28529411764705886 on epoch=224
05/18/2022 17:49:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.64 on epoch=227
05/18/2022 17:49:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
05/18/2022 17:49:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.68 on epoch=232
05/18/2022 17:49:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.64 on epoch=234
05/18/2022 17:49:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.66 on epoch=237
05/18/2022 17:49:55 - INFO - __main__ - Global step 950 Train loss 0.68 Classification-F1 0.4977671451355662 on epoch=237
05/18/2022 17:49:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4748452696728559 -> 0.4977671451355662 on epoch=237, global_step=950
05/18/2022 17:49:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=239
05/18/2022 17:49:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=242
05/18/2022 17:49:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.61 on epoch=244
05/18/2022 17:50:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=247
05/18/2022 17:50:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.63 on epoch=249
05/18/2022 17:50:03 - INFO - __main__ - Global step 1000 Train loss 0.65 Classification-F1 0.4481406355645707 on epoch=249
05/18/2022 17:50:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=252
05/18/2022 17:50:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.55 on epoch=254
05/18/2022 17:50:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=257
05/18/2022 17:50:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=259
05/18/2022 17:50:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=262
05/18/2022 17:50:10 - INFO - __main__ - Global step 1050 Train loss 0.58 Classification-F1 0.4827403846153846 on epoch=262
05/18/2022 17:50:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.57 on epoch=264
05/18/2022 17:50:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.56 on epoch=267
05/18/2022 17:50:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.67 on epoch=269
05/18/2022 17:50:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=272
05/18/2022 17:50:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.62 on epoch=274
05/18/2022 17:50:19 - INFO - __main__ - Global step 1100 Train loss 0.58 Classification-F1 0.4570535714285714 on epoch=274
05/18/2022 17:50:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=277
05/18/2022 17:50:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.69 on epoch=279
05/18/2022 17:50:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.57 on epoch=282
05/18/2022 17:50:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.60 on epoch=284
05/18/2022 17:50:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=287
05/18/2022 17:50:27 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.4894964953755546 on epoch=287
05/18/2022 17:50:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=289
05/18/2022 17:50:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=292
05/18/2022 17:50:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.54 on epoch=294
05/18/2022 17:50:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.57 on epoch=297
05/18/2022 17:50:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=299
05/18/2022 17:50:35 - INFO - __main__ - Global step 1200 Train loss 0.55 Classification-F1 0.4996124031007752 on epoch=299
05/18/2022 17:50:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4977671451355662 -> 0.4996124031007752 on epoch=299, global_step=1200
05/18/2022 17:50:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=302
05/18/2022 17:50:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.59 on epoch=304
05/18/2022 17:50:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=307
05/18/2022 17:50:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=309
05/18/2022 17:50:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.64 on epoch=312
05/18/2022 17:50:44 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.4426688815060908 on epoch=312
05/18/2022 17:50:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=314
05/18/2022 17:50:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.55 on epoch=317
05/18/2022 17:50:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.67 on epoch=319
05/18/2022 17:50:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=322
05/18/2022 17:50:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=324
05/18/2022 17:50:52 - INFO - __main__ - Global step 1300 Train loss 0.56 Classification-F1 0.4904729269520558 on epoch=324
05/18/2022 17:50:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=327
05/18/2022 17:50:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=329
05/18/2022 17:50:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=332
05/18/2022 17:50:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=334
05/18/2022 17:51:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=337
05/18/2022 17:51:01 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.46131316663752403 on epoch=337
05/18/2022 17:51:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=339
05/18/2022 17:51:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=342
05/18/2022 17:51:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=344
05/18/2022 17:51:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=347
05/18/2022 17:51:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.68 on epoch=349
05/18/2022 17:51:09 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.50447261663286 on epoch=349
05/18/2022 17:51:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4996124031007752 -> 0.50447261663286 on epoch=349, global_step=1400
05/18/2022 17:51:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=352
05/18/2022 17:51:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=354
05/18/2022 17:51:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=357
05/18/2022 17:51:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=359
05/18/2022 17:51:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=362
05/18/2022 17:51:18 - INFO - __main__ - Global step 1450 Train loss 0.50 Classification-F1 0.4360431235431236 on epoch=362
05/18/2022 17:51:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=364
05/18/2022 17:51:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=367
05/18/2022 17:51:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=369
05/18/2022 17:51:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=372
05/18/2022 17:51:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=374
05/18/2022 17:51:27 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.4460710821060058 on epoch=374
05/18/2022 17:51:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=377
05/18/2022 17:51:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=379
05/18/2022 17:51:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=382
05/18/2022 17:51:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=384
05/18/2022 17:51:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=387
05/18/2022 17:51:35 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.44002516586593454 on epoch=387
05/18/2022 17:51:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=389
05/18/2022 17:51:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=392
05/18/2022 17:51:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=394
05/18/2022 17:51:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=397
05/18/2022 17:51:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=399
05/18/2022 17:51:44 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.5808646276938959 on epoch=399
05/18/2022 17:51:44 - INFO - __main__ - Saving model with best Classification-F1: 0.50447261663286 -> 0.5808646276938959 on epoch=399, global_step=1600
05/18/2022 17:51:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
05/18/2022 17:51:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=404
05/18/2022 17:51:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=407
05/18/2022 17:51:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=409
05/18/2022 17:51:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=412
05/18/2022 17:51:52 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.5275133398011157 on epoch=412
05/18/2022 17:51:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=414
05/18/2022 17:51:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=417
05/18/2022 17:51:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=419
05/18/2022 17:51:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=422
05/18/2022 17:52:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=424
05/18/2022 17:52:00 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.5514652014652015 on epoch=424
05/18/2022 17:52:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=427
05/18/2022 17:52:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=429
05/18/2022 17:52:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=432
05/18/2022 17:52:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=434
05/18/2022 17:52:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=437
05/18/2022 17:52:09 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.6003366003366003 on epoch=437
05/18/2022 17:52:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5808646276938959 -> 0.6003366003366003 on epoch=437, global_step=1750
05/18/2022 17:52:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=439
05/18/2022 17:52:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=442
05/18/2022 17:52:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=444
05/18/2022 17:52:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=447
05/18/2022 17:52:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=449
05/18/2022 17:52:16 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.589957264957265 on epoch=449
05/18/2022 17:52:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
05/18/2022 17:52:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=454
05/18/2022 17:52:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=457
05/18/2022 17:52:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=459
05/18/2022 17:52:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=462
05/18/2022 17:52:25 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.5808333333333333 on epoch=462
05/18/2022 17:52:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
05/18/2022 17:52:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=467
05/18/2022 17:52:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=469
05/18/2022 17:52:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=472
05/18/2022 17:52:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=474
05/18/2022 17:52:34 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.5605098633115875 on epoch=474
05/18/2022 17:52:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=477
05/18/2022 17:52:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=479
05/18/2022 17:52:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=482
05/18/2022 17:52:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=484
05/18/2022 17:52:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=487
05/18/2022 17:52:42 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.5396161740558292 on epoch=487
05/18/2022 17:52:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=489
05/18/2022 17:52:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
05/18/2022 17:52:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=494
05/18/2022 17:52:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=497
05/18/2022 17:52:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=499
05/18/2022 17:52:50 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.5860820334504544 on epoch=499
05/18/2022 17:52:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=502
05/18/2022 17:52:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=504
05/18/2022 17:52:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=507
05/18/2022 17:52:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=509
05/18/2022 17:52:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=512
05/18/2022 17:52:58 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.5508520215416767 on epoch=512
05/18/2022 17:53:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=514
05/18/2022 17:53:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.32 on epoch=517
05/18/2022 17:53:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=519
05/18/2022 17:53:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=522
05/18/2022 17:53:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/18/2022 17:53:07 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.5910897635035566 on epoch=524
05/18/2022 17:53:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=527
05/18/2022 17:53:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=529
05/18/2022 17:53:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=532
05/18/2022 17:53:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=534
05/18/2022 17:53:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
05/18/2022 17:53:14 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.5583287416120999 on epoch=537
05/18/2022 17:53:16 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=539
05/18/2022 17:53:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
05/18/2022 17:53:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=544
05/18/2022 17:53:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
05/18/2022 17:53:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=549
05/18/2022 17:53:23 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.5884646962233169 on epoch=549
05/18/2022 17:53:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=552
05/18/2022 17:53:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
05/18/2022 17:53:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=557
05/18/2022 17:53:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
05/18/2022 17:53:30 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=562
05/18/2022 17:53:31 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.5746871109774335 on epoch=562
05/18/2022 17:53:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=564
05/18/2022 17:53:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
05/18/2022 17:53:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=569
05/18/2022 17:53:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=572
05/18/2022 17:53:39 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
05/18/2022 17:53:40 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.5202442485051181 on epoch=574
05/18/2022 17:53:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
05/18/2022 17:53:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=579
05/18/2022 17:53:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
05/18/2022 17:53:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=584
05/18/2022 17:53:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 17:53:48 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.5845460845460846 on epoch=587
05/18/2022 17:53:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
05/18/2022 17:53:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
05/18/2022 17:53:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
05/18/2022 17:53:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
05/18/2022 17:53:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/18/2022 17:53:56 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.594176245210728 on epoch=599
05/18/2022 17:53:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/18/2022 17:53:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=604
05/18/2022 17:54:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=607
05/18/2022 17:54:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
05/18/2022 17:54:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
05/18/2022 17:54:04 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.5644059795436664 on epoch=612
05/18/2022 17:54:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/18/2022 17:54:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
05/18/2022 17:54:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
05/18/2022 17:54:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=622
05/18/2022 17:54:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
05/18/2022 17:54:13 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.5903518641246552 on epoch=624
05/18/2022 17:54:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/18/2022 17:54:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=629
05/18/2022 17:54:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
05/18/2022 17:54:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/18/2022 17:54:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
05/18/2022 17:54:21 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6098818507049875 on epoch=637
05/18/2022 17:54:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6003366003366003 -> 0.6098818507049875 on epoch=637, global_step=2550
05/18/2022 17:54:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=639
05/18/2022 17:54:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
05/18/2022 17:54:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/18/2022 17:54:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 17:54:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
05/18/2022 17:54:29 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.608893200743795 on epoch=649
05/18/2022 17:54:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
05/18/2022 17:54:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
05/18/2022 17:54:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 17:54:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=659
05/18/2022 17:54:36 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=662
05/18/2022 17:54:36 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.5660384331116038 on epoch=662
05/18/2022 17:54:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
05/18/2022 17:54:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/18/2022 17:54:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
05/18/2022 17:54:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
05/18/2022 17:54:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
05/18/2022 17:54:44 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6122652063828534 on epoch=674
05/18/2022 17:54:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6098818507049875 -> 0.6122652063828534 on epoch=674, global_step=2700
05/18/2022 17:54:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/18/2022 17:54:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
05/18/2022 17:54:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
05/18/2022 17:54:50 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=684
05/18/2022 17:54:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/18/2022 17:54:52 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.5404014262709915 on epoch=687
05/18/2022 17:54:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
05/18/2022 17:54:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=692
05/18/2022 17:54:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/18/2022 17:54:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=697
05/18/2022 17:55:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
05/18/2022 17:55:01 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.5916008248857085 on epoch=699
05/18/2022 17:55:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/18/2022 17:55:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
05/18/2022 17:55:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
05/18/2022 17:55:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
05/18/2022 17:55:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
05/18/2022 17:55:09 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5568943943943944 on epoch=712
05/18/2022 17:55:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
05/18/2022 17:55:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
05/18/2022 17:55:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
05/18/2022 17:55:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/18/2022 17:55:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
05/18/2022 17:55:18 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.628906955736224 on epoch=724
05/18/2022 17:55:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6122652063828534 -> 0.628906955736224 on epoch=724, global_step=2900
05/18/2022 17:55:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
05/18/2022 17:55:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=729
05/18/2022 17:55:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/18/2022 17:55:24 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
05/18/2022 17:55:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
05/18/2022 17:55:27 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.5808230293455423 on epoch=737
05/18/2022 17:55:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
05/18/2022 17:55:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
05/18/2022 17:55:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
05/18/2022 17:55:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/18/2022 17:55:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
05/18/2022 17:55:35 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:55:35 - INFO - __main__ - Printing 3 examples
05/18/2022 17:55:35 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 17:55:35 - INFO - __main__ - ['others']
05/18/2022 17:55:35 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 17:55:35 - INFO - __main__ - ['others']
05/18/2022 17:55:35 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 17:55:35 - INFO - __main__ - ['others']
05/18/2022 17:55:35 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:55:35 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:55:36 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:55:36 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:55:36 - INFO - __main__ - Printing 3 examples
05/18/2022 17:55:36 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:55:36 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:55:36 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:55:36 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6287660256410257 on epoch=749
05/18/2022 17:55:36 - INFO - __main__ - save last model!
05/18/2022 17:55:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 17:55:36 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 17:55:36 - INFO - __main__ - Printing 3 examples
05/18/2022 17:55:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 17:55:36 - INFO - __main__ - ['others']
05/18/2022 17:55:36 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:55:38 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:55:42 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:55:42 - INFO - __main__ - task name: emo
05/18/2022 17:55:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:55:42 - INFO - __main__ - Starting training!
05/18/2022 17:55:45 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 17:56:59 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.2_8_predictions.txt
05/18/2022 17:56:59 - INFO - __main__ - Classification-F1 on test data: 0.3631
05/18/2022 17:56:59 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.628906955736224, test_performance=0.3631340472735185
05/18/2022 17:56:59 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
05/18/2022 17:57:00 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:57:00 - INFO - __main__ - Printing 3 examples
05/18/2022 17:57:00 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 17:57:00 - INFO - __main__ - ['others']
05/18/2022 17:57:00 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 17:57:00 - INFO - __main__ - ['others']
05/18/2022 17:57:00 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 17:57:00 - INFO - __main__ - ['others']
05/18/2022 17:57:00 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:57:00 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:57:01 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 17:57:01 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 17:57:01 - INFO - __main__ - Printing 3 examples
05/18/2022 17:57:01 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 17:57:01 - INFO - __main__ - ['others']
05/18/2022 17:57:01 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 17:57:01 - INFO - __main__ - ['others']
05/18/2022 17:57:01 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 17:57:01 - INFO - __main__ - ['others']
05/18/2022 17:57:01 - INFO - __main__ - Tokenizing Input ...
05/18/2022 17:57:01 - INFO - __main__ - Tokenizing Output ...
05/18/2022 17:57:01 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 17:57:07 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 17:57:07 - INFO - __main__ - task name: emo
05/18/2022 17:57:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 17:57:07 - INFO - __main__ - Starting training!
05/18/2022 17:57:09 - INFO - __main__ - Step 10 Global step 10 Train loss 6.00 on epoch=2
05/18/2022 17:57:10 - INFO - __main__ - Step 20 Global step 20 Train loss 2.56 on epoch=4
05/18/2022 17:57:12 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=7
05/18/2022 17:57:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.56 on epoch=9
05/18/2022 17:57:15 - INFO - __main__ - Step 50 Global step 50 Train loss 1.29 on epoch=12
05/18/2022 17:57:16 - INFO - __main__ - Global step 50 Train loss 2.65 Classification-F1 0.1 on epoch=12
05/18/2022 17:57:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
05/18/2022 17:57:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=14
05/18/2022 17:57:19 - INFO - __main__ - Step 70 Global step 70 Train loss 1.12 on epoch=17
05/18/2022 17:57:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=19
05/18/2022 17:57:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=22
05/18/2022 17:57:23 - INFO - __main__ - Step 100 Global step 100 Train loss 1.15 on epoch=24
05/18/2022 17:57:24 - INFO - __main__ - Global step 100 Train loss 1.10 Classification-F1 0.23529411764705882 on epoch=24
05/18/2022 17:57:24 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.23529411764705882 on epoch=24, global_step=100
05/18/2022 17:57:26 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
05/18/2022 17:57:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.09 on epoch=29
05/18/2022 17:57:28 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=32
05/18/2022 17:57:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=34
05/18/2022 17:57:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
05/18/2022 17:57:32 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
05/18/2022 17:57:33 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=39
05/18/2022 17:57:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=42
05/18/2022 17:57:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
05/18/2022 17:57:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
05/18/2022 17:57:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
05/18/2022 17:57:40 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.09493670886075949 on epoch=49
05/18/2022 17:57:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
05/18/2022 17:57:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
05/18/2022 17:57:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
05/18/2022 17:57:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
05/18/2022 17:57:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=62
05/18/2022 17:57:49 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
05/18/2022 17:57:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=64
05/18/2022 17:57:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=67
05/18/2022 17:57:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=69
05/18/2022 17:57:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
05/18/2022 17:57:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
05/18/2022 17:57:58 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.30243161094224924 on epoch=74
05/18/2022 17:57:58 - INFO - __main__ - Saving model with best Classification-F1: 0.23529411764705882 -> 0.30243161094224924 on epoch=74, global_step=300
05/18/2022 17:57:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.85 on epoch=77
05/18/2022 17:58:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=79
05/18/2022 17:58:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
05/18/2022 17:58:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
05/18/2022 17:58:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
05/18/2022 17:58:05 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.25146713615023475 on epoch=87
05/18/2022 17:58:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
05/18/2022 17:58:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=92
05/18/2022 17:58:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=94
05/18/2022 17:58:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=97
05/18/2022 17:58:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=99
05/18/2022 17:58:14 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.3693926846100759 on epoch=99
05/18/2022 17:58:14 - INFO - __main__ - Saving model with best Classification-F1: 0.30243161094224924 -> 0.3693926846100759 on epoch=99, global_step=400
05/18/2022 17:58:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
05/18/2022 17:58:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
05/18/2022 17:58:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=107
05/18/2022 17:58:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.73 on epoch=109
05/18/2022 17:58:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=112
05/18/2022 17:58:22 - INFO - __main__ - Global step 450 Train loss 0.75 Classification-F1 0.5256011730205279 on epoch=112
05/18/2022 17:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3693926846100759 -> 0.5256011730205279 on epoch=112, global_step=450
05/18/2022 17:58:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=114
05/18/2022 17:58:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=117
05/18/2022 17:58:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=119
05/18/2022 17:58:29 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=122
05/18/2022 17:58:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=124
05/18/2022 17:58:31 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.43770146520146525 on epoch=124
05/18/2022 17:58:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=127
05/18/2022 17:58:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=129
05/18/2022 17:58:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=132
05/18/2022 17:58:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.59 on epoch=134
05/18/2022 17:58:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=137
05/18/2022 17:58:39 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6362503622138511 on epoch=137
05/18/2022 17:58:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5256011730205279 -> 0.6362503622138511 on epoch=137, global_step=550
05/18/2022 17:58:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=139
05/18/2022 17:58:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=142
05/18/2022 17:58:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
05/18/2022 17:58:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=147
05/18/2022 17:58:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=149
05/18/2022 17:58:48 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.5696218866950574 on epoch=149
05/18/2022 17:58:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
05/18/2022 17:58:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=154
05/18/2022 17:58:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=157
05/18/2022 17:58:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=159
05/18/2022 17:58:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=162
05/18/2022 17:58:56 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.5788206536845375 on epoch=162
05/18/2022 17:58:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=164
05/18/2022 17:58:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=167
05/18/2022 17:59:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=169
05/18/2022 17:59:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=172
05/18/2022 17:59:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=174
05/18/2022 17:59:05 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6240589198036006 on epoch=174
05/18/2022 17:59:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=177
05/18/2022 17:59:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=179
05/18/2022 17:59:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=182
05/18/2022 17:59:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=184
05/18/2022 17:59:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=187
05/18/2022 17:59:13 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.6534873188405798 on epoch=187
05/18/2022 17:59:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6362503622138511 -> 0.6534873188405798 on epoch=187, global_step=750
05/18/2022 17:59:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
05/18/2022 17:59:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
05/18/2022 17:59:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
05/18/2022 17:59:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
05/18/2022 17:59:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
05/18/2022 17:59:21 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.6971108326371485 on epoch=199
05/18/2022 17:59:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6534873188405798 -> 0.6971108326371485 on epoch=199, global_step=800
05/18/2022 17:59:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
05/18/2022 17:59:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
05/18/2022 17:59:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
05/18/2022 17:59:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
05/18/2022 17:59:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
05/18/2022 17:59:30 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.6976453726453726 on epoch=212
05/18/2022 17:59:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6971108326371485 -> 0.6976453726453726 on epoch=212, global_step=850
05/18/2022 17:59:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
05/18/2022 17:59:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
05/18/2022 17:59:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
05/18/2022 17:59:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
05/18/2022 17:59:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
05/18/2022 17:59:38 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.7237191898703055 on epoch=224
05/18/2022 17:59:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6976453726453726 -> 0.7237191898703055 on epoch=224, global_step=900
05/18/2022 17:59:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
05/18/2022 17:59:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
05/18/2022 17:59:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
05/18/2022 17:59:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
05/18/2022 17:59:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
05/18/2022 17:59:46 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.6448323323323324 on epoch=237
05/18/2022 17:59:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
05/18/2022 17:59:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
05/18/2022 17:59:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=244
05/18/2022 17:59:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
05/18/2022 17:59:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
05/18/2022 17:59:55 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.681699586333207 on epoch=249
05/18/2022 17:59:57 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
05/18/2022 17:59:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=254
05/18/2022 17:59:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
05/18/2022 18:00:01 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
05/18/2022 18:00:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
05/18/2022 18:00:03 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7413458245111471 on epoch=262
05/18/2022 18:00:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7237191898703055 -> 0.7413458245111471 on epoch=262, global_step=1050
05/18/2022 18:00:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
05/18/2022 18:00:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
05/18/2022 18:00:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
05/18/2022 18:00:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
05/18/2022 18:00:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
05/18/2022 18:00:12 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6960562240873698 on epoch=274
05/18/2022 18:00:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=277
05/18/2022 18:00:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=279
05/18/2022 18:00:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=282
05/18/2022 18:00:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=284
05/18/2022 18:00:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=287
05/18/2022 18:00:20 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.7172799422799423 on epoch=287
05/18/2022 18:00:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
05/18/2022 18:00:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=292
05/18/2022 18:00:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
05/18/2022 18:00:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
05/18/2022 18:00:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
05/18/2022 18:00:28 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.725094696969697 on epoch=299
05/18/2022 18:00:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
05/18/2022 18:00:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
05/18/2022 18:00:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
05/18/2022 18:00:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
05/18/2022 18:00:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
05/18/2022 18:00:36 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7251838235294118 on epoch=312
05/18/2022 18:00:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=314
05/18/2022 18:00:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
05/18/2022 18:00:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
05/18/2022 18:00:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
05/18/2022 18:00:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
05/18/2022 18:00:45 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.7730654761904763 on epoch=324
05/18/2022 18:00:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7413458245111471 -> 0.7730654761904763 on epoch=324, global_step=1300
05/18/2022 18:00:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
05/18/2022 18:00:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=329
05/18/2022 18:00:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
05/18/2022 18:00:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
05/18/2022 18:00:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
05/18/2022 18:00:54 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7094438188188188 on epoch=337
05/18/2022 18:00:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
05/18/2022 18:00:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
05/18/2022 18:00:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=344
05/18/2022 18:00:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
05/18/2022 18:01:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
05/18/2022 18:01:02 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.6929050638728058 on epoch=349
05/18/2022 18:01:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
05/18/2022 18:01:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
05/18/2022 18:01:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
05/18/2022 18:01:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
05/18/2022 18:01:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=362
05/18/2022 18:01:10 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7074674317617865 on epoch=362
05/18/2022 18:01:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
05/18/2022 18:01:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
05/18/2022 18:01:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
05/18/2022 18:01:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
05/18/2022 18:01:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
05/18/2022 18:01:18 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7343693693693694 on epoch=374
05/18/2022 18:01:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
05/18/2022 18:01:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
05/18/2022 18:01:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
05/18/2022 18:01:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
05/18/2022 18:01:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
05/18/2022 18:01:26 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7226088056680162 on epoch=387
05/18/2022 18:01:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
05/18/2022 18:01:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
05/18/2022 18:01:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=394
05/18/2022 18:01:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
05/18/2022 18:01:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
05/18/2022 18:01:34 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7359047202797202 on epoch=399
05/18/2022 18:01:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
05/18/2022 18:01:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
05/18/2022 18:01:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
05/18/2022 18:01:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
05/18/2022 18:01:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
05/18/2022 18:01:42 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.7080849141824751 on epoch=412
05/18/2022 18:01:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
05/18/2022 18:01:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
05/18/2022 18:01:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
05/18/2022 18:01:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
05/18/2022 18:01:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
05/18/2022 18:01:49 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7786585015237768 on epoch=424
05/18/2022 18:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7730654761904763 -> 0.7786585015237768 on epoch=424, global_step=1700
05/18/2022 18:01:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=427
05/18/2022 18:01:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
05/18/2022 18:01:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
05/18/2022 18:01:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
05/18/2022 18:01:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
05/18/2022 18:01:58 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.7598400650582814 on epoch=437
05/18/2022 18:01:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
05/18/2022 18:02:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/18/2022 18:02:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=444
05/18/2022 18:02:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
05/18/2022 18:02:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
05/18/2022 18:02:06 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.735356926799758 on epoch=449
05/18/2022 18:02:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=452
05/18/2022 18:02:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
05/18/2022 18:02:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
05/18/2022 18:02:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
05/18/2022 18:02:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
05/18/2022 18:02:14 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.730179403139555 on epoch=462
05/18/2022 18:02:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
05/18/2022 18:02:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
05/18/2022 18:02:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
05/18/2022 18:02:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
05/18/2022 18:02:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=474
05/18/2022 18:02:22 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.742189956297554 on epoch=474
05/18/2022 18:02:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
05/18/2022 18:02:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=479
05/18/2022 18:02:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
05/18/2022 18:02:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
05/18/2022 18:02:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
05/18/2022 18:02:30 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.7407874960057123 on epoch=487
05/18/2022 18:02:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
05/18/2022 18:02:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
05/18/2022 18:02:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
05/18/2022 18:02:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=497
05/18/2022 18:02:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=499
05/18/2022 18:02:38 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7222964943553178 on epoch=499
05/18/2022 18:02:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
05/18/2022 18:02:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
05/18/2022 18:02:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
05/18/2022 18:02:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
05/18/2022 18:02:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
05/18/2022 18:02:47 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7716310160427807 on epoch=512
05/18/2022 18:02:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/18/2022 18:02:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
05/18/2022 18:02:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=519
05/18/2022 18:02:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
05/18/2022 18:02:54 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
05/18/2022 18:02:54 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7567353330220977 on epoch=524
05/18/2022 18:02:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
05/18/2022 18:02:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=529
05/18/2022 18:02:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
05/18/2022 18:03:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
05/18/2022 18:03:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
05/18/2022 18:03:03 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.729488416988417 on epoch=537
05/18/2022 18:03:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
05/18/2022 18:03:06 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
05/18/2022 18:03:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
05/18/2022 18:03:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
05/18/2022 18:03:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=549
05/18/2022 18:03:11 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6863435110114428 on epoch=549
05/18/2022 18:03:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
05/18/2022 18:03:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
05/18/2022 18:03:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/18/2022 18:03:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/18/2022 18:03:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
05/18/2022 18:03:20 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7294976205277153 on epoch=562
05/18/2022 18:03:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/18/2022 18:03:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=567
05/18/2022 18:03:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
05/18/2022 18:03:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/18/2022 18:03:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
05/18/2022 18:03:29 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7078815397126592 on epoch=574
05/18/2022 18:03:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
05/18/2022 18:03:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=579
05/18/2022 18:03:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=582
05/18/2022 18:03:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
05/18/2022 18:03:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
05/18/2022 18:03:36 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7019628647214855 on epoch=587
05/18/2022 18:03:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
05/18/2022 18:03:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
05/18/2022 18:03:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
05/18/2022 18:03:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
05/18/2022 18:03:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
05/18/2022 18:03:45 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7261042003985553 on epoch=599
05/18/2022 18:03:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
05/18/2022 18:03:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
05/18/2022 18:03:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
05/18/2022 18:03:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
05/18/2022 18:03:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/18/2022 18:03:54 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.741014391014391 on epoch=612
05/18/2022 18:03:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
05/18/2022 18:03:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
05/18/2022 18:03:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/18/2022 18:04:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
05/18/2022 18:04:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
05/18/2022 18:04:02 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7287562532344316 on epoch=624
05/18/2022 18:04:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
05/18/2022 18:04:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
05/18/2022 18:04:07 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
05/18/2022 18:04:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
05/18/2022 18:04:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
05/18/2022 18:04:11 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7294372294372294 on epoch=637
05/18/2022 18:04:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 18:04:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/18/2022 18:04:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
05/18/2022 18:04:17 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
05/18/2022 18:04:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
05/18/2022 18:04:19 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7011429879076938 on epoch=649
05/18/2022 18:04:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/18/2022 18:04:22 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
05/18/2022 18:04:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
05/18/2022 18:04:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 18:04:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
05/18/2022 18:04:27 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7174789915966387 on epoch=662
05/18/2022 18:04:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
05/18/2022 18:04:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
05/18/2022 18:04:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=669
05/18/2022 18:04:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
05/18/2022 18:04:35 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/18/2022 18:04:36 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7144940476190477 on epoch=674
05/18/2022 18:04:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/18/2022 18:04:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
05/18/2022 18:04:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
05/18/2022 18:04:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
05/18/2022 18:04:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
05/18/2022 18:04:44 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7431245225362872 on epoch=687
05/18/2022 18:04:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
05/18/2022 18:04:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
05/18/2022 18:04:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
05/18/2022 18:04:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
05/18/2022 18:04:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
05/18/2022 18:04:52 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7222612691362691 on epoch=699
05/18/2022 18:04:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
05/18/2022 18:04:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
05/18/2022 18:04:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/18/2022 18:04:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/18/2022 18:04:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
05/18/2022 18:05:00 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7607071830653589 on epoch=712
05/18/2022 18:05:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=714
05/18/2022 18:05:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/18/2022 18:05:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
05/18/2022 18:05:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
05/18/2022 18:05:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
05/18/2022 18:05:09 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7327976190476191 on epoch=724
05/18/2022 18:05:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
05/18/2022 18:05:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
05/18/2022 18:05:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
05/18/2022 18:05:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/18/2022 18:05:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
05/18/2022 18:05:17 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7308712121212121 on epoch=737
05/18/2022 18:05:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
05/18/2022 18:05:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
05/18/2022 18:05:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/18/2022 18:05:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=747
05/18/2022 18:05:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
05/18/2022 18:05:25 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7497556207233625 on epoch=749
05/18/2022 18:05:25 - INFO - __main__ - save last model!
05/18/2022 18:05:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 18:05:25 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 18:05:25 - INFO - __main__ - Printing 3 examples
05/18/2022 18:05:25 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:05:25 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:05:25 - INFO - __main__ - Printing 3 examples
05/18/2022 18:05:25 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:05:25 - INFO - __main__ - ['others']
05/18/2022 18:05:25 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:05:25 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:05:26 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:05:26 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:05:26 - INFO - __main__ - Printing 3 examples
05/18/2022 18:05:26 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:05:26 - INFO - __main__ - ['others']
05/18/2022 18:05:26 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:05:26 - INFO - __main__ - ['others']
05/18/2022 18:05:26 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:05:26 - INFO - __main__ - ['others']
05/18/2022 18:05:26 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:05:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:05:26 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:05:27 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:05:31 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:05:31 - INFO - __main__ - task name: emo
05/18/2022 18:05:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:05:31 - INFO - __main__ - Starting training!
05/18/2022 18:05:33 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 18:06:50 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.5_8_predictions.txt
05/18/2022 18:06:50 - INFO - __main__ - Classification-F1 on test data: 0.5439
05/18/2022 18:06:50 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.7786585015237768, test_performance=0.5439438445845611
05/18/2022 18:06:50 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
05/18/2022 18:06:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:06:51 - INFO - __main__ - Printing 3 examples
05/18/2022 18:06:51 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:06:51 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:06:51 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:06:51 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:06:51 - INFO - __main__ - Printing 3 examples
05/18/2022 18:06:51 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:06:51 - INFO - __main__ - ['others']
05/18/2022 18:06:51 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:06:51 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:06:51 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:06:58 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:06:58 - INFO - __main__ - task name: emo
05/18/2022 18:06:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:06:58 - INFO - __main__ - Starting training!
05/18/2022 18:07:00 - INFO - __main__ - Step 10 Global step 10 Train loss 7.25 on epoch=2
05/18/2022 18:07:01 - INFO - __main__ - Step 20 Global step 20 Train loss 4.56 on epoch=4
05/18/2022 18:07:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=7
05/18/2022 18:07:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=9
05/18/2022 18:07:05 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
05/18/2022 18:07:06 - INFO - __main__ - Global step 50 Train loss 3.48 Classification-F1 0.16 on epoch=12
05/18/2022 18:07:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16 on epoch=12, global_step=50
05/18/2022 18:07:08 - INFO - __main__ - Step 60 Global step 60 Train loss 1.21 on epoch=14
05/18/2022 18:07:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=17
05/18/2022 18:07:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.19 on epoch=19
05/18/2022 18:07:12 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
05/18/2022 18:07:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.11 on epoch=24
05/18/2022 18:07:15 - INFO - __main__ - Global step 100 Train loss 1.15 Classification-F1 0.22863666014350945 on epoch=24
05/18/2022 18:07:15 - INFO - __main__ - Saving model with best Classification-F1: 0.16 -> 0.22863666014350945 on epoch=24, global_step=100
05/18/2022 18:07:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.10 on epoch=27
05/18/2022 18:07:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.10 on epoch=29
05/18/2022 18:07:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
05/18/2022 18:07:21 - INFO - __main__ - Step 140 Global step 140 Train loss 1.29 on epoch=34
05/18/2022 18:07:22 - INFO - __main__ - Step 150 Global step 150 Train loss 2.28 on epoch=37
05/18/2022 18:07:23 - INFO - __main__ - Global step 150 Train loss 1.33 Classification-F1 0.10126582278481013 on epoch=37
05/18/2022 18:07:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
05/18/2022 18:07:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
05/18/2022 18:07:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=44
05/18/2022 18:07:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
05/18/2022 18:07:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
05/18/2022 18:07:31 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=49
05/18/2022 18:07:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
05/18/2022 18:07:35 - INFO - __main__ - Step 220 Global step 220 Train loss 1.01 on epoch=54
05/18/2022 18:07:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
05/18/2022 18:07:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.96 on epoch=59
05/18/2022 18:07:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=62
05/18/2022 18:07:41 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.10126582278481013 on epoch=62
05/18/2022 18:07:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=64
05/18/2022 18:07:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=67
05/18/2022 18:07:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
05/18/2022 18:07:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
05/18/2022 18:07:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=74
05/18/2022 18:07:49 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=74
05/18/2022 18:07:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=77
05/18/2022 18:07:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.94 on epoch=79
05/18/2022 18:07:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
05/18/2022 18:07:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.97 on epoch=84
05/18/2022 18:07:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
05/18/2022 18:07:57 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.2413283248081841 on epoch=87
05/18/2022 18:07:57 - INFO - __main__ - Saving model with best Classification-F1: 0.22863666014350945 -> 0.2413283248081841 on epoch=87, global_step=350
05/18/2022 18:07:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
05/18/2022 18:08:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
05/18/2022 18:08:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
05/18/2022 18:08:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 18:08:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=99
05/18/2022 18:08:06 - INFO - __main__ - Global step 400 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=99
05/18/2022 18:08:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.89 on epoch=102
05/18/2022 18:08:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
05/18/2022 18:08:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
05/18/2022 18:08:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
05/18/2022 18:08:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
05/18/2022 18:08:14 - INFO - __main__ - Global step 450 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=112
05/18/2022 18:08:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
05/18/2022 18:08:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.99 on epoch=117
05/18/2022 18:08:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.91 on epoch=119
05/18/2022 18:08:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=122
05/18/2022 18:08:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=124
05/18/2022 18:08:23 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.13067758749069247 on epoch=124
05/18/2022 18:08:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=127
05/18/2022 18:08:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=129
05/18/2022 18:08:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
05/18/2022 18:08:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.88 on epoch=134
05/18/2022 18:08:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
05/18/2022 18:08:31 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=137
05/18/2022 18:08:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=139
05/18/2022 18:08:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.26 on epoch=142
05/18/2022 18:08:35 - INFO - __main__ - Step 580 Global step 580 Train loss 2.14 on epoch=144
05/18/2022 18:08:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.22 on epoch=147
05/18/2022 18:08:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.80 on epoch=149
05/18/2022 18:08:39 - INFO - __main__ - Global step 600 Train loss 1.25 Classification-F1 0.38642597058684447 on epoch=149
05/18/2022 18:08:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2413283248081841 -> 0.38642597058684447 on epoch=149, global_step=600
05/18/2022 18:08:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
05/18/2022 18:08:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
05/18/2022 18:08:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.83 on epoch=157
05/18/2022 18:08:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=159
05/18/2022 18:08:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.86 on epoch=162
05/18/2022 18:08:47 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.37962962962962965 on epoch=162
05/18/2022 18:08:48 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=164
05/18/2022 18:08:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
05/18/2022 18:08:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.88 on epoch=169
05/18/2022 18:08:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
05/18/2022 18:08:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
05/18/2022 18:08:56 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.2911686413695913 on epoch=174
05/18/2022 18:08:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.77 on epoch=177
05/18/2022 18:08:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=179
05/18/2022 18:09:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
05/18/2022 18:09:02 - INFO - __main__ - Step 740 Global step 740 Train loss 1.06 on epoch=184
05/18/2022 18:09:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
05/18/2022 18:09:05 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.28257853257853255 on epoch=187
05/18/2022 18:09:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.79 on epoch=189
05/18/2022 18:09:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
05/18/2022 18:09:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
05/18/2022 18:09:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
05/18/2022 18:09:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
05/18/2022 18:09:12 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.3539840249366842 on epoch=199
05/18/2022 18:09:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/18/2022 18:09:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.87 on epoch=204
05/18/2022 18:09:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.79 on epoch=207
05/18/2022 18:09:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
05/18/2022 18:09:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=212
05/18/2022 18:09:21 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.21405228758169936 on epoch=212
05/18/2022 18:09:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
05/18/2022 18:09:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
05/18/2022 18:09:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
05/18/2022 18:09:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=222
05/18/2022 18:09:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=224
05/18/2022 18:09:30 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.2984826932195353 on epoch=224
05/18/2022 18:09:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
05/18/2022 18:09:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
05/18/2022 18:09:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
05/18/2022 18:09:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
05/18/2022 18:09:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.77 on epoch=237
05/18/2022 18:09:38 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.19143413367942896 on epoch=237
05/18/2022 18:09:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.91 on epoch=239
05/18/2022 18:09:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=242
05/18/2022 18:09:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.80 on epoch=244
05/18/2022 18:09:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.91 on epoch=247
05/18/2022 18:09:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.90 on epoch=249
05/18/2022 18:09:47 - INFO - __main__ - Global step 1000 Train loss 0.86 Classification-F1 0.36345523329129886 on epoch=249
05/18/2022 18:09:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=252
05/18/2022 18:09:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
05/18/2022 18:09:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.92 on epoch=257
05/18/2022 18:09:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.79 on epoch=259
05/18/2022 18:09:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
05/18/2022 18:09:55 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.4168293404318249 on epoch=262
05/18/2022 18:09:55 - INFO - __main__ - Saving model with best Classification-F1: 0.38642597058684447 -> 0.4168293404318249 on epoch=262, global_step=1050
05/18/2022 18:09:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.86 on epoch=264
05/18/2022 18:09:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
05/18/2022 18:10:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
05/18/2022 18:10:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.84 on epoch=272
05/18/2022 18:10:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.78 on epoch=274
05/18/2022 18:10:04 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.4325233720180529 on epoch=274
05/18/2022 18:10:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4168293404318249 -> 0.4325233720180529 on epoch=274, global_step=1100
05/18/2022 18:10:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.75 on epoch=277
05/18/2022 18:10:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
05/18/2022 18:10:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.84 on epoch=282
05/18/2022 18:10:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.77 on epoch=284
05/18/2022 18:10:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=287
05/18/2022 18:10:13 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.407051282051282 on epoch=287
05/18/2022 18:10:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
05/18/2022 18:10:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=292
05/18/2022 18:10:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.76 on epoch=294
05/18/2022 18:10:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
05/18/2022 18:10:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.78 on epoch=299
05/18/2022 18:10:22 - INFO - __main__ - Global step 1200 Train loss 0.78 Classification-F1 0.27847782258064513 on epoch=299
05/18/2022 18:10:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=302
05/18/2022 18:10:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.75 on epoch=304
05/18/2022 18:10:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
05/18/2022 18:10:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.78 on epoch=309
05/18/2022 18:10:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.78 on epoch=312
05/18/2022 18:10:31 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.43126047520457456 on epoch=312
05/18/2022 18:10:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.83 on epoch=314
05/18/2022 18:10:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.78 on epoch=317
05/18/2022 18:10:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=319
05/18/2022 18:10:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.90 on epoch=322
05/18/2022 18:10:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.71 on epoch=324
05/18/2022 18:10:40 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3192307692307692 on epoch=324
05/18/2022 18:10:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.72 on epoch=327
05/18/2022 18:10:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=329
05/18/2022 18:10:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.75 on epoch=332
05/18/2022 18:10:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.81 on epoch=334
05/18/2022 18:10:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.81 on epoch=337
05/18/2022 18:10:49 - INFO - __main__ - Global step 1350 Train loss 0.77 Classification-F1 0.44395477368782277 on epoch=337
05/18/2022 18:10:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4325233720180529 -> 0.44395477368782277 on epoch=337, global_step=1350
05/18/2022 18:10:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=339
05/18/2022 18:10:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.79 on epoch=342
05/18/2022 18:10:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.69 on epoch=344
05/18/2022 18:10:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.77 on epoch=347
05/18/2022 18:10:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=349
05/18/2022 18:10:57 - INFO - __main__ - Global step 1400 Train loss 0.76 Classification-F1 0.5084768053562889 on epoch=349
05/18/2022 18:10:57 - INFO - __main__ - Saving model with best Classification-F1: 0.44395477368782277 -> 0.5084768053562889 on epoch=349, global_step=1400
05/18/2022 18:10:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
05/18/2022 18:11:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
05/18/2022 18:11:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.67 on epoch=357
05/18/2022 18:11:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=359
05/18/2022 18:11:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=362
05/18/2022 18:11:05 - INFO - __main__ - Global step 1450 Train loss 0.75 Classification-F1 0.5543263043263044 on epoch=362
05/18/2022 18:11:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5084768053562889 -> 0.5543263043263044 on epoch=362, global_step=1450
05/18/2022 18:11:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.67 on epoch=364
05/18/2022 18:11:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.69 on epoch=367
05/18/2022 18:11:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
05/18/2022 18:11:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.82 on epoch=372
05/18/2022 18:11:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=374
05/18/2022 18:11:14 - INFO - __main__ - Global step 1500 Train loss 0.72 Classification-F1 0.44948979591836735 on epoch=374
05/18/2022 18:11:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.65 on epoch=377
05/18/2022 18:11:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.72 on epoch=379
05/18/2022 18:11:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.63 on epoch=382
05/18/2022 18:11:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.63 on epoch=384
05/18/2022 18:11:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=387
05/18/2022 18:11:23 - INFO - __main__ - Global step 1550 Train loss 0.66 Classification-F1 0.6259834368530021 on epoch=387
05/18/2022 18:11:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5543263043263044 -> 0.6259834368530021 on epoch=387, global_step=1550
05/18/2022 18:11:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.64 on epoch=389
05/18/2022 18:11:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.71 on epoch=392
05/18/2022 18:11:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=394
05/18/2022 18:11:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=397
05/18/2022 18:11:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.62 on epoch=399
05/18/2022 18:11:31 - INFO - __main__ - Global step 1600 Train loss 0.69 Classification-F1 0.4357371794871795 on epoch=399
05/18/2022 18:11:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.63 on epoch=402
05/18/2022 18:11:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.62 on epoch=404
05/18/2022 18:11:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=407
05/18/2022 18:11:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.69 on epoch=409
05/18/2022 18:11:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=412
05/18/2022 18:11:39 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.6327606177606178 on epoch=412
05/18/2022 18:11:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6259834368530021 -> 0.6327606177606178 on epoch=412, global_step=1650
05/18/2022 18:11:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.62 on epoch=414
05/18/2022 18:11:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=417
05/18/2022 18:11:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=419
05/18/2022 18:11:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=422
05/18/2022 18:11:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.63 on epoch=424
05/18/2022 18:11:48 - INFO - __main__ - Global step 1700 Train loss 0.61 Classification-F1 0.4947163733500942 on epoch=424
05/18/2022 18:11:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=427
05/18/2022 18:11:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=429
05/18/2022 18:11:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=432
05/18/2022 18:11:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=434
05/18/2022 18:11:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=437
05/18/2022 18:11:56 - INFO - __main__ - Global step 1750 Train loss 0.51 Classification-F1 0.6671040778986805 on epoch=437
05/18/2022 18:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6327606177606178 -> 0.6671040778986805 on epoch=437, global_step=1750
05/18/2022 18:11:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=439
05/18/2022 18:12:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=442
05/18/2022 18:12:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=444
05/18/2022 18:12:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.54 on epoch=447
05/18/2022 18:12:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=449
05/18/2022 18:12:05 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.528545588778147 on epoch=449
05/18/2022 18:12:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.53 on epoch=452
05/18/2022 18:12:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.60 on epoch=454
05/18/2022 18:12:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=457
05/18/2022 18:12:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=459
05/18/2022 18:12:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=462
05/18/2022 18:12:13 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.6643907563025211 on epoch=462
05/18/2022 18:12:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=464
05/18/2022 18:12:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=467
05/18/2022 18:12:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=469
05/18/2022 18:12:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=472
05/18/2022 18:12:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=474
05/18/2022 18:12:22 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.4240777338603426 on epoch=474
05/18/2022 18:12:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=477
05/18/2022 18:12:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=479
05/18/2022 18:12:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=482
05/18/2022 18:12:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=484
05/18/2022 18:12:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=487
05/18/2022 18:12:30 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.6006599609540786 on epoch=487
05/18/2022 18:12:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=489
05/18/2022 18:12:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=492
05/18/2022 18:12:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=494
05/18/2022 18:12:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=497
05/18/2022 18:12:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=499
05/18/2022 18:12:39 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.6193924539512775 on epoch=499
05/18/2022 18:12:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=502
05/18/2022 18:12:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=504
05/18/2022 18:12:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=507
05/18/2022 18:12:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=509
05/18/2022 18:12:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=512
05/18/2022 18:12:47 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.7395988420181968 on epoch=512
05/18/2022 18:12:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6671040778986805 -> 0.7395988420181968 on epoch=512, global_step=2050
05/18/2022 18:12:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=514
05/18/2022 18:12:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=517
05/18/2022 18:12:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=519
05/18/2022 18:12:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.34 on epoch=522
05/18/2022 18:12:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=524
05/18/2022 18:12:56 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.6911196911196911 on epoch=524
05/18/2022 18:12:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=527
05/18/2022 18:12:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=529
05/18/2022 18:13:00 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=532
05/18/2022 18:13:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=534
05/18/2022 18:13:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=537
05/18/2022 18:13:04 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.5780812324929971 on epoch=537
05/18/2022 18:13:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=539
05/18/2022 18:13:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=542
05/18/2022 18:13:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=544
05/18/2022 18:13:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=547
05/18/2022 18:13:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=549
05/18/2022 18:13:12 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.5815820150268066 on epoch=549
05/18/2022 18:13:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=552
05/18/2022 18:13:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=554
05/18/2022 18:13:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=557
05/18/2022 18:13:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=559
05/18/2022 18:13:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=562
05/18/2022 18:13:20 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.6918529060369668 on epoch=562
05/18/2022 18:13:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=564
05/18/2022 18:13:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=567
05/18/2022 18:13:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=569
05/18/2022 18:13:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=572
05/18/2022 18:13:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=574
05/18/2022 18:13:29 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.5646213073538655 on epoch=574
05/18/2022 18:13:30 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=577
05/18/2022 18:13:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=579
05/18/2022 18:13:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=582
05/18/2022 18:13:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=584
05/18/2022 18:13:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=587
05/18/2022 18:13:37 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.7080126787557128 on epoch=587
05/18/2022 18:13:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=589
05/18/2022 18:13:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
05/18/2022 18:13:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=594
05/18/2022 18:13:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=597
05/18/2022 18:13:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=599
05/18/2022 18:13:44 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.7292178542178541 on epoch=599
05/18/2022 18:13:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=602
05/18/2022 18:13:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=604
05/18/2022 18:13:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
05/18/2022 18:13:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=609
05/18/2022 18:13:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
05/18/2022 18:13:52 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7457208927797163 on epoch=612
05/18/2022 18:13:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7395988420181968 -> 0.7457208927797163 on epoch=612, global_step=2450
05/18/2022 18:13:53 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=614
05/18/2022 18:13:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=617
05/18/2022 18:13:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=619
05/18/2022 18:13:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
05/18/2022 18:13:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=624
05/18/2022 18:14:00 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.7292690417690417 on epoch=624
05/18/2022 18:14:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
05/18/2022 18:14:02 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=629
05/18/2022 18:14:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=632
05/18/2022 18:14:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=634
05/18/2022 18:14:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=637
05/18/2022 18:14:07 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.7446969696969697 on epoch=637
05/18/2022 18:14:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=639
05/18/2022 18:14:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=642
05/18/2022 18:14:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=644
05/18/2022 18:14:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=647
05/18/2022 18:14:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=649
05/18/2022 18:14:15 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7442067736185383 on epoch=649
05/18/2022 18:14:17 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=652
05/18/2022 18:14:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=654
05/18/2022 18:14:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=657
05/18/2022 18:14:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=659
05/18/2022 18:14:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=662
05/18/2022 18:14:23 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.7044750914719955 on epoch=662
05/18/2022 18:14:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=664
05/18/2022 18:14:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=667
05/18/2022 18:14:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=669
05/18/2022 18:14:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
05/18/2022 18:14:31 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=674
05/18/2022 18:14:32 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.7217503217503218 on epoch=674
05/18/2022 18:14:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=677
05/18/2022 18:14:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.22 on epoch=679
05/18/2022 18:14:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=682
05/18/2022 18:14:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=684
05/18/2022 18:14:40 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=687
05/18/2022 18:14:41 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.7318295739348372 on epoch=687
05/18/2022 18:14:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
05/18/2022 18:14:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=692
05/18/2022 18:14:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=694
05/18/2022 18:14:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=697
05/18/2022 18:14:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=699
05/18/2022 18:14:49 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.7187028657616894 on epoch=699
05/18/2022 18:14:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=702
05/18/2022 18:14:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=704
05/18/2022 18:14:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
05/18/2022 18:14:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=709
05/18/2022 18:14:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
05/18/2022 18:14:58 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6897692091190544 on epoch=712
05/18/2022 18:14:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/18/2022 18:15:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=717
05/18/2022 18:15:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=719
05/18/2022 18:15:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=722
05/18/2022 18:15:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
05/18/2022 18:15:06 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7292178542178541 on epoch=724
05/18/2022 18:15:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
05/18/2022 18:15:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
05/18/2022 18:15:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=732
05/18/2022 18:15:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/18/2022 18:15:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=737
05/18/2022 18:15:15 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7062712062712062 on epoch=737
05/18/2022 18:15:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
05/18/2022 18:15:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=742
05/18/2022 18:15:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
05/18/2022 18:15:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
05/18/2022 18:15:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=749
05/18/2022 18:15:23 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:15:23 - INFO - __main__ - Printing 3 examples
05/18/2022 18:15:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:15:23 - INFO - __main__ - ['others']
05/18/2022 18:15:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:15:23 - INFO - __main__ - ['others']
05/18/2022 18:15:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:15:23 - INFO - __main__ - ['others']
05/18/2022 18:15:23 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:15:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:15:24 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:15:24 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:15:24 - INFO - __main__ - Printing 3 examples
05/18/2022 18:15:24 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:15:24 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:15:24 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7217503217503218 on epoch=749
05/18/2022 18:15:24 - INFO - __main__ - save last model!
05/18/2022 18:15:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 18:15:24 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 18:15:24 - INFO - __main__ - Printing 3 examples
05/18/2022 18:15:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 18:15:24 - INFO - __main__ - ['others']
05/18/2022 18:15:24 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:15:24 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:15:26 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:15:29 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:15:29 - INFO - __main__ - task name: emo
05/18/2022 18:15:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:15:29 - INFO - __main__ - Starting training!
05/18/2022 18:15:32 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 18:16:52 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.4_8_predictions.txt
05/18/2022 18:16:52 - INFO - __main__ - Classification-F1 on test data: 0.1554
05/18/2022 18:16:52 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7457208927797163, test_performance=0.15542408855152712
05/18/2022 18:16:53 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
05/18/2022 18:16:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:16:54 - INFO - __main__ - Printing 3 examples
05/18/2022 18:16:54 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:16:54 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:16:54 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:16:54 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:16:54 - INFO - __main__ - Printing 3 examples
05/18/2022 18:16:54 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:16:54 - INFO - __main__ - ['others']
05/18/2022 18:16:54 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:16:54 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:16:54 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:17:00 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:17:00 - INFO - __main__ - task name: emo
05/18/2022 18:17:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:17:01 - INFO - __main__ - Starting training!
05/18/2022 18:17:02 - INFO - __main__ - Step 10 Global step 10 Train loss 7.36 on epoch=2
05/18/2022 18:17:04 - INFO - __main__ - Step 20 Global step 20 Train loss 5.29 on epoch=4
05/18/2022 18:17:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=7
05/18/2022 18:17:07 - INFO - __main__ - Step 40 Global step 40 Train loss 2.14 on epoch=9
05/18/2022 18:17:08 - INFO - __main__ - Step 50 Global step 50 Train loss 2.09 on epoch=12
05/18/2022 18:17:09 - INFO - __main__ - Global step 50 Train loss 3.93 Classification-F1 0.15702087286527516 on epoch=12
05/18/2022 18:17:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15702087286527516 on epoch=12, global_step=50
05/18/2022 18:17:11 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=14
05/18/2022 18:17:12 - INFO - __main__ - Step 70 Global step 70 Train loss 1.67 on epoch=17
05/18/2022 18:17:13 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=19
05/18/2022 18:17:15 - INFO - __main__ - Step 90 Global step 90 Train loss 2.63 on epoch=22
05/18/2022 18:17:16 - INFO - __main__ - Step 100 Global step 100 Train loss 2.89 on epoch=24
05/18/2022 18:17:17 - INFO - __main__ - Global step 100 Train loss 2.04 Classification-F1 0.1 on epoch=24
05/18/2022 18:17:19 - INFO - __main__ - Step 110 Global step 110 Train loss 1.42 on epoch=27
05/18/2022 18:17:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.79 on epoch=29
05/18/2022 18:17:21 - INFO - __main__ - Step 130 Global step 130 Train loss 1.35 on epoch=32
05/18/2022 18:17:23 - INFO - __main__ - Step 140 Global step 140 Train loss 1.34 on epoch=34
05/18/2022 18:17:24 - INFO - __main__ - Step 150 Global step 150 Train loss 1.37 on epoch=37
05/18/2022 18:17:25 - INFO - __main__ - Global step 150 Train loss 1.45 Classification-F1 0.12368421052631579 on epoch=37
05/18/2022 18:17:26 - INFO - __main__ - Step 160 Global step 160 Train loss 1.21 on epoch=39
05/18/2022 18:17:28 - INFO - __main__ - Step 170 Global step 170 Train loss 1.25 on epoch=42
05/18/2022 18:17:30 - INFO - __main__ - Step 180 Global step 180 Train loss 1.20 on epoch=44
05/18/2022 18:17:31 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=47
05/18/2022 18:17:33 - INFO - __main__ - Step 200 Global step 200 Train loss 1.19 on epoch=49
05/18/2022 18:17:34 - INFO - __main__ - Global step 200 Train loss 1.19 Classification-F1 0.1 on epoch=49
05/18/2022 18:17:35 - INFO - __main__ - Step 210 Global step 210 Train loss 1.15 on epoch=52
05/18/2022 18:17:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.21 on epoch=54
05/18/2022 18:17:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.16 on epoch=57
05/18/2022 18:17:39 - INFO - __main__ - Step 240 Global step 240 Train loss 1.30 on epoch=59
05/18/2022 18:17:40 - INFO - __main__ - Step 250 Global step 250 Train loss 1.10 on epoch=62
05/18/2022 18:17:41 - INFO - __main__ - Global step 250 Train loss 1.18 Classification-F1 0.1457326892109501 on epoch=62
05/18/2022 18:17:43 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
05/18/2022 18:17:44 - INFO - __main__ - Step 270 Global step 270 Train loss 1.23 on epoch=67
05/18/2022 18:17:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.24 on epoch=69
05/18/2022 18:17:47 - INFO - __main__ - Step 290 Global step 290 Train loss 1.08 on epoch=72
05/18/2022 18:17:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.24 on epoch=74
05/18/2022 18:17:50 - INFO - __main__ - Global step 300 Train loss 1.17 Classification-F1 0.1796875 on epoch=74
05/18/2022 18:17:50 - INFO - __main__ - Saving model with best Classification-F1: 0.15702087286527516 -> 0.1796875 on epoch=74, global_step=300
05/18/2022 18:17:52 - INFO - __main__ - Step 310 Global step 310 Train loss 1.11 on epoch=77
05/18/2022 18:17:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
05/18/2022 18:17:55 - INFO - __main__ - Step 330 Global step 330 Train loss 1.07 on epoch=82
05/18/2022 18:17:56 - INFO - __main__ - Step 340 Global step 340 Train loss 1.06 on epoch=84
05/18/2022 18:17:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.08 on epoch=87
05/18/2022 18:17:58 - INFO - __main__ - Global step 350 Train loss 1.06 Classification-F1 0.267551133222775 on epoch=87
05/18/2022 18:17:58 - INFO - __main__ - Saving model with best Classification-F1: 0.1796875 -> 0.267551133222775 on epoch=87, global_step=350
05/18/2022 18:18:00 - INFO - __main__ - Step 360 Global step 360 Train loss 1.12 on epoch=89
05/18/2022 18:18:01 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=92
05/18/2022 18:18:03 - INFO - __main__ - Step 380 Global step 380 Train loss 1.03 on epoch=94
05/18/2022 18:18:04 - INFO - __main__ - Step 390 Global step 390 Train loss 1.03 on epoch=97
05/18/2022 18:18:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.98 on epoch=99
05/18/2022 18:18:07 - INFO - __main__ - Global step 400 Train loss 1.06 Classification-F1 0.1640625 on epoch=99
05/18/2022 18:18:08 - INFO - __main__ - Step 410 Global step 410 Train loss 1.10 on epoch=102
05/18/2022 18:18:10 - INFO - __main__ - Step 420 Global step 420 Train loss 1.02 on epoch=104
05/18/2022 18:18:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.96 on epoch=107
05/18/2022 18:18:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
05/18/2022 18:18:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
05/18/2022 18:18:15 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.1808979236547279 on epoch=112
05/18/2022 18:18:17 - INFO - __main__ - Step 460 Global step 460 Train loss 1.05 on epoch=114
05/18/2022 18:18:18 - INFO - __main__ - Step 470 Global step 470 Train loss 1.11 on epoch=117
05/18/2022 18:18:20 - INFO - __main__ - Step 480 Global step 480 Train loss 1.01 on epoch=119
05/18/2022 18:18:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
05/18/2022 18:18:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=124
05/18/2022 18:18:24 - INFO - __main__ - Global step 500 Train loss 1.01 Classification-F1 0.15 on epoch=124
05/18/2022 18:18:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=127
05/18/2022 18:18:27 - INFO - __main__ - Step 520 Global step 520 Train loss 1.01 on epoch=129
05/18/2022 18:18:28 - INFO - __main__ - Step 530 Global step 530 Train loss 1.06 on epoch=132
05/18/2022 18:18:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.99 on epoch=134
05/18/2022 18:18:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.14 on epoch=137
05/18/2022 18:18:32 - INFO - __main__ - Global step 550 Train loss 1.03 Classification-F1 0.3299220272904484 on epoch=137
05/18/2022 18:18:32 - INFO - __main__ - Saving model with best Classification-F1: 0.267551133222775 -> 0.3299220272904484 on epoch=137, global_step=550
05/18/2022 18:18:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
05/18/2022 18:18:34 - INFO - __main__ - Step 570 Global step 570 Train loss 1.01 on epoch=142
05/18/2022 18:18:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=144
05/18/2022 18:18:38 - INFO - __main__ - Step 590 Global step 590 Train loss 1.02 on epoch=147
05/18/2022 18:18:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.97 on epoch=149
05/18/2022 18:18:40 - INFO - __main__ - Global step 600 Train loss 0.96 Classification-F1 0.21124551971326164 on epoch=149
05/18/2022 18:18:41 - INFO - __main__ - Step 610 Global step 610 Train loss 1.02 on epoch=152
05/18/2022 18:18:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.90 on epoch=154
05/18/2022 18:18:44 - INFO - __main__ - Step 630 Global step 630 Train loss 1.01 on epoch=157
05/18/2022 18:18:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
05/18/2022 18:18:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.91 on epoch=162
05/18/2022 18:18:48 - INFO - __main__ - Global step 650 Train loss 0.95 Classification-F1 0.22306058164237053 on epoch=162
05/18/2022 18:18:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.86 on epoch=164
05/18/2022 18:18:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.95 on epoch=167
05/18/2022 18:18:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=169
05/18/2022 18:18:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.83 on epoch=172
05/18/2022 18:18:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.92 on epoch=174
05/18/2022 18:18:57 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.13067758749069247 on epoch=174
05/18/2022 18:18:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=177
05/18/2022 18:18:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
05/18/2022 18:19:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.87 on epoch=182
05/18/2022 18:19:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
05/18/2022 18:19:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.81 on epoch=187
05/18/2022 18:19:05 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.26006991944064445 on epoch=187
05/18/2022 18:19:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.94 on epoch=189
05/18/2022 18:19:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=192
05/18/2022 18:19:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
05/18/2022 18:19:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.92 on epoch=197
05/18/2022 18:19:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.89 on epoch=199
05/18/2022 18:19:13 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.17234848484848483 on epoch=199
05/18/2022 18:19:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
05/18/2022 18:19:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=204
05/18/2022 18:19:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.87 on epoch=207
05/18/2022 18:19:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
05/18/2022 18:19:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
05/18/2022 18:19:22 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.3576189096328186 on epoch=212
05/18/2022 18:19:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3299220272904484 -> 0.3576189096328186 on epoch=212, global_step=850
05/18/2022 18:19:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.87 on epoch=214
05/18/2022 18:19:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.92 on epoch=217
05/18/2022 18:19:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 18:19:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=222
05/18/2022 18:19:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.79 on epoch=224
05/18/2022 18:19:30 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.2861111111111111 on epoch=224
05/18/2022 18:19:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=227
05/18/2022 18:19:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
05/18/2022 18:19:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=232
05/18/2022 18:19:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
05/18/2022 18:19:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=237
05/18/2022 18:19:37 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.4451149425287356 on epoch=237
05/18/2022 18:19:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3576189096328186 -> 0.4451149425287356 on epoch=237, global_step=950
05/18/2022 18:19:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
05/18/2022 18:19:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.79 on epoch=242
05/18/2022 18:19:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.64 on epoch=244
05/18/2022 18:19:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.82 on epoch=247
05/18/2022 18:19:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.80 on epoch=249
05/18/2022 18:19:45 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.3641998844304812 on epoch=249
05/18/2022 18:19:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=252
05/18/2022 18:19:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=254
05/18/2022 18:19:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.70 on epoch=257
05/18/2022 18:19:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.73 on epoch=259
05/18/2022 18:19:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=262
05/18/2022 18:19:53 - INFO - __main__ - Global step 1050 Train loss 0.72 Classification-F1 0.2702508908230621 on epoch=262
05/18/2022 18:19:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.72 on epoch=264
05/18/2022 18:19:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.67 on epoch=267
05/18/2022 18:19:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=269
05/18/2022 18:19:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=272
05/18/2022 18:20:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=274
05/18/2022 18:20:01 - INFO - __main__ - Global step 1100 Train loss 0.71 Classification-F1 0.5492424242424242 on epoch=274
05/18/2022 18:20:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4451149425287356 -> 0.5492424242424242 on epoch=274, global_step=1100
05/18/2022 18:20:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=277
05/18/2022 18:20:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.56 on epoch=279
05/18/2022 18:20:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=282
05/18/2022 18:20:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.57 on epoch=284
05/18/2022 18:20:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=287
05/18/2022 18:20:10 - INFO - __main__ - Global step 1150 Train loss 0.59 Classification-F1 0.5808479532163742 on epoch=287
05/18/2022 18:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5492424242424242 -> 0.5808479532163742 on epoch=287, global_step=1150
05/18/2022 18:20:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.56 on epoch=289
05/18/2022 18:20:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.57 on epoch=292
05/18/2022 18:20:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.56 on epoch=294
05/18/2022 18:20:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=297
05/18/2022 18:20:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=299
05/18/2022 18:20:18 - INFO - __main__ - Global step 1200 Train loss 0.53 Classification-F1 0.6105462519936204 on epoch=299
05/18/2022 18:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5808479532163742 -> 0.6105462519936204 on epoch=299, global_step=1200
05/18/2022 18:20:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=302
05/18/2022 18:20:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=304
05/18/2022 18:20:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=307
05/18/2022 18:20:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=309
05/18/2022 18:20:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=312
05/18/2022 18:20:27 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.6128102615358987 on epoch=312
05/18/2022 18:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6105462519936204 -> 0.6128102615358987 on epoch=312, global_step=1250
05/18/2022 18:20:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=314
05/18/2022 18:20:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=317
05/18/2022 18:20:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=319
05/18/2022 18:20:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=322
05/18/2022 18:20:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=324
05/18/2022 18:20:35 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.6492436974789917 on epoch=324
05/18/2022 18:20:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6128102615358987 -> 0.6492436974789917 on epoch=324, global_step=1300
05/18/2022 18:20:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=327
05/18/2022 18:20:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=329
05/18/2022 18:20:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=332
05/18/2022 18:20:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=334
05/18/2022 18:20:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=337
05/18/2022 18:20:43 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.6534199522102747 on epoch=337
05/18/2022 18:20:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6492436974789917 -> 0.6534199522102747 on epoch=337, global_step=1350
05/18/2022 18:20:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=339
05/18/2022 18:20:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=342
05/18/2022 18:20:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=344
05/18/2022 18:20:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=347
05/18/2022 18:20:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=349
05/18/2022 18:20:51 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.638344634089315 on epoch=349
05/18/2022 18:20:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=352
05/18/2022 18:20:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=354
05/18/2022 18:20:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
05/18/2022 18:20:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=359
05/18/2022 18:20:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=362
05/18/2022 18:21:00 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.737296494355318 on epoch=362
05/18/2022 18:21:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6534199522102747 -> 0.737296494355318 on epoch=362, global_step=1450
05/18/2022 18:21:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=364
05/18/2022 18:21:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=367
05/18/2022 18:21:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=369
05/18/2022 18:21:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=372
05/18/2022 18:21:07 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=374
05/18/2022 18:21:08 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.6859806859806861 on epoch=374
05/18/2022 18:21:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=377
05/18/2022 18:21:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
05/18/2022 18:21:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=382
05/18/2022 18:21:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=384
05/18/2022 18:21:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=387
05/18/2022 18:21:16 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.6804061784897024 on epoch=387
05/18/2022 18:21:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=389
05/18/2022 18:21:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=392
05/18/2022 18:21:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=394
05/18/2022 18:21:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
05/18/2022 18:21:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
05/18/2022 18:21:24 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6937181746964356 on epoch=399
05/18/2022 18:21:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=402
05/18/2022 18:21:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=404
05/18/2022 18:21:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=407
05/18/2022 18:21:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
05/18/2022 18:21:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=412
05/18/2022 18:21:33 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.689766081871345 on epoch=412
05/18/2022 18:21:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=414
05/18/2022 18:21:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=417
05/18/2022 18:21:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
05/18/2022 18:21:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=422
05/18/2022 18:21:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=424
05/18/2022 18:21:41 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.7751277272968041 on epoch=424
05/18/2022 18:21:41 - INFO - __main__ - Saving model with best Classification-F1: 0.737296494355318 -> 0.7751277272968041 on epoch=424, global_step=1700
05/18/2022 18:21:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
05/18/2022 18:21:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=429
05/18/2022 18:21:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=432
05/18/2022 18:21:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=434
05/18/2022 18:21:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
05/18/2022 18:21:49 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7170412391000627 on epoch=437
05/18/2022 18:21:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
05/18/2022 18:21:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
05/18/2022 18:21:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
05/18/2022 18:21:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
05/18/2022 18:21:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
05/18/2022 18:21:58 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7457062677650913 on epoch=449
05/18/2022 18:22:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
05/18/2022 18:22:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
05/18/2022 18:22:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=457
05/18/2022 18:22:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
05/18/2022 18:22:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
05/18/2022 18:22:07 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7518551587301587 on epoch=462
05/18/2022 18:22:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
05/18/2022 18:22:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
05/18/2022 18:22:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
05/18/2022 18:22:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
05/18/2022 18:22:14 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
05/18/2022 18:22:15 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.7515697515697515 on epoch=474
05/18/2022 18:22:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
05/18/2022 18:22:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
05/18/2022 18:22:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
05/18/2022 18:22:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
05/18/2022 18:22:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
05/18/2022 18:22:23 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.724159126671912 on epoch=487
05/18/2022 18:22:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
05/18/2022 18:22:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
05/18/2022 18:22:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
05/18/2022 18:22:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=497
05/18/2022 18:22:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
05/18/2022 18:22:32 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7371970527341373 on epoch=499
05/18/2022 18:22:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
05/18/2022 18:22:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
05/18/2022 18:22:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
05/18/2022 18:22:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
05/18/2022 18:22:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/18/2022 18:22:40 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6777177993210601 on epoch=512
05/18/2022 18:22:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
05/18/2022 18:22:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
05/18/2022 18:22:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=519
05/18/2022 18:22:46 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=522
05/18/2022 18:22:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
05/18/2022 18:22:49 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7220740178742603 on epoch=524
05/18/2022 18:22:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
05/18/2022 18:22:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
05/18/2022 18:22:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=532
05/18/2022 18:22:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=534
05/18/2022 18:22:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
05/18/2022 18:22:57 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7096495472454551 on epoch=537
05/18/2022 18:22:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
05/18/2022 18:23:00 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=542
05/18/2022 18:23:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
05/18/2022 18:23:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
05/18/2022 18:23:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
05/18/2022 18:23:06 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7059698995182866 on epoch=549
05/18/2022 18:23:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=552
05/18/2022 18:23:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
05/18/2022 18:23:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
05/18/2022 18:23:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
05/18/2022 18:23:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
05/18/2022 18:23:13 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.7364182364182363 on epoch=562
05/18/2022 18:23:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
05/18/2022 18:23:16 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
05/18/2022 18:23:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
05/18/2022 18:23:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
05/18/2022 18:23:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
05/18/2022 18:23:22 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.709022449559534 on epoch=574
05/18/2022 18:23:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
05/18/2022 18:23:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
05/18/2022 18:23:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
05/18/2022 18:23:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
05/18/2022 18:23:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
05/18/2022 18:23:29 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7235946420729029 on epoch=587
05/18/2022 18:23:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
05/18/2022 18:23:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=592
05/18/2022 18:23:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=594
05/18/2022 18:23:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=597
05/18/2022 18:23:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
05/18/2022 18:23:38 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.717296494355318 on epoch=599
05/18/2022 18:23:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
05/18/2022 18:23:40 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
05/18/2022 18:23:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
05/18/2022 18:23:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/18/2022 18:23:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
05/18/2022 18:23:46 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7450980392156864 on epoch=612
05/18/2022 18:23:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
05/18/2022 18:23:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
05/18/2022 18:23:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
05/18/2022 18:23:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
05/18/2022 18:23:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/18/2022 18:23:54 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7673611111111112 on epoch=624
05/18/2022 18:23:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=627
05/18/2022 18:23:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
05/18/2022 18:23:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
05/18/2022 18:24:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
05/18/2022 18:24:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
05/18/2022 18:24:02 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7096495472454551 on epoch=637
05/18/2022 18:24:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
05/18/2022 18:24:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
05/18/2022 18:24:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
05/18/2022 18:24:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
05/18/2022 18:24:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/18/2022 18:24:10 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7591431556948799 on epoch=649
05/18/2022 18:24:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
05/18/2022 18:24:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=654
05/18/2022 18:24:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
05/18/2022 18:24:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
05/18/2022 18:24:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=662
05/18/2022 18:24:18 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7589361831689198 on epoch=662
05/18/2022 18:24:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
05/18/2022 18:24:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
05/18/2022 18:24:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=669
05/18/2022 18:24:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
05/18/2022 18:24:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=674
05/18/2022 18:24:27 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7588638650930086 on epoch=674
05/18/2022 18:24:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
05/18/2022 18:24:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
05/18/2022 18:24:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
05/18/2022 18:24:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
05/18/2022 18:24:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
05/18/2022 18:24:35 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7595444117183248 on epoch=687
05/18/2022 18:24:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
05/18/2022 18:24:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
05/18/2022 18:24:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
05/18/2022 18:24:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/18/2022 18:24:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=699
05/18/2022 18:24:44 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7303030303030303 on epoch=699
05/18/2022 18:24:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
05/18/2022 18:24:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
05/18/2022 18:24:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/18/2022 18:24:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
05/18/2022 18:24:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
05/18/2022 18:24:52 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7512554112554113 on epoch=712
05/18/2022 18:24:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
05/18/2022 18:24:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
05/18/2022 18:24:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=719
05/18/2022 18:24:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
05/18/2022 18:25:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
05/18/2022 18:25:01 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.731764705882353 on epoch=724
05/18/2022 18:25:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
05/18/2022 18:25:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
05/18/2022 18:25:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
05/18/2022 18:25:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
05/18/2022 18:25:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
05/18/2022 18:25:09 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7311853832442069 on epoch=737
05/18/2022 18:25:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
05/18/2022 18:25:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
05/18/2022 18:25:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/18/2022 18:25:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
05/18/2022 18:25:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
05/18/2022 18:25:18 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7075155369273016 on epoch=749
05/18/2022 18:25:18 - INFO - __main__ - save last model!
05/18/2022 18:25:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 18:25:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 18:25:18 - INFO - __main__ - Printing 3 examples
05/18/2022 18:25:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:25:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:25:18 - INFO - __main__ - Printing 3 examples
05/18/2022 18:25:18 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:25:18 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:25:18 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:25:18 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:25:18 - INFO - __main__ - Printing 3 examples
05/18/2022 18:25:18 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:25:18 - INFO - __main__ - ['others']
05/18/2022 18:25:18 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:25:18 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:25:18 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:25:20 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:25:25 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:25:25 - INFO - __main__ - task name: emo
05/18/2022 18:25:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:25:25 - INFO - __main__ - Starting training!
05/18/2022 18:25:26 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 18:26:42 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.3_8_predictions.txt
05/18/2022 18:26:42 - INFO - __main__ - Classification-F1 on test data: 0.4074
05/18/2022 18:26:42 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7751277272968041, test_performance=0.4074195859958414
05/18/2022 18:26:42 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
05/18/2022 18:26:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:26:43 - INFO - __main__ - Printing 3 examples
05/18/2022 18:26:43 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:26:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:26:43 - INFO - __main__ - Loaded 64 examples from train data
05/18/2022 18:26:43 - INFO - __main__ - Start tokenizing ... 64 instances
05/18/2022 18:26:43 - INFO - __main__ - Printing 3 examples
05/18/2022 18:26:43 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
05/18/2022 18:26:43 - INFO - __main__ - ['others']
05/18/2022 18:26:43 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:26:43 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:26:43 - INFO - __main__ - Loaded 64 examples from dev data
05/18/2022 18:26:49 - INFO - __main__ - try to initialize prompt embeddings
05/18/2022 18:26:49 - INFO - __main__ - task name: emo
05/18/2022 18:26:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
05/18/2022 18:26:50 - INFO - __main__ - Starting training!
05/18/2022 18:26:52 - INFO - __main__ - Step 10 Global step 10 Train loss 7.26 on epoch=2
05/18/2022 18:26:54 - INFO - __main__ - Step 20 Global step 20 Train loss 5.00 on epoch=4
05/18/2022 18:26:56 - INFO - __main__ - Step 30 Global step 30 Train loss 3.37 on epoch=7
05/18/2022 18:26:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
05/18/2022 18:26:59 - INFO - __main__ - Step 50 Global step 50 Train loss 2.16 on epoch=12
05/18/2022 18:27:00 - INFO - __main__ - Global step 50 Train loss 4.07 Classification-F1 0.20782188214039002 on epoch=12
05/18/2022 18:27:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20782188214039002 on epoch=12, global_step=50
05/18/2022 18:27:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.85 on epoch=14
05/18/2022 18:27:04 - INFO - __main__ - Step 70 Global step 70 Train loss 1.75 on epoch=17
05/18/2022 18:27:06 - INFO - __main__ - Step 80 Global step 80 Train loss 1.61 on epoch=19
05/18/2022 18:27:07 - INFO - __main__ - Step 90 Global step 90 Train loss 1.66 on epoch=22
05/18/2022 18:27:09 - INFO - __main__ - Step 100 Global step 100 Train loss 1.55 on epoch=24
05/18/2022 18:27:10 - INFO - __main__ - Global step 100 Train loss 1.69 Classification-F1 0.2306338028169014 on epoch=24
05/18/2022 18:27:10 - INFO - __main__ - Saving model with best Classification-F1: 0.20782188214039002 -> 0.2306338028169014 on epoch=24, global_step=100
05/18/2022 18:27:11 - INFO - __main__ - Step 110 Global step 110 Train loss 1.38 on epoch=27
05/18/2022 18:27:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.42 on epoch=29
05/18/2022 18:27:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.38 on epoch=32
05/18/2022 18:27:16 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=34
05/18/2022 18:27:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.33 on epoch=37
05/18/2022 18:27:18 - INFO - __main__ - Global step 150 Train loss 1.34 Classification-F1 0.13347763347763347 on epoch=37
05/18/2022 18:27:19 - INFO - __main__ - Step 160 Global step 160 Train loss 1.18 on epoch=39
05/18/2022 18:27:21 - INFO - __main__ - Step 170 Global step 170 Train loss 1.10 on epoch=42
05/18/2022 18:27:22 - INFO - __main__ - Step 180 Global step 180 Train loss 1.15 on epoch=44
05/18/2022 18:27:24 - INFO - __main__ - Step 190 Global step 190 Train loss 1.17 on epoch=47
05/18/2022 18:27:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.06 on epoch=49
05/18/2022 18:27:26 - INFO - __main__ - Global step 200 Train loss 1.13 Classification-F1 0.3577061259286323 on epoch=49
05/18/2022 18:27:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2306338028169014 -> 0.3577061259286323 on epoch=49, global_step=200
05/18/2022 18:27:28 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=52
05/18/2022 18:27:29 - INFO - __main__ - Step 220 Global step 220 Train loss 1.04 on epoch=54
05/18/2022 18:27:31 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
05/18/2022 18:27:32 - INFO - __main__ - Step 240 Global step 240 Train loss 1.04 on epoch=59
05/18/2022 18:27:34 - INFO - __main__ - Step 250 Global step 250 Train loss 1.07 on epoch=62
05/18/2022 18:27:35 - INFO - __main__ - Global step 250 Train loss 1.07 Classification-F1 0.4106176414120187 on epoch=62
05/18/2022 18:27:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3577061259286323 -> 0.4106176414120187 on epoch=62, global_step=250
05/18/2022 18:27:36 - INFO - __main__ - Step 260 Global step 260 Train loss 1.04 on epoch=64
05/18/2022 18:27:37 - INFO - __main__ - Step 270 Global step 270 Train loss 1.09 on epoch=67
05/18/2022 18:27:39 - INFO - __main__ - Step 280 Global step 280 Train loss 1.05 on epoch=69
05/18/2022 18:27:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
05/18/2022 18:27:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
05/18/2022 18:27:43 - INFO - __main__ - Global step 300 Train loss 0.99 Classification-F1 0.42694805194805197 on epoch=74
05/18/2022 18:27:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4106176414120187 -> 0.42694805194805197 on epoch=74, global_step=300
05/18/2022 18:27:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
05/18/2022 18:27:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=79
05/18/2022 18:27:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
05/18/2022 18:27:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
05/18/2022 18:27:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=87
05/18/2022 18:27:51 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.4998961183743792 on epoch=87
05/18/2022 18:27:51 - INFO - __main__ - Saving model with best Classification-F1: 0.42694805194805197 -> 0.4998961183743792 on epoch=87, global_step=350
05/18/2022 18:27:52 - INFO - __main__ - Step 360 Global step 360 Train loss 1.01 on epoch=89
05/18/2022 18:27:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
05/18/2022 18:27:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=94
05/18/2022 18:27:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
05/18/2022 18:27:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
05/18/2022 18:27:59 - INFO - __main__ - Global step 400 Train loss 0.91 Classification-F1 0.32757021727609964 on epoch=99
05/18/2022 18:28:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=102
05/18/2022 18:28:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=104
05/18/2022 18:28:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
05/18/2022 18:28:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.98 on epoch=109
05/18/2022 18:28:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.94 on epoch=112
05/18/2022 18:28:08 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.411144883485309 on epoch=112
05/18/2022 18:28:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=114
05/18/2022 18:28:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
05/18/2022 18:28:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.88 on epoch=119
05/18/2022 18:28:13 - INFO - __main__ - Step 490 Global step 490 Train loss 1.08 on epoch=122
05/18/2022 18:28:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.78 on epoch=124
05/18/2022 18:28:15 - INFO - __main__ - Global step 500 Train loss 0.89 Classification-F1 0.43686868686868685 on epoch=124
05/18/2022 18:28:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
05/18/2022 18:28:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=129
05/18/2022 18:28:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
05/18/2022 18:28:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
05/18/2022 18:28:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.87 on epoch=137
05/18/2022 18:28:23 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.5263736263736264 on epoch=137
05/18/2022 18:28:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4998961183743792 -> 0.5263736263736264 on epoch=137, global_step=550
05/18/2022 18:28:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.82 on epoch=139
05/18/2022 18:28:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
05/18/2022 18:28:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
05/18/2022 18:28:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
05/18/2022 18:28:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.88 on epoch=149
05/18/2022 18:28:31 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.42036474164133736 on epoch=149
05/18/2022 18:28:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
05/18/2022 18:28:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
05/18/2022 18:28:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=157
05/18/2022 18:28:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=159
05/18/2022 18:28:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.81 on epoch=162
05/18/2022 18:28:39 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.5959549411162315 on epoch=162
05/18/2022 18:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5263736263736264 -> 0.5959549411162315 on epoch=162, global_step=650
05/18/2022 18:28:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.92 on epoch=164
05/18/2022 18:28:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=167
05/18/2022 18:28:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.80 on epoch=169
05/18/2022 18:28:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
05/18/2022 18:28:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
05/18/2022 18:28:47 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3660749751737835 on epoch=174
05/18/2022 18:28:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.81 on epoch=177
05/18/2022 18:28:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
05/18/2022 18:28:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.77 on epoch=182
05/18/2022 18:28:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.85 on epoch=184
05/18/2022 18:28:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=187
05/18/2022 18:28:56 - INFO - __main__ - Global step 750 Train loss 0.79 Classification-F1 0.5951576576576576 on epoch=187
05/18/2022 18:28:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.79 on epoch=189
05/18/2022 18:28:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.85 on epoch=192
05/18/2022 18:29:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=194
05/18/2022 18:29:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.75 on epoch=197
05/18/2022 18:29:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
05/18/2022 18:29:04 - INFO - __main__ - Global step 800 Train loss 0.78 Classification-F1 0.4294349128540305 on epoch=199
05/18/2022 18:29:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=202
05/18/2022 18:29:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.74 on epoch=204
05/18/2022 18:29:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
05/18/2022 18:29:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.87 on epoch=209
05/18/2022 18:29:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
05/18/2022 18:29:13 - INFO - __main__ - Global step 850 Train loss 0.77 Classification-F1 0.5567890701431074 on epoch=212
05/18/2022 18:29:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
05/18/2022 18:29:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=217
05/18/2022 18:29:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
05/18/2022 18:29:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
05/18/2022 18:29:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.71 on epoch=224
05/18/2022 18:29:22 - INFO - __main__ - Global step 900 Train loss 0.75 Classification-F1 0.4935483870967742 on epoch=224
05/18/2022 18:29:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.67 on epoch=227
05/18/2022 18:29:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.68 on epoch=229
05/18/2022 18:29:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=232
05/18/2022 18:29:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.72 on epoch=234
05/18/2022 18:29:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=237
05/18/2022 18:29:30 - INFO - __main__ - Global step 950 Train loss 0.70 Classification-F1 0.6682961460446248 on epoch=237
05/18/2022 18:29:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5959549411162315 -> 0.6682961460446248 on epoch=237, global_step=950
05/18/2022 18:29:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.57 on epoch=239
05/18/2022 18:29:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.69 on epoch=242
05/18/2022 18:29:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.67 on epoch=244
05/18/2022 18:29:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.60 on epoch=247
05/18/2022 18:29:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=249
05/18/2022 18:29:38 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.5476312741222209 on epoch=249
05/18/2022 18:29:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=252
05/18/2022 18:29:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.64 on epoch=254
05/18/2022 18:29:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=257
05/18/2022 18:29:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.57 on epoch=259
05/18/2022 18:29:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=262
05/18/2022 18:29:46 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.5992052635007491 on epoch=262
05/18/2022 18:29:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=264
05/18/2022 18:29:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.56 on epoch=267
05/18/2022 18:29:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
05/18/2022 18:29:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=272
05/18/2022 18:29:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=274
05/18/2022 18:29:55 - INFO - __main__ - Global step 1100 Train loss 0.55 Classification-F1 0.5926767676767677 on epoch=274
05/18/2022 18:29:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=277
05/18/2022 18:29:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=279
05/18/2022 18:30:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=282
05/18/2022 18:30:02 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=284
05/18/2022 18:30:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=287
05/18/2022 18:30:04 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.6741119431643625 on epoch=287
05/18/2022 18:30:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6682961460446248 -> 0.6741119431643625 on epoch=287, global_step=1150
05/18/2022 18:30:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=289
05/18/2022 18:30:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=292
05/18/2022 18:30:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=294
05/18/2022 18:30:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=297
05/18/2022 18:30:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=299
05/18/2022 18:30:13 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.5976049159348661 on epoch=299
05/18/2022 18:30:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=302
05/18/2022 18:30:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=304
05/18/2022 18:30:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=307
05/18/2022 18:30:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=309
05/18/2022 18:30:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=312
05/18/2022 18:30:21 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.71829086473299 on epoch=312
05/18/2022 18:30:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6741119431643625 -> 0.71829086473299 on epoch=312, global_step=1250
05/18/2022 18:30:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=314
05/18/2022 18:30:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=317
05/18/2022 18:30:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=319
05/18/2022 18:30:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=322
05/18/2022 18:30:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=324
05/18/2022 18:30:29 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.6756823659696437 on epoch=324
05/18/2022 18:30:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=327
05/18/2022 18:30:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=329
05/18/2022 18:30:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=332
05/18/2022 18:30:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=334
05/18/2022 18:30:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=337
05/18/2022 18:30:37 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.6813431824342641 on epoch=337
05/18/2022 18:30:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=339
05/18/2022 18:30:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=342
05/18/2022 18:30:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=344
05/18/2022 18:30:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=347
05/18/2022 18:30:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=349
05/18/2022 18:30:46 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.6535557184750733 on epoch=349
05/18/2022 18:30:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=352
05/18/2022 18:30:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=354
05/18/2022 18:30:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
05/18/2022 18:30:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=359
05/18/2022 18:30:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=362
05/18/2022 18:30:53 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.6860906862745099 on epoch=362
05/18/2022 18:30:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=364
05/18/2022 18:30:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=367
05/18/2022 18:30:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=369
05/18/2022 18:30:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=372
05/18/2022 18:31:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=374
05/18/2022 18:31:02 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.7019327731092437 on epoch=374
05/18/2022 18:31:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=377
05/18/2022 18:31:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=379
05/18/2022 18:31:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=382
05/18/2022 18:31:08 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=384
05/18/2022 18:31:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=387
05/18/2022 18:31:11 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.6752822341057635 on epoch=387
05/18/2022 18:31:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=389
05/18/2022 18:31:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=392
05/18/2022 18:31:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=394
05/18/2022 18:31:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=397
05/18/2022 18:31:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=399
05/18/2022 18:31:19 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.6736479207067443 on epoch=399
05/18/2022 18:31:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=402
05/18/2022 18:31:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=404
05/18/2022 18:31:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=407
05/18/2022 18:31:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=409
05/18/2022 18:31:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=412
05/18/2022 18:31:27 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.6859106314848313 on epoch=412
05/18/2022 18:31:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=414
05/18/2022 18:31:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=417
05/18/2022 18:31:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
05/18/2022 18:31:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=422
05/18/2022 18:31:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=424
05/18/2022 18:31:36 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7572979512317748 on epoch=424
05/18/2022 18:31:36 - INFO - __main__ - Saving model with best Classification-F1: 0.71829086473299 -> 0.7572979512317748 on epoch=424, global_step=1700
05/18/2022 18:31:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=427
05/18/2022 18:31:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=429
05/18/2022 18:31:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=432
05/18/2022 18:31:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=434
05/18/2022 18:31:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=437
05/18/2022 18:31:44 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.7012364903885817 on epoch=437
05/18/2022 18:31:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=439
05/18/2022 18:31:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=442
05/18/2022 18:31:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=444
05/18/2022 18:31:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=447
05/18/2022 18:31:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=449
05/18/2022 18:31:52 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.6809207553989338 on epoch=449
05/18/2022 18:31:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=452
05/18/2022 18:31:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
05/18/2022 18:31:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=457
05/18/2022 18:31:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=459
05/18/2022 18:32:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=462
05/18/2022 18:32:01 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7431607744107743 on epoch=462
05/18/2022 18:32:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=464
05/18/2022 18:32:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=467
05/18/2022 18:32:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=469
05/18/2022 18:32:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=472
05/18/2022 18:32:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=474
05/18/2022 18:32:10 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.6795093795093795 on epoch=474
05/18/2022 18:32:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=477
05/18/2022 18:32:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=479
05/18/2022 18:32:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=482
05/18/2022 18:32:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
05/18/2022 18:32:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
05/18/2022 18:32:17 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7524480095068331 on epoch=487
05/18/2022 18:32:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
05/18/2022 18:32:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=492
05/18/2022 18:32:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
05/18/2022 18:32:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
05/18/2022 18:32:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
05/18/2022 18:32:26 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.716017316017316 on epoch=499
05/18/2022 18:32:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=502
05/18/2022 18:32:29 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=504
05/18/2022 18:32:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=507
05/18/2022 18:32:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
05/18/2022 18:32:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
05/18/2022 18:32:34 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.6656318827371459 on epoch=512
05/18/2022 18:32:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=514
05/18/2022 18:32:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=517
05/18/2022 18:32:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
05/18/2022 18:32:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=522
05/18/2022 18:32:41 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
05/18/2022 18:32:42 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.6837218337218337 on epoch=524
05/18/2022 18:32:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
05/18/2022 18:32:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
05/18/2022 18:32:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=532
05/18/2022 18:32:48 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=534
05/18/2022 18:32:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
05/18/2022 18:32:51 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7397937710437711 on epoch=537
05/18/2022 18:32:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=539
05/18/2022 18:32:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
05/18/2022 18:32:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
05/18/2022 18:32:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=547
05/18/2022 18:32:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=549
05/18/2022 18:32:59 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7450472942800308 on epoch=549
05/18/2022 18:33:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=552
05/18/2022 18:33:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=554
05/18/2022 18:33:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
05/18/2022 18:33:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
05/18/2022 18:33:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=562
05/18/2022 18:33:07 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7450472942800308 on epoch=562
05/18/2022 18:33:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
05/18/2022 18:33:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
05/18/2022 18:33:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
05/18/2022 18:33:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
05/18/2022 18:33:14 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
05/18/2022 18:33:16 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7315656565656565 on epoch=574
05/18/2022 18:33:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
05/18/2022 18:33:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=579
05/18/2022 18:33:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
05/18/2022 18:33:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
05/18/2022 18:33:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
05/18/2022 18:33:25 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.7177421271538919 on epoch=587
05/18/2022 18:33:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=589
05/18/2022 18:33:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
05/18/2022 18:33:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=594
05/18/2022 18:33:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
05/18/2022 18:33:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
05/18/2022 18:33:33 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7704650673400674 on epoch=599
05/18/2022 18:33:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7572979512317748 -> 0.7704650673400674 on epoch=599, global_step=2400
05/18/2022 18:33:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=602
05/18/2022 18:33:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
05/18/2022 18:33:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=607
05/18/2022 18:33:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
05/18/2022 18:33:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
05/18/2022 18:33:42 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.743462451595058 on epoch=612
05/18/2022 18:33:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
05/18/2022 18:33:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=617
05/18/2022 18:33:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
05/18/2022 18:33:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
05/18/2022 18:33:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
05/18/2022 18:33:50 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7526455026455026 on epoch=624
05/18/2022 18:33:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
05/18/2022 18:33:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=629
05/18/2022 18:33:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
05/18/2022 18:33:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
05/18/2022 18:33:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=637
05/18/2022 18:33:58 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7313886776802377 on epoch=637
05/18/2022 18:34:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
05/18/2022 18:34:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=642
05/18/2022 18:34:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
05/18/2022 18:34:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
05/18/2022 18:34:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
05/18/2022 18:34:07 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7512955182072829 on epoch=649
05/18/2022 18:34:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
05/18/2022 18:34:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
05/18/2022 18:34:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
05/18/2022 18:34:13 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=659
05/18/2022 18:34:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
05/18/2022 18:34:15 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.7450472942800308 on epoch=662
05/18/2022 18:34:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
05/18/2022 18:34:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=667
05/18/2022 18:34:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
05/18/2022 18:34:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=672
05/18/2022 18:34:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
05/18/2022 18:34:24 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7182800982800983 on epoch=674
05/18/2022 18:34:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
05/18/2022 18:34:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=679
05/18/2022 18:34:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=682
05/18/2022 18:34:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
05/18/2022 18:34:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
05/18/2022 18:34:32 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7031106148753209 on epoch=687
05/18/2022 18:34:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
05/18/2022 18:34:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
05/18/2022 18:34:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
05/18/2022 18:34:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
05/18/2022 18:34:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
05/18/2022 18:34:40 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7524480095068331 on epoch=699
05/18/2022 18:34:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
05/18/2022 18:34:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=704
05/18/2022 18:34:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
05/18/2022 18:34:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
05/18/2022 18:34:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
05/18/2022 18:34:49 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7383640324816796 on epoch=712
05/18/2022 18:34:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
05/18/2022 18:34:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
05/18/2022 18:34:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
05/18/2022 18:34:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
05/18/2022 18:34:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
05/18/2022 18:34:57 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6939679320003863 on epoch=724
05/18/2022 18:34:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=727
05/18/2022 18:34:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
05/18/2022 18:35:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
05/18/2022 18:35:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
05/18/2022 18:35:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
05/18/2022 18:35:05 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7660014005602241 on epoch=737
05/18/2022 18:35:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=739
05/18/2022 18:35:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
05/18/2022 18:35:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
05/18/2022 18:35:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=747
05/18/2022 18:35:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=749
05/18/2022 18:35:13 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.7520424836601307 on epoch=749
05/18/2022 18:35:13 - INFO - __main__ - save last model!
05/18/2022 18:35:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/18/2022 18:35:13 - INFO - __main__ - Start tokenizing ... 5509 instances
05/18/2022 18:35:13 - INFO - __main__ - Printing 3 examples
05/18/2022 18:35:13 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/18/2022 18:35:13 - INFO - __main__ - ['others']
05/18/2022 18:35:13 - INFO - __main__ -  [emo] what you like very little things ok
05/18/2022 18:35:13 - INFO - __main__ - ['others']
05/18/2022 18:35:13 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/18/2022 18:35:13 - INFO - __main__ - ['others']
05/18/2022 18:35:13 - INFO - __main__ - Tokenizing Input ...
05/18/2022 18:35:15 - INFO - __main__ - Tokenizing Output ...
05/18/2022 18:35:21 - INFO - __main__ - Loaded 5509 examples from test data
05/18/2022 18:36:37 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.2_8_predictions.txt
05/18/2022 18:36:37 - INFO - __main__ - Classification-F1 on test data: 0.4131
05/18/2022 18:36:37 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7704650673400674, test_performance=0.41314744943622894
05/21/2022 21:23:24 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
05/21/2022 21:23:24 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
05/21/2022 21:23:24 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
05/21/2022 21:23:24 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
05/21/2022 21:23:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:23:25 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:23:25 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:23:25 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:25 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:23:25 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:25 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 21:23:25 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
05/21/2022 21:23:29 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/02/2022 11:43:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/02/2022 11:43:58 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
06/02/2022 11:43:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-base-cls2cls', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-cls2cls/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/02/2022 11:43:58 - INFO - __main__ - models/T5-base-cls2cls/singletask-emo
06/02/2022 11:44:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/02/2022 11:44:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/02/2022 11:44:00 - INFO - __main__ - args.device: cuda:0
06/02/2022 11:44:00 - INFO - __main__ - Using 2 gpus
06/02/2022 11:44:00 - INFO - __main__ - args.device: cuda:1
06/02/2022 11:44:00 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/02/2022 11:44:00 - INFO - __main__ - Using 2 gpus
06/02/2022 11:44:00 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
06/02/2022 11:44:05 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
06/02/2022 11:44:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:44:06 - INFO - __main__ - Printing 3 examples
06/02/2022 11:44:06 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:44:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:44:06 - INFO - __main__ - Printing 3 examples
06/02/2022 11:44:06 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:44:06 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:44:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:44:06 - INFO - __main__ - Printing 3 examples
06/02/2022 11:44:06 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:44:06 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:44:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:44:06 - INFO - __main__ - Printing 3 examples
06/02/2022 11:44:06 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:44:06 - INFO - __main__ - ['others']
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:44:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:44:06 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:44:06 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:44:12 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:44:12 - INFO - __main__ - task name: emo
06/02/2022 11:44:12 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:44:12 - INFO - __main__ - task name: emo
06/02/2022 11:44:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:44:12 - INFO - __main__ - Starting training!
06/02/2022 11:44:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:44:12 - INFO - __main__ - Starting training!
06/02/2022 11:44:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.18 on epoch=2
06/02/2022 11:44:15 - INFO - __main__ - Step 20 Global step 20 Train loss 2.71 on epoch=4
06/02/2022 11:44:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.69 on epoch=7
06/02/2022 11:44:18 - INFO - __main__ - Step 40 Global step 40 Train loss 1.46 on epoch=9
06/02/2022 11:44:19 - INFO - __main__ - Step 50 Global step 50 Train loss 1.36 on epoch=12
06/02/2022 11:44:20 - INFO - __main__ - Global step 50 Train loss 2.68 Classification-F1 0.13167388167388167 on epoch=12
06/02/2022 11:44:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13167388167388167 on epoch=12, global_step=50
06/02/2022 11:44:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.23 on epoch=14
06/02/2022 11:44:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=17
06/02/2022 11:44:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.08 on epoch=19
06/02/2022 11:44:24 - INFO - __main__ - Step 90 Global step 90 Train loss 2.30 on epoch=22
06/02/2022 11:44:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.38 on epoch=24
06/02/2022 11:44:26 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.16731898238747556 on epoch=24
06/02/2022 11:44:26 - INFO - __main__ - Saving model with best Classification-F1: 0.13167388167388167 -> 0.16731898238747556 on epoch=24, global_step=100
06/02/2022 11:44:27 - INFO - __main__ - Step 110 Global step 110 Train loss 1.09 on epoch=27
06/02/2022 11:44:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.04 on epoch=29
06/02/2022 11:44:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=32
06/02/2022 11:44:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.10 on epoch=34
06/02/2022 11:44:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.01 on epoch=37
06/02/2022 11:44:33 - INFO - __main__ - Global step 150 Train loss 1.05 Classification-F1 0.20467032967032966 on epoch=37
06/02/2022 11:44:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16731898238747556 -> 0.20467032967032966 on epoch=37, global_step=150
06/02/2022 11:44:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=39
06/02/2022 11:44:35 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
06/02/2022 11:44:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.84 on epoch=44
06/02/2022 11:44:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=47
06/02/2022 11:44:39 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=49
06/02/2022 11:44:40 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.20462794918330307 on epoch=49
06/02/2022 11:44:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=52
06/02/2022 11:44:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
06/02/2022 11:44:44 - INFO - __main__ - Step 230 Global step 230 Train loss 1.02 on epoch=57
06/02/2022 11:44:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
06/02/2022 11:44:46 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
06/02/2022 11:44:47 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.1237183868762816 on epoch=62
06/02/2022 11:44:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
06/02/2022 11:44:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=67
06/02/2022 11:44:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=69
06/02/2022 11:44:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.93 on epoch=72
06/02/2022 11:44:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
06/02/2022 11:44:53 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.23333333333333336 on epoch=74
06/02/2022 11:44:53 - INFO - __main__ - Saving model with best Classification-F1: 0.20467032967032966 -> 0.23333333333333336 on epoch=74, global_step=300
06/02/2022 11:44:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.99 on epoch=77
06/02/2022 11:44:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.96 on epoch=79
06/02/2022 11:44:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.91 on epoch=82
06/02/2022 11:44:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=84
06/02/2022 11:45:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=87
06/02/2022 11:45:00 - INFO - __main__ - Global step 350 Train loss 0.95 Classification-F1 0.35590909090909095 on epoch=87
06/02/2022 11:45:00 - INFO - __main__ - Saving model with best Classification-F1: 0.23333333333333336 -> 0.35590909090909095 on epoch=87, global_step=350
06/02/2022 11:45:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
06/02/2022 11:45:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
06/02/2022 11:45:04 - INFO - __main__ - Step 380 Global step 380 Train loss 1.04 on epoch=94
06/02/2022 11:45:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.97 on epoch=97
06/02/2022 11:45:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
06/02/2022 11:45:07 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.31721967963386727 on epoch=99
06/02/2022 11:45:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.88 on epoch=102
06/02/2022 11:45:10 - INFO - __main__ - Step 420 Global step 420 Train loss 1.01 on epoch=104
06/02/2022 11:45:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.97 on epoch=107
06/02/2022 11:45:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
06/02/2022 11:45:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
06/02/2022 11:45:14 - INFO - __main__ - Global step 450 Train loss 0.94 Classification-F1 0.20787037037037037 on epoch=112
06/02/2022 11:45:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.89 on epoch=114
06/02/2022 11:45:17 - INFO - __main__ - Step 470 Global step 470 Train loss 1.00 on epoch=117
06/02/2022 11:45:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.84 on epoch=119
06/02/2022 11:45:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=122
06/02/2022 11:45:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.84 on epoch=124
06/02/2022 11:45:21 - INFO - __main__ - Global step 500 Train loss 0.90 Classification-F1 0.2714472309299895 on epoch=124
06/02/2022 11:45:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.81 on epoch=127
06/02/2022 11:45:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.98 on epoch=129
06/02/2022 11:45:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.97 on epoch=132
06/02/2022 11:45:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=134
06/02/2022 11:45:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.81 on epoch=137
06/02/2022 11:45:28 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.4278902384165542 on epoch=137
06/02/2022 11:45:28 - INFO - __main__ - Saving model with best Classification-F1: 0.35590909090909095 -> 0.4278902384165542 on epoch=137, global_step=550
06/02/2022 11:45:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=139
06/02/2022 11:45:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=142
06/02/2022 11:45:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.80 on epoch=144
06/02/2022 11:45:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=147
06/02/2022 11:45:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.70 on epoch=149
06/02/2022 11:45:34 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.45240292041078306 on epoch=149
06/02/2022 11:45:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4278902384165542 -> 0.45240292041078306 on epoch=149, global_step=600
06/02/2022 11:45:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=152
06/02/2022 11:45:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
06/02/2022 11:45:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.85 on epoch=157
06/02/2022 11:45:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
06/02/2022 11:45:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=162
06/02/2022 11:45:41 - INFO - __main__ - Global step 650 Train loss 0.81 Classification-F1 0.37473684210526315 on epoch=162
06/02/2022 11:45:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.75 on epoch=164
06/02/2022 11:45:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.79 on epoch=167
06/02/2022 11:45:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=169
06/02/2022 11:45:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.74 on epoch=172
06/02/2022 11:45:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.69 on epoch=174
06/02/2022 11:45:48 - INFO - __main__ - Global step 700 Train loss 0.74 Classification-F1 0.5099479392252636 on epoch=174
06/02/2022 11:45:48 - INFO - __main__ - Saving model with best Classification-F1: 0.45240292041078306 -> 0.5099479392252636 on epoch=174, global_step=700
06/02/2022 11:45:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
06/02/2022 11:45:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.68 on epoch=179
06/02/2022 11:45:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.73 on epoch=182
06/02/2022 11:45:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.72 on epoch=184
06/02/2022 11:45:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.79 on epoch=187
06/02/2022 11:45:55 - INFO - __main__ - Global step 750 Train loss 0.71 Classification-F1 0.4421961550993809 on epoch=187
06/02/2022 11:45:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.71 on epoch=189
06/02/2022 11:45:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.67 on epoch=192
06/02/2022 11:45:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=194
06/02/2022 11:46:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=197
06/02/2022 11:46:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=199
06/02/2022 11:46:02 - INFO - __main__ - Global step 800 Train loss 0.68 Classification-F1 0.5292084726867335 on epoch=199
06/02/2022 11:46:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5099479392252636 -> 0.5292084726867335 on epoch=199, global_step=800
06/02/2022 11:46:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=202
06/02/2022 11:46:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=204
06/02/2022 11:46:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=207
06/02/2022 11:46:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.66 on epoch=209
06/02/2022 11:46:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=212
06/02/2022 11:46:08 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.5165201465201466 on epoch=212
06/02/2022 11:46:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=214
06/02/2022 11:46:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.65 on epoch=217
06/02/2022 11:46:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.68 on epoch=219
06/02/2022 11:46:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.65 on epoch=222
06/02/2022 11:46:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.60 on epoch=224
06/02/2022 11:46:15 - INFO - __main__ - Global step 900 Train loss 0.65 Classification-F1 0.6607378129117258 on epoch=224
06/02/2022 11:46:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5292084726867335 -> 0.6607378129117258 on epoch=224, global_step=900
06/02/2022 11:46:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=227
06/02/2022 11:46:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=229
06/02/2022 11:46:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=232
06/02/2022 11:46:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=234
06/02/2022 11:46:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=237
06/02/2022 11:46:22 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.5820289855072465 on epoch=237
06/02/2022 11:46:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=239
06/02/2022 11:46:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=242
06/02/2022 11:46:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=244
06/02/2022 11:46:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=247
06/02/2022 11:46:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=249
06/02/2022 11:46:29 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.5699570071423617 on epoch=249
06/02/2022 11:46:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=252
06/02/2022 11:46:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=254
06/02/2022 11:46:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=257
06/02/2022 11:46:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=259
06/02/2022 11:46:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=262
06/02/2022 11:46:36 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.6853264978264978 on epoch=262
06/02/2022 11:46:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6607378129117258 -> 0.6853264978264978 on epoch=262, global_step=1050
06/02/2022 11:46:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=264
06/02/2022 11:46:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=267
06/02/2022 11:46:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=269
06/02/2022 11:46:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=272
06/02/2022 11:46:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=274
06/02/2022 11:46:43 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.6228692451627496 on epoch=274
06/02/2022 11:46:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=277
06/02/2022 11:46:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=279
06/02/2022 11:46:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=282
06/02/2022 11:46:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=284
06/02/2022 11:46:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=287
06/02/2022 11:46:49 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.6309842056212291 on epoch=287
06/02/2022 11:46:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=289
06/02/2022 11:46:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=292
06/02/2022 11:46:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=294
06/02/2022 11:46:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=297
06/02/2022 11:46:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=299
06/02/2022 11:46:56 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.5927816736792894 on epoch=299
06/02/2022 11:46:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=302
06/02/2022 11:46:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=304
06/02/2022 11:47:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=307
06/02/2022 11:47:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=309
06/02/2022 11:47:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
06/02/2022 11:47:03 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7234546703296703 on epoch=312
06/02/2022 11:47:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6853264978264978 -> 0.7234546703296703 on epoch=312, global_step=1250
06/02/2022 11:47:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
06/02/2022 11:47:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=317
06/02/2022 11:47:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=319
06/02/2022 11:47:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=322
06/02/2022 11:47:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=324
06/02/2022 11:47:10 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.7526578998353192 on epoch=324
06/02/2022 11:47:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7234546703296703 -> 0.7526578998353192 on epoch=324, global_step=1300
06/02/2022 11:47:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=327
06/02/2022 11:47:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=329
06/02/2022 11:47:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=332
06/02/2022 11:47:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
06/02/2022 11:47:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=337
06/02/2022 11:47:17 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.7337591602297484 on epoch=337
06/02/2022 11:47:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=339
06/02/2022 11:47:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=342
06/02/2022 11:47:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=344
06/02/2022 11:47:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=347
06/02/2022 11:47:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=349
06/02/2022 11:47:23 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7202581636011263 on epoch=349
06/02/2022 11:47:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=352
06/02/2022 11:47:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=354
06/02/2022 11:47:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=357
06/02/2022 11:47:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=359
06/02/2022 11:47:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=362
06/02/2022 11:47:30 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7565488565488565 on epoch=362
06/02/2022 11:47:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7526578998353192 -> 0.7565488565488565 on epoch=362, global_step=1450
06/02/2022 11:47:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=364
06/02/2022 11:47:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=367
06/02/2022 11:47:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=369
06/02/2022 11:47:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=372
06/02/2022 11:47:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
06/02/2022 11:47:37 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.761908656677354 on epoch=374
06/02/2022 11:47:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7565488565488565 -> 0.761908656677354 on epoch=374, global_step=1500
06/02/2022 11:47:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=377
06/02/2022 11:47:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=379
06/02/2022 11:47:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=382
06/02/2022 11:47:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=384
06/02/2022 11:47:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=387
06/02/2022 11:47:44 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.6417748917748918 on epoch=387
06/02/2022 11:47:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=389
06/02/2022 11:47:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=392
06/02/2022 11:47:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
06/02/2022 11:47:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=397
06/02/2022 11:47:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=399
06/02/2022 11:47:51 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.7303729485524617 on epoch=399
06/02/2022 11:47:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=402
06/02/2022 11:47:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=404
06/02/2022 11:47:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=407
06/02/2022 11:47:56 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=409
06/02/2022 11:47:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
06/02/2022 11:47:57 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.708935384000799 on epoch=412
06/02/2022 11:47:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=414
06/02/2022 11:48:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=417
06/02/2022 11:48:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=419
06/02/2022 11:48:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=422
06/02/2022 11:48:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=424
06/02/2022 11:48:04 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.7247512528584635 on epoch=424
06/02/2022 11:48:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
06/02/2022 11:48:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=429
06/02/2022 11:48:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
06/02/2022 11:48:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
06/02/2022 11:48:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=437
06/02/2022 11:48:11 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.7121025660053761 on epoch=437
06/02/2022 11:48:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
06/02/2022 11:48:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=442
06/02/2022 11:48:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
06/02/2022 11:48:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
06/02/2022 11:48:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/02/2022 11:48:18 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.7068360891437552 on epoch=449
06/02/2022 11:48:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=452
06/02/2022 11:48:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=454
06/02/2022 11:48:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=457
06/02/2022 11:48:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=459
06/02/2022 11:48:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=462
06/02/2022 11:48:24 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.69168457399224 on epoch=462
06/02/2022 11:48:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=464
06/02/2022 11:48:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=467
06/02/2022 11:48:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
06/02/2022 11:48:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=472
06/02/2022 11:48:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
06/02/2022 11:48:31 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.7065972222222222 on epoch=474
06/02/2022 11:48:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
06/02/2022 11:48:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
06/02/2022 11:48:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=482
06/02/2022 11:48:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=484
06/02/2022 11:48:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=487
06/02/2022 11:48:38 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7301963088408101 on epoch=487
06/02/2022 11:48:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=489
06/02/2022 11:48:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=492
06/02/2022 11:48:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/02/2022 11:48:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=497
06/02/2022 11:48:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=499
06/02/2022 11:48:45 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.682937834224599 on epoch=499
06/02/2022 11:48:46 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
06/02/2022 11:48:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/02/2022 11:48:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/02/2022 11:48:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=509
06/02/2022 11:48:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
06/02/2022 11:48:52 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.6904268292682927 on epoch=512
06/02/2022 11:48:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
06/02/2022 11:48:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
06/02/2022 11:48:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/02/2022 11:48:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
06/02/2022 11:48:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=524
06/02/2022 11:48:58 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7225906120023767 on epoch=524
06/02/2022 11:49:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
06/02/2022 11:49:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
06/02/2022 11:49:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/02/2022 11:49:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
06/02/2022 11:49:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=537
06/02/2022 11:49:05 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.6533492822966507 on epoch=537
06/02/2022 11:49:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.06 on epoch=539
06/02/2022 11:49:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
06/02/2022 11:49:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/02/2022 11:49:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
06/02/2022 11:49:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/02/2022 11:49:12 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.6487903225806453 on epoch=549
06/02/2022 11:49:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
06/02/2022 11:49:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=554
06/02/2022 11:49:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/02/2022 11:49:17 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
06/02/2022 11:49:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
06/02/2022 11:49:19 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6660903714983412 on epoch=562
06/02/2022 11:49:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/02/2022 11:49:21 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
06/02/2022 11:49:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
06/02/2022 11:49:24 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
06/02/2022 11:49:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=574
06/02/2022 11:49:26 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.6954954954954955 on epoch=574
06/02/2022 11:49:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/02/2022 11:49:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=579
06/02/2022 11:49:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
06/02/2022 11:49:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=584
06/02/2022 11:49:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 11:49:32 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7238032934807128 on epoch=587
06/02/2022 11:49:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=589
06/02/2022 11:49:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
06/02/2022 11:49:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/02/2022 11:49:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/02/2022 11:49:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/02/2022 11:49:39 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6705008488964346 on epoch=599
06/02/2022 11:49:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=602
06/02/2022 11:49:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
06/02/2022 11:49:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=607
06/02/2022 11:49:44 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
06/02/2022 11:49:46 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/02/2022 11:49:46 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.6876793150370517 on epoch=612
06/02/2022 11:49:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=614
06/02/2022 11:49:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/02/2022 11:49:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=619
06/02/2022 11:49:51 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
06/02/2022 11:49:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=624
06/02/2022 11:49:53 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.6794153225806452 on epoch=624
06/02/2022 11:49:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/02/2022 11:49:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
06/02/2022 11:49:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 11:49:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/02/2022 11:49:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/02/2022 11:50:00 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6682319518716578 on epoch=637
06/02/2022 11:50:01 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 11:50:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=642
06/02/2022 11:50:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/02/2022 11:50:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 11:50:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=649
06/02/2022 11:50:06 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.673202614379085 on epoch=649
06/02/2022 11:50:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/02/2022 11:50:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=654
06/02/2022 11:50:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 11:50:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/02/2022 11:50:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/02/2022 11:50:13 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.6879984051036683 on epoch=662
06/02/2022 11:50:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=664
06/02/2022 11:50:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=667
06/02/2022 11:50:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/02/2022 11:50:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/02/2022 11:50:19 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/02/2022 11:50:20 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6455653907904626 on epoch=674
06/02/2022 11:50:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/02/2022 11:50:22 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/02/2022 11:50:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/02/2022 11:50:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=684
06/02/2022 11:50:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/02/2022 11:50:26 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6894792486231314 on epoch=687
06/02/2022 11:50:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
06/02/2022 11:50:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=692
06/02/2022 11:50:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 11:50:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/02/2022 11:50:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/02/2022 11:50:33 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.638413789428815 on epoch=699
06/02/2022 11:50:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
06/02/2022 11:50:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=704
06/02/2022 11:50:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=707
06/02/2022 11:50:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
06/02/2022 11:50:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=712
06/02/2022 11:50:40 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.6827112004947892 on epoch=712
06/02/2022 11:50:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/02/2022 11:50:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
06/02/2022 11:50:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 11:50:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/02/2022 11:50:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
06/02/2022 11:50:47 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6219251336898396 on epoch=724
06/02/2022 11:50:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/02/2022 11:50:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/02/2022 11:50:51 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 11:50:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=734
06/02/2022 11:50:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/02/2022 11:50:54 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6671957671957672 on epoch=737
06/02/2022 11:50:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
06/02/2022 11:50:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=742
06/02/2022 11:50:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
06/02/2022 11:50:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/02/2022 11:51:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/02/2022 11:51:00 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.6745391705069125 on epoch=749
06/02/2022 11:51:00 - INFO - __main__ - save last model!
06/02/2022 11:51:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 11:51:01 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 11:51:01 - INFO - __main__ - Printing 3 examples
06/02/2022 11:51:01 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:51:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:51:01 - INFO - __main__ - Printing 3 examples
06/02/2022 11:51:01 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:51:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:51:01 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:51:01 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:51:01 - INFO - __main__ - Printing 3 examples
06/02/2022 11:51:01 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:51:01 - INFO - __main__ - ['others']
06/02/2022 11:51:01 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:51:01 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:51:01 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:51:03 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:51:07 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:51:07 - INFO - __main__ - task name: emo
06/02/2022 11:51:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:51:07 - INFO - __main__ - Starting training!
06/02/2022 11:51:08 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 11:51:51 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.5_8_predictions.txt
06/02/2022 11:51:51 - INFO - __main__ - Classification-F1 on test data: 0.2531
06/02/2022 11:51:51 - INFO - __main__ - prefix=emo_16_100, lr=0.5, bsz=8, dev_performance=0.761908656677354, test_performance=0.2530649663645602
06/02/2022 11:51:51 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.4, bsz=8 ...
06/02/2022 11:51:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:51:52 - INFO - __main__ - Printing 3 examples
06/02/2022 11:51:52 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:51:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:51:52 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:51:52 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:51:52 - INFO - __main__ - Printing 3 examples
06/02/2022 11:51:52 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:51:52 - INFO - __main__ - ['others']
06/02/2022 11:51:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:51:52 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:51:52 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:51:58 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:51:58 - INFO - __main__ - task name: emo
06/02/2022 11:51:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:51:58 - INFO - __main__ - Starting training!
06/02/2022 11:52:00 - INFO - __main__ - Step 10 Global step 10 Train loss 6.44 on epoch=2
06/02/2022 11:52:01 - INFO - __main__ - Step 20 Global step 20 Train loss 4.27 on epoch=4
06/02/2022 11:52:02 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=7
06/02/2022 11:52:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.57 on epoch=9
06/02/2022 11:52:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.54 on epoch=12
06/02/2022 11:52:05 - INFO - __main__ - Global step 50 Train loss 3.24 Classification-F1 0.09493670886075949 on epoch=12
06/02/2022 11:52:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09493670886075949 on epoch=12, global_step=50
06/02/2022 11:52:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.38 on epoch=14
06/02/2022 11:52:07 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=17
06/02/2022 11:52:09 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=19
06/02/2022 11:52:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
06/02/2022 11:52:11 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
06/02/2022 11:52:12 - INFO - __main__ - Global step 100 Train loss 1.20 Classification-F1 0.1 on epoch=24
06/02/2022 11:52:12 - INFO - __main__ - Saving model with best Classification-F1: 0.09493670886075949 -> 0.1 on epoch=24, global_step=100
06/02/2022 11:52:13 - INFO - __main__ - Step 110 Global step 110 Train loss 1.24 on epoch=27
06/02/2022 11:52:14 - INFO - __main__ - Step 120 Global step 120 Train loss 1.24 on epoch=29
06/02/2022 11:52:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=32
06/02/2022 11:52:17 - INFO - __main__ - Step 140 Global step 140 Train loss 1.06 on epoch=34
06/02/2022 11:52:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
06/02/2022 11:52:18 - INFO - __main__ - Global step 150 Train loss 1.10 Classification-F1 0.12905551550108146 on epoch=37
06/02/2022 11:52:18 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.12905551550108146 on epoch=37, global_step=150
06/02/2022 11:52:20 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
06/02/2022 11:52:21 - INFO - __main__ - Step 170 Global step 170 Train loss 1.18 on epoch=42
06/02/2022 11:52:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.96 on epoch=44
06/02/2022 11:52:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
06/02/2022 11:52:25 - INFO - __main__ - Step 200 Global step 200 Train loss 1.04 on epoch=49
06/02/2022 11:52:25 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.1900100944552599 on epoch=49
06/02/2022 11:52:25 - INFO - __main__ - Saving model with best Classification-F1: 0.12905551550108146 -> 0.1900100944552599 on epoch=49, global_step=200
06/02/2022 11:52:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
06/02/2022 11:52:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
06/02/2022 11:52:29 - INFO - __main__ - Step 230 Global step 230 Train loss 1.69 on epoch=57
06/02/2022 11:52:30 - INFO - __main__ - Step 240 Global step 240 Train loss 1.07 on epoch=59
06/02/2022 11:52:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
06/02/2022 11:52:32 - INFO - __main__ - Global step 250 Train loss 1.13 Classification-F1 0.17045454545454547 on epoch=62
06/02/2022 11:52:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.98 on epoch=64
06/02/2022 11:52:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.90 on epoch=67
06/02/2022 11:52:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.96 on epoch=69
06/02/2022 11:52:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.87 on epoch=72
06/02/2022 11:52:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=74
06/02/2022 11:52:38 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.13034188034188032 on epoch=74
06/02/2022 11:52:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.98 on epoch=77
06/02/2022 11:52:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=79
06/02/2022 11:52:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.93 on epoch=82
06/02/2022 11:52:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
06/02/2022 11:52:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.03 on epoch=87
06/02/2022 11:52:45 - INFO - __main__ - Global step 350 Train loss 0.96 Classification-F1 0.254981884057971 on epoch=87
06/02/2022 11:52:45 - INFO - __main__ - Saving model with best Classification-F1: 0.1900100944552599 -> 0.254981884057971 on epoch=87, global_step=350
06/02/2022 11:52:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.91 on epoch=89
06/02/2022 11:52:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=92
06/02/2022 11:52:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=94
06/02/2022 11:52:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
06/02/2022 11:52:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=99
06/02/2022 11:52:52 - INFO - __main__ - Global step 400 Train loss 0.90 Classification-F1 0.15666666666666668 on epoch=99
06/02/2022 11:52:53 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=102
06/02/2022 11:52:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=104
06/02/2022 11:52:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=107
06/02/2022 11:52:57 - INFO - __main__ - Step 440 Global step 440 Train loss 2.93 on epoch=109
06/02/2022 11:52:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.91 on epoch=112
06/02/2022 11:52:59 - INFO - __main__ - Global step 450 Train loss 1.29 Classification-F1 0.22850678733031676 on epoch=112
06/02/2022 11:53:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
06/02/2022 11:53:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=117
06/02/2022 11:53:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.91 on epoch=119
06/02/2022 11:53:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.86 on epoch=122
06/02/2022 11:53:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=124
06/02/2022 11:53:06 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.20689324116743468 on epoch=124
06/02/2022 11:53:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.82 on epoch=127
06/02/2022 11:53:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=129
06/02/2022 11:53:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
06/02/2022 11:53:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
06/02/2022 11:53:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.92 on epoch=137
06/02/2022 11:53:13 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.17839889579020016 on epoch=137
06/02/2022 11:53:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
06/02/2022 11:53:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
06/02/2022 11:53:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.93 on epoch=144
06/02/2022 11:53:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.86 on epoch=147
06/02/2022 11:53:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=149
06/02/2022 11:53:19 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.22464925012094825 on epoch=149
06/02/2022 11:53:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
06/02/2022 11:53:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
06/02/2022 11:53:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.78 on epoch=157
06/02/2022 11:53:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.85 on epoch=159
06/02/2022 11:53:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.93 on epoch=162
06/02/2022 11:53:26 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.3562233589087809 on epoch=162
06/02/2022 11:53:26 - INFO - __main__ - Saving model with best Classification-F1: 0.254981884057971 -> 0.3562233589087809 on epoch=162, global_step=650
06/02/2022 11:53:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
06/02/2022 11:53:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
06/02/2022 11:53:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.81 on epoch=169
06/02/2022 11:53:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.81 on epoch=172
06/02/2022 11:53:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.87 on epoch=174
06/02/2022 11:53:33 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.2226190476190476 on epoch=174
06/02/2022 11:53:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=177
06/02/2022 11:53:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
06/02/2022 11:53:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.80 on epoch=182
06/02/2022 11:53:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.86 on epoch=184
06/02/2022 11:53:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.88 on epoch=187
06/02/2022 11:53:39 - INFO - __main__ - Global step 750 Train loss 0.84 Classification-F1 0.3186975591151793 on epoch=187
06/02/2022 11:53:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.76 on epoch=189
06/02/2022 11:53:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
06/02/2022 11:53:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.84 on epoch=194
06/02/2022 11:53:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.90 on epoch=197
06/02/2022 11:53:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.81 on epoch=199
06/02/2022 11:53:46 - INFO - __main__ - Global step 800 Train loss 0.83 Classification-F1 0.37909560723514213 on epoch=199
06/02/2022 11:53:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3562233589087809 -> 0.37909560723514213 on epoch=199, global_step=800
06/02/2022 11:53:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.87 on epoch=202
06/02/2022 11:53:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.83 on epoch=204
06/02/2022 11:53:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=207
06/02/2022 11:53:51 - INFO - __main__ - Step 840 Global step 840 Train loss 0.72 on epoch=209
06/02/2022 11:53:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.77 on epoch=212
06/02/2022 11:53:53 - INFO - __main__ - Global step 850 Train loss 0.78 Classification-F1 0.32999999999999996 on epoch=212
06/02/2022 11:53:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=214
06/02/2022 11:53:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.82 on epoch=217
06/02/2022 11:53:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
06/02/2022 11:53:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
06/02/2022 11:53:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.67 on epoch=224
06/02/2022 11:54:00 - INFO - __main__ - Global step 900 Train loss 0.75 Classification-F1 0.45565666217840134 on epoch=224
06/02/2022 11:54:00 - INFO - __main__ - Saving model with best Classification-F1: 0.37909560723514213 -> 0.45565666217840134 on epoch=224, global_step=900
06/02/2022 11:54:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=227
06/02/2022 11:54:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.75 on epoch=229
06/02/2022 11:54:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=232
06/02/2022 11:54:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.70 on epoch=234
06/02/2022 11:54:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=237
06/02/2022 11:54:06 - INFO - __main__ - Global step 950 Train loss 0.76 Classification-F1 0.47884891688771003 on epoch=237
06/02/2022 11:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.45565666217840134 -> 0.47884891688771003 on epoch=237, global_step=950
06/02/2022 11:54:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=239
06/02/2022 11:54:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.69 on epoch=242
06/02/2022 11:54:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.73 on epoch=244
06/02/2022 11:54:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
06/02/2022 11:54:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.71 on epoch=249
06/02/2022 11:54:13 - INFO - __main__ - Global step 1000 Train loss 0.73 Classification-F1 0.31720301418439717 on epoch=249
06/02/2022 11:54:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.69 on epoch=252
06/02/2022 11:54:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=254
06/02/2022 11:54:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.63 on epoch=257
06/02/2022 11:54:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=259
06/02/2022 11:54:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.59 on epoch=262
06/02/2022 11:54:20 - INFO - __main__ - Global step 1050 Train loss 0.66 Classification-F1 0.40933006535947714 on epoch=262
06/02/2022 11:54:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.69 on epoch=264
06/02/2022 11:54:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.73 on epoch=267
06/02/2022 11:54:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.68 on epoch=269
06/02/2022 11:54:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.70 on epoch=272
06/02/2022 11:54:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=274
06/02/2022 11:54:26 - INFO - __main__ - Global step 1100 Train loss 0.70 Classification-F1 0.4119991702228544 on epoch=274
06/02/2022 11:54:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.69 on epoch=277
06/02/2022 11:54:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.71 on epoch=279
06/02/2022 11:54:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=282
06/02/2022 11:54:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=284
06/02/2022 11:54:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.71 on epoch=287
06/02/2022 11:54:33 - INFO - __main__ - Global step 1150 Train loss 0.68 Classification-F1 0.43418407019384286 on epoch=287
06/02/2022 11:54:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.62 on epoch=289
06/02/2022 11:54:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=292
06/02/2022 11:54:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.63 on epoch=294
06/02/2022 11:54:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.63 on epoch=297
06/02/2022 11:54:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.68 on epoch=299
06/02/2022 11:54:40 - INFO - __main__ - Global step 1200 Train loss 0.65 Classification-F1 0.4762289068231842 on epoch=299
06/02/2022 11:54:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.59 on epoch=302
06/02/2022 11:54:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.55 on epoch=304
06/02/2022 11:54:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.65 on epoch=307
06/02/2022 11:54:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.66 on epoch=309
06/02/2022 11:54:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=312
06/02/2022 11:54:46 - INFO - __main__ - Global step 1250 Train loss 0.61 Classification-F1 0.4868693284936479 on epoch=312
06/02/2022 11:54:46 - INFO - __main__ - Saving model with best Classification-F1: 0.47884891688771003 -> 0.4868693284936479 on epoch=312, global_step=1250
06/02/2022 11:54:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=314
06/02/2022 11:54:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=317
06/02/2022 11:54:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=319
06/02/2022 11:54:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=322
06/02/2022 11:54:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=324
06/02/2022 11:54:53 - INFO - __main__ - Global step 1300 Train loss 0.57 Classification-F1 0.4975213434656159 on epoch=324
06/02/2022 11:54:53 - INFO - __main__ - Saving model with best Classification-F1: 0.4868693284936479 -> 0.4975213434656159 on epoch=324, global_step=1300
06/02/2022 11:54:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.61 on epoch=327
06/02/2022 11:54:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=329
06/02/2022 11:54:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=332
06/02/2022 11:54:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=334
06/02/2022 11:54:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=337
06/02/2022 11:55:00 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.5557858807858808 on epoch=337
06/02/2022 11:55:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4975213434656159 -> 0.5557858807858808 on epoch=337, global_step=1350
06/02/2022 11:55:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.59 on epoch=339
06/02/2022 11:55:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.60 on epoch=342
06/02/2022 11:55:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.54 on epoch=344
06/02/2022 11:55:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=347
06/02/2022 11:55:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=349
06/02/2022 11:55:06 - INFO - __main__ - Global step 1400 Train loss 0.52 Classification-F1 0.5193452380952381 on epoch=349
06/02/2022 11:55:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.45 on epoch=352
06/02/2022 11:55:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=354
06/02/2022 11:55:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=357
06/02/2022 11:55:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=359
06/02/2022 11:55:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.56 on epoch=362
06/02/2022 11:55:13 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.5438405797101449 on epoch=362
06/02/2022 11:55:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=364
06/02/2022 11:55:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=367
06/02/2022 11:55:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=369
06/02/2022 11:55:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=372
06/02/2022 11:55:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=374
06/02/2022 11:55:20 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.6573461091753775 on epoch=374
06/02/2022 11:55:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5557858807858808 -> 0.6573461091753775 on epoch=374, global_step=1500
06/02/2022 11:55:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=377
06/02/2022 11:55:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=379
06/02/2022 11:55:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=382
06/02/2022 11:55:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=384
06/02/2022 11:55:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=387
06/02/2022 11:55:27 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.5182076879334729 on epoch=387
06/02/2022 11:55:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=389
06/02/2022 11:55:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=392
06/02/2022 11:55:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=394
06/02/2022 11:55:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=397
06/02/2022 11:55:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=399
06/02/2022 11:55:33 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.6053958944281526 on epoch=399
06/02/2022 11:55:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=402
06/02/2022 11:55:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=404
06/02/2022 11:55:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=407
06/02/2022 11:55:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=409
06/02/2022 11:55:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=412
06/02/2022 11:55:40 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.6244154788697174 on epoch=412
06/02/2022 11:55:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=414
06/02/2022 11:55:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=417
06/02/2022 11:55:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=419
06/02/2022 11:55:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.53 on epoch=422
06/02/2022 11:55:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.04 on epoch=424
06/02/2022 11:55:47 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.5518833018833018 on epoch=424
06/02/2022 11:55:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.44 on epoch=427
06/02/2022 11:55:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.99 on epoch=429
06/02/2022 11:55:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.04 on epoch=432
06/02/2022 11:55:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 2.18 on epoch=434
06/02/2022 11:55:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.42 on epoch=437
06/02/2022 11:55:53 - INFO - __main__ - Global step 1750 Train loss 1.42 Classification-F1 0.6193001443001444 on epoch=437
06/02/2022 11:55:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=439
06/02/2022 11:55:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=442
06/02/2022 11:55:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=444
06/02/2022 11:55:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=447
06/02/2022 11:56:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=449
06/02/2022 11:56:00 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.649154334038055 on epoch=449
06/02/2022 11:56:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=452
06/02/2022 11:56:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=454
06/02/2022 11:56:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=457
06/02/2022 11:56:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=459
06/02/2022 11:56:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=462
06/02/2022 11:56:07 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.6545479440512302 on epoch=462
06/02/2022 11:56:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=464
06/02/2022 11:56:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=467
06/02/2022 11:56:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=469
06/02/2022 11:56:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=472
06/02/2022 11:56:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=474
06/02/2022 11:56:14 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.6397594457903013 on epoch=474
06/02/2022 11:56:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=477
06/02/2022 11:56:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=479
06/02/2022 11:56:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=482
06/02/2022 11:56:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=484
06/02/2022 11:56:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=487
06/02/2022 11:56:21 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.6471163245356795 on epoch=487
06/02/2022 11:56:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=489
06/02/2022 11:56:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=492
06/02/2022 11:56:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=494
06/02/2022 11:56:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=497
06/02/2022 11:56:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=499
06/02/2022 11:56:27 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.6744972769166317 on epoch=499
06/02/2022 11:56:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6573461091753775 -> 0.6744972769166317 on epoch=499, global_step=2000
06/02/2022 11:56:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=502
06/02/2022 11:56:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=504
06/02/2022 11:56:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=507
06/02/2022 11:56:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.25 on epoch=509
06/02/2022 11:56:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=512
06/02/2022 11:56:34 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.6180351906158357 on epoch=512
06/02/2022 11:56:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=514
06/02/2022 11:56:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=517
06/02/2022 11:56:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=519
06/02/2022 11:56:39 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=522
06/02/2022 11:56:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=524
06/02/2022 11:56:41 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.7293948412698413 on epoch=524
06/02/2022 11:56:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6744972769166317 -> 0.7293948412698413 on epoch=524, global_step=2100
06/02/2022 11:56:42 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=527
06/02/2022 11:56:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.21 on epoch=529
06/02/2022 11:56:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=532
06/02/2022 11:56:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=534
06/02/2022 11:56:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=537
06/02/2022 11:56:47 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.6038401253918495 on epoch=537
06/02/2022 11:56:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=539
06/02/2022 11:56:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.29 on epoch=542
06/02/2022 11:56:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=544
06/02/2022 11:56:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=547
06/02/2022 11:56:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=549
06/02/2022 11:56:54 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.6256121939030485 on epoch=549
06/02/2022 11:56:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=552
06/02/2022 11:56:57 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=554
06/02/2022 11:56:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=557
06/02/2022 11:56:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=559
06/02/2022 11:57:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=562
06/02/2022 11:57:01 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.6831206741239924 on epoch=562
06/02/2022 11:57:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=564
06/02/2022 11:57:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=567
06/02/2022 11:57:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=569
06/02/2022 11:57:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=572
06/02/2022 11:57:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=574
06/02/2022 11:57:08 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.5966525665738559 on epoch=574
06/02/2022 11:57:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.26 on epoch=577
06/02/2022 11:57:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=579
06/02/2022 11:57:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=582
06/02/2022 11:57:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.21 on epoch=584
06/02/2022 11:57:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=587
06/02/2022 11:57:15 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7104489713185366 on epoch=587
06/02/2022 11:57:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=589
06/02/2022 11:57:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=592
06/02/2022 11:57:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.22 on epoch=594
06/02/2022 11:57:20 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
06/02/2022 11:57:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=599
06/02/2022 11:57:22 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.6020658263305323 on epoch=599
06/02/2022 11:57:23 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=602
06/02/2022 11:57:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=604
06/02/2022 11:57:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=607
06/02/2022 11:57:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.30 on epoch=609
06/02/2022 11:57:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
06/02/2022 11:57:29 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.6704166666666667 on epoch=612
06/02/2022 11:57:30 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=614
06/02/2022 11:57:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=617
06/02/2022 11:57:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=619
06/02/2022 11:57:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=622
06/02/2022 11:57:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=624
06/02/2022 11:57:36 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.6969655797101451 on epoch=624
06/02/2022 11:57:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=627
06/02/2022 11:57:38 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=629
06/02/2022 11:57:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=632
06/02/2022 11:57:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.26 on epoch=634
06/02/2022 11:57:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=637
06/02/2022 11:57:43 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.597962382445141 on epoch=637
06/02/2022 11:57:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=639
06/02/2022 11:57:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=642
06/02/2022 11:57:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=644
06/02/2022 11:57:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=647
06/02/2022 11:57:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=649
06/02/2022 11:57:50 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.6381465425644949 on epoch=649
06/02/2022 11:57:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=652
06/02/2022 11:57:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=654
06/02/2022 11:57:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=657
06/02/2022 11:57:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=659
06/02/2022 11:57:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=662
06/02/2022 11:57:57 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.6005291005291006 on epoch=662
06/02/2022 11:57:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=664
06/02/2022 11:57:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=667
06/02/2022 11:58:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=669
06/02/2022 11:58:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
06/02/2022 11:58:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=674
06/02/2022 11:58:04 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.711571330547065 on epoch=674
06/02/2022 11:58:05 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=677
06/02/2022 11:58:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=679
06/02/2022 11:58:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.24 on epoch=682
06/02/2022 11:58:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=684
06/02/2022 11:58:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=687
06/02/2022 11:58:11 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.6750633267661132 on epoch=687
06/02/2022 11:58:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=689
06/02/2022 11:58:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=692
06/02/2022 11:58:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=694
06/02/2022 11:58:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=697
06/02/2022 11:58:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=699
06/02/2022 11:58:18 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.6688405797101449 on epoch=699
06/02/2022 11:58:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=702
06/02/2022 11:58:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.20 on epoch=704
06/02/2022 11:58:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=707
06/02/2022 11:58:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=709
06/02/2022 11:58:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=712
06/02/2022 11:58:25 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.6119554204660588 on epoch=712
06/02/2022 11:58:26 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=714
06/02/2022 11:58:27 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=717
06/02/2022 11:58:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=719
06/02/2022 11:58:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=722
06/02/2022 11:58:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=724
06/02/2022 11:58:32 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.6885416666666666 on epoch=724
06/02/2022 11:58:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=727
06/02/2022 11:58:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=729
06/02/2022 11:58:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=732
06/02/2022 11:58:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=734
06/02/2022 11:58:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=737
06/02/2022 11:58:39 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.6618773946360154 on epoch=737
06/02/2022 11:58:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=739
06/02/2022 11:58:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=742
06/02/2022 11:58:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=744
06/02/2022 11:58:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=747
06/02/2022 11:58:45 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
06/02/2022 11:58:46 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.6327404479578393 on epoch=749
06/02/2022 11:58:46 - INFO - __main__ - save last model!
06/02/2022 11:58:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 11:58:46 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 11:58:46 - INFO - __main__ - Printing 3 examples
06/02/2022 11:58:46 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:58:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:58:46 - INFO - __main__ - Printing 3 examples
06/02/2022 11:58:46 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:58:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:58:46 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:58:46 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:58:46 - INFO - __main__ - Printing 3 examples
06/02/2022 11:58:46 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:58:46 - INFO - __main__ - ['others']
06/02/2022 11:58:46 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:58:46 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:58:46 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:58:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:58:52 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:58:52 - INFO - __main__ - task name: emo
06/02/2022 11:58:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:58:52 - INFO - __main__ - Starting training!
06/02/2022 11:58:53 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 11:59:36 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.4_8_predictions.txt
06/02/2022 11:59:36 - INFO - __main__ - Classification-F1 on test data: 0.2039
06/02/2022 11:59:36 - INFO - __main__ - prefix=emo_16_100, lr=0.4, bsz=8, dev_performance=0.7293948412698413, test_performance=0.20390935460005227
06/02/2022 11:59:36 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.3, bsz=8 ...
06/02/2022 11:59:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:59:37 - INFO - __main__ - Printing 3 examples
06/02/2022 11:59:37 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:59:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:59:37 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 11:59:37 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 11:59:37 - INFO - __main__ - Printing 3 examples
06/02/2022 11:59:37 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 11:59:37 - INFO - __main__ - ['others']
06/02/2022 11:59:37 - INFO - __main__ - Tokenizing Input ...
06/02/2022 11:59:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 11:59:37 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 11:59:43 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 11:59:43 - INFO - __main__ - task name: emo
06/02/2022 11:59:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 11:59:44 - INFO - __main__ - Starting training!
06/02/2022 11:59:45 - INFO - __main__ - Step 10 Global step 10 Train loss 7.74 on epoch=2
06/02/2022 11:59:46 - INFO - __main__ - Step 20 Global step 20 Train loss 8.06 on epoch=4
06/02/2022 11:59:48 - INFO - __main__ - Step 30 Global step 30 Train loss 8.00 on epoch=7
06/02/2022 11:59:49 - INFO - __main__ - Step 40 Global step 40 Train loss 8.08 on epoch=9
06/02/2022 11:59:50 - INFO - __main__ - Step 50 Global step 50 Train loss 8.17 on epoch=12
06/02/2022 12:00:07 - INFO - __main__ - Global step 50 Train loss 8.01 Classification-F1 0.0 on epoch=12
06/02/2022 12:00:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=12, global_step=50
06/02/2022 12:00:08 - INFO - __main__ - Step 60 Global step 60 Train loss 8.36 on epoch=14
06/02/2022 12:00:10 - INFO - __main__ - Step 70 Global step 70 Train loss 8.62 on epoch=17
06/02/2022 12:00:11 - INFO - __main__ - Step 80 Global step 80 Train loss 8.70 on epoch=19
06/02/2022 12:00:12 - INFO - __main__ - Step 90 Global step 90 Train loss 8.78 on epoch=22
06/02/2022 12:00:13 - INFO - __main__ - Step 100 Global step 100 Train loss 8.88 on epoch=24
06/02/2022 12:00:31 - INFO - __main__ - Global step 100 Train loss 8.67 Classification-F1 0.0 on epoch=24
06/02/2022 12:00:32 - INFO - __main__ - Step 110 Global step 110 Train loss 8.73 on epoch=27
06/02/2022 12:00:33 - INFO - __main__ - Step 120 Global step 120 Train loss 8.75 on epoch=29
06/02/2022 12:00:35 - INFO - __main__ - Step 130 Global step 130 Train loss 8.68 on epoch=32
06/02/2022 12:00:36 - INFO - __main__ - Step 140 Global step 140 Train loss 8.79 on epoch=34
06/02/2022 12:00:37 - INFO - __main__ - Step 150 Global step 150 Train loss 8.80 on epoch=37
06/02/2022 12:00:48 - INFO - __main__ - Global step 150 Train loss 8.75 Classification-F1 0.0 on epoch=37
06/02/2022 12:00:49 - INFO - __main__ - Step 160 Global step 160 Train loss 8.88 on epoch=39
06/02/2022 12:00:50 - INFO - __main__ - Step 170 Global step 170 Train loss 8.80 on epoch=42
06/02/2022 12:00:52 - INFO - __main__ - Step 180 Global step 180 Train loss 8.88 on epoch=44
06/02/2022 12:00:53 - INFO - __main__ - Step 190 Global step 190 Train loss 8.73 on epoch=47
06/02/2022 12:00:54 - INFO - __main__ - Step 200 Global step 200 Train loss 8.88 on epoch=49
06/02/2022 12:01:09 - INFO - __main__ - Global step 200 Train loss 8.83 Classification-F1 0.0 on epoch=49
06/02/2022 12:01:10 - INFO - __main__ - Step 210 Global step 210 Train loss 8.89 on epoch=52
06/02/2022 12:01:12 - INFO - __main__ - Step 220 Global step 220 Train loss 8.86 on epoch=54
06/02/2022 12:01:13 - INFO - __main__ - Step 230 Global step 230 Train loss 8.75 on epoch=57
06/02/2022 12:01:14 - INFO - __main__ - Step 240 Global step 240 Train loss 8.89 on epoch=59
06/02/2022 12:01:16 - INFO - __main__ - Step 250 Global step 250 Train loss 8.77 on epoch=62
06/02/2022 12:01:30 - INFO - __main__ - Global step 250 Train loss 8.83 Classification-F1 0.0 on epoch=62
06/02/2022 12:01:32 - INFO - __main__ - Step 260 Global step 260 Train loss 8.78 on epoch=64
06/02/2022 12:01:33 - INFO - __main__ - Step 270 Global step 270 Train loss 8.80 on epoch=67
06/02/2022 12:01:34 - INFO - __main__ - Step 280 Global step 280 Train loss 8.79 on epoch=69
06/02/2022 12:01:35 - INFO - __main__ - Step 290 Global step 290 Train loss 8.81 on epoch=72
06/02/2022 12:01:37 - INFO - __main__ - Step 300 Global step 300 Train loss 8.82 on epoch=74
06/02/2022 12:01:49 - INFO - __main__ - Global step 300 Train loss 8.80 Classification-F1 0.0 on epoch=74
06/02/2022 12:01:51 - INFO - __main__ - Step 310 Global step 310 Train loss 8.86 on epoch=77
06/02/2022 12:01:52 - INFO - __main__ - Step 320 Global step 320 Train loss 8.86 on epoch=79
06/02/2022 12:01:53 - INFO - __main__ - Step 330 Global step 330 Train loss 8.81 on epoch=82
06/02/2022 12:01:55 - INFO - __main__ - Step 340 Global step 340 Train loss 8.83 on epoch=84
06/02/2022 12:01:56 - INFO - __main__ - Step 350 Global step 350 Train loss 8.81 on epoch=87
06/02/2022 12:02:06 - INFO - __main__ - Global step 350 Train loss 8.83 Classification-F1 0.0 on epoch=87
06/02/2022 12:02:07 - INFO - __main__ - Step 360 Global step 360 Train loss 8.92 on epoch=89
06/02/2022 12:02:08 - INFO - __main__ - Step 370 Global step 370 Train loss 8.91 on epoch=92
06/02/2022 12:02:10 - INFO - __main__ - Step 380 Global step 380 Train loss 8.93 on epoch=94
06/02/2022 12:02:11 - INFO - __main__ - Step 390 Global step 390 Train loss 8.91 on epoch=97
06/02/2022 12:02:12 - INFO - __main__ - Step 400 Global step 400 Train loss 8.89 on epoch=99
06/02/2022 12:02:33 - INFO - __main__ - Global step 400 Train loss 8.91 Classification-F1 0.0 on epoch=99
06/02/2022 12:02:34 - INFO - __main__ - Step 410 Global step 410 Train loss 8.85 on epoch=102
06/02/2022 12:02:35 - INFO - __main__ - Step 420 Global step 420 Train loss 8.88 on epoch=104
06/02/2022 12:02:37 - INFO - __main__ - Step 430 Global step 430 Train loss 8.82 on epoch=107
06/02/2022 12:02:38 - INFO - __main__ - Step 440 Global step 440 Train loss 8.97 on epoch=109
06/02/2022 12:02:39 - INFO - __main__ - Step 450 Global step 450 Train loss 8.80 on epoch=112
06/02/2022 12:02:56 - INFO - __main__ - Global step 450 Train loss 8.86 Classification-F1 0.0 on epoch=112
06/02/2022 12:02:58 - INFO - __main__ - Step 460 Global step 460 Train loss 8.77 on epoch=114
06/02/2022 12:02:59 - INFO - __main__ - Step 470 Global step 470 Train loss 8.88 on epoch=117
06/02/2022 12:03:00 - INFO - __main__ - Step 480 Global step 480 Train loss 8.89 on epoch=119
06/02/2022 12:03:02 - INFO - __main__ - Step 490 Global step 490 Train loss 8.85 on epoch=122
06/02/2022 12:03:03 - INFO - __main__ - Step 500 Global step 500 Train loss 8.92 on epoch=124
06/02/2022 12:03:20 - INFO - __main__ - Global step 500 Train loss 8.86 Classification-F1 0.0 on epoch=124
06/02/2022 12:03:21 - INFO - __main__ - Step 510 Global step 510 Train loss 8.91 on epoch=127
06/02/2022 12:03:22 - INFO - __main__ - Step 520 Global step 520 Train loss 8.82 on epoch=129
06/02/2022 12:03:24 - INFO - __main__ - Step 530 Global step 530 Train loss 8.83 on epoch=132
06/02/2022 12:03:25 - INFO - __main__ - Step 540 Global step 540 Train loss 8.77 on epoch=134
06/02/2022 12:03:26 - INFO - __main__ - Step 550 Global step 550 Train loss 8.89 on epoch=137
06/02/2022 12:03:33 - INFO - __main__ - Global step 550 Train loss 8.85 Classification-F1 0.0 on epoch=137
06/02/2022 12:03:35 - INFO - __main__ - Step 560 Global step 560 Train loss 8.82 on epoch=139
06/02/2022 12:03:36 - INFO - __main__ - Step 570 Global step 570 Train loss 8.70 on epoch=142
06/02/2022 12:03:37 - INFO - __main__ - Step 580 Global step 580 Train loss 8.67 on epoch=144
06/02/2022 12:03:39 - INFO - __main__ - Step 590 Global step 590 Train loss 8.68 on epoch=147
06/02/2022 12:03:40 - INFO - __main__ - Step 600 Global step 600 Train loss 8.82 on epoch=149
06/02/2022 12:04:00 - INFO - __main__ - Global step 600 Train loss 8.74 Classification-F1 0.0 on epoch=149
06/02/2022 12:04:02 - INFO - __main__ - Step 610 Global step 610 Train loss 8.83 on epoch=152
06/02/2022 12:04:03 - INFO - __main__ - Step 620 Global step 620 Train loss 8.80 on epoch=154
06/02/2022 12:04:04 - INFO - __main__ - Step 630 Global step 630 Train loss 8.85 on epoch=157
06/02/2022 12:04:06 - INFO - __main__ - Step 640 Global step 640 Train loss 8.97 on epoch=159
06/02/2022 12:04:07 - INFO - __main__ - Step 650 Global step 650 Train loss 8.84 on epoch=162
06/02/2022 12:04:22 - INFO - __main__ - Global step 650 Train loss 8.86 Classification-F1 0.0 on epoch=162
06/02/2022 12:04:23 - INFO - __main__ - Step 660 Global step 660 Train loss 8.94 on epoch=164
06/02/2022 12:04:24 - INFO - __main__ - Step 670 Global step 670 Train loss 8.81 on epoch=167
06/02/2022 12:04:26 - INFO - __main__ - Step 680 Global step 680 Train loss 8.95 on epoch=169
06/02/2022 12:04:27 - INFO - __main__ - Step 690 Global step 690 Train loss 8.86 on epoch=172
06/02/2022 12:04:28 - INFO - __main__ - Step 700 Global step 700 Train loss 8.96 on epoch=174
06/02/2022 12:04:35 - INFO - __main__ - Global step 700 Train loss 8.90 Classification-F1 0.0 on epoch=174
06/02/2022 12:04:36 - INFO - __main__ - Step 710 Global step 710 Train loss 8.84 on epoch=177
06/02/2022 12:04:38 - INFO - __main__ - Step 720 Global step 720 Train loss 8.90 on epoch=179
06/02/2022 12:04:39 - INFO - __main__ - Step 730 Global step 730 Train loss 8.76 on epoch=182
06/02/2022 12:04:40 - INFO - __main__ - Step 740 Global step 740 Train loss 8.88 on epoch=184
06/02/2022 12:04:41 - INFO - __main__ - Step 750 Global step 750 Train loss 8.74 on epoch=187
06/02/2022 12:04:53 - INFO - __main__ - Global step 750 Train loss 8.83 Classification-F1 0.0 on epoch=187
06/02/2022 12:04:55 - INFO - __main__ - Step 760 Global step 760 Train loss 8.91 on epoch=189
06/02/2022 12:04:56 - INFO - __main__ - Step 770 Global step 770 Train loss 8.78 on epoch=192
06/02/2022 12:04:57 - INFO - __main__ - Step 780 Global step 780 Train loss 8.86 on epoch=194
06/02/2022 12:04:58 - INFO - __main__ - Step 790 Global step 790 Train loss 8.88 on epoch=197
06/02/2022 12:05:00 - INFO - __main__ - Step 800 Global step 800 Train loss 8.84 on epoch=199
06/02/2022 12:05:13 - INFO - __main__ - Global step 800 Train loss 8.85 Classification-F1 0.0 on epoch=199
06/02/2022 12:05:14 - INFO - __main__ - Step 810 Global step 810 Train loss 8.64 on epoch=202
06/02/2022 12:05:15 - INFO - __main__ - Step 820 Global step 820 Train loss 8.95 on epoch=204
06/02/2022 12:05:17 - INFO - __main__ - Step 830 Global step 830 Train loss 8.71 on epoch=207
06/02/2022 12:05:18 - INFO - __main__ - Step 840 Global step 840 Train loss 8.87 on epoch=209
06/02/2022 12:05:19 - INFO - __main__ - Step 850 Global step 850 Train loss 8.75 on epoch=212
06/02/2022 12:05:26 - INFO - __main__ - Global step 850 Train loss 8.79 Classification-F1 0.0 on epoch=212
06/02/2022 12:05:28 - INFO - __main__ - Step 860 Global step 860 Train loss 8.73 on epoch=214
06/02/2022 12:05:29 - INFO - __main__ - Step 870 Global step 870 Train loss 8.70 on epoch=217
06/02/2022 12:05:30 - INFO - __main__ - Step 880 Global step 880 Train loss 8.81 on epoch=219
06/02/2022 12:05:32 - INFO - __main__ - Step 890 Global step 890 Train loss 8.72 on epoch=222
06/02/2022 12:05:33 - INFO - __main__ - Step 900 Global step 900 Train loss 8.78 on epoch=224
06/02/2022 12:05:47 - INFO - __main__ - Global step 900 Train loss 8.75 Classification-F1 0.0 on epoch=224
06/02/2022 12:05:48 - INFO - __main__ - Step 910 Global step 910 Train loss 8.76 on epoch=227
06/02/2022 12:05:50 - INFO - __main__ - Step 920 Global step 920 Train loss 8.77 on epoch=229
06/02/2022 12:05:51 - INFO - __main__ - Step 930 Global step 930 Train loss 8.81 on epoch=232
06/02/2022 12:05:52 - INFO - __main__ - Step 940 Global step 940 Train loss 8.73 on epoch=234
06/02/2022 12:05:54 - INFO - __main__ - Step 950 Global step 950 Train loss 8.68 on epoch=237
06/02/2022 12:06:02 - INFO - __main__ - Global step 950 Train loss 8.75 Classification-F1 0.0 on epoch=237
06/02/2022 12:06:03 - INFO - __main__ - Step 960 Global step 960 Train loss 8.58 on epoch=239
06/02/2022 12:06:05 - INFO - __main__ - Step 970 Global step 970 Train loss 8.62 on epoch=242
06/02/2022 12:06:06 - INFO - __main__ - Step 980 Global step 980 Train loss 8.72 on epoch=244
06/02/2022 12:06:07 - INFO - __main__ - Step 990 Global step 990 Train loss 8.68 on epoch=247
06/02/2022 12:06:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 8.62 on epoch=249
06/02/2022 12:06:18 - INFO - __main__ - Global step 1000 Train loss 8.64 Classification-F1 0.0 on epoch=249
06/02/2022 12:06:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 8.61 on epoch=252
06/02/2022 12:06:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 8.76 on epoch=254
06/02/2022 12:06:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 8.66 on epoch=257
06/02/2022 12:06:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 8.73 on epoch=259
06/02/2022 12:06:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 8.50 on epoch=262
06/02/2022 12:06:39 - INFO - __main__ - Global step 1050 Train loss 8.65 Classification-F1 0.0 on epoch=262
06/02/2022 12:06:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 8.64 on epoch=264
06/02/2022 12:06:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 8.67 on epoch=267
06/02/2022 12:06:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 8.70 on epoch=269
06/02/2022 12:06:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 8.67 on epoch=272
06/02/2022 12:06:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 8.67 on epoch=274
06/02/2022 12:06:59 - INFO - __main__ - Global step 1100 Train loss 8.67 Classification-F1 0.0 on epoch=274
06/02/2022 12:07:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 8.61 on epoch=277
06/02/2022 12:07:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 8.75 on epoch=279
06/02/2022 12:07:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 8.72 on epoch=282
06/02/2022 12:07:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 8.73 on epoch=284
06/02/2022 12:07:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 8.71 on epoch=287
06/02/2022 12:07:10 - INFO - __main__ - Global step 1150 Train loss 8.70 Classification-F1 0.0 on epoch=287
06/02/2022 12:07:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 8.70 on epoch=289
06/02/2022 12:07:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 8.79 on epoch=292
06/02/2022 12:07:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 8.73 on epoch=294
06/02/2022 12:07:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 8.69 on epoch=297
06/02/2022 12:07:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 8.76 on epoch=299
06/02/2022 12:07:28 - INFO - __main__ - Global step 1200 Train loss 8.73 Classification-F1 0.0 on epoch=299
06/02/2022 12:07:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 8.73 on epoch=302
06/02/2022 12:07:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 8.70 on epoch=304
06/02/2022 12:07:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 8.73 on epoch=307
06/02/2022 12:07:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 8.64 on epoch=309
06/02/2022 12:07:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 8.69 on epoch=312
06/02/2022 12:07:47 - INFO - __main__ - Global step 1250 Train loss 8.70 Classification-F1 0.0 on epoch=312
06/02/2022 12:07:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 8.69 on epoch=314
06/02/2022 12:07:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 8.65 on epoch=317
06/02/2022 12:07:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 8.74 on epoch=319
06/02/2022 12:07:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 8.65 on epoch=322
06/02/2022 12:07:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 8.71 on epoch=324
06/02/2022 12:08:03 - INFO - __main__ - Global step 1300 Train loss 8.69 Classification-F1 0.0 on epoch=324
06/02/2022 12:08:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 8.80 on epoch=327
06/02/2022 12:08:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 8.74 on epoch=329
06/02/2022 12:08:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 8.61 on epoch=332
06/02/2022 12:08:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 8.61 on epoch=334
06/02/2022 12:08:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 8.64 on epoch=337
06/02/2022 12:08:21 - INFO - __main__ - Global step 1350 Train loss 8.68 Classification-F1 0.0 on epoch=337
06/02/2022 12:08:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 8.76 on epoch=339
06/02/2022 12:08:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 8.58 on epoch=342
06/02/2022 12:08:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 8.78 on epoch=344
06/02/2022 12:08:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 8.70 on epoch=347
06/02/2022 12:08:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 8.74 on epoch=349
06/02/2022 12:08:45 - INFO - __main__ - Global step 1400 Train loss 8.71 Classification-F1 0.0 on epoch=349
06/02/2022 12:08:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 8.65 on epoch=352
06/02/2022 12:08:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 8.65 on epoch=354
06/02/2022 12:08:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 8.64 on epoch=357
06/02/2022 12:08:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 8.75 on epoch=359
06/02/2022 12:08:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 8.64 on epoch=362
06/02/2022 12:08:59 - INFO - __main__ - Global step 1450 Train loss 8.67 Classification-F1 0.0 on epoch=362
06/02/2022 12:09:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 8.66 on epoch=364
06/02/2022 12:09:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 8.62 on epoch=367
06/02/2022 12:09:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 8.76 on epoch=369
06/02/2022 12:09:05 - INFO - __main__ - Step 1490 Global step 1490 Train loss 8.72 on epoch=372
06/02/2022 12:09:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 8.63 on epoch=374
06/02/2022 12:09:09 - INFO - __main__ - Global step 1500 Train loss 8.68 Classification-F1 0.0 on epoch=374
06/02/2022 12:09:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 8.69 on epoch=377
06/02/2022 12:09:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 8.69 on epoch=379
06/02/2022 12:09:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 8.65 on epoch=382
06/02/2022 12:09:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 8.58 on epoch=384
06/02/2022 12:09:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 8.66 on epoch=387
06/02/2022 12:09:25 - INFO - __main__ - Global step 1550 Train loss 8.65 Classification-F1 0.0 on epoch=387
06/02/2022 12:09:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 8.67 on epoch=389
06/02/2022 12:09:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 8.69 on epoch=392
06/02/2022 12:09:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 8.59 on epoch=394
06/02/2022 12:09:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 8.62 on epoch=397
06/02/2022 12:09:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 8.64 on epoch=399
06/02/2022 12:09:38 - INFO - __main__ - Global step 1600 Train loss 8.64 Classification-F1 0.0 on epoch=399
06/02/2022 12:09:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 8.63 on epoch=402
06/02/2022 12:09:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 8.61 on epoch=404
06/02/2022 12:09:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 8.55 on epoch=407
06/02/2022 12:09:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 8.62 on epoch=409
06/02/2022 12:09:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 8.58 on epoch=412
06/02/2022 12:09:51 - INFO - __main__ - Global step 1650 Train loss 8.60 Classification-F1 0.0 on epoch=412
06/02/2022 12:09:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 8.51 on epoch=414
06/02/2022 12:09:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 8.60 on epoch=417
06/02/2022 12:09:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 8.60 on epoch=419
06/02/2022 12:09:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 8.40 on epoch=422
06/02/2022 12:09:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 8.51 on epoch=424
06/02/2022 12:10:07 - INFO - __main__ - Global step 1700 Train loss 8.52 Classification-F1 0.0 on epoch=424
06/02/2022 12:10:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 8.67 on epoch=427
06/02/2022 12:10:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 8.54 on epoch=429
06/02/2022 12:10:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 8.47 on epoch=432
06/02/2022 12:10:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 8.58 on epoch=434
06/02/2022 12:10:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 8.54 on epoch=437
06/02/2022 12:10:20 - INFO - __main__ - Global step 1750 Train loss 8.56 Classification-F1 0.0 on epoch=437
06/02/2022 12:10:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 8.57 on epoch=439
06/02/2022 12:10:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 8.54 on epoch=442
06/02/2022 12:10:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 8.46 on epoch=444
06/02/2022 12:10:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 8.56 on epoch=447
06/02/2022 12:10:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 8.48 on epoch=449
06/02/2022 12:10:39 - INFO - __main__ - Global step 1800 Train loss 8.52 Classification-F1 0.0 on epoch=449
06/02/2022 12:10:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 8.54 on epoch=452
06/02/2022 12:10:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 8.42 on epoch=454
06/02/2022 12:10:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 8.43 on epoch=457
06/02/2022 12:10:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 8.42 on epoch=459
06/02/2022 12:10:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 8.40 on epoch=462
06/02/2022 12:10:53 - INFO - __main__ - Global step 1850 Train loss 8.44 Classification-F1 0.0 on epoch=462
06/02/2022 12:10:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 8.52 on epoch=464
06/02/2022 12:10:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 8.41 on epoch=467
06/02/2022 12:10:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 8.50 on epoch=469
06/02/2022 12:10:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 8.39 on epoch=472
06/02/2022 12:10:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 8.51 on epoch=474
06/02/2022 12:11:05 - INFO - __main__ - Global step 1900 Train loss 8.46 Classification-F1 0.0 on epoch=474
06/02/2022 12:11:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 8.49 on epoch=477
06/02/2022 12:11:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 8.54 on epoch=479
06/02/2022 12:11:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 8.38 on epoch=482
06/02/2022 12:11:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 8.56 on epoch=484
06/02/2022 12:11:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 8.48 on epoch=487
06/02/2022 12:11:17 - INFO - __main__ - Global step 1950 Train loss 8.49 Classification-F1 0.0 on epoch=487
06/02/2022 12:11:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 8.53 on epoch=489
06/02/2022 12:11:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 8.47 on epoch=492
06/02/2022 12:11:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 8.50 on epoch=494
06/02/2022 12:11:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 8.38 on epoch=497
06/02/2022 12:11:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 8.48 on epoch=499
06/02/2022 12:11:32 - INFO - __main__ - Global step 2000 Train loss 8.47 Classification-F1 0.0 on epoch=499
06/02/2022 12:11:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 8.43 on epoch=502
06/02/2022 12:11:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 8.50 on epoch=504
06/02/2022 12:11:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 8.39 on epoch=507
06/02/2022 12:11:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 8.37 on epoch=509
06/02/2022 12:11:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 8.49 on epoch=512
06/02/2022 12:11:49 - INFO - __main__ - Global step 2050 Train loss 8.44 Classification-F1 0.0 on epoch=512
06/02/2022 12:11:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 8.38 on epoch=514
06/02/2022 12:11:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 8.38 on epoch=517
06/02/2022 12:11:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 8.54 on epoch=519
06/02/2022 12:11:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 8.32 on epoch=522
06/02/2022 12:11:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 8.43 on epoch=524
06/02/2022 12:12:07 - INFO - __main__ - Global step 2100 Train loss 8.41 Classification-F1 0.0 on epoch=524
06/02/2022 12:12:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 8.34 on epoch=527
06/02/2022 12:12:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 8.47 on epoch=529
06/02/2022 12:12:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 8.34 on epoch=532
06/02/2022 12:12:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 8.48 on epoch=534
06/02/2022 12:12:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 8.40 on epoch=537
06/02/2022 12:12:22 - INFO - __main__ - Global step 2150 Train loss 8.40 Classification-F1 0.0 on epoch=537
06/02/2022 12:12:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 8.41 on epoch=539
06/02/2022 12:12:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 8.33 on epoch=542
06/02/2022 12:12:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 8.43 on epoch=544
06/02/2022 12:12:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 8.36 on epoch=547
06/02/2022 12:12:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 8.29 on epoch=549
06/02/2022 12:12:46 - INFO - __main__ - Global step 2200 Train loss 8.37 Classification-F1 0.0 on epoch=549
06/02/2022 12:12:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 8.35 on epoch=552
06/02/2022 12:12:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 8.40 on epoch=554
06/02/2022 12:12:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 8.43 on epoch=557
06/02/2022 12:12:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 8.42 on epoch=559
06/02/2022 12:12:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 8.45 on epoch=562
06/02/2022 12:12:56 - INFO - __main__ - Global step 2250 Train loss 8.41 Classification-F1 0.0 on epoch=562
06/02/2022 12:12:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 8.42 on epoch=564
06/02/2022 12:12:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 8.29 on epoch=567
06/02/2022 12:13:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 8.47 on epoch=569
06/02/2022 12:13:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 8.44 on epoch=572
06/02/2022 12:13:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 8.48 on epoch=574
06/02/2022 12:13:14 - INFO - __main__ - Global step 2300 Train loss 8.42 Classification-F1 0.0 on epoch=574
06/02/2022 12:13:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 8.34 on epoch=577
06/02/2022 12:13:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 8.32 on epoch=579
06/02/2022 12:13:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 8.24 on epoch=582
06/02/2022 12:13:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 8.28 on epoch=584
06/02/2022 12:13:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 8.28 on epoch=587
06/02/2022 12:13:25 - INFO - __main__ - Global step 2350 Train loss 8.29 Classification-F1 0.0 on epoch=587
06/02/2022 12:13:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 8.38 on epoch=589
06/02/2022 12:13:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 8.24 on epoch=592
06/02/2022 12:13:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 8.33 on epoch=594
06/02/2022 12:13:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 8.14 on epoch=597
06/02/2022 12:13:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 8.25 on epoch=599
06/02/2022 12:13:38 - INFO - __main__ - Global step 2400 Train loss 8.27 Classification-F1 0.0 on epoch=599
06/02/2022 12:13:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 8.17 on epoch=602
06/02/2022 12:13:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 8.28 on epoch=604
06/02/2022 12:13:42 - INFO - __main__ - Step 2430 Global step 2430 Train loss 8.18 on epoch=607
06/02/2022 12:13:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 8.30 on epoch=609
06/02/2022 12:13:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 8.17 on epoch=612
06/02/2022 12:13:56 - INFO - __main__ - Global step 2450 Train loss 8.22 Classification-F1 0.0 on epoch=612
06/02/2022 12:13:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 8.23 on epoch=614
06/02/2022 12:13:59 - INFO - __main__ - Step 2470 Global step 2470 Train loss 8.07 on epoch=617
06/02/2022 12:14:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 8.14 on epoch=619
06/02/2022 12:14:01 - INFO - __main__ - Step 2490 Global step 2490 Train loss 8.20 on epoch=622
06/02/2022 12:14:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 8.23 on epoch=624
06/02/2022 12:14:09 - INFO - __main__ - Global step 2500 Train loss 8.17 Classification-F1 0.0 on epoch=624
06/02/2022 12:14:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 8.14 on epoch=627
06/02/2022 12:14:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 8.05 on epoch=629
06/02/2022 12:14:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 8.25 on epoch=632
06/02/2022 12:14:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 8.14 on epoch=634
06/02/2022 12:14:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 7.88 on epoch=637
06/02/2022 12:14:23 - INFO - __main__ - Global step 2550 Train loss 8.09 Classification-F1 0.0 on epoch=637
06/02/2022 12:14:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 8.18 on epoch=639
06/02/2022 12:14:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 8.17 on epoch=642
06/02/2022 12:14:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 7.99 on epoch=644
06/02/2022 12:14:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 7.97 on epoch=647
06/02/2022 12:14:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 8.04 on epoch=649
06/02/2022 12:14:37 - INFO - __main__ - Global step 2600 Train loss 8.07 Classification-F1 0.0 on epoch=649
06/02/2022 12:14:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 8.13 on epoch=652
06/02/2022 12:14:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 8.01 on epoch=654
06/02/2022 12:14:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 7.93 on epoch=657
06/02/2022 12:14:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 7.91 on epoch=659
06/02/2022 12:14:43 - INFO - __main__ - Step 2650 Global step 2650 Train loss 7.85 on epoch=662
06/02/2022 12:14:51 - INFO - __main__ - Global step 2650 Train loss 7.96 Classification-F1 0.0 on epoch=662
06/02/2022 12:14:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 7.90 on epoch=664
06/02/2022 12:14:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 7.83 on epoch=667
06/02/2022 12:14:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 7.95 on epoch=669
06/02/2022 12:14:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 7.90 on epoch=672
06/02/2022 12:14:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 7.89 on epoch=674
06/02/2022 12:15:03 - INFO - __main__ - Global step 2700 Train loss 7.89 Classification-F1 0.0 on epoch=674
06/02/2022 12:15:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 7.77 on epoch=677
06/02/2022 12:15:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 7.96 on epoch=679
06/02/2022 12:15:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 7.89 on epoch=682
06/02/2022 12:15:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 7.78 on epoch=684
06/02/2022 12:15:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 7.92 on epoch=687
06/02/2022 12:15:15 - INFO - __main__ - Global step 2750 Train loss 7.87 Classification-F1 0.0 on epoch=687
06/02/2022 12:15:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 7.78 on epoch=689
06/02/2022 12:15:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 7.81 on epoch=692
06/02/2022 12:15:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 8.05 on epoch=694
06/02/2022 12:15:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 7.77 on epoch=697
06/02/2022 12:15:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 7.91 on epoch=699
06/02/2022 12:15:38 - INFO - __main__ - Global step 2800 Train loss 7.86 Classification-F1 0.0 on epoch=699
06/02/2022 12:15:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 7.88 on epoch=702
06/02/2022 12:15:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 7.83 on epoch=704
06/02/2022 12:15:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 7.87 on epoch=707
06/02/2022 12:15:43 - INFO - __main__ - Step 2840 Global step 2840 Train loss 7.86 on epoch=709
06/02/2022 12:15:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 7.81 on epoch=712
06/02/2022 12:15:48 - INFO - __main__ - Global step 2850 Train loss 7.85 Classification-F1 0.0 on epoch=712
06/02/2022 12:15:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 7.81 on epoch=714
06/02/2022 12:15:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 7.72 on epoch=717
06/02/2022 12:15:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 7.79 on epoch=719
06/02/2022 12:15:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 7.66 on epoch=722
06/02/2022 12:15:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 7.87 on epoch=724
06/02/2022 12:16:04 - INFO - __main__ - Global step 2900 Train loss 7.77 Classification-F1 0.0 on epoch=724
06/02/2022 12:16:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 7.56 on epoch=727
06/02/2022 12:16:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 7.70 on epoch=729
06/02/2022 12:16:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 7.65 on epoch=732
06/02/2022 12:16:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 7.66 on epoch=734
06/02/2022 12:16:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 7.44 on epoch=737
06/02/2022 12:16:15 - INFO - __main__ - Global step 2950 Train loss 7.60 Classification-F1 0.0 on epoch=737
06/02/2022 12:16:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 7.52 on epoch=739
06/02/2022 12:16:18 - INFO - __main__ - Step 2970 Global step 2970 Train loss 7.70 on epoch=742
06/02/2022 12:16:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 7.64 on epoch=744
06/02/2022 12:16:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 7.45 on epoch=747
06/02/2022 12:16:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 7.58 on epoch=749
06/02/2022 12:16:23 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:16:23 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:23 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:23 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:16:23 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:16:23 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:23 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 12:16:23 - INFO - __main__ - ['others']
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:23 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:16:29 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:16:29 - INFO - __main__ - task name: emo
06/02/2022 12:16:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:16:29 - INFO - __main__ - Starting training!
06/02/2022 12:16:31 - INFO - __main__ - Global step 3000 Train loss 7.58 Classification-F1 0.0 on epoch=749
06/02/2022 12:16:31 - INFO - __main__ - save last model!
06/02/2022 12:16:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 12:16:31 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 12:16:31 - INFO - __main__ - Printing 3 examples
06/02/2022 12:16:31 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 12:16:31 - INFO - __main__ - ['others']
06/02/2022 12:16:31 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 12:16:31 - INFO - __main__ - ['others']
06/02/2022 12:16:31 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 12:16:31 - INFO - __main__ - ['others']
06/02/2022 12:16:31 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:16:33 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:16:38 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 12:24:13 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.3_8_predictions.txt
06/02/2022 12:24:14 - INFO - __main__ - Classification-F1 on test data: 0.0000
06/02/2022 12:24:14 - INFO - __main__ - prefix=emo_16_100, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.0
06/02/2022 12:24:14 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.2, bsz=8 ...
06/02/2022 12:24:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:24:15 - INFO - __main__ - Printing 3 examples
06/02/2022 12:24:15 - INFO - __main__ -  [emo] how cause yes am listening
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:24:15 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:24:15 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:24:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:24:15 - INFO - __main__ - Printing 3 examples
06/02/2022 12:24:15 - INFO - __main__ -  [emo] ok i wiil ask u some questions done what is ur full name
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ -  [emo] give your num i send message to this num no to tjis
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ -  [emo] what is docker vagrant and docker are different beasts what is vagrant
06/02/2022 12:24:15 - INFO - __main__ - ['others']
06/02/2022 12:24:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:24:15 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:24:15 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:24:20 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:24:20 - INFO - __main__ - task name: emo
06/02/2022 12:24:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:24:21 - INFO - __main__ - Starting training!
06/02/2022 12:24:22 - INFO - __main__ - Step 10 Global step 10 Train loss 7.11 on epoch=2
06/02/2022 12:24:23 - INFO - __main__ - Step 20 Global step 20 Train loss 5.35 on epoch=4
06/02/2022 12:24:25 - INFO - __main__ - Step 30 Global step 30 Train loss 3.59 on epoch=7
06/02/2022 12:24:26 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=9
06/02/2022 12:24:27 - INFO - __main__ - Step 50 Global step 50 Train loss 1.97 on epoch=12
06/02/2022 12:24:28 - INFO - __main__ - Global step 50 Train loss 4.09 Classification-F1 0.16862745098039217 on epoch=12
06/02/2022 12:24:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16862745098039217 on epoch=12, global_step=50
06/02/2022 12:24:29 - INFO - __main__ - Step 60 Global step 60 Train loss 1.58 on epoch=14
06/02/2022 12:24:30 - INFO - __main__ - Step 70 Global step 70 Train loss 1.71 on epoch=17
06/02/2022 12:24:31 - INFO - __main__ - Step 80 Global step 80 Train loss 1.42 on epoch=19
06/02/2022 12:24:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.43 on epoch=22
06/02/2022 12:24:34 - INFO - __main__ - Step 100 Global step 100 Train loss 1.32 on epoch=24
06/02/2022 12:24:34 - INFO - __main__ - Global step 100 Train loss 1.49 Classification-F1 0.25681818181818183 on epoch=24
06/02/2022 12:24:34 - INFO - __main__ - Saving model with best Classification-F1: 0.16862745098039217 -> 0.25681818181818183 on epoch=24, global_step=100
06/02/2022 12:24:36 - INFO - __main__ - Step 110 Global step 110 Train loss 1.26 on epoch=27
06/02/2022 12:24:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.20 on epoch=29
06/02/2022 12:24:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=32
06/02/2022 12:24:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=34
06/02/2022 12:24:40 - INFO - __main__ - Step 150 Global step 150 Train loss 1.03 on epoch=37
06/02/2022 12:24:41 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.2556818181818182 on epoch=37
06/02/2022 12:24:42 - INFO - __main__ - Step 160 Global step 160 Train loss 1.16 on epoch=39
06/02/2022 12:24:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.11 on epoch=42
06/02/2022 12:24:45 - INFO - __main__ - Step 180 Global step 180 Train loss 1.11 on epoch=44
06/02/2022 12:24:46 - INFO - __main__ - Step 190 Global step 190 Train loss 1.66 on epoch=47
06/02/2022 12:24:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.03 on epoch=49
06/02/2022 12:24:48 - INFO - __main__ - Global step 200 Train loss 1.21 Classification-F1 0.17764705882352944 on epoch=49
06/02/2022 12:24:49 - INFO - __main__ - Step 210 Global step 210 Train loss 1.02 on epoch=52
06/02/2022 12:24:50 - INFO - __main__ - Step 220 Global step 220 Train loss 1.04 on epoch=54
06/02/2022 12:24:52 - INFO - __main__ - Step 230 Global step 230 Train loss 1.04 on epoch=57
06/02/2022 12:24:53 - INFO - __main__ - Step 240 Global step 240 Train loss 1.10 on epoch=59
06/02/2022 12:24:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=62
06/02/2022 12:24:55 - INFO - __main__ - Global step 250 Train loss 1.02 Classification-F1 0.19927536231884058 on epoch=62
06/02/2022 12:24:56 - INFO - __main__ - Step 260 Global step 260 Train loss 1.07 on epoch=64
06/02/2022 12:24:57 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=67
06/02/2022 12:24:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.98 on epoch=69
06/02/2022 12:25:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.96 on epoch=72
06/02/2022 12:25:01 - INFO - __main__ - Step 300 Global step 300 Train loss 1.18 on epoch=74
06/02/2022 12:25:01 - INFO - __main__ - Global step 300 Train loss 1.04 Classification-F1 0.22978926875917893 on epoch=74
06/02/2022 12:25:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
06/02/2022 12:25:04 - INFO - __main__ - Step 320 Global step 320 Train loss 1.15 on epoch=79
06/02/2022 12:25:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.03 on epoch=82
06/02/2022 12:25:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
06/02/2022 12:25:08 - INFO - __main__ - Step 350 Global step 350 Train loss 1.00 on epoch=87
06/02/2022 12:25:08 - INFO - __main__ - Global step 350 Train loss 1.01 Classification-F1 0.1565276828434723 on epoch=87
06/02/2022 12:25:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
06/02/2022 12:25:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
06/02/2022 12:25:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.98 on epoch=94
06/02/2022 12:25:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.97 on epoch=97
06/02/2022 12:25:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
06/02/2022 12:25:15 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.3599154775462808 on epoch=99
06/02/2022 12:25:15 - INFO - __main__ - Saving model with best Classification-F1: 0.25681818181818183 -> 0.3599154775462808 on epoch=99, global_step=400
06/02/2022 12:25:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
06/02/2022 12:25:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
06/02/2022 12:25:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=107
06/02/2022 12:25:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
06/02/2022 12:25:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.97 on epoch=112
06/02/2022 12:25:22 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.22993546758958827 on epoch=112
06/02/2022 12:25:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.97 on epoch=114
06/02/2022 12:25:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.93 on epoch=117
06/02/2022 12:25:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
06/02/2022 12:25:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.99 on epoch=122
06/02/2022 12:25:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.88 on epoch=124
06/02/2022 12:25:29 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.31889834881320955 on epoch=124
06/02/2022 12:25:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.93 on epoch=127
06/02/2022 12:25:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.91 on epoch=129
06/02/2022 12:25:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.87 on epoch=132
06/02/2022 12:25:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.79 on epoch=134
06/02/2022 12:25:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
06/02/2022 12:25:35 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.3182957393483709 on epoch=137
06/02/2022 12:25:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.86 on epoch=139
06/02/2022 12:25:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.87 on epoch=142
06/02/2022 12:25:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
06/02/2022 12:25:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.88 on epoch=147
06/02/2022 12:25:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
06/02/2022 12:25:42 - INFO - __main__ - Global step 600 Train loss 0.87 Classification-F1 0.40730284208545076 on epoch=149
06/02/2022 12:25:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3599154775462808 -> 0.40730284208545076 on epoch=149, global_step=600
06/02/2022 12:25:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.87 on epoch=152
06/02/2022 12:25:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.85 on epoch=154
06/02/2022 12:25:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=157
06/02/2022 12:25:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.95 on epoch=159
06/02/2022 12:25:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.88 on epoch=162
06/02/2022 12:25:49 - INFO - __main__ - Global step 650 Train loss 0.90 Classification-F1 0.39870006252651796 on epoch=162
06/02/2022 12:25:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.96 on epoch=164
06/02/2022 12:25:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.96 on epoch=167
06/02/2022 12:25:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=169
06/02/2022 12:25:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.95 on epoch=172
06/02/2022 12:25:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
06/02/2022 12:25:56 - INFO - __main__ - Global step 700 Train loss 0.94 Classification-F1 0.29485380116959065 on epoch=174
06/02/2022 12:25:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=177
06/02/2022 12:25:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
06/02/2022 12:25:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
06/02/2022 12:26:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.90 on epoch=184
06/02/2022 12:26:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.90 on epoch=187
06/02/2022 12:26:03 - INFO - __main__ - Global step 750 Train loss 0.89 Classification-F1 0.26525725232621783 on epoch=187
06/02/2022 12:26:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=189
06/02/2022 12:26:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.87 on epoch=192
06/02/2022 12:26:06 - INFO - __main__ - Step 780 Global step 780 Train loss 1.02 on epoch=194
06/02/2022 12:26:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.88 on epoch=197
06/02/2022 12:26:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.86 on epoch=199
06/02/2022 12:26:09 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.2969569362784046 on epoch=199
06/02/2022 12:26:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
06/02/2022 12:26:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.84 on epoch=204
06/02/2022 12:26:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.89 on epoch=207
06/02/2022 12:26:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.80 on epoch=209
06/02/2022 12:26:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.83 on epoch=212
06/02/2022 12:26:16 - INFO - __main__ - Global step 850 Train loss 0.85 Classification-F1 0.32734704600194553 on epoch=212
06/02/2022 12:26:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.93 on epoch=214
06/02/2022 12:26:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.87 on epoch=217
06/02/2022 12:26:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.85 on epoch=219
06/02/2022 12:26:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.72 on epoch=222
06/02/2022 12:26:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.74 on epoch=224
06/02/2022 12:26:23 - INFO - __main__ - Global step 900 Train loss 0.82 Classification-F1 0.4368421052631579 on epoch=224
06/02/2022 12:26:23 - INFO - __main__ - Saving model with best Classification-F1: 0.40730284208545076 -> 0.4368421052631579 on epoch=224, global_step=900
06/02/2022 12:26:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=227
06/02/2022 12:26:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.69 on epoch=229
06/02/2022 12:26:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.72 on epoch=232
06/02/2022 12:26:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
06/02/2022 12:26:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.84 on epoch=237
06/02/2022 12:26:30 - INFO - __main__ - Global step 950 Train loss 0.76 Classification-F1 0.4957393483709273 on epoch=237
06/02/2022 12:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4368421052631579 -> 0.4957393483709273 on epoch=237, global_step=950
06/02/2022 12:26:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.79 on epoch=239
06/02/2022 12:26:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.75 on epoch=242
06/02/2022 12:26:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.75 on epoch=244
06/02/2022 12:26:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
06/02/2022 12:26:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.74 on epoch=249
06/02/2022 12:26:36 - INFO - __main__ - Global step 1000 Train loss 0.75 Classification-F1 0.42917592917592917 on epoch=249
06/02/2022 12:26:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.65 on epoch=252
06/02/2022 12:26:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.74 on epoch=254
06/02/2022 12:26:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.75 on epoch=257
06/02/2022 12:26:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.77 on epoch=259
06/02/2022 12:26:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.77 on epoch=262
06/02/2022 12:26:43 - INFO - __main__ - Global step 1050 Train loss 0.74 Classification-F1 0.5277605032595959 on epoch=262
06/02/2022 12:26:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4957393483709273 -> 0.5277605032595959 on epoch=262, global_step=1050
06/02/2022 12:26:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.70 on epoch=264
06/02/2022 12:26:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.71 on epoch=267
06/02/2022 12:26:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=269
06/02/2022 12:26:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.65 on epoch=272
06/02/2022 12:26:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.62 on epoch=274
06/02/2022 12:26:50 - INFO - __main__ - Global step 1100 Train loss 0.67 Classification-F1 0.4886155913978495 on epoch=274
06/02/2022 12:26:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.76 on epoch=277
06/02/2022 12:26:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.67 on epoch=279
06/02/2022 12:26:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.63 on epoch=282
06/02/2022 12:26:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.66 on epoch=284
06/02/2022 12:26:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.60 on epoch=287
06/02/2022 12:26:57 - INFO - __main__ - Global step 1150 Train loss 0.66 Classification-F1 0.5288832694347011 on epoch=287
06/02/2022 12:26:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5277605032595959 -> 0.5288832694347011 on epoch=287, global_step=1150
06/02/2022 12:26:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.76 on epoch=289
06/02/2022 12:26:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.69 on epoch=292
06/02/2022 12:27:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.70 on epoch=294
06/02/2022 12:27:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.70 on epoch=297
06/02/2022 12:27:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.66 on epoch=299
06/02/2022 12:27:04 - INFO - __main__ - Global step 1200 Train loss 0.70 Classification-F1 0.4613408521303257 on epoch=299
06/02/2022 12:27:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.67 on epoch=302
06/02/2022 12:27:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.61 on epoch=304
06/02/2022 12:27:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.59 on epoch=307
06/02/2022 12:27:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.64 on epoch=309
06/02/2022 12:27:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.70 on epoch=312
06/02/2022 12:27:10 - INFO - __main__ - Global step 1250 Train loss 0.64 Classification-F1 0.5095348837209303 on epoch=312
06/02/2022 12:27:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.60 on epoch=314
06/02/2022 12:27:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=317
06/02/2022 12:27:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=319
06/02/2022 12:27:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=322
06/02/2022 12:27:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=324
06/02/2022 12:27:17 - INFO - __main__ - Global step 1300 Train loss 0.59 Classification-F1 0.49619453044375644 on epoch=324
06/02/2022 12:27:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.65 on epoch=327
06/02/2022 12:27:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=329
06/02/2022 12:27:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.63 on epoch=332
06/02/2022 12:27:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=334
06/02/2022 12:27:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=337
06/02/2022 12:27:24 - INFO - __main__ - Global step 1350 Train loss 0.57 Classification-F1 0.5477634892957474 on epoch=337
06/02/2022 12:27:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5288832694347011 -> 0.5477634892957474 on epoch=337, global_step=1350
06/02/2022 12:27:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=339
06/02/2022 12:27:26 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.53 on epoch=342
06/02/2022 12:27:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=344
06/02/2022 12:27:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.62 on epoch=347
06/02/2022 12:27:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=349
06/02/2022 12:27:31 - INFO - __main__ - Global step 1400 Train loss 0.53 Classification-F1 0.5512076562373676 on epoch=349
06/02/2022 12:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5477634892957474 -> 0.5512076562373676 on epoch=349, global_step=1400
06/02/2022 12:27:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.56 on epoch=352
06/02/2022 12:27:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=354
06/02/2022 12:27:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=357
06/02/2022 12:27:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=359
06/02/2022 12:27:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=362
06/02/2022 12:27:38 - INFO - __main__ - Global step 1450 Train loss 0.52 Classification-F1 0.49241071428571426 on epoch=362
06/02/2022 12:27:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=364
06/02/2022 12:27:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=367
06/02/2022 12:27:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=369
06/02/2022 12:27:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.60 on epoch=372
06/02/2022 12:27:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=374
06/02/2022 12:27:44 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.5289014377927053 on epoch=374
06/02/2022 12:27:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=377
06/02/2022 12:27:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=379
06/02/2022 12:27:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=382
06/02/2022 12:27:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=384
06/02/2022 12:27:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=387
06/02/2022 12:27:51 - INFO - __main__ - Global step 1550 Train loss 0.45 Classification-F1 0.5286935286935287 on epoch=387
06/02/2022 12:27:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=389
06/02/2022 12:27:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=392
06/02/2022 12:27:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=394
06/02/2022 12:27:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=397
06/02/2022 12:27:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=399
06/02/2022 12:27:58 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.5513287582253099 on epoch=399
06/02/2022 12:27:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5512076562373676 -> 0.5513287582253099 on epoch=399, global_step=1600
06/02/2022 12:27:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
06/02/2022 12:28:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=404
06/02/2022 12:28:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=407
06/02/2022 12:28:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=409
06/02/2022 12:28:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=412
06/02/2022 12:28:05 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.5564039408866995 on epoch=412
06/02/2022 12:28:05 - INFO - __main__ - Saving model with best Classification-F1: 0.5513287582253099 -> 0.5564039408866995 on epoch=412, global_step=1650
06/02/2022 12:28:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=414
06/02/2022 12:28:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=417
06/02/2022 12:28:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=419
06/02/2022 12:28:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=422
06/02/2022 12:28:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=424
06/02/2022 12:28:12 - INFO - __main__ - Global step 1700 Train loss 0.37 Classification-F1 0.5642857142857143 on epoch=424
06/02/2022 12:28:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5564039408866995 -> 0.5642857142857143 on epoch=424, global_step=1700
06/02/2022 12:28:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=427
06/02/2022 12:28:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=429
06/02/2022 12:28:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=432
06/02/2022 12:28:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=434
06/02/2022 12:28:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=437
06/02/2022 12:28:19 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.5333520968195891 on epoch=437
06/02/2022 12:28:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=439
06/02/2022 12:28:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=442
06/02/2022 12:28:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=444
06/02/2022 12:28:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=447
06/02/2022 12:28:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=449
06/02/2022 12:28:25 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.5365141232575201 on epoch=449
06/02/2022 12:28:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=452
06/02/2022 12:28:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=454
06/02/2022 12:28:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=457
06/02/2022 12:28:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=459
06/02/2022 12:28:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=462
06/02/2022 12:28:32 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.5798275509650722 on epoch=462
06/02/2022 12:28:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5642857142857143 -> 0.5798275509650722 on epoch=462, global_step=1850
06/02/2022 12:28:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=464
06/02/2022 12:28:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=467
06/02/2022 12:28:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=469
06/02/2022 12:28:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=472
06/02/2022 12:28:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=474
06/02/2022 12:28:39 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.711154768899334 on epoch=474
06/02/2022 12:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5798275509650722 -> 0.711154768899334 on epoch=474, global_step=1900
06/02/2022 12:28:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=477
06/02/2022 12:28:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=479
06/02/2022 12:28:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=482
06/02/2022 12:28:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=484
06/02/2022 12:28:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=487
06/02/2022 12:28:46 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.6131160572337042 on epoch=487
06/02/2022 12:28:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=489
06/02/2022 12:28:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=492
06/02/2022 12:28:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=494
06/02/2022 12:28:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=497
06/02/2022 12:28:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=499
06/02/2022 12:28:53 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.6489826302729529 on epoch=499
06/02/2022 12:28:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.37 on epoch=502
06/02/2022 12:28:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=504
06/02/2022 12:28:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=507
06/02/2022 12:28:58 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=509
06/02/2022 12:28:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=512
06/02/2022 12:28:59 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.6571637426900585 on epoch=512
06/02/2022 12:29:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=514
06/02/2022 12:29:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=517
06/02/2022 12:29:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=519
06/02/2022 12:29:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=522
06/02/2022 12:29:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=524
06/02/2022 12:29:06 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.6810065237651444 on epoch=524
06/02/2022 12:29:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=527
06/02/2022 12:29:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=529
06/02/2022 12:29:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=532
06/02/2022 12:29:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=534
06/02/2022 12:29:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=537
06/02/2022 12:29:13 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.6877552177858439 on epoch=537
06/02/2022 12:29:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=539
06/02/2022 12:29:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=542
06/02/2022 12:29:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=544
06/02/2022 12:29:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=547
06/02/2022 12:29:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=549
06/02/2022 12:29:19 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7176134781397939 on epoch=549
06/02/2022 12:29:19 - INFO - __main__ - Saving model with best Classification-F1: 0.711154768899334 -> 0.7176134781397939 on epoch=549, global_step=2200
06/02/2022 12:29:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=552
06/02/2022 12:29:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=554
06/02/2022 12:29:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=557
06/02/2022 12:29:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=559
06/02/2022 12:29:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=562
06/02/2022 12:29:26 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.6728534155597723 on epoch=562
06/02/2022 12:29:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=564
06/02/2022 12:29:29 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=567
06/02/2022 12:29:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=569
06/02/2022 12:29:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=572
06/02/2022 12:29:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=574
06/02/2022 12:29:33 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.68592060960482 on epoch=574
06/02/2022 12:29:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
06/02/2022 12:29:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=579
06/02/2022 12:29:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=582
06/02/2022 12:29:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=584
06/02/2022 12:29:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=587
06/02/2022 12:29:40 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.6957264957264956 on epoch=587
06/02/2022 12:29:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=589
06/02/2022 12:29:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
06/02/2022 12:29:44 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=594
06/02/2022 12:29:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
06/02/2022 12:29:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=599
06/02/2022 12:29:47 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.6990643764837313 on epoch=599
06/02/2022 12:29:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=602
06/02/2022 12:29:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=604
06/02/2022 12:29:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
06/02/2022 12:29:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
06/02/2022 12:29:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
06/02/2022 12:29:53 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.6823953073953073 on epoch=612
06/02/2022 12:29:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=614
06/02/2022 12:29:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=617
06/02/2022 12:29:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
06/02/2022 12:29:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
06/02/2022 12:30:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
06/02/2022 12:30:00 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7117117117117118 on epoch=624
06/02/2022 12:30:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=627
06/02/2022 12:30:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=629
06/02/2022 12:30:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=632
06/02/2022 12:30:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=634
06/02/2022 12:30:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=637
06/02/2022 12:30:07 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.6728707250446381 on epoch=637
06/02/2022 12:30:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=639
06/02/2022 12:30:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=642
06/02/2022 12:30:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=644
06/02/2022 12:30:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 12:30:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
06/02/2022 12:30:14 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.667251361912096 on epoch=649
06/02/2022 12:30:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
06/02/2022 12:30:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=654
06/02/2022 12:30:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=657
06/02/2022 12:30:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
06/02/2022 12:30:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=662
06/02/2022 12:30:20 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.7161709258483453 on epoch=662
06/02/2022 12:30:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=664
06/02/2022 12:30:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=667
06/02/2022 12:30:24 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/02/2022 12:30:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
06/02/2022 12:30:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
06/02/2022 12:30:27 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6646198149147333 on epoch=674
06/02/2022 12:30:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=677
06/02/2022 12:30:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
06/02/2022 12:30:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=682
06/02/2022 12:30:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=684
06/02/2022 12:30:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/02/2022 12:30:34 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.6503923519009726 on epoch=687
06/02/2022 12:30:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
06/02/2022 12:30:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=692
06/02/2022 12:30:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/02/2022 12:30:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=697
06/02/2022 12:30:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/02/2022 12:30:41 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.6825593761077633 on epoch=699
06/02/2022 12:30:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
06/02/2022 12:30:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=704
06/02/2022 12:30:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=707
06/02/2022 12:30:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=709
06/02/2022 12:30:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
06/02/2022 12:30:48 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.6763578715191618 on epoch=712
06/02/2022 12:30:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=714
06/02/2022 12:30:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/02/2022 12:30:51 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 12:30:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/02/2022 12:30:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/02/2022 12:30:54 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6606959706959707 on epoch=724
06/02/2022 12:30:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=727
06/02/2022 12:30:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
06/02/2022 12:30:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/02/2022 12:30:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=734
06/02/2022 12:31:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=737
06/02/2022 12:31:01 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7098684210526316 on epoch=737
06/02/2022 12:31:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
06/02/2022 12:31:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
06/02/2022 12:31:05 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=744
06/02/2022 12:31:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=747
06/02/2022 12:31:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=749
06/02/2022 12:31:08 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.6968641361468187 on epoch=749
06/02/2022 12:31:08 - INFO - __main__ - save last model!
06/02/2022 12:31:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 12:31:08 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 12:31:08 - INFO - __main__ - Printing 3 examples
06/02/2022 12:31:08 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 12:31:08 - INFO - __main__ - ['others']
06/02/2022 12:31:08 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 12:31:08 - INFO - __main__ - ['others']
06/02/2022 12:31:08 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 12:31:08 - INFO - __main__ - ['others']
06/02/2022 12:31:08 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:31:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:31:09 - INFO - __main__ - Printing 3 examples
06/02/2022 12:31:09 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:31:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:31:09 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:31:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:31:09 - INFO - __main__ - Printing 3 examples
06/02/2022 12:31:09 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:31:09 - INFO - __main__ - ['others']
06/02/2022 12:31:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:31:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:31:09 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:31:10 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:31:15 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:31:15 - INFO - __main__ - task name: emo
06/02/2022 12:31:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:31:15 - INFO - __main__ - Starting training!
06/02/2022 12:31:15 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 12:31:58 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_100_0.2_8_predictions.txt
06/02/2022 12:31:58 - INFO - __main__ - Classification-F1 on test data: 0.3346
06/02/2022 12:31:58 - INFO - __main__ - prefix=emo_16_100, lr=0.2, bsz=8, dev_performance=0.7176134781397939, test_performance=0.33456786551417467
06/02/2022 12:31:58 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.5, bsz=8 ...
06/02/2022 12:31:59 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:31:59 - INFO - __main__ - Printing 3 examples
06/02/2022 12:31:59 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:31:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:31:59 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:31:59 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:31:59 - INFO - __main__ - Printing 3 examples
06/02/2022 12:31:59 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:31:59 - INFO - __main__ - ['others']
06/02/2022 12:31:59 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:31:59 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:31:59 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:32:05 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:32:05 - INFO - __main__ - task name: emo
06/02/2022 12:32:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:32:06 - INFO - __main__ - Starting training!
06/02/2022 12:32:07 - INFO - __main__ - Step 10 Global step 10 Train loss 5.61 on epoch=2
06/02/2022 12:32:08 - INFO - __main__ - Step 20 Global step 20 Train loss 2.75 on epoch=4
06/02/2022 12:32:10 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=7
06/02/2022 12:32:11 - INFO - __main__ - Step 40 Global step 40 Train loss 1.28 on epoch=9
06/02/2022 12:32:12 - INFO - __main__ - Step 50 Global step 50 Train loss 1.26 on epoch=12
06/02/2022 12:32:13 - INFO - __main__ - Global step 50 Train loss 2.55 Classification-F1 0.1 on epoch=12
06/02/2022 12:32:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
06/02/2022 12:32:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=14
06/02/2022 12:32:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.45 on epoch=17
06/02/2022 12:32:16 - INFO - __main__ - Step 80 Global step 80 Train loss 1.04 on epoch=19
06/02/2022 12:32:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=22
06/02/2022 12:32:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=24
06/02/2022 12:32:20 - INFO - __main__ - Global step 100 Train loss 1.09 Classification-F1 0.10273972602739727 on epoch=24
06/02/2022 12:32:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.10273972602739727 on epoch=24, global_step=100
06/02/2022 12:32:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
06/02/2022 12:32:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=29
06/02/2022 12:32:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=32
06/02/2022 12:32:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=34
06/02/2022 12:32:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.92 on epoch=37
06/02/2022 12:32:26 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.10389610389610389 on epoch=37
06/02/2022 12:32:26 - INFO - __main__ - Saving model with best Classification-F1: 0.10273972602739727 -> 0.10389610389610389 on epoch=37, global_step=150
06/02/2022 12:32:28 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=39
06/02/2022 12:32:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.90 on epoch=42
06/02/2022 12:32:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=44
06/02/2022 12:32:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=47
06/02/2022 12:32:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
06/02/2022 12:32:33 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.1 on epoch=49
06/02/2022 12:32:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=52
06/02/2022 12:32:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=54
06/02/2022 12:32:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=57
06/02/2022 12:32:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=59
06/02/2022 12:32:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=62
06/02/2022 12:32:40 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.2341430499325236 on epoch=62
06/02/2022 12:32:40 - INFO - __main__ - Saving model with best Classification-F1: 0.10389610389610389 -> 0.2341430499325236 on epoch=62, global_step=250
06/02/2022 12:32:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=64
06/02/2022 12:32:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=67
06/02/2022 12:32:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
06/02/2022 12:32:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=72
06/02/2022 12:32:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
06/02/2022 12:32:47 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.1622222222222222 on epoch=74
06/02/2022 12:32:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
06/02/2022 12:32:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=79
06/02/2022 12:32:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=82
06/02/2022 12:32:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=84
06/02/2022 12:32:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
06/02/2022 12:32:54 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.24116959064327484 on epoch=87
06/02/2022 12:32:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2341430499325236 -> 0.24116959064327484 on epoch=87, global_step=350
06/02/2022 12:32:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=89
06/02/2022 12:32:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.76 on epoch=92
06/02/2022 12:32:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=94
06/02/2022 12:32:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.80 on epoch=97
06/02/2022 12:33:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=99
06/02/2022 12:33:00 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.5567877140627351 on epoch=99
06/02/2022 12:33:00 - INFO - __main__ - Saving model with best Classification-F1: 0.24116959064327484 -> 0.5567877140627351 on epoch=99, global_step=400
06/02/2022 12:33:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=102
06/02/2022 12:33:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=104
06/02/2022 12:33:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=107
06/02/2022 12:33:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
06/02/2022 12:33:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.66 on epoch=112
06/02/2022 12:33:07 - INFO - __main__ - Global step 450 Train loss 0.66 Classification-F1 0.43861870989530566 on epoch=112
06/02/2022 12:33:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.65 on epoch=114
06/02/2022 12:33:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=117
06/02/2022 12:33:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.69 on epoch=119
06/02/2022 12:33:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=122
06/02/2022 12:33:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=124
06/02/2022 12:33:14 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.3578259605729084 on epoch=124
06/02/2022 12:33:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=127
06/02/2022 12:33:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=129
06/02/2022 12:33:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=132
06/02/2022 12:33:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
06/02/2022 12:33:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=137
06/02/2022 12:33:21 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.47337178188242024 on epoch=137
06/02/2022 12:33:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=139
06/02/2022 12:33:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=142
06/02/2022 12:33:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
06/02/2022 12:33:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=147
06/02/2022 12:33:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=149
06/02/2022 12:33:28 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.40147058823529413 on epoch=149
06/02/2022 12:33:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=152
06/02/2022 12:33:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=154
06/02/2022 12:33:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=157
06/02/2022 12:33:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=159
06/02/2022 12:33:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=162
06/02/2022 12:33:34 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.5366430020283975 on epoch=162
06/02/2022 12:33:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=164
06/02/2022 12:33:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=167
06/02/2022 12:33:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=169
06/02/2022 12:33:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=172
06/02/2022 12:33:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=174
06/02/2022 12:33:41 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.5989372469635628 on epoch=174
06/02/2022 12:33:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5567877140627351 -> 0.5989372469635628 on epoch=174, global_step=700
06/02/2022 12:33:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=177
06/02/2022 12:33:44 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=179
06/02/2022 12:33:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=182
06/02/2022 12:33:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=184
06/02/2022 12:33:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=187
06/02/2022 12:33:48 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.589676529978254 on epoch=187
06/02/2022 12:33:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=189
06/02/2022 12:33:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=192
06/02/2022 12:33:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=194
06/02/2022 12:33:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=197
06/02/2022 12:33:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=199
06/02/2022 12:33:55 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.5045333923042901 on epoch=199
06/02/2022 12:33:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=202
06/02/2022 12:33:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=204
06/02/2022 12:33:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
06/02/2022 12:34:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=209
06/02/2022 12:34:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=212
06/02/2022 12:34:01 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.5160670746877642 on epoch=212
06/02/2022 12:34:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=214
06/02/2022 12:34:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=217
06/02/2022 12:34:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=219
06/02/2022 12:34:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=222
06/02/2022 12:34:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=224
06/02/2022 12:34:08 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.5922532129428681 on epoch=224
06/02/2022 12:34:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.29 on epoch=227
06/02/2022 12:34:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=229
06/02/2022 12:34:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=232
06/02/2022 12:34:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=234
06/02/2022 12:34:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=237
06/02/2022 12:34:15 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.6075980392156862 on epoch=237
06/02/2022 12:34:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5989372469635628 -> 0.6075980392156862 on epoch=237, global_step=950
06/02/2022 12:34:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=239
06/02/2022 12:34:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=242
06/02/2022 12:34:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=244
06/02/2022 12:34:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=247
06/02/2022 12:34:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
06/02/2022 12:34:22 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5919104459150226 on epoch=249
06/02/2022 12:34:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=252
06/02/2022 12:34:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=254
06/02/2022 12:34:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
06/02/2022 12:34:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=259
06/02/2022 12:34:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=262
06/02/2022 12:34:28 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.6313095238095238 on epoch=262
06/02/2022 12:34:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6075980392156862 -> 0.6313095238095238 on epoch=262, global_step=1050
06/02/2022 12:34:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=264
06/02/2022 12:34:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
06/02/2022 12:34:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=269
06/02/2022 12:34:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=272
06/02/2022 12:34:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=274
06/02/2022 12:34:35 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.5923983386458032 on epoch=274
06/02/2022 12:34:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=277
06/02/2022 12:34:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=279
06/02/2022 12:34:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
06/02/2022 12:34:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=284
06/02/2022 12:34:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=287
06/02/2022 12:34:42 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6302443517813536 on epoch=287
06/02/2022 12:34:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.05 on epoch=289
06/02/2022 12:34:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
06/02/2022 12:34:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=294
06/02/2022 12:34:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
06/02/2022 12:34:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=299
06/02/2022 12:34:49 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.5980653815580286 on epoch=299
06/02/2022 12:34:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=302
06/02/2022 12:34:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=304
06/02/2022 12:34:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
06/02/2022 12:34:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=309
06/02/2022 12:34:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
06/02/2022 12:34:56 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5952750206782466 on epoch=312
06/02/2022 12:34:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/02/2022 12:34:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=317
06/02/2022 12:35:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=319
06/02/2022 12:35:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=322
06/02/2022 12:35:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
06/02/2022 12:35:03 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.6009523809523809 on epoch=324
06/02/2022 12:35:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=327
06/02/2022 12:35:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=329
06/02/2022 12:35:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=332
06/02/2022 12:35:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=334
06/02/2022 12:35:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
06/02/2022 12:35:10 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6402646815550042 on epoch=337
06/02/2022 12:35:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6313095238095238 -> 0.6402646815550042 on epoch=337, global_step=1350
06/02/2022 12:35:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=339
06/02/2022 12:35:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=342
06/02/2022 12:35:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=344
06/02/2022 12:35:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.03 on epoch=347
06/02/2022 12:35:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
06/02/2022 12:35:17 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.6174768518518519 on epoch=349
06/02/2022 12:35:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=352
06/02/2022 12:35:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=354
06/02/2022 12:35:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/02/2022 12:35:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=359
06/02/2022 12:35:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=362
06/02/2022 12:35:24 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.599946524064171 on epoch=362
06/02/2022 12:35:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=364
06/02/2022 12:35:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=367
06/02/2022 12:35:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=369
06/02/2022 12:35:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/02/2022 12:35:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=374
06/02/2022 12:35:32 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.621875 on epoch=374
06/02/2022 12:35:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/02/2022 12:35:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=379
06/02/2022 12:35:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=382
06/02/2022 12:35:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/02/2022 12:35:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=387
06/02/2022 12:35:39 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.658655462184874 on epoch=387
06/02/2022 12:35:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6402646815550042 -> 0.658655462184874 on epoch=387, global_step=1550
06/02/2022 12:35:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=389
06/02/2022 12:35:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/02/2022 12:35:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
06/02/2022 12:35:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/02/2022 12:35:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=399
06/02/2022 12:35:46 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6660287081339712 on epoch=399
06/02/2022 12:35:46 - INFO - __main__ - Saving model with best Classification-F1: 0.658655462184874 -> 0.6660287081339712 on epoch=399, global_step=1600
06/02/2022 12:35:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=402
06/02/2022 12:35:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
06/02/2022 12:35:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
06/02/2022 12:35:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=409
06/02/2022 12:35:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/02/2022 12:35:54 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6261904761904762 on epoch=412
06/02/2022 12:35:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=414
06/02/2022 12:35:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/02/2022 12:35:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/02/2022 12:36:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/02/2022 12:36:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=424
06/02/2022 12:36:02 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.601984126984127 on epoch=424
06/02/2022 12:36:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=427
06/02/2022 12:36:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/02/2022 12:36:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=432
06/02/2022 12:36:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/02/2022 12:36:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=437
06/02/2022 12:36:09 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.705448717948718 on epoch=437
06/02/2022 12:36:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6660287081339712 -> 0.705448717948718 on epoch=437, global_step=1750
06/02/2022 12:36:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=439
06/02/2022 12:36:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=442
06/02/2022 12:36:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
06/02/2022 12:36:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/02/2022 12:36:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=449
06/02/2022 12:36:17 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6735455090718248 on epoch=449
06/02/2022 12:36:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=452
06/02/2022 12:36:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=454
06/02/2022 12:36:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/02/2022 12:36:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/02/2022 12:36:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=462
06/02/2022 12:36:24 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6916852210969858 on epoch=462
06/02/2022 12:36:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=464
06/02/2022 12:36:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/02/2022 12:36:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/02/2022 12:36:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=472
06/02/2022 12:36:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=474
06/02/2022 12:36:31 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.6704433497536946 on epoch=474
06/02/2022 12:36:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/02/2022 12:36:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/02/2022 12:36:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/02/2022 12:36:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/02/2022 12:36:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=487
06/02/2022 12:36:39 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6560442654192654 on epoch=487
06/02/2022 12:36:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=489
06/02/2022 12:36:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/02/2022 12:36:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=494
06/02/2022 12:36:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=497
06/02/2022 12:36:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
06/02/2022 12:36:46 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.694352146263911 on epoch=499
06/02/2022 12:36:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/02/2022 12:36:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/02/2022 12:36:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.00 on epoch=507
06/02/2022 12:36:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/02/2022 12:36:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
06/02/2022 12:36:53 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6788868857180052 on epoch=512
06/02/2022 12:36:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.00 on epoch=514
06/02/2022 12:36:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.00 on epoch=517
06/02/2022 12:36:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/02/2022 12:36:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/02/2022 12:36:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.00 on epoch=524
06/02/2022 12:37:00 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6507103409246141 on epoch=524
06/02/2022 12:37:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/02/2022 12:37:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/02/2022 12:37:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=532
06/02/2022 12:37:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/02/2022 12:37:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/02/2022 12:37:07 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6292393880629175 on epoch=537
06/02/2022 12:37:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/02/2022 12:37:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=542
06/02/2022 12:37:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/02/2022 12:37:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/02/2022 12:37:13 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.00 on epoch=549
06/02/2022 12:37:14 - INFO - __main__ - Global step 2200 Train loss 0.01 Classification-F1 0.6801782682512734 on epoch=549
06/02/2022 12:37:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/02/2022 12:37:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/02/2022 12:37:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=557
06/02/2022 12:37:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
06/02/2022 12:37:20 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/02/2022 12:37:21 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6262288786482334 on epoch=562
06/02/2022 12:37:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/02/2022 12:37:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/02/2022 12:37:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/02/2022 12:37:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.00 on epoch=572
06/02/2022 12:37:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/02/2022 12:37:28 - INFO - __main__ - Global step 2300 Train loss 0.01 Classification-F1 0.6236559139784945 on epoch=574
06/02/2022 12:37:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
06/02/2022 12:37:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/02/2022 12:37:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.00 on epoch=582
06/02/2022 12:37:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
06/02/2022 12:37:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/02/2022 12:37:35 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.60625 on epoch=587
06/02/2022 12:37:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/02/2022 12:37:37 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/02/2022 12:37:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/02/2022 12:37:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/02/2022 12:37:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/02/2022 12:37:42 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.623214708435256 on epoch=599
06/02/2022 12:37:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.00 on epoch=602
06/02/2022 12:37:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=604
06/02/2022 12:37:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/02/2022 12:37:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=609
06/02/2022 12:37:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/02/2022 12:37:49 - INFO - __main__ - Global step 2450 Train loss 0.00 Classification-F1 0.6059139784946236 on epoch=612
06/02/2022 12:37:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/02/2022 12:37:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/02/2022 12:37:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/02/2022 12:37:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/02/2022 12:37:55 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/02/2022 12:37:55 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.650775694893342 on epoch=624
06/02/2022 12:37:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/02/2022 12:37:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/02/2022 12:37:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/02/2022 12:38:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=634
06/02/2022 12:38:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/02/2022 12:38:02 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.635844263263618 on epoch=637
06/02/2022 12:38:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 12:38:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/02/2022 12:38:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=644
06/02/2022 12:38:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/02/2022 12:38:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
06/02/2022 12:38:09 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6483754690651242 on epoch=649
06/02/2022 12:38:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/02/2022 12:38:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/02/2022 12:38:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/02/2022 12:38:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 12:38:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/02/2022 12:38:16 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6293153815580286 on epoch=662
06/02/2022 12:38:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/02/2022 12:38:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/02/2022 12:38:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/02/2022 12:38:21 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/02/2022 12:38:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/02/2022 12:38:23 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6349217571644041 on epoch=674
06/02/2022 12:38:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/02/2022 12:38:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/02/2022 12:38:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=682
06/02/2022 12:38:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=684
06/02/2022 12:38:29 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/02/2022 12:38:29 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5988727198196352 on epoch=687
06/02/2022 12:38:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/02/2022 12:38:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/02/2022 12:38:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/02/2022 12:38:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/02/2022 12:38:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/02/2022 12:38:36 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.5824655299844534 on epoch=699
06/02/2022 12:38:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/02/2022 12:38:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/02/2022 12:38:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/02/2022 12:38:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/02/2022 12:38:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/02/2022 12:38:43 - INFO - __main__ - Global step 2850 Train loss 0.00 Classification-F1 0.5932351994851994 on epoch=712
06/02/2022 12:38:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/02/2022 12:38:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/02/2022 12:38:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/02/2022 12:38:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/02/2022 12:38:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/02/2022 12:38:50 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6105090311986865 on epoch=724
06/02/2022 12:38:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/02/2022 12:38:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 12:38:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 12:38:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/02/2022 12:38:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 12:38:57 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.5960365853658537 on epoch=737
06/02/2022 12:38:58 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/02/2022 12:38:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/02/2022 12:39:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/02/2022 12:39:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/02/2022 12:39:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/02/2022 12:39:03 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.6295001364127017 on epoch=749
06/02/2022 12:39:03 - INFO - __main__ - save last model!
06/02/2022 12:39:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 12:39:04 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 12:39:04 - INFO - __main__ - Printing 3 examples
06/02/2022 12:39:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:39:04 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:39:04 - INFO - __main__ - Printing 3 examples
06/02/2022 12:39:04 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:39:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:39:04 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:39:04 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:39:04 - INFO - __main__ - Printing 3 examples
06/02/2022 12:39:04 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:39:04 - INFO - __main__ - ['others']
06/02/2022 12:39:04 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:39:04 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:39:04 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:39:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:39:10 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:39:10 - INFO - __main__ - task name: emo
06/02/2022 12:39:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:39:11 - INFO - __main__ - Starting training!
06/02/2022 12:39:11 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 12:39:54 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.5_8_predictions.txt
06/02/2022 12:39:54 - INFO - __main__ - Classification-F1 on test data: 0.2904
06/02/2022 12:39:54 - INFO - __main__ - prefix=emo_16_13, lr=0.5, bsz=8, dev_performance=0.705448717948718, test_performance=0.29042110643879665
06/02/2022 12:39:54 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.4, bsz=8 ...
06/02/2022 12:39:55 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:39:55 - INFO - __main__ - Printing 3 examples
06/02/2022 12:39:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:39:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:39:55 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:39:55 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:39:55 - INFO - __main__ - Printing 3 examples
06/02/2022 12:39:55 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:39:55 - INFO - __main__ - ['others']
06/02/2022 12:39:55 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:39:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:39:55 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:40:01 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:40:01 - INFO - __main__ - task name: emo
06/02/2022 12:40:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:40:01 - INFO - __main__ - Starting training!
06/02/2022 12:40:03 - INFO - __main__ - Step 10 Global step 10 Train loss 6.54 on epoch=2
06/02/2022 12:40:04 - INFO - __main__ - Step 20 Global step 20 Train loss 4.10 on epoch=4
06/02/2022 12:40:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
06/02/2022 12:40:06 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=9
06/02/2022 12:40:08 - INFO - __main__ - Step 50 Global step 50 Train loss 1.54 on epoch=12
06/02/2022 12:40:08 - INFO - __main__ - Global step 50 Train loss 3.29 Classification-F1 0.2684178743961353 on epoch=12
06/02/2022 12:40:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2684178743961353 on epoch=12, global_step=50
06/02/2022 12:40:09 - INFO - __main__ - Step 60 Global step 60 Train loss 1.31 on epoch=14
06/02/2022 12:40:11 - INFO - __main__ - Step 70 Global step 70 Train loss 1.32 on epoch=17
06/02/2022 12:40:12 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=19
06/02/2022 12:40:13 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
06/02/2022 12:40:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.19 on epoch=24
06/02/2022 12:40:15 - INFO - __main__ - Global step 100 Train loss 1.22 Classification-F1 0.17154886148007592 on epoch=24
06/02/2022 12:40:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
06/02/2022 12:40:17 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=29
06/02/2022 12:40:19 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
06/02/2022 12:40:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
06/02/2022 12:40:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=37
06/02/2022 12:40:22 - INFO - __main__ - Global step 150 Train loss 1.06 Classification-F1 0.30303030303030304 on epoch=37
06/02/2022 12:40:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2684178743961353 -> 0.30303030303030304 on epoch=37, global_step=150
06/02/2022 12:40:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
06/02/2022 12:40:24 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
06/02/2022 12:40:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.07 on epoch=44
06/02/2022 12:40:27 - INFO - __main__ - Step 190 Global step 190 Train loss 1.06 on epoch=47
06/02/2022 12:40:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.93 on epoch=49
06/02/2022 12:40:29 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.3472222222222222 on epoch=49
06/02/2022 12:40:29 - INFO - __main__ - Saving model with best Classification-F1: 0.30303030303030304 -> 0.3472222222222222 on epoch=49, global_step=200
06/02/2022 12:40:30 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
06/02/2022 12:40:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.87 on epoch=54
06/02/2022 12:40:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=57
06/02/2022 12:40:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
06/02/2022 12:40:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=62
06/02/2022 12:40:36 - INFO - __main__ - Global step 250 Train loss 0.92 Classification-F1 0.3617162407484988 on epoch=62
06/02/2022 12:40:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3472222222222222 -> 0.3617162407484988 on epoch=62, global_step=250
06/02/2022 12:40:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=64
06/02/2022 12:40:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
06/02/2022 12:40:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=69
06/02/2022 12:40:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=72
06/02/2022 12:40:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.93 on epoch=74
06/02/2022 12:40:42 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.46434676434676436 on epoch=74
06/02/2022 12:40:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3617162407484988 -> 0.46434676434676436 on epoch=74, global_step=300
06/02/2022 12:40:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=77
06/02/2022 12:40:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=79
06/02/2022 12:40:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=82
06/02/2022 12:40:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=84
06/02/2022 12:40:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=87
06/02/2022 12:40:49 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.6576178451178452 on epoch=87
06/02/2022 12:40:49 - INFO - __main__ - Saving model with best Classification-F1: 0.46434676434676436 -> 0.6576178451178452 on epoch=87, global_step=350
06/02/2022 12:40:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=89
06/02/2022 12:40:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=92
06/02/2022 12:40:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=94
06/02/2022 12:40:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=97
06/02/2022 12:40:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=99
06/02/2022 12:40:56 - INFO - __main__ - Global step 400 Train loss 0.70 Classification-F1 0.45570026046884393 on epoch=99
06/02/2022 12:40:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=102
06/02/2022 12:40:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=104
06/02/2022 12:41:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.63 on epoch=107
06/02/2022 12:41:01 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=109
06/02/2022 12:41:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.62 on epoch=112
06/02/2022 12:41:03 - INFO - __main__ - Global step 450 Train loss 0.68 Classification-F1 0.5725490196078431 on epoch=112
06/02/2022 12:41:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=114
06/02/2022 12:41:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=117
06/02/2022 12:41:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=119
06/02/2022 12:41:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
06/02/2022 12:41:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.69 on epoch=124
06/02/2022 12:41:10 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.5094322344322344 on epoch=124
06/02/2022 12:41:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=127
06/02/2022 12:41:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=129
06/02/2022 12:41:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=132
06/02/2022 12:41:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.62 on epoch=134
06/02/2022 12:41:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=137
06/02/2022 12:41:17 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.5462902752376436 on epoch=137
06/02/2022 12:41:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=139
06/02/2022 12:41:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=142
06/02/2022 12:41:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=144
06/02/2022 12:41:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=147
06/02/2022 12:41:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=149
06/02/2022 12:41:24 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.442621809635279 on epoch=149
06/02/2022 12:41:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=152
06/02/2022 12:41:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=154
06/02/2022 12:41:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=157
06/02/2022 12:41:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=159
06/02/2022 12:41:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
06/02/2022 12:41:30 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.5686937933067964 on epoch=162
06/02/2022 12:41:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=164
06/02/2022 12:41:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=167
06/02/2022 12:41:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=169
06/02/2022 12:41:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=172
06/02/2022 12:41:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=174
06/02/2022 12:41:37 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.6699560358069813 on epoch=174
06/02/2022 12:41:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6576178451178452 -> 0.6699560358069813 on epoch=174, global_step=700
06/02/2022 12:41:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=177
06/02/2022 12:41:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=179
06/02/2022 12:41:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=182
06/02/2022 12:41:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=184
06/02/2022 12:41:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=187
06/02/2022 12:41:43 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.652076692746064 on epoch=187
06/02/2022 12:41:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=189
06/02/2022 12:41:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=192
06/02/2022 12:41:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=194
06/02/2022 12:41:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=197
06/02/2022 12:41:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=199
06/02/2022 12:41:50 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.6501758205612161 on epoch=199
06/02/2022 12:41:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=202
06/02/2022 12:41:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=204
06/02/2022 12:41:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=207
06/02/2022 12:41:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=209
06/02/2022 12:41:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
06/02/2022 12:41:57 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.6468504795117698 on epoch=212
06/02/2022 12:41:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
06/02/2022 12:41:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
06/02/2022 12:42:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=219
06/02/2022 12:42:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=222
06/02/2022 12:42:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/02/2022 12:42:04 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.5912949239562142 on epoch=224
06/02/2022 12:42:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.15 on epoch=227
06/02/2022 12:42:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=229
06/02/2022 12:42:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=232
06/02/2022 12:42:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=234
06/02/2022 12:42:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=237
06/02/2022 12:42:10 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.6216764418377322 on epoch=237
06/02/2022 12:42:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=239
06/02/2022 12:42:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=242
06/02/2022 12:42:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=244
06/02/2022 12:42:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=247
06/02/2022 12:42:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=249
06/02/2022 12:42:17 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.5541827541827542 on epoch=249
06/02/2022 12:42:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=252
06/02/2022 12:42:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
06/02/2022 12:42:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=257
06/02/2022 12:42:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
06/02/2022 12:42:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=262
06/02/2022 12:42:24 - INFO - __main__ - Global step 1050 Train loss 0.09 Classification-F1 0.6175516795865633 on epoch=262
06/02/2022 12:42:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=264
06/02/2022 12:42:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
06/02/2022 12:42:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=269
06/02/2022 12:42:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=272
06/02/2022 12:42:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=274
06/02/2022 12:42:30 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.5755170755170755 on epoch=274
06/02/2022 12:42:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=277
06/02/2022 12:42:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/02/2022 12:42:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=282
06/02/2022 12:42:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=284
06/02/2022 12:42:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.05 on epoch=287
06/02/2022 12:42:37 - INFO - __main__ - Global step 1150 Train loss 0.05 Classification-F1 0.5880821998469058 on epoch=287
06/02/2022 12:42:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=289
06/02/2022 12:42:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=292
06/02/2022 12:42:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=294
06/02/2022 12:42:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=297
06/02/2022 12:42:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=299
06/02/2022 12:42:44 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.6215142596712396 on epoch=299
06/02/2022 12:42:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=302
06/02/2022 12:42:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
06/02/2022 12:42:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=307
06/02/2022 12:42:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=309
06/02/2022 12:42:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=312
06/02/2022 12:42:51 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.618034732633683 on epoch=312
06/02/2022 12:42:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=314
06/02/2022 12:42:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=317
06/02/2022 12:42:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=319
06/02/2022 12:42:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=322
06/02/2022 12:42:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=324
06/02/2022 12:42:57 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.5586424934251021 on epoch=324
06/02/2022 12:42:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
06/02/2022 12:43:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=329
06/02/2022 12:43:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=332
06/02/2022 12:43:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=334
06/02/2022 12:43:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=337
06/02/2022 12:43:04 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5685808814841073 on epoch=337
06/02/2022 12:43:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.03 on epoch=339
06/02/2022 12:43:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=342
06/02/2022 12:43:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=344
06/02/2022 12:43:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=347
06/02/2022 12:43:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=349
06/02/2022 12:43:11 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.6462041156840934 on epoch=349
06/02/2022 12:43:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=352
06/02/2022 12:43:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
06/02/2022 12:43:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/02/2022 12:43:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=359
06/02/2022 12:43:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=362
06/02/2022 12:43:18 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.6545925881095038 on epoch=362
06/02/2022 12:43:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=364
06/02/2022 12:43:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=367
06/02/2022 12:43:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=369
06/02/2022 12:43:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/02/2022 12:43:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=374
06/02/2022 12:43:24 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.6176009428161509 on epoch=374
06/02/2022 12:43:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=377
06/02/2022 12:43:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=379
06/02/2022 12:43:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=382
06/02/2022 12:43:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=384
06/02/2022 12:43:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/02/2022 12:43:31 - INFO - __main__ - Global step 1550 Train loss 0.03 Classification-F1 0.6135101769947982 on epoch=387
06/02/2022 12:43:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
06/02/2022 12:43:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=392
06/02/2022 12:43:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=394
06/02/2022 12:43:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/02/2022 12:43:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
06/02/2022 12:43:38 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6738095238095239 on epoch=399
06/02/2022 12:43:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6699560358069813 -> 0.6738095238095239 on epoch=399, global_step=1600
06/02/2022 12:43:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=402
06/02/2022 12:43:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/02/2022 12:43:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/02/2022 12:43:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/02/2022 12:43:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=412
06/02/2022 12:43:45 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6526091026091027 on epoch=412
06/02/2022 12:43:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=414
06/02/2022 12:43:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=417
06/02/2022 12:43:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=419
06/02/2022 12:43:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=422
06/02/2022 12:43:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
06/02/2022 12:43:51 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.6212788563701342 on epoch=424
06/02/2022 12:43:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=427
06/02/2022 12:43:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
06/02/2022 12:43:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/02/2022 12:43:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/02/2022 12:43:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/02/2022 12:43:58 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 0.6187140804597702 on epoch=437
06/02/2022 12:43:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=439
06/02/2022 12:44:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
06/02/2022 12:44:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/02/2022 12:44:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=447
06/02/2022 12:44:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/02/2022 12:44:05 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.581246687864335 on epoch=449
06/02/2022 12:44:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/02/2022 12:44:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=454
06/02/2022 12:44:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/02/2022 12:44:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=459
06/02/2022 12:44:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/02/2022 12:44:12 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.6200583230616601 on epoch=462
06/02/2022 12:44:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
06/02/2022 12:44:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=467
06/02/2022 12:44:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
06/02/2022 12:44:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
06/02/2022 12:44:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
06/02/2022 12:44:18 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.6053921568627452 on epoch=474
06/02/2022 12:44:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/02/2022 12:44:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/02/2022 12:44:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/02/2022 12:44:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/02/2022 12:44:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
06/02/2022 12:44:25 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.6239622865275142 on epoch=487
06/02/2022 12:44:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/02/2022 12:44:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/02/2022 12:44:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=494
06/02/2022 12:44:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/02/2022 12:44:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/02/2022 12:44:32 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.5208680208680209 on epoch=499
06/02/2022 12:44:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/02/2022 12:44:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/02/2022 12:44:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/02/2022 12:44:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.00 on epoch=509
06/02/2022 12:44:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.00 on epoch=512
06/02/2022 12:44:39 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.5853601055806938 on epoch=512
06/02/2022 12:44:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/02/2022 12:44:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/02/2022 12:44:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.00 on epoch=519
06/02/2022 12:44:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=522
06/02/2022 12:44:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=524
06/02/2022 12:44:46 - INFO - __main__ - Global step 2100 Train loss 0.01 Classification-F1 0.6126155712362609 on epoch=524
06/02/2022 12:44:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/02/2022 12:44:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/02/2022 12:44:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=532
06/02/2022 12:44:51 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=534
06/02/2022 12:44:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/02/2022 12:44:52 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6000865800865801 on epoch=537
06/02/2022 12:44:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/02/2022 12:44:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.00 on epoch=542
06/02/2022 12:44:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/02/2022 12:44:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
06/02/2022 12:44:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/02/2022 12:44:59 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6654463780418789 on epoch=549
06/02/2022 12:45:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=552
06/02/2022 12:45:02 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
06/02/2022 12:45:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/02/2022 12:45:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=559
06/02/2022 12:45:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/02/2022 12:45:06 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.5600788288288288 on epoch=562
06/02/2022 12:45:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
06/02/2022 12:45:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=567
06/02/2022 12:45:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/02/2022 12:45:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/02/2022 12:45:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/02/2022 12:45:13 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6070869741922373 on epoch=574
06/02/2022 12:45:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=577
06/02/2022 12:45:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.00 on epoch=579
06/02/2022 12:45:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/02/2022 12:45:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/02/2022 12:45:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=587
06/02/2022 12:45:19 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6330362620685202 on epoch=587
06/02/2022 12:45:21 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/02/2022 12:45:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=592
06/02/2022 12:45:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/02/2022 12:45:24 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/02/2022 12:45:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=599
06/02/2022 12:45:26 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.5996642246642246 on epoch=599
06/02/2022 12:45:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/02/2022 12:45:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/02/2022 12:45:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.00 on epoch=607
06/02/2022 12:45:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
06/02/2022 12:45:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/02/2022 12:45:33 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.6713424299631197 on epoch=612
06/02/2022 12:45:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.00 on epoch=614
06/02/2022 12:45:35 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.00 on epoch=617
06/02/2022 12:45:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/02/2022 12:45:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=622
06/02/2022 12:45:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=624
06/02/2022 12:45:40 - INFO - __main__ - Global step 2500 Train loss 0.00 Classification-F1 0.622481684981685 on epoch=624
06/02/2022 12:45:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=627
06/02/2022 12:45:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/02/2022 12:45:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
06/02/2022 12:45:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=634
06/02/2022 12:45:46 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/02/2022 12:45:46 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.6427849927849928 on epoch=637
06/02/2022 12:45:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 12:45:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 12:45:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/02/2022 12:45:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 12:45:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.00 on epoch=649
06/02/2022 12:45:53 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.6294642857142857 on epoch=649
06/02/2022 12:45:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/02/2022 12:45:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/02/2022 12:45:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/02/2022 12:45:58 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 12:45:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/02/2022 12:46:00 - INFO - __main__ - Global step 2650 Train loss 0.00 Classification-F1 0.6718864468864468 on epoch=662
06/02/2022 12:46:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/02/2022 12:46:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.00 on epoch=667
06/02/2022 12:46:03 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/02/2022 12:46:05 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/02/2022 12:46:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/02/2022 12:46:06 - INFO - __main__ - Global step 2700 Train loss 0.00 Classification-F1 0.6011738640814728 on epoch=674
06/02/2022 12:46:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/02/2022 12:46:09 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.00 on epoch=679
06/02/2022 12:46:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/02/2022 12:46:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=684
06/02/2022 12:46:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/02/2022 12:46:13 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6503643020483628 on epoch=687
06/02/2022 12:46:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=689
06/02/2022 12:46:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/02/2022 12:46:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/02/2022 12:46:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/02/2022 12:46:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/02/2022 12:46:20 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6291208876643146 on epoch=699
06/02/2022 12:46:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/02/2022 12:46:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/02/2022 12:46:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/02/2022 12:46:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
06/02/2022 12:46:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/02/2022 12:46:26 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6236415130568356 on epoch=712
06/02/2022 12:46:28 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/02/2022 12:46:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/02/2022 12:46:30 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 12:46:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=722
06/02/2022 12:46:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 12:46:33 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6273477480374032 on epoch=724
06/02/2022 12:46:34 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 12:46:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 12:46:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 12:46:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
06/02/2022 12:46:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 12:46:40 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6413616625310173 on epoch=737
06/02/2022 12:46:41 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/02/2022 12:46:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/02/2022 12:46:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/02/2022 12:46:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/02/2022 12:46:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/02/2022 12:46:47 - INFO - __main__ - Global step 3000 Train loss 0.00 Classification-F1 0.600219298245614 on epoch=749
06/02/2022 12:46:47 - INFO - __main__ - save last model!
06/02/2022 12:46:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 12:46:47 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 12:46:47 - INFO - __main__ - Printing 3 examples
06/02/2022 12:46:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:46:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:46:47 - INFO - __main__ - Printing 3 examples
06/02/2022 12:46:47 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:46:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:46:47 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:46:47 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:46:47 - INFO - __main__ - Printing 3 examples
06/02/2022 12:46:47 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:46:47 - INFO - __main__ - ['others']
06/02/2022 12:46:47 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:46:47 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:46:47 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:46:49 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:46:53 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:46:53 - INFO - __main__ - task name: emo
06/02/2022 12:46:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:46:54 - INFO - __main__ - Starting training!
06/02/2022 12:46:54 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 12:47:38 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.4_8_predictions.txt
06/02/2022 12:47:38 - INFO - __main__ - Classification-F1 on test data: 0.2017
06/02/2022 12:47:38 - INFO - __main__ - prefix=emo_16_13, lr=0.4, bsz=8, dev_performance=0.6738095238095239, test_performance=0.201668123716036
06/02/2022 12:47:38 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.3, bsz=8 ...
06/02/2022 12:47:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:47:39 - INFO - __main__ - Printing 3 examples
06/02/2022 12:47:39 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:47:39 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:47:39 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:47:39 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:47:39 - INFO - __main__ - Printing 3 examples
06/02/2022 12:47:39 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:47:39 - INFO - __main__ - ['others']
06/02/2022 12:47:39 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:47:39 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:47:39 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:47:45 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:47:45 - INFO - __main__ - task name: emo
06/02/2022 12:47:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:47:45 - INFO - __main__ - Starting training!
06/02/2022 12:47:46 - INFO - __main__ - Step 10 Global step 10 Train loss 6.69 on epoch=2
06/02/2022 12:47:48 - INFO - __main__ - Step 20 Global step 20 Train loss 3.95 on epoch=4
06/02/2022 12:47:49 - INFO - __main__ - Step 30 Global step 30 Train loss 2.62 on epoch=7
06/02/2022 12:47:50 - INFO - __main__ - Step 40 Global step 40 Train loss 2.16 on epoch=9
06/02/2022 12:47:51 - INFO - __main__ - Step 50 Global step 50 Train loss 2.10 on epoch=12
06/02/2022 12:47:52 - INFO - __main__ - Global step 50 Train loss 3.51 Classification-F1 0.10144927536231885 on epoch=12
06/02/2022 12:47:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10144927536231885 on epoch=12, global_step=50
06/02/2022 12:47:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.67 on epoch=14
06/02/2022 12:47:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.59 on epoch=17
06/02/2022 12:47:56 - INFO - __main__ - Step 80 Global step 80 Train loss 1.20 on epoch=19
06/02/2022 12:47:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.44 on epoch=22
06/02/2022 12:47:58 - INFO - __main__ - Step 100 Global step 100 Train loss 1.31 on epoch=24
06/02/2022 12:47:59 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.12213225371120107 on epoch=24
06/02/2022 12:47:59 - INFO - __main__ - Saving model with best Classification-F1: 0.10144927536231885 -> 0.12213225371120107 on epoch=24, global_step=100
06/02/2022 12:48:00 - INFO - __main__ - Step 110 Global step 110 Train loss 1.06 on epoch=27
06/02/2022 12:48:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.22 on epoch=29
06/02/2022 12:48:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.09 on epoch=32
06/02/2022 12:48:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=34
06/02/2022 12:48:05 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
06/02/2022 12:48:05 - INFO - __main__ - Global step 150 Train loss 1.09 Classification-F1 0.40138352638352637 on epoch=37
06/02/2022 12:48:05 - INFO - __main__ - Saving model with best Classification-F1: 0.12213225371120107 -> 0.40138352638352637 on epoch=37, global_step=150
06/02/2022 12:48:07 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
06/02/2022 12:48:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.01 on epoch=42
06/02/2022 12:48:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=44
06/02/2022 12:48:10 - INFO - __main__ - Step 190 Global step 190 Train loss 1.02 on epoch=47
06/02/2022 12:48:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=49
06/02/2022 12:48:12 - INFO - __main__ - Global step 200 Train loss 1.00 Classification-F1 0.12151702786377708 on epoch=49
06/02/2022 12:48:13 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
06/02/2022 12:48:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.98 on epoch=54
06/02/2022 12:48:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
06/02/2022 12:48:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.95 on epoch=59
06/02/2022 12:48:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
06/02/2022 12:48:18 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.16984126984126985 on epoch=62
06/02/2022 12:48:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.99 on epoch=64
06/02/2022 12:48:21 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=67
06/02/2022 12:48:22 - INFO - __main__ - Step 280 Global step 280 Train loss 1.00 on epoch=69
06/02/2022 12:48:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
06/02/2022 12:48:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.94 on epoch=74
06/02/2022 12:48:25 - INFO - __main__ - Global step 300 Train loss 0.93 Classification-F1 0.2569444444444445 on epoch=74
06/02/2022 12:48:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.96 on epoch=77
06/02/2022 12:48:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.88 on epoch=79
06/02/2022 12:48:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=82
06/02/2022 12:48:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
06/02/2022 12:48:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.93 on epoch=87
06/02/2022 12:48:32 - INFO - __main__ - Global step 350 Train loss 0.89 Classification-F1 0.39856150793650796 on epoch=87
06/02/2022 12:48:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=89
06/02/2022 12:48:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
06/02/2022 12:48:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=94
06/02/2022 12:48:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.91 on epoch=97
06/02/2022 12:48:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=99
06/02/2022 12:48:39 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3163396886801142 on epoch=99
06/02/2022 12:48:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=102
06/02/2022 12:48:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=104
06/02/2022 12:48:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.88 on epoch=107
06/02/2022 12:48:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.90 on epoch=109
06/02/2022 12:48:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.83 on epoch=112
06/02/2022 12:48:45 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.4258231594524602 on epoch=112
06/02/2022 12:48:45 - INFO - __main__ - Saving model with best Classification-F1: 0.40138352638352637 -> 0.4258231594524602 on epoch=112, global_step=450
06/02/2022 12:48:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=114
06/02/2022 12:48:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=117
06/02/2022 12:48:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=119
06/02/2022 12:48:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=122
06/02/2022 12:48:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=124
06/02/2022 12:48:52 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.4842929905408684 on epoch=124
06/02/2022 12:48:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4258231594524602 -> 0.4842929905408684 on epoch=124, global_step=500
06/02/2022 12:48:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.83 on epoch=127
06/02/2022 12:48:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.71 on epoch=129
06/02/2022 12:48:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=132
06/02/2022 12:48:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=134
06/02/2022 12:48:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.73 on epoch=137
06/02/2022 12:48:58 - INFO - __main__ - Global step 550 Train loss 0.76 Classification-F1 0.5536096296965862 on epoch=137
06/02/2022 12:48:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4842929905408684 -> 0.5536096296965862 on epoch=137, global_step=550
06/02/2022 12:48:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
06/02/2022 12:49:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=142
06/02/2022 12:49:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=144
06/02/2022 12:49:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.68 on epoch=147
06/02/2022 12:49:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
06/02/2022 12:49:05 - INFO - __main__ - Global step 600 Train loss 0.70 Classification-F1 0.3006657608695652 on epoch=149
06/02/2022 12:49:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
06/02/2022 12:49:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=154
06/02/2022 12:49:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.69 on epoch=157
06/02/2022 12:49:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.64 on epoch=159
06/02/2022 12:49:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=162
06/02/2022 12:49:11 - INFO - __main__ - Global step 650 Train loss 0.68 Classification-F1 0.6143790849673203 on epoch=162
06/02/2022 12:49:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5536096296965862 -> 0.6143790849673203 on epoch=162, global_step=650
06/02/2022 12:49:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
06/02/2022 12:49:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=167
06/02/2022 12:49:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=169
06/02/2022 12:49:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.57 on epoch=172
06/02/2022 12:49:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=174
06/02/2022 12:49:18 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.5917131414113646 on epoch=174
06/02/2022 12:49:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=177
06/02/2022 12:49:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=179
06/02/2022 12:49:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=182
06/02/2022 12:49:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=184
06/02/2022 12:49:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=187
06/02/2022 12:49:25 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.7233090185676393 on epoch=187
06/02/2022 12:49:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6143790849673203 -> 0.7233090185676393 on epoch=187, global_step=750
06/02/2022 12:49:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=189
06/02/2022 12:49:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=192
06/02/2022 12:49:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=194
06/02/2022 12:49:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=197
06/02/2022 12:49:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=199
06/02/2022 12:49:32 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.7262679848886745 on epoch=199
06/02/2022 12:49:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7233090185676393 -> 0.7262679848886745 on epoch=199, global_step=800
06/02/2022 12:49:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=202
06/02/2022 12:49:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=204
06/02/2022 12:49:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=207
06/02/2022 12:49:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=209
06/02/2022 12:49:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=212
06/02/2022 12:49:38 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.6486973345525977 on epoch=212
06/02/2022 12:49:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=214
06/02/2022 12:49:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
06/02/2022 12:49:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=219
06/02/2022 12:49:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=222
06/02/2022 12:49:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=224
06/02/2022 12:49:45 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.7582815551565552 on epoch=224
06/02/2022 12:49:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7262679848886745 -> 0.7582815551565552 on epoch=224, global_step=900
06/02/2022 12:49:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=227
06/02/2022 12:49:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=229
06/02/2022 12:49:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=232
06/02/2022 12:49:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=234
06/02/2022 12:49:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=237
06/02/2022 12:49:52 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.7763573232323232 on epoch=237
06/02/2022 12:49:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7582815551565552 -> 0.7763573232323232 on epoch=237, global_step=950
06/02/2022 12:49:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=239
06/02/2022 12:49:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=242
06/02/2022 12:49:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=244
06/02/2022 12:49:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=247
06/02/2022 12:49:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=249
06/02/2022 12:49:58 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.7375717656789764 on epoch=249
06/02/2022 12:50:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=252
06/02/2022 12:50:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=254
06/02/2022 12:50:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=257
06/02/2022 12:50:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=259
06/02/2022 12:50:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=262
06/02/2022 12:50:05 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.6953441295546559 on epoch=262
06/02/2022 12:50:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=264
06/02/2022 12:50:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=267
06/02/2022 12:50:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=269
06/02/2022 12:50:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=272
06/02/2022 12:50:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=274
06/02/2022 12:50:12 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.6715547965547966 on epoch=274
06/02/2022 12:50:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=277
06/02/2022 12:50:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=279
06/02/2022 12:50:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=282
06/02/2022 12:50:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=284
06/02/2022 12:50:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
06/02/2022 12:50:18 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.6791583416583418 on epoch=287
06/02/2022 12:50:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=289
06/02/2022 12:50:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=292
06/02/2022 12:50:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
06/02/2022 12:50:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=297
06/02/2022 12:50:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=299
06/02/2022 12:50:25 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.6876050420168067 on epoch=299
06/02/2022 12:50:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=302
06/02/2022 12:50:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=304
06/02/2022 12:50:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=307
06/02/2022 12:50:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=309
06/02/2022 12:50:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.11 on epoch=312
06/02/2022 12:50:32 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.7076100370218016 on epoch=312
06/02/2022 12:50:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=314
06/02/2022 12:50:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=317
06/02/2022 12:50:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=319
06/02/2022 12:50:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=322
06/02/2022 12:50:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=324
06/02/2022 12:50:38 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.6588209088209087 on epoch=324
06/02/2022 12:50:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=327
06/02/2022 12:50:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=329
06/02/2022 12:50:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=332
06/02/2022 12:50:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=334
06/02/2022 12:50:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
06/02/2022 12:50:45 - INFO - __main__ - Global step 1350 Train loss 0.10 Classification-F1 0.6964937363834423 on epoch=337
06/02/2022 12:50:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=339
06/02/2022 12:50:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=342
06/02/2022 12:50:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
06/02/2022 12:50:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=347
06/02/2022 12:50:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=349
06/02/2022 12:50:52 - INFO - __main__ - Global step 1400 Train loss 0.11 Classification-F1 0.676174696864352 on epoch=349
06/02/2022 12:50:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=352
06/02/2022 12:50:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=354
06/02/2022 12:50:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
06/02/2022 12:50:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
06/02/2022 12:50:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=362
06/02/2022 12:50:58 - INFO - __main__ - Global step 1450 Train loss 0.10 Classification-F1 0.6543306346929015 on epoch=362
06/02/2022 12:51:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=364
06/02/2022 12:51:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=367
06/02/2022 12:51:02 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=369
06/02/2022 12:51:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=372
06/02/2022 12:51:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
06/02/2022 12:51:05 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.5870138633296528 on epoch=374
06/02/2022 12:51:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
06/02/2022 12:51:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=379
06/02/2022 12:51:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=382
06/02/2022 12:51:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=384
06/02/2022 12:51:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=387
06/02/2022 12:51:12 - INFO - __main__ - Global step 1550 Train loss 0.08 Classification-F1 0.6777027027027027 on epoch=387
06/02/2022 12:51:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=389
06/02/2022 12:51:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
06/02/2022 12:51:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=394
06/02/2022 12:51:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=397
06/02/2022 12:51:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/02/2022 12:51:19 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.6803440385070623 on epoch=399
06/02/2022 12:51:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=402
06/02/2022 12:51:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=404
06/02/2022 12:51:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=407
06/02/2022 12:51:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=409
06/02/2022 12:51:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=412
06/02/2022 12:51:25 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.6088924588924589 on epoch=412
06/02/2022 12:51:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
06/02/2022 12:51:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=417
06/02/2022 12:51:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
06/02/2022 12:51:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
06/02/2022 12:51:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=424
06/02/2022 12:51:32 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.5172422949265054 on epoch=424
06/02/2022 12:51:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=427
06/02/2022 12:51:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/02/2022 12:51:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=432
06/02/2022 12:51:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=434
06/02/2022 12:51:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=437
06/02/2022 12:51:39 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.65 on epoch=437
06/02/2022 12:51:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=439
06/02/2022 12:51:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=442
06/02/2022 12:51:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=444
06/02/2022 12:51:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=447
06/02/2022 12:51:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=449
06/02/2022 12:51:46 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.6506274046319813 on epoch=449
06/02/2022 12:51:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
06/02/2022 12:51:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=454
06/02/2022 12:51:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/02/2022 12:51:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/02/2022 12:51:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
06/02/2022 12:51:52 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.5858337065233616 on epoch=462
06/02/2022 12:51:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=464
06/02/2022 12:51:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
06/02/2022 12:51:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/02/2022 12:51:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/02/2022 12:51:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=474
06/02/2022 12:51:59 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6542471042471043 on epoch=474
06/02/2022 12:52:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=477
06/02/2022 12:52:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/02/2022 12:52:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/02/2022 12:52:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/02/2022 12:52:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/02/2022 12:52:06 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.6509575569358178 on epoch=487
06/02/2022 12:52:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
06/02/2022 12:52:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=492
06/02/2022 12:52:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/02/2022 12:52:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/02/2022 12:52:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/02/2022 12:52:12 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.6411844089862666 on epoch=499
06/02/2022 12:52:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=502
06/02/2022 12:52:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.00 on epoch=504
06/02/2022 12:52:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
06/02/2022 12:52:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=509
06/02/2022 12:52:18 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=512
06/02/2022 12:52:19 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.644855193539404 on epoch=512
06/02/2022 12:52:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=514
06/02/2022 12:52:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/02/2022 12:52:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
06/02/2022 12:52:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=522
06/02/2022 12:52:25 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
06/02/2022 12:52:26 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6541353383458647 on epoch=524
06/02/2022 12:52:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=527
06/02/2022 12:52:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=529
06/02/2022 12:52:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=532
06/02/2022 12:52:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.00 on epoch=534
06/02/2022 12:52:32 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=537
06/02/2022 12:52:32 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.6657738095238096 on epoch=537
06/02/2022 12:52:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
06/02/2022 12:52:35 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=542
06/02/2022 12:52:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=544
06/02/2022 12:52:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.00 on epoch=547
06/02/2022 12:52:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/02/2022 12:52:39 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6670250896057347 on epoch=549
06/02/2022 12:52:40 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/02/2022 12:52:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/02/2022 12:52:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
06/02/2022 12:52:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
06/02/2022 12:52:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/02/2022 12:52:46 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5983147235564428 on epoch=562
06/02/2022 12:52:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/02/2022 12:52:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
06/02/2022 12:52:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/02/2022 12:52:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/02/2022 12:52:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/02/2022 12:52:53 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.5745576108479334 on epoch=574
06/02/2022 12:52:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
06/02/2022 12:52:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=579
06/02/2022 12:52:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/02/2022 12:52:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/02/2022 12:52:59 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 12:53:00 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.6473494098494099 on epoch=587
06/02/2022 12:53:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
06/02/2022 12:53:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/02/2022 12:53:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/02/2022 12:53:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/02/2022 12:53:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/02/2022 12:53:06 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.633822623571935 on epoch=599
06/02/2022 12:53:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/02/2022 12:53:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/02/2022 12:53:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=607
06/02/2022 12:53:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/02/2022 12:53:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/02/2022 12:53:13 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.6444163135339606 on epoch=612
06/02/2022 12:53:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=614
06/02/2022 12:53:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=617
06/02/2022 12:53:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
06/02/2022 12:53:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=622
06/02/2022 12:53:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/02/2022 12:53:20 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.686740890688259 on epoch=624
06/02/2022 12:53:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
06/02/2022 12:53:22 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/02/2022 12:53:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/02/2022 12:53:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/02/2022 12:53:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/02/2022 12:53:26 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6939881057528116 on epoch=637
06/02/2022 12:53:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=639
06/02/2022 12:53:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/02/2022 12:53:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/02/2022 12:53:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 12:53:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=649
06/02/2022 12:53:33 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.6602893215796442 on epoch=649
06/02/2022 12:53:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/02/2022 12:53:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/02/2022 12:53:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/02/2022 12:53:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/02/2022 12:53:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
06/02/2022 12:53:40 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6758824320210036 on epoch=662
06/02/2022 12:53:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/02/2022 12:53:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/02/2022 12:53:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/02/2022 12:53:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/02/2022 12:53:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
06/02/2022 12:53:47 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.6631001681404908 on epoch=674
06/02/2022 12:53:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/02/2022 12:53:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/02/2022 12:53:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/02/2022 12:53:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
06/02/2022 12:53:53 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/02/2022 12:53:53 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.6073432012633464 on epoch=687
06/02/2022 12:53:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/02/2022 12:53:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/02/2022 12:53:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 12:53:58 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/02/2022 12:53:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/02/2022 12:54:00 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.6546980304913956 on epoch=699
06/02/2022 12:54:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=702
06/02/2022 12:54:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/02/2022 12:54:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/02/2022 12:54:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/02/2022 12:54:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/02/2022 12:54:07 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.681759715380405 on epoch=712
06/02/2022 12:54:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/02/2022 12:54:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
06/02/2022 12:54:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 12:54:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/02/2022 12:54:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
06/02/2022 12:54:13 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5630812324929971 on epoch=724
06/02/2022 12:54:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 12:54:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=729
06/02/2022 12:54:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 12:54:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/02/2022 12:54:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=737
06/02/2022 12:54:20 - INFO - __main__ - Global step 2950 Train loss 0.00 Classification-F1 0.540960020371785 on epoch=737
06/02/2022 12:54:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/02/2022 12:54:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/02/2022 12:54:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
06/02/2022 12:54:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
06/02/2022 12:54:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/02/2022 12:54:27 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6816293183940243 on epoch=749
06/02/2022 12:54:27 - INFO - __main__ - save last model!
06/02/2022 12:54:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 12:54:27 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 12:54:27 - INFO - __main__ - Printing 3 examples
06/02/2022 12:54:27 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:54:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:54:27 - INFO - __main__ - Printing 3 examples
06/02/2022 12:54:27 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:54:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:54:27 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:54:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:54:27 - INFO - __main__ - Printing 3 examples
06/02/2022 12:54:27 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:54:27 - INFO - __main__ - ['others']
06/02/2022 12:54:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:54:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:54:27 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:54:29 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:54:33 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:54:33 - INFO - __main__ - task name: emo
06/02/2022 12:54:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:54:34 - INFO - __main__ - Starting training!
06/02/2022 12:54:34 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 12:55:17 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.3_8_predictions.txt
06/02/2022 12:55:17 - INFO - __main__ - Classification-F1 on test data: 0.2495
06/02/2022 12:55:17 - INFO - __main__ - prefix=emo_16_13, lr=0.3, bsz=8, dev_performance=0.7763573232323232, test_performance=0.24947396221510335
06/02/2022 12:55:17 - INFO - __main__ - Running ... prefix=emo_16_13, lr=0.2, bsz=8 ...
06/02/2022 12:55:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:55:18 - INFO - __main__ - Printing 3 examples
06/02/2022 12:55:18 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:55:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:55:18 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 12:55:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 12:55:18 - INFO - __main__ - Printing 3 examples
06/02/2022 12:55:18 - INFO - __main__ -  [emo] ok thx you and you  ok tell me about your  family
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ -  [emo] i did ask now you did tell ms
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ -  [emo] buddy how you tell me your contact no
06/02/2022 12:55:18 - INFO - __main__ - ['others']
06/02/2022 12:55:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 12:55:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 12:55:18 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 12:55:24 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 12:55:24 - INFO - __main__ - task name: emo
06/02/2022 12:55:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 12:55:25 - INFO - __main__ - Starting training!
06/02/2022 12:55:26 - INFO - __main__ - Step 10 Global step 10 Train loss 7.49 on epoch=2
06/02/2022 12:55:27 - INFO - __main__ - Step 20 Global step 20 Train loss 5.06 on epoch=4
06/02/2022 12:55:29 - INFO - __main__ - Step 30 Global step 30 Train loss 3.32 on epoch=7
06/02/2022 12:55:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.30 on epoch=9
06/02/2022 12:55:31 - INFO - __main__ - Step 50 Global step 50 Train loss 1.99 on epoch=12
06/02/2022 12:55:32 - INFO - __main__ - Global step 50 Train loss 4.03 Classification-F1 0.18881544156530936 on epoch=12
06/02/2022 12:55:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18881544156530936 on epoch=12, global_step=50
06/02/2022 12:55:33 - INFO - __main__ - Step 60 Global step 60 Train loss 1.65 on epoch=14
06/02/2022 12:55:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.49 on epoch=17
06/02/2022 12:55:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.49 on epoch=19
06/02/2022 12:55:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.62 on epoch=22
06/02/2022 12:55:38 - INFO - __main__ - Step 100 Global step 100 Train loss 1.33 on epoch=24
06/02/2022 12:55:38 - INFO - __main__ - Global step 100 Train loss 1.52 Classification-F1 0.24002382370458603 on epoch=24
06/02/2022 12:55:38 - INFO - __main__ - Saving model with best Classification-F1: 0.18881544156530936 -> 0.24002382370458603 on epoch=24, global_step=100
06/02/2022 12:55:40 - INFO - __main__ - Step 110 Global step 110 Train loss 1.28 on epoch=27
06/02/2022 12:55:41 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
06/02/2022 12:55:42 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=32
06/02/2022 12:55:44 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
06/02/2022 12:55:45 - INFO - __main__ - Step 150 Global step 150 Train loss 1.02 on epoch=37
06/02/2022 12:55:45 - INFO - __main__ - Global step 150 Train loss 1.12 Classification-F1 0.1307631160572337 on epoch=37
06/02/2022 12:55:47 - INFO - __main__ - Step 160 Global step 160 Train loss 1.23 on epoch=39
06/02/2022 12:55:48 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=42
06/02/2022 12:55:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.02 on epoch=44
06/02/2022 12:55:50 - INFO - __main__ - Step 190 Global step 190 Train loss 1.13 on epoch=47
06/02/2022 12:55:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
06/02/2022 12:55:52 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.253734827264239 on epoch=49
06/02/2022 12:55:52 - INFO - __main__ - Saving model with best Classification-F1: 0.24002382370458603 -> 0.253734827264239 on epoch=49, global_step=200
06/02/2022 12:55:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.03 on epoch=52
06/02/2022 12:55:55 - INFO - __main__ - Step 220 Global step 220 Train loss 1.03 on epoch=54
06/02/2022 12:55:56 - INFO - __main__ - Step 230 Global step 230 Train loss 1.10 on epoch=57
06/02/2022 12:55:57 - INFO - __main__ - Step 240 Global step 240 Train loss 1.01 on epoch=59
06/02/2022 12:55:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.98 on epoch=62
06/02/2022 12:55:59 - INFO - __main__ - Global step 250 Train loss 1.03 Classification-F1 0.10126582278481013 on epoch=62
06/02/2022 12:56:00 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
06/02/2022 12:56:01 - INFO - __main__ - Step 270 Global step 270 Train loss 1.05 on epoch=67
06/02/2022 12:56:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=69
06/02/2022 12:56:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
06/02/2022 12:56:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.95 on epoch=74
06/02/2022 12:56:05 - INFO - __main__ - Global step 300 Train loss 1.01 Classification-F1 0.2907925407925408 on epoch=74
06/02/2022 12:56:05 - INFO - __main__ - Saving model with best Classification-F1: 0.253734827264239 -> 0.2907925407925408 on epoch=74, global_step=300
06/02/2022 12:56:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
06/02/2022 12:56:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=79
06/02/2022 12:56:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
06/02/2022 12:56:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.93 on epoch=84
06/02/2022 12:56:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=87
06/02/2022 12:56:12 - INFO - __main__ - Global step 350 Train loss 0.95 Classification-F1 0.23853640951694305 on epoch=87
06/02/2022 12:56:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.85 on epoch=89
06/02/2022 12:56:14 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=92
06/02/2022 12:56:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=94
06/02/2022 12:56:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=97
06/02/2022 12:56:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.93 on epoch=99
06/02/2022 12:56:19 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.28420920607048794 on epoch=99
06/02/2022 12:56:20 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=102
06/02/2022 12:56:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.88 on epoch=104
06/02/2022 12:56:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=107
06/02/2022 12:56:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=109
06/02/2022 12:56:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=112
06/02/2022 12:56:25 - INFO - __main__ - Global step 450 Train loss 0.88 Classification-F1 0.2880911517925248 on epoch=112
06/02/2022 12:56:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=114
06/02/2022 12:56:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=117
06/02/2022 12:56:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=119
06/02/2022 12:56:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=122
06/02/2022 12:56:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=124
06/02/2022 12:56:32 - INFO - __main__ - Global step 500 Train loss 0.81 Classification-F1 0.42064818809318383 on epoch=124
06/02/2022 12:56:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2907925407925408 -> 0.42064818809318383 on epoch=124, global_step=500
06/02/2022 12:56:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.80 on epoch=127
06/02/2022 12:56:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=129
06/02/2022 12:56:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=132
06/02/2022 12:56:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.74 on epoch=134
06/02/2022 12:56:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=137
06/02/2022 12:56:39 - INFO - __main__ - Global step 550 Train loss 0.75 Classification-F1 0.42822479928635143 on epoch=137
06/02/2022 12:56:39 - INFO - __main__ - Saving model with best Classification-F1: 0.42064818809318383 -> 0.42822479928635143 on epoch=137, global_step=550
06/02/2022 12:56:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
06/02/2022 12:56:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=142
06/02/2022 12:56:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=144
06/02/2022 12:56:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.71 on epoch=147
06/02/2022 12:56:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.69 on epoch=149
06/02/2022 12:56:45 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.5597436879077727 on epoch=149
06/02/2022 12:56:45 - INFO - __main__ - Saving model with best Classification-F1: 0.42822479928635143 -> 0.5597436879077727 on epoch=149, global_step=600
06/02/2022 12:56:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.66 on epoch=152
06/02/2022 12:56:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.66 on epoch=154
06/02/2022 12:56:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=157
06/02/2022 12:56:50 - INFO - __main__ - Step 640 Global step 640 Train loss 0.62 on epoch=159
06/02/2022 12:56:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=162
06/02/2022 12:56:52 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.2673397199705232 on epoch=162
06/02/2022 12:56:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.69 on epoch=164
06/02/2022 12:56:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.69 on epoch=167
06/02/2022 12:56:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=169
06/02/2022 12:56:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
06/02/2022 12:56:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=174
06/02/2022 12:56:58 - INFO - __main__ - Global step 700 Train loss 0.65 Classification-F1 0.628027378027378 on epoch=174
06/02/2022 12:56:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5597436879077727 -> 0.628027378027378 on epoch=174, global_step=700
06/02/2022 12:57:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
06/02/2022 12:57:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=179
06/02/2022 12:57:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=182
06/02/2022 12:57:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.65 on epoch=184
06/02/2022 12:57:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=187
06/02/2022 12:57:05 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.507422969187675 on epoch=187
06/02/2022 12:57:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=189
06/02/2022 12:57:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=192
06/02/2022 12:57:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.62 on epoch=194
06/02/2022 12:57:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=197
06/02/2022 12:57:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=199
06/02/2022 12:57:12 - INFO - __main__ - Global step 800 Train loss 0.57 Classification-F1 0.5013631022326674 on epoch=199
06/02/2022 12:57:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=202
06/02/2022 12:57:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=204
06/02/2022 12:57:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.61 on epoch=207
06/02/2022 12:57:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=209
06/02/2022 12:57:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.57 on epoch=212
06/02/2022 12:57:18 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.5204678362573099 on epoch=212
06/02/2022 12:57:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.52 on epoch=214
06/02/2022 12:57:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
06/02/2022 12:57:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=219
06/02/2022 12:57:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.63 on epoch=222
06/02/2022 12:57:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=224
06/02/2022 12:57:25 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.5591525591525591 on epoch=224
06/02/2022 12:57:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=227
06/02/2022 12:57:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=229
06/02/2022 12:57:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=232
06/02/2022 12:57:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
06/02/2022 12:57:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=237
06/02/2022 12:57:32 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.5092585196033472 on epoch=237
06/02/2022 12:57:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.52 on epoch=239
06/02/2022 12:57:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=242
06/02/2022 12:57:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=244
06/02/2022 12:57:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.53 on epoch=247
06/02/2022 12:57:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=249
06/02/2022 12:57:38 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.5330746187363834 on epoch=249
06/02/2022 12:57:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=252
06/02/2022 12:57:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
06/02/2022 12:57:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=257
06/02/2022 12:57:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=259
06/02/2022 12:57:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=262
06/02/2022 12:57:45 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.4796171171171171 on epoch=262
06/02/2022 12:57:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=264
06/02/2022 12:57:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=267
06/02/2022 12:57:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=269
06/02/2022 12:57:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=272
06/02/2022 12:57:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=274
06/02/2022 12:57:51 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.5478367777062233 on epoch=274
06/02/2022 12:57:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=277
06/02/2022 12:57:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=279
06/02/2022 12:57:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=282
06/02/2022 12:57:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=284
06/02/2022 12:57:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=287
06/02/2022 12:57:58 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.5986768018018017 on epoch=287
06/02/2022 12:57:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=289
06/02/2022 12:58:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
06/02/2022 12:58:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=294
06/02/2022 12:58:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=297
06/02/2022 12:58:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=299
06/02/2022 12:58:05 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.5719725276680545 on epoch=299
06/02/2022 12:58:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=302
06/02/2022 12:58:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=304
06/02/2022 12:58:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=307
06/02/2022 12:58:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=309
06/02/2022 12:58:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=312
06/02/2022 12:58:11 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.5070529320822905 on epoch=312
06/02/2022 12:58:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=314
06/02/2022 12:58:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=317
06/02/2022 12:58:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=319
06/02/2022 12:58:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=322
06/02/2022 12:58:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=324
06/02/2022 12:58:18 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.49637834931952585 on epoch=324
06/02/2022 12:58:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=327
06/02/2022 12:58:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=329
06/02/2022 12:58:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=332
06/02/2022 12:58:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
06/02/2022 12:58:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=337
06/02/2022 12:58:25 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.5351178622231254 on epoch=337
06/02/2022 12:58:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=339
06/02/2022 12:58:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=342
06/02/2022 12:58:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=344
06/02/2022 12:58:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=347
06/02/2022 12:58:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=349
06/02/2022 12:58:31 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.5510984326841104 on epoch=349
06/02/2022 12:58:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=352
06/02/2022 12:58:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=354
06/02/2022 12:58:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=357
06/02/2022 12:58:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=359
06/02/2022 12:58:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=362
06/02/2022 12:58:38 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.5308557431728164 on epoch=362
06/02/2022 12:58:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=364
06/02/2022 12:58:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=367
06/02/2022 12:58:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=369
06/02/2022 12:58:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=372
06/02/2022 12:58:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=374
06/02/2022 12:58:44 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.5554737253847376 on epoch=374
06/02/2022 12:58:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=377
06/02/2022 12:58:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=379
06/02/2022 12:58:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=382
06/02/2022 12:58:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=384
06/02/2022 12:58:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=387
06/02/2022 12:58:51 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.5773707773707774 on epoch=387
06/02/2022 12:58:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=389
06/02/2022 12:58:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=392
06/02/2022 12:58:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=394
06/02/2022 12:58:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=397
06/02/2022 12:58:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=399
06/02/2022 12:58:58 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.5628369331817609 on epoch=399
06/02/2022 12:58:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=402
06/02/2022 12:59:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=404
06/02/2022 12:59:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=407
06/02/2022 12:59:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=409
06/02/2022 12:59:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=412
06/02/2022 12:59:04 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.5397368421052633 on epoch=412
06/02/2022 12:59:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=414
06/02/2022 12:59:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=417
06/02/2022 12:59:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=419
06/02/2022 12:59:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=422
06/02/2022 12:59:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=424
06/02/2022 12:59:11 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.610042735042735 on epoch=424
06/02/2022 12:59:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=427
06/02/2022 12:59:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=429
06/02/2022 12:59:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=432
06/02/2022 12:59:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
06/02/2022 12:59:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=437
06/02/2022 12:59:18 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.6061947856065504 on epoch=437
06/02/2022 12:59:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
06/02/2022 12:59:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=442
06/02/2022 12:59:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=444
06/02/2022 12:59:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
06/02/2022 12:59:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=449
06/02/2022 12:59:24 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.6196613190730837 on epoch=449
06/02/2022 12:59:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=452
06/02/2022 12:59:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=454
06/02/2022 12:59:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=457
06/02/2022 12:59:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
06/02/2022 12:59:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=462
06/02/2022 12:59:31 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.6136012469503931 on epoch=462
06/02/2022 12:59:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
06/02/2022 12:59:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=467
06/02/2022 12:59:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=469
06/02/2022 12:59:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
06/02/2022 12:59:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=474
06/02/2022 12:59:37 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.5865237651444548 on epoch=474
06/02/2022 12:59:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=477
06/02/2022 12:59:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
06/02/2022 12:59:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
06/02/2022 12:59:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
06/02/2022 12:59:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
06/02/2022 12:59:44 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.5886580086580087 on epoch=487
06/02/2022 12:59:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
06/02/2022 12:59:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=492
06/02/2022 12:59:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=494
06/02/2022 12:59:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=497
06/02/2022 12:59:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
06/02/2022 12:59:51 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.560923098822106 on epoch=499
06/02/2022 12:59:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=502
06/02/2022 12:59:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
06/02/2022 12:59:55 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=507
06/02/2022 12:59:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=509
06/02/2022 12:59:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=512
06/02/2022 12:59:58 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.5791211884961884 on epoch=512
06/02/2022 12:59:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=514
06/02/2022 13:00:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=517
06/02/2022 13:00:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=519
06/02/2022 13:00:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=522
06/02/2022 13:00:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=524
06/02/2022 13:00:04 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.5721834471834472 on epoch=524
06/02/2022 13:00:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=527
06/02/2022 13:00:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=529
06/02/2022 13:00:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=532
06/02/2022 13:00:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=534
06/02/2022 13:00:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
06/02/2022 13:00:11 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.5746403170087381 on epoch=537
06/02/2022 13:00:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=539
06/02/2022 13:00:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=542
06/02/2022 13:00:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
06/02/2022 13:00:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
06/02/2022 13:00:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
06/02/2022 13:00:18 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.5468986568986569 on epoch=549
06/02/2022 13:00:19 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=552
06/02/2022 13:00:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=554
06/02/2022 13:00:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/02/2022 13:00:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=559
06/02/2022 13:00:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=562
06/02/2022 13:00:24 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5394951332451332 on epoch=562
06/02/2022 13:00:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
06/02/2022 13:00:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=567
06/02/2022 13:00:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=569
06/02/2022 13:00:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
06/02/2022 13:00:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
06/02/2022 13:00:31 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.5604964878671775 on epoch=574
06/02/2022 13:00:32 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=577
06/02/2022 13:00:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=579
06/02/2022 13:00:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=582
06/02/2022 13:00:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=584
06/02/2022 13:00:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=587
06/02/2022 13:00:38 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.560540239487608 on epoch=587
06/02/2022 13:00:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
06/02/2022 13:00:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=592
06/02/2022 13:00:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/02/2022 13:00:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=597
06/02/2022 13:00:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/02/2022 13:00:44 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.5743181818181818 on epoch=599
06/02/2022 13:00:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=602
06/02/2022 13:00:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/02/2022 13:00:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/02/2022 13:00:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/02/2022 13:00:51 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/02/2022 13:00:51 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.5877345884605413 on epoch=612
06/02/2022 13:00:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/02/2022 13:00:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=617
06/02/2022 13:00:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=619
06/02/2022 13:00:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=622
06/02/2022 13:00:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=624
06/02/2022 13:00:58 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.5628363896165753 on epoch=624
06/02/2022 13:00:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/02/2022 13:01:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
06/02/2022 13:01:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/02/2022 13:01:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=634
06/02/2022 13:01:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=637
06/02/2022 13:01:05 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.5397386397386397 on epoch=637
06/02/2022 13:01:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
06/02/2022 13:01:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 13:01:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=644
06/02/2022 13:01:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 13:01:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=649
06/02/2022 13:01:12 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.5789760348583879 on epoch=649
06/02/2022 13:01:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/02/2022 13:01:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=654
06/02/2022 13:01:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 13:01:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=659
06/02/2022 13:01:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
06/02/2022 13:01:18 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5035170361247947 on epoch=662
06/02/2022 13:01:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/02/2022 13:01:21 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=667
06/02/2022 13:01:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=669
06/02/2022 13:01:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/02/2022 13:01:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/02/2022 13:01:25 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5579911524500907 on epoch=674
06/02/2022 13:01:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/02/2022 13:01:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=679
06/02/2022 13:01:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=682
06/02/2022 13:01:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/02/2022 13:01:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
06/02/2022 13:01:32 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.5570210286561789 on epoch=687
06/02/2022 13:01:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=689
06/02/2022 13:01:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/02/2022 13:01:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 13:01:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/02/2022 13:01:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/02/2022 13:01:39 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.5284085491816304 on epoch=699
06/02/2022 13:01:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=702
06/02/2022 13:01:41 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/02/2022 13:01:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=707
06/02/2022 13:01:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/02/2022 13:01:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/02/2022 13:01:45 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.5392308897243108 on epoch=712
06/02/2022 13:01:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=714
06/02/2022 13:01:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/02/2022 13:01:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=719
06/02/2022 13:01:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
06/02/2022 13:01:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=724
06/02/2022 13:01:52 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.577970177970178 on epoch=724
06/02/2022 13:01:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/02/2022 13:01:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 13:01:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
06/02/2022 13:01:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/02/2022 13:01:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=737
06/02/2022 13:01:59 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5347985347985348 on epoch=737
06/02/2022 13:02:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=739
06/02/2022 13:02:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
06/02/2022 13:02:03 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/02/2022 13:02:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/02/2022 13:02:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/02/2022 13:02:06 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.5908119658119658 on epoch=749
06/02/2022 13:02:06 - INFO - __main__ - save last model!
06/02/2022 13:02:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:02:06 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:02:06 - INFO - __main__ - Printing 3 examples
06/02/2022 13:02:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:02:06 - INFO - __main__ - ['others']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:02:06 - INFO - __main__ - ['others']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:02:06 - INFO - __main__ - ['others']
06/02/2022 13:02:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:02:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:02:06 - INFO - __main__ - Printing 3 examples
06/02/2022 13:02:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:02:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:02:06 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:02:06 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:02:06 - INFO - __main__ - Printing 3 examples
06/02/2022 13:02:06 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:02:06 - INFO - __main__ - ['sad']
06/02/2022 13:02:06 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:02:06 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:02:06 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:02:08 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:02:12 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:02:12 - INFO - __main__ - task name: emo
06/02/2022 13:02:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:02:13 - INFO - __main__ - Starting training!
06/02/2022 13:02:13 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:02:56 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_13_0.2_8_predictions.txt
06/02/2022 13:02:56 - INFO - __main__ - Classification-F1 on test data: 0.3483
06/02/2022 13:02:56 - INFO - __main__ - prefix=emo_16_13, lr=0.2, bsz=8, dev_performance=0.628027378027378, test_performance=0.3482672792591976
06/02/2022 13:02:56 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.5, bsz=8 ...
06/02/2022 13:02:57 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:02:57 - INFO - __main__ - Printing 3 examples
06/02/2022 13:02:57 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:02:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:02:57 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:02:57 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:02:57 - INFO - __main__ - Printing 3 examples
06/02/2022 13:02:57 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:02:57 - INFO - __main__ - ['sad']
06/02/2022 13:02:57 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:02:57 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:02:57 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:03:03 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:03:03 - INFO - __main__ - task name: emo
06/02/2022 13:03:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:03:04 - INFO - __main__ - Starting training!
06/02/2022 13:03:05 - INFO - __main__ - Step 10 Global step 10 Train loss 6.32 on epoch=2
06/02/2022 13:03:06 - INFO - __main__ - Step 20 Global step 20 Train loss 3.73 on epoch=4
06/02/2022 13:03:07 - INFO - __main__ - Step 30 Global step 30 Train loss 2.45 on epoch=7
06/02/2022 13:03:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.99 on epoch=9
06/02/2022 13:03:10 - INFO - __main__ - Step 50 Global step 50 Train loss 2.45 on epoch=12
06/02/2022 13:03:10 - INFO - __main__ - Global step 50 Train loss 3.39 Classification-F1 0.13067758749069247 on epoch=12
06/02/2022 13:03:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
06/02/2022 13:03:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.67 on epoch=14
06/02/2022 13:03:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.78 on epoch=17
06/02/2022 13:03:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.36 on epoch=19
06/02/2022 13:03:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.34 on epoch=22
06/02/2022 13:03:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.25 on epoch=24
06/02/2022 13:03:17 - INFO - __main__ - Global step 100 Train loss 1.48 Classification-F1 0.16132939662351425 on epoch=24
06/02/2022 13:03:17 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.16132939662351425 on epoch=24, global_step=100
06/02/2022 13:03:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.31 on epoch=27
06/02/2022 13:03:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
06/02/2022 13:03:21 - INFO - __main__ - Step 130 Global step 130 Train loss 1.20 on epoch=32
06/02/2022 13:03:22 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
06/02/2022 13:03:23 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
06/02/2022 13:03:24 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.1794380308904873 on epoch=37
06/02/2022 13:03:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16132939662351425 -> 0.1794380308904873 on epoch=37, global_step=150
06/02/2022 13:03:25 - INFO - __main__ - Step 160 Global step 160 Train loss 1.10 on epoch=39
06/02/2022 13:03:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.13 on epoch=42
06/02/2022 13:03:28 - INFO - __main__ - Step 180 Global step 180 Train loss 1.06 on epoch=44
06/02/2022 13:03:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
06/02/2022 13:03:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.08 on epoch=49
06/02/2022 13:03:31 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.10256410256410256 on epoch=49
06/02/2022 13:03:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=52
06/02/2022 13:03:33 - INFO - __main__ - Step 220 Global step 220 Train loss 1.13 on epoch=54
06/02/2022 13:03:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.97 on epoch=57
06/02/2022 13:03:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.92 on epoch=59
06/02/2022 13:03:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=62
06/02/2022 13:03:37 - INFO - __main__ - Global step 250 Train loss 0.98 Classification-F1 0.24016531713900136 on epoch=62
06/02/2022 13:03:37 - INFO - __main__ - Saving model with best Classification-F1: 0.1794380308904873 -> 0.24016531713900136 on epoch=62, global_step=250
06/02/2022 13:03:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.93 on epoch=64
06/02/2022 13:03:40 - INFO - __main__ - Step 270 Global step 270 Train loss 1.06 on epoch=67
06/02/2022 13:03:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=69
06/02/2022 13:03:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
06/02/2022 13:03:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
06/02/2022 13:03:44 - INFO - __main__ - Global step 300 Train loss 0.95 Classification-F1 0.23973727422003283 on epoch=74
06/02/2022 13:03:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
06/02/2022 13:03:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=79
06/02/2022 13:03:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.90 on epoch=82
06/02/2022 13:03:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.99 on epoch=84
06/02/2022 13:03:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.82 on epoch=87
06/02/2022 13:03:51 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.3074074074074074 on epoch=87
06/02/2022 13:03:51 - INFO - __main__ - Saving model with best Classification-F1: 0.24016531713900136 -> 0.3074074074074074 on epoch=87, global_step=350
06/02/2022 13:03:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=89
06/02/2022 13:03:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
06/02/2022 13:03:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
06/02/2022 13:03:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.88 on epoch=97
06/02/2022 13:03:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=99
06/02/2022 13:03:58 - INFO - __main__ - Global step 400 Train loss 0.83 Classification-F1 0.4158653846153846 on epoch=99
06/02/2022 13:03:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3074074074074074 -> 0.4158653846153846 on epoch=99, global_step=400
06/02/2022 13:03:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.74 on epoch=102
06/02/2022 13:04:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=104
06/02/2022 13:04:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=107
06/02/2022 13:04:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=109
06/02/2022 13:04:04 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=112
06/02/2022 13:04:05 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.4131224525961368 on epoch=112
06/02/2022 13:04:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.87 on epoch=114
06/02/2022 13:04:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=117
06/02/2022 13:04:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=119
06/02/2022 13:04:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.72 on epoch=122
06/02/2022 13:04:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=124
06/02/2022 13:04:11 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.4901573787409701 on epoch=124
06/02/2022 13:04:11 - INFO - __main__ - Saving model with best Classification-F1: 0.4158653846153846 -> 0.4901573787409701 on epoch=124, global_step=500
06/02/2022 13:04:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=127
06/02/2022 13:04:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.65 on epoch=129
06/02/2022 13:04:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.60 on epoch=132
06/02/2022 13:04:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=134
06/02/2022 13:04:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=137
06/02/2022 13:04:18 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.4578508727676319 on epoch=137
06/02/2022 13:04:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=139
06/02/2022 13:04:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=142
06/02/2022 13:04:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=144
06/02/2022 13:04:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=147
06/02/2022 13:04:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=149
06/02/2022 13:04:25 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.5666699893673578 on epoch=149
06/02/2022 13:04:25 - INFO - __main__ - Saving model with best Classification-F1: 0.4901573787409701 -> 0.5666699893673578 on epoch=149, global_step=600
06/02/2022 13:04:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=152
06/02/2022 13:04:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=154
06/02/2022 13:04:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=157
06/02/2022 13:04:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=159
06/02/2022 13:04:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=162
06/02/2022 13:04:32 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.655924419140697 on epoch=162
06/02/2022 13:04:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5666699893673578 -> 0.655924419140697 on epoch=162, global_step=650
06/02/2022 13:04:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=164
06/02/2022 13:04:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=167
06/02/2022 13:04:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=169
06/02/2022 13:04:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=172
06/02/2022 13:04:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=174
06/02/2022 13:04:39 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.5489911906791702 on epoch=174
06/02/2022 13:04:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=177
06/02/2022 13:04:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=179
06/02/2022 13:04:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=182
06/02/2022 13:04:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=184
06/02/2022 13:04:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=187
06/02/2022 13:04:46 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.6084459148975278 on epoch=187
06/02/2022 13:04:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=189
06/02/2022 13:04:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=192
06/02/2022 13:04:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
06/02/2022 13:04:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=197
06/02/2022 13:04:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
06/02/2022 13:04:52 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.5004047356988534 on epoch=199
06/02/2022 13:04:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=202
06/02/2022 13:04:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=204
06/02/2022 13:04:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=207
06/02/2022 13:04:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=209
06/02/2022 13:04:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=212
06/02/2022 13:04:59 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.6189301777682588 on epoch=212
06/02/2022 13:05:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=214
06/02/2022 13:05:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=217
06/02/2022 13:05:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.15 on epoch=219
06/02/2022 13:05:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=222
06/02/2022 13:05:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.13 on epoch=224
06/02/2022 13:05:06 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.7488530668677729 on epoch=224
06/02/2022 13:05:06 - INFO - __main__ - Saving model with best Classification-F1: 0.655924419140697 -> 0.7488530668677729 on epoch=224, global_step=900
06/02/2022 13:05:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=227
06/02/2022 13:05:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=229
06/02/2022 13:05:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=232
06/02/2022 13:05:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
06/02/2022 13:05:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=237
06/02/2022 13:05:13 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.6670518680445151 on epoch=237
06/02/2022 13:05:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=239
06/02/2022 13:05:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=242
06/02/2022 13:05:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=244
06/02/2022 13:05:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=247
06/02/2022 13:05:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=249
06/02/2022 13:05:19 - INFO - __main__ - Global step 1000 Train loss 0.14 Classification-F1 0.6785633995103149 on epoch=249
06/02/2022 13:05:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=252
06/02/2022 13:05:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.06 on epoch=254
06/02/2022 13:05:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.06 on epoch=257
06/02/2022 13:05:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=259
06/02/2022 13:05:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=262
06/02/2022 13:05:26 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7047754585705249 on epoch=262
06/02/2022 13:05:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=264
06/02/2022 13:05:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=267
06/02/2022 13:05:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=269
06/02/2022 13:05:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=272
06/02/2022 13:05:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=274
06/02/2022 13:05:33 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.6591662709309768 on epoch=274
06/02/2022 13:05:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=277
06/02/2022 13:05:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.04 on epoch=279
06/02/2022 13:05:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=282
06/02/2022 13:05:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=284
06/02/2022 13:05:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=287
06/02/2022 13:05:40 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.6998268398268399 on epoch=287
06/02/2022 13:05:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=289
06/02/2022 13:05:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=292
06/02/2022 13:05:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=294
06/02/2022 13:05:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.09 on epoch=297
06/02/2022 13:05:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=299
06/02/2022 13:05:47 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6405445111277821 on epoch=299
06/02/2022 13:05:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=302
06/02/2022 13:05:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=304
06/02/2022 13:05:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=307
06/02/2022 13:05:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=309
06/02/2022 13:05:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=312
06/02/2022 13:05:53 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.668123012718601 on epoch=312
06/02/2022 13:05:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=314
06/02/2022 13:05:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=317
06/02/2022 13:05:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=319
06/02/2022 13:05:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=322
06/02/2022 13:06:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=324
06/02/2022 13:06:00 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.649469696969697 on epoch=324
06/02/2022 13:06:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=327
06/02/2022 13:06:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=329
06/02/2022 13:06:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=332
06/02/2022 13:06:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=334
06/02/2022 13:06:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=337
06/02/2022 13:06:07 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.6777381990285216 on epoch=337
06/02/2022 13:06:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=339
06/02/2022 13:06:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/02/2022 13:06:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=344
06/02/2022 13:06:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.09 on epoch=347
06/02/2022 13:06:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=349
06/02/2022 13:06:14 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.5801806171470806 on epoch=349
06/02/2022 13:06:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=352
06/02/2022 13:06:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=354
06/02/2022 13:06:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=357
06/02/2022 13:06:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=359
06/02/2022 13:06:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=362
06/02/2022 13:06:20 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.6714285714285714 on epoch=362
06/02/2022 13:06:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=364
06/02/2022 13:06:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
06/02/2022 13:06:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=369
06/02/2022 13:06:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=372
06/02/2022 13:06:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=374
06/02/2022 13:06:27 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.64582225016524 on epoch=374
06/02/2022 13:06:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=377
06/02/2022 13:06:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=379
06/02/2022 13:06:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=382
06/02/2022 13:06:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=384
06/02/2022 13:06:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=387
06/02/2022 13:06:34 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.6904115853658537 on epoch=387
06/02/2022 13:06:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/02/2022 13:06:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=392
06/02/2022 13:06:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=394
06/02/2022 13:06:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=397
06/02/2022 13:06:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=399
06/02/2022 13:06:41 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.7023172905525847 on epoch=399
06/02/2022 13:06:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.08 on epoch=402
06/02/2022 13:06:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=404
06/02/2022 13:06:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/02/2022 13:06:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
06/02/2022 13:06:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=412
06/02/2022 13:06:48 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.632901575777576 on epoch=412
06/02/2022 13:06:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=414
06/02/2022 13:06:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=417
06/02/2022 13:06:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=419
06/02/2022 13:06:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=422
06/02/2022 13:06:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
06/02/2022 13:06:54 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6569711538461538 on epoch=424
06/02/2022 13:06:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=427
06/02/2022 13:06:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=429
06/02/2022 13:06:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=432
06/02/2022 13:07:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=434
06/02/2022 13:07:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/02/2022 13:07:01 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.6337903225806452 on epoch=437
06/02/2022 13:07:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=439
06/02/2022 13:07:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=442
06/02/2022 13:07:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=444
06/02/2022 13:07:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=447
06/02/2022 13:07:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/02/2022 13:07:08 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.6971093954891162 on epoch=449
06/02/2022 13:07:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/02/2022 13:07:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/02/2022 13:07:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=457
06/02/2022 13:07:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.02 on epoch=459
06/02/2022 13:07:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/02/2022 13:07:15 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.6368627149877151 on epoch=462
06/02/2022 13:07:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=464
06/02/2022 13:07:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=467
06/02/2022 13:07:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=469
06/02/2022 13:07:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=472
06/02/2022 13:07:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/02/2022 13:07:22 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.656075119236884 on epoch=474
06/02/2022 13:07:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=477
06/02/2022 13:07:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=479
06/02/2022 13:07:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=482
06/02/2022 13:07:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=484
06/02/2022 13:07:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=487
06/02/2022 13:07:28 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6361512460233297 on epoch=487
06/02/2022 13:07:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/02/2022 13:07:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
06/02/2022 13:07:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/02/2022 13:07:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=497
06/02/2022 13:07:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=499
06/02/2022 13:07:35 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7034208027151575 on epoch=499
06/02/2022 13:07:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/02/2022 13:07:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=504
06/02/2022 13:07:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/02/2022 13:07:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=509
06/02/2022 13:07:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/02/2022 13:07:42 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.6416197799449863 on epoch=512
06/02/2022 13:07:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/02/2022 13:07:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=517
06/02/2022 13:07:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/02/2022 13:07:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=522
06/02/2022 13:07:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
06/02/2022 13:07:49 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.6922619047619047 on epoch=524
06/02/2022 13:07:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=527
06/02/2022 13:07:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.00 on epoch=529
06/02/2022 13:07:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.00 on epoch=532
06/02/2022 13:07:54 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=534
06/02/2022 13:07:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=537
06/02/2022 13:07:56 - INFO - __main__ - Global step 2150 Train loss 0.01 Classification-F1 0.6465544871794872 on epoch=537
06/02/2022 13:07:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/02/2022 13:07:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=542
06/02/2022 13:07:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=544
06/02/2022 13:08:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/02/2022 13:08:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/02/2022 13:08:02 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6206358182533438 on epoch=549
06/02/2022 13:08:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.00 on epoch=552
06/02/2022 13:08:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.00 on epoch=554
06/02/2022 13:08:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/02/2022 13:08:07 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=559
06/02/2022 13:08:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/02/2022 13:08:09 - INFO - __main__ - Global step 2250 Train loss 0.01 Classification-F1 0.6621794871794872 on epoch=562
06/02/2022 13:08:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/02/2022 13:08:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/02/2022 13:08:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/02/2022 13:08:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=572
06/02/2022 13:08:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/02/2022 13:08:16 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.6065668202764977 on epoch=574
06/02/2022 13:08:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=577
06/02/2022 13:08:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/02/2022 13:08:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=582
06/02/2022 13:08:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=584
06/02/2022 13:08:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=587
06/02/2022 13:08:23 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.6086209052263065 on epoch=587
06/02/2022 13:08:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=589
06/02/2022 13:08:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/02/2022 13:08:27 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.00 on epoch=594
06/02/2022 13:08:28 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/02/2022 13:08:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/02/2022 13:08:30 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.6925526024363233 on epoch=599
06/02/2022 13:08:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/02/2022 13:08:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/02/2022 13:08:34 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/02/2022 13:08:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.00 on epoch=609
06/02/2022 13:08:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/02/2022 13:08:37 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.5791981239990849 on epoch=612
06/02/2022 13:08:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/02/2022 13:08:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=617
06/02/2022 13:08:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.00 on epoch=619
06/02/2022 13:08:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/02/2022 13:08:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/02/2022 13:08:44 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.657490694789082 on epoch=624
06/02/2022 13:08:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/02/2022 13:08:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.08 on epoch=629
06/02/2022 13:08:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 13:08:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/02/2022 13:08:50 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=637
06/02/2022 13:08:50 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.6779569892473118 on epoch=637
06/02/2022 13:08:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 13:08:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/02/2022 13:08:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/02/2022 13:08:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/02/2022 13:08:57 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/02/2022 13:08:57 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6850559350559351 on epoch=649
06/02/2022 13:08:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
06/02/2022 13:09:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/02/2022 13:09:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 13:09:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=659
06/02/2022 13:09:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.00 on epoch=662
06/02/2022 13:09:04 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.6986895457483693 on epoch=662
06/02/2022 13:09:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/02/2022 13:09:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
06/02/2022 13:09:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/02/2022 13:09:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/02/2022 13:09:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/02/2022 13:09:11 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.6685782967032967 on epoch=674
06/02/2022 13:09:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=677
06/02/2022 13:09:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
06/02/2022 13:09:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/02/2022 13:09:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/02/2022 13:09:17 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/02/2022 13:09:18 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6473921469497809 on epoch=687
06/02/2022 13:09:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/02/2022 13:09:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.00 on epoch=692
06/02/2022 13:09:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=694
06/02/2022 13:09:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=697
06/02/2022 13:09:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/02/2022 13:09:25 - INFO - __main__ - Global step 2800 Train loss 0.00 Classification-F1 0.6752747252747252 on epoch=699
06/02/2022 13:09:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/02/2022 13:09:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/02/2022 13:09:29 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
06/02/2022 13:09:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=709
06/02/2022 13:09:31 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/02/2022 13:09:32 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.6515407464752585 on epoch=712
06/02/2022 13:09:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/02/2022 13:09:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/02/2022 13:09:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/02/2022 13:09:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=722
06/02/2022 13:09:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 13:09:39 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6896756978653531 on epoch=724
06/02/2022 13:09:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 13:09:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 13:09:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=732
06/02/2022 13:09:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/02/2022 13:09:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 13:09:45 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.653372668997669 on epoch=737
06/02/2022 13:09:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/02/2022 13:09:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/02/2022 13:09:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
06/02/2022 13:09:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/02/2022 13:09:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/02/2022 13:09:52 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6190015479876161 on epoch=749
06/02/2022 13:09:52 - INFO - __main__ - save last model!
06/02/2022 13:09:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:09:52 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:09:52 - INFO - __main__ - Printing 3 examples
06/02/2022 13:09:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:09:52 - INFO - __main__ - ['others']
06/02/2022 13:09:52 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:09:52 - INFO - __main__ - ['others']
06/02/2022 13:09:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:09:52 - INFO - __main__ - ['others']
06/02/2022 13:09:52 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:09:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:09:53 - INFO - __main__ - Printing 3 examples
06/02/2022 13:09:53 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:09:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:09:53 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:09:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:09:53 - INFO - __main__ - Printing 3 examples
06/02/2022 13:09:53 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:09:53 - INFO - __main__ - ['sad']
06/02/2022 13:09:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:09:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:09:53 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:09:54 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:09:59 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:09:59 - INFO - __main__ - task name: emo
06/02/2022 13:09:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:09:59 - INFO - __main__ - Starting training!
06/02/2022 13:10:00 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:10:42 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.5_8_predictions.txt
06/02/2022 13:10:42 - INFO - __main__ - Classification-F1 on test data: 0.3970
06/02/2022 13:10:42 - INFO - __main__ - prefix=emo_16_21, lr=0.5, bsz=8, dev_performance=0.7488530668677729, test_performance=0.39696243311422963
06/02/2022 13:10:42 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.4, bsz=8 ...
06/02/2022 13:10:43 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:10:43 - INFO - __main__ - Printing 3 examples
06/02/2022 13:10:43 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:10:43 - INFO - __main__ - ['sad']
06/02/2022 13:10:43 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:10:43 - INFO - __main__ - ['sad']
06/02/2022 13:10:43 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:10:43 - INFO - __main__ - ['sad']
06/02/2022 13:10:43 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:10:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:10:44 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:10:44 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:10:44 - INFO - __main__ - Printing 3 examples
06/02/2022 13:10:44 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:10:44 - INFO - __main__ - ['sad']
06/02/2022 13:10:44 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:10:44 - INFO - __main__ - ['sad']
06/02/2022 13:10:44 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:10:44 - INFO - __main__ - ['sad']
06/02/2022 13:10:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:10:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:10:44 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:10:49 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:10:49 - INFO - __main__ - task name: emo
06/02/2022 13:10:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:10:50 - INFO - __main__ - Starting training!
06/02/2022 13:10:51 - INFO - __main__ - Step 10 Global step 10 Train loss 6.31 on epoch=2
06/02/2022 13:10:53 - INFO - __main__ - Step 20 Global step 20 Train loss 3.79 on epoch=4
06/02/2022 13:10:54 - INFO - __main__ - Step 30 Global step 30 Train loss 3.02 on epoch=7
06/02/2022 13:10:55 - INFO - __main__ - Step 40 Global step 40 Train loss 2.07 on epoch=9
06/02/2022 13:10:56 - INFO - __main__ - Step 50 Global step 50 Train loss 2.56 on epoch=12
06/02/2022 13:10:57 - INFO - __main__ - Global step 50 Train loss 3.55 Classification-F1 0.15842490842490842 on epoch=12
06/02/2022 13:10:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15842490842490842 on epoch=12, global_step=50
06/02/2022 13:10:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.48 on epoch=14
06/02/2022 13:10:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.36 on epoch=17
06/02/2022 13:11:00 - INFO - __main__ - Step 80 Global step 80 Train loss 1.28 on epoch=19
06/02/2022 13:11:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.18 on epoch=22
06/02/2022 13:11:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.34 on epoch=24
06/02/2022 13:11:03 - INFO - __main__ - Global step 100 Train loss 1.33 Classification-F1 0.20237348858038512 on epoch=24
06/02/2022 13:11:03 - INFO - __main__ - Saving model with best Classification-F1: 0.15842490842490842 -> 0.20237348858038512 on epoch=24, global_step=100
06/02/2022 13:11:05 - INFO - __main__ - Step 110 Global step 110 Train loss 1.26 on epoch=27
06/02/2022 13:11:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.22 on epoch=29
06/02/2022 13:11:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.19 on epoch=32
06/02/2022 13:11:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.24 on epoch=34
06/02/2022 13:11:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.15 on epoch=37
06/02/2022 13:11:10 - INFO - __main__ - Global step 150 Train loss 1.21 Classification-F1 0.12351351351351353 on epoch=37
06/02/2022 13:11:11 - INFO - __main__ - Step 160 Global step 160 Train loss 1.09 on epoch=39
06/02/2022 13:11:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=42
06/02/2022 13:11:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
06/02/2022 13:11:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.10 on epoch=47
06/02/2022 13:11:16 - INFO - __main__ - Step 200 Global step 200 Train loss 1.08 on epoch=49
06/02/2022 13:11:17 - INFO - __main__ - Global step 200 Train loss 1.06 Classification-F1 0.12779973649538867 on epoch=49
06/02/2022 13:11:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.01 on epoch=52
06/02/2022 13:11:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.95 on epoch=54
06/02/2022 13:11:20 - INFO - __main__ - Step 230 Global step 230 Train loss 1.09 on epoch=57
06/02/2022 13:11:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.00 on epoch=59
06/02/2022 13:11:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.96 on epoch=62
06/02/2022 13:11:23 - INFO - __main__ - Global step 250 Train loss 1.00 Classification-F1 0.1 on epoch=62
06/02/2022 13:11:24 - INFO - __main__ - Step 260 Global step 260 Train loss 1.10 on epoch=64
06/02/2022 13:11:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.99 on epoch=67
06/02/2022 13:11:27 - INFO - __main__ - Step 280 Global step 280 Train loss 1.12 on epoch=69
06/02/2022 13:11:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.05 on epoch=72
06/02/2022 13:11:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.87 on epoch=74
06/02/2022 13:11:30 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.23881673881673882 on epoch=74
06/02/2022 13:11:30 - INFO - __main__ - Saving model with best Classification-F1: 0.20237348858038512 -> 0.23881673881673882 on epoch=74, global_step=300
06/02/2022 13:11:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=77
06/02/2022 13:11:32 - INFO - __main__ - Step 320 Global step 320 Train loss 1.02 on epoch=79
06/02/2022 13:11:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.99 on epoch=82
06/02/2022 13:11:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.97 on epoch=84
06/02/2022 13:11:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=87
06/02/2022 13:11:36 - INFO - __main__ - Global step 350 Train loss 0.98 Classification-F1 0.16749134092033646 on epoch=87
06/02/2022 13:11:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=89
06/02/2022 13:11:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.96 on epoch=92
06/02/2022 13:11:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.94 on epoch=94
06/02/2022 13:11:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=97
06/02/2022 13:11:42 - INFO - __main__ - Step 400 Global step 400 Train loss 1.04 on epoch=99
06/02/2022 13:11:43 - INFO - __main__ - Global step 400 Train loss 0.97 Classification-F1 0.2163760896637609 on epoch=99
06/02/2022 13:11:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=102
06/02/2022 13:11:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.97 on epoch=104
06/02/2022 13:11:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.87 on epoch=107
06/02/2022 13:11:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=109
06/02/2022 13:11:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=112
06/02/2022 13:11:50 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.1 on epoch=112
06/02/2022 13:11:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.93 on epoch=114
06/02/2022 13:11:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=117
06/02/2022 13:11:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.87 on epoch=119
06/02/2022 13:11:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.85 on epoch=122
06/02/2022 13:11:56 - INFO - __main__ - Step 500 Global step 500 Train loss 1.03 on epoch=124
06/02/2022 13:11:56 - INFO - __main__ - Global step 500 Train loss 0.95 Classification-F1 0.32504054253280257 on epoch=124
06/02/2022 13:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.23881673881673882 -> 0.32504054253280257 on epoch=124, global_step=500
06/02/2022 13:11:57 - INFO - __main__ - Step 510 Global step 510 Train loss 1.01 on epoch=127
06/02/2022 13:11:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
06/02/2022 13:12:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.94 on epoch=132
06/02/2022 13:12:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
06/02/2022 13:12:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.95 on epoch=137
06/02/2022 13:12:03 - INFO - __main__ - Global step 550 Train loss 0.95 Classification-F1 0.20606060606060606 on epoch=137
06/02/2022 13:12:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.83 on epoch=139
06/02/2022 13:12:05 - INFO - __main__ - Step 570 Global step 570 Train loss 1.05 on epoch=142
06/02/2022 13:12:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.91 on epoch=144
06/02/2022 13:12:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.94 on epoch=147
06/02/2022 13:12:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.92 on epoch=149
06/02/2022 13:12:09 - INFO - __main__ - Global step 600 Train loss 0.93 Classification-F1 0.2507869249394673 on epoch=149
06/02/2022 13:12:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
06/02/2022 13:12:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
06/02/2022 13:12:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.81 on epoch=157
06/02/2022 13:12:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.81 on epoch=159
06/02/2022 13:12:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.91 on epoch=162
06/02/2022 13:12:16 - INFO - __main__ - Global step 650 Train loss 0.84 Classification-F1 0.1581196581196581 on epoch=162
06/02/2022 13:12:17 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
06/02/2022 13:12:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.88 on epoch=167
06/02/2022 13:12:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.87 on epoch=169
06/02/2022 13:12:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.89 on epoch=172
06/02/2022 13:12:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
06/02/2022 13:12:23 - INFO - __main__ - Global step 700 Train loss 0.87 Classification-F1 0.2183447749809306 on epoch=174
06/02/2022 13:12:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.80 on epoch=177
06/02/2022 13:12:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
06/02/2022 13:12:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.89 on epoch=182
06/02/2022 13:12:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.74 on epoch=184
06/02/2022 13:12:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.86 on epoch=187
06/02/2022 13:12:29 - INFO - __main__ - Global step 750 Train loss 0.82 Classification-F1 0.25298141644717587 on epoch=187
06/02/2022 13:12:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.87 on epoch=189
06/02/2022 13:12:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.76 on epoch=192
06/02/2022 13:12:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=194
06/02/2022 13:12:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.74 on epoch=197
06/02/2022 13:12:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.68 on epoch=199
06/02/2022 13:12:36 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.4523892773892775 on epoch=199
06/02/2022 13:12:36 - INFO - __main__ - Saving model with best Classification-F1: 0.32504054253280257 -> 0.4523892773892775 on epoch=199, global_step=800
06/02/2022 13:12:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=202
06/02/2022 13:12:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=204
06/02/2022 13:12:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.75 on epoch=207
06/02/2022 13:12:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.57 on epoch=209
06/02/2022 13:12:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=212
06/02/2022 13:12:42 - INFO - __main__ - Global step 850 Train loss 0.66 Classification-F1 0.3693671630094044 on epoch=212
06/02/2022 13:12:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.65 on epoch=214
06/02/2022 13:12:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=217
06/02/2022 13:12:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.57 on epoch=219
06/02/2022 13:12:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
06/02/2022 13:12:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=224
06/02/2022 13:12:49 - INFO - __main__ - Global step 900 Train loss 0.56 Classification-F1 0.4577269577269577 on epoch=224
06/02/2022 13:12:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4523892773892775 -> 0.4577269577269577 on epoch=224, global_step=900
06/02/2022 13:12:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.59 on epoch=227
06/02/2022 13:12:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=229
06/02/2022 13:12:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.57 on epoch=232
06/02/2022 13:12:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
06/02/2022 13:12:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=237
06/02/2022 13:12:56 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.5815445665445665 on epoch=237
06/02/2022 13:12:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4577269577269577 -> 0.5815445665445665 on epoch=237, global_step=950
06/02/2022 13:12:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=239
06/02/2022 13:12:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=242
06/02/2022 13:12:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=244
06/02/2022 13:13:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=247
06/02/2022 13:13:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=249
06/02/2022 13:13:02 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.571078431372549 on epoch=249
06/02/2022 13:13:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=252
06/02/2022 13:13:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=254
06/02/2022 13:13:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=257
06/02/2022 13:13:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=259
06/02/2022 13:13:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=262
06/02/2022 13:13:09 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6380412670735252 on epoch=262
06/02/2022 13:13:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5815445665445665 -> 0.6380412670735252 on epoch=262, global_step=1050
06/02/2022 13:13:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=264
06/02/2022 13:13:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=267
06/02/2022 13:13:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=269
06/02/2022 13:13:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=272
06/02/2022 13:13:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=274
06/02/2022 13:13:15 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.6388556618819776 on epoch=274
06/02/2022 13:13:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6380412670735252 -> 0.6388556618819776 on epoch=274, global_step=1100
06/02/2022 13:13:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=277
06/02/2022 13:13:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=279
06/02/2022 13:13:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=282
06/02/2022 13:13:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=284
06/02/2022 13:13:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=287
06/02/2022 13:13:22 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.6070660522273426 on epoch=287
06/02/2022 13:13:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
06/02/2022 13:13:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=292
06/02/2022 13:13:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
06/02/2022 13:13:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=297
06/02/2022 13:13:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=299
06/02/2022 13:13:29 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.575595238095238 on epoch=299
06/02/2022 13:13:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=302
06/02/2022 13:13:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=304
06/02/2022 13:13:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=307
06/02/2022 13:13:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=309
06/02/2022 13:13:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=312
06/02/2022 13:13:35 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.6286043221527092 on epoch=312
06/02/2022 13:13:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=314
06/02/2022 13:13:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=317
06/02/2022 13:13:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=319
06/02/2022 13:13:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=322
06/02/2022 13:13:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=324
06/02/2022 13:13:42 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.624421347486363 on epoch=324
06/02/2022 13:13:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=327
06/02/2022 13:13:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
06/02/2022 13:13:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=332
06/02/2022 13:13:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=334
06/02/2022 13:13:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=337
06/02/2022 13:13:49 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.6638405948889821 on epoch=337
06/02/2022 13:13:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6388556618819776 -> 0.6638405948889821 on epoch=337, global_step=1350
06/02/2022 13:13:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
06/02/2022 13:13:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=342
06/02/2022 13:13:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=344
06/02/2022 13:13:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=347
06/02/2022 13:13:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=349
06/02/2022 13:13:55 - INFO - __main__ - Global step 1400 Train loss 0.10 Classification-F1 0.6286043221527092 on epoch=349
06/02/2022 13:13:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=352
06/02/2022 13:13:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=354
06/02/2022 13:13:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
06/02/2022 13:14:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=359
06/02/2022 13:14:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=362
06/02/2022 13:14:02 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.685530462184874 on epoch=362
06/02/2022 13:14:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6638405948889821 -> 0.685530462184874 on epoch=362, global_step=1450
06/02/2022 13:14:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=364
06/02/2022 13:14:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=367
06/02/2022 13:14:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=369
06/02/2022 13:14:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=372
06/02/2022 13:14:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=374
06/02/2022 13:14:08 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.6556878306878307 on epoch=374
06/02/2022 13:14:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
06/02/2022 13:14:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=379
06/02/2022 13:14:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=382
06/02/2022 13:14:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=384
06/02/2022 13:14:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=387
06/02/2022 13:14:15 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.7045038468668892 on epoch=387
06/02/2022 13:14:15 - INFO - __main__ - Saving model with best Classification-F1: 0.685530462184874 -> 0.7045038468668892 on epoch=387, global_step=1550
06/02/2022 13:14:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=389
06/02/2022 13:14:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=392
06/02/2022 13:14:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=394
06/02/2022 13:14:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=397
06/02/2022 13:14:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=399
06/02/2022 13:14:22 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.6388888888888888 on epoch=399
06/02/2022 13:14:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=402
06/02/2022 13:14:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=404
06/02/2022 13:14:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=407
06/02/2022 13:14:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/02/2022 13:14:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=412
06/02/2022 13:14:28 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.6836038961038962 on epoch=412
06/02/2022 13:14:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
06/02/2022 13:14:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=417
06/02/2022 13:14:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
06/02/2022 13:14:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=422
06/02/2022 13:14:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=424
06/02/2022 13:14:35 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.6391402714932127 on epoch=424
06/02/2022 13:14:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=427
06/02/2022 13:14:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=429
06/02/2022 13:14:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=432
06/02/2022 13:14:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=434
06/02/2022 13:14:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=437
06/02/2022 13:14:41 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.7131597774244833 on epoch=437
06/02/2022 13:14:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7045038468668892 -> 0.7131597774244833 on epoch=437, global_step=1750
06/02/2022 13:14:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
06/02/2022 13:14:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
06/02/2022 13:14:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=444
06/02/2022 13:14:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=447
06/02/2022 13:14:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=449
06/02/2022 13:14:48 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7322850875981806 on epoch=449
06/02/2022 13:14:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7131597774244833 -> 0.7322850875981806 on epoch=449, global_step=1800
06/02/2022 13:14:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=452
06/02/2022 13:14:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=454
06/02/2022 13:14:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=457
06/02/2022 13:14:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
06/02/2022 13:14:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
06/02/2022 13:14:55 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7176030668677729 on epoch=462
06/02/2022 13:14:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
06/02/2022 13:14:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
06/02/2022 13:14:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/02/2022 13:15:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/02/2022 13:15:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/02/2022 13:15:02 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7011467086834734 on epoch=474
06/02/2022 13:15:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=477
06/02/2022 13:15:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=479
06/02/2022 13:15:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/02/2022 13:15:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=484
06/02/2022 13:15:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
06/02/2022 13:15:08 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7458593114665222 on epoch=487
06/02/2022 13:15:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7322850875981806 -> 0.7458593114665222 on epoch=487, global_step=1950
06/02/2022 13:15:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=489
06/02/2022 13:15:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/02/2022 13:15:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/02/2022 13:15:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=497
06/02/2022 13:15:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/02/2022 13:15:15 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7157426188866352 on epoch=499
06/02/2022 13:15:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=502
06/02/2022 13:15:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
06/02/2022 13:15:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=507
06/02/2022 13:15:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/02/2022 13:15:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=512
06/02/2022 13:15:22 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.747259118701244 on epoch=512
06/02/2022 13:15:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7458593114665222 -> 0.747259118701244 on epoch=512, global_step=2050
06/02/2022 13:15:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=514
06/02/2022 13:15:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/02/2022 13:15:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=519
06/02/2022 13:15:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
06/02/2022 13:15:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/02/2022 13:15:29 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6876427494074553 on epoch=524
06/02/2022 13:15:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=527
06/02/2022 13:15:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=529
06/02/2022 13:15:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=532
06/02/2022 13:15:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=534
06/02/2022 13:15:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.00 on epoch=537
06/02/2022 13:15:36 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7160292355525621 on epoch=537
06/02/2022 13:15:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.00 on epoch=539
06/02/2022 13:15:38 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=542
06/02/2022 13:15:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
06/02/2022 13:15:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
06/02/2022 13:15:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/02/2022 13:15:43 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7021618324749255 on epoch=549
06/02/2022 13:15:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=552
06/02/2022 13:15:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/02/2022 13:15:47 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/02/2022 13:15:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/02/2022 13:15:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.00 on epoch=562
06/02/2022 13:15:50 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.6686487854251013 on epoch=562
06/02/2022 13:15:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=564
06/02/2022 13:15:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
06/02/2022 13:15:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/02/2022 13:15:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=572
06/02/2022 13:15:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.00 on epoch=574
06/02/2022 13:15:57 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7288515406162466 on epoch=574
06/02/2022 13:15:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=577
06/02/2022 13:15:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/02/2022 13:16:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/02/2022 13:16:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.00 on epoch=584
06/02/2022 13:16:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
06/02/2022 13:16:04 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7023535851122058 on epoch=587
06/02/2022 13:16:05 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=589
06/02/2022 13:16:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/02/2022 13:16:08 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
06/02/2022 13:16:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
06/02/2022 13:16:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=599
06/02/2022 13:16:11 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.7164713064713065 on epoch=599
06/02/2022 13:16:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/02/2022 13:16:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/02/2022 13:16:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/02/2022 13:16:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/02/2022 13:16:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=612
06/02/2022 13:16:18 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7264160401002507 on epoch=612
06/02/2022 13:16:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/02/2022 13:16:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/02/2022 13:16:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/02/2022 13:16:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/02/2022 13:16:24 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/02/2022 13:16:25 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.6993571821158027 on epoch=624
06/02/2022 13:16:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=627
06/02/2022 13:16:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/02/2022 13:16:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 13:16:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/02/2022 13:16:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/02/2022 13:16:32 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.6808565531475749 on epoch=637
06/02/2022 13:16:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=639
06/02/2022 13:16:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.00 on epoch=642
06/02/2022 13:16:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/02/2022 13:16:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/02/2022 13:16:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
06/02/2022 13:16:39 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.6836843336843337 on epoch=649
06/02/2022 13:16:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/02/2022 13:16:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=654
06/02/2022 13:16:42 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/02/2022 13:16:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 13:16:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/02/2022 13:16:46 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.6543136598580147 on epoch=662
06/02/2022 13:16:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/02/2022 13:16:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/02/2022 13:16:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=669
06/02/2022 13:16:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=672
06/02/2022 13:16:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=674
06/02/2022 13:16:53 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7273873963847 on epoch=674
06/02/2022 13:16:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/02/2022 13:16:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/02/2022 13:16:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/02/2022 13:16:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/02/2022 13:16:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=687
06/02/2022 13:17:00 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.6584576305628937 on epoch=687
06/02/2022 13:17:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
06/02/2022 13:17:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=692
06/02/2022 13:17:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/02/2022 13:17:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=697
06/02/2022 13:17:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=699
06/02/2022 13:17:07 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.6726093956644241 on epoch=699
06/02/2022 13:17:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=702
06/02/2022 13:17:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
06/02/2022 13:17:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/02/2022 13:17:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
06/02/2022 13:17:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=712
06/02/2022 13:17:13 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.670185291858679 on epoch=712
06/02/2022 13:17:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/02/2022 13:17:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/02/2022 13:17:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/02/2022 13:17:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/02/2022 13:17:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 13:17:20 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.6536931818181818 on epoch=724
06/02/2022 13:17:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/02/2022 13:17:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=729
06/02/2022 13:17:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 13:17:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=734
06/02/2022 13:17:27 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 13:17:27 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.6513888888888889 on epoch=737
06/02/2022 13:17:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=739
06/02/2022 13:17:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/02/2022 13:17:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/02/2022 13:17:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=747
06/02/2022 13:17:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/02/2022 13:17:34 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.6336024024451483 on epoch=749
06/02/2022 13:17:34 - INFO - __main__ - save last model!
06/02/2022 13:17:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:17:34 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:17:34 - INFO - __main__ - Printing 3 examples
06/02/2022 13:17:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:17:34 - INFO - __main__ - ['others']
06/02/2022 13:17:34 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:17:34 - INFO - __main__ - ['others']
06/02/2022 13:17:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:17:34 - INFO - __main__ - ['others']
06/02/2022 13:17:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:17:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:17:35 - INFO - __main__ - Printing 3 examples
06/02/2022 13:17:35 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:17:35 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:17:35 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:17:35 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:17:35 - INFO - __main__ - Printing 3 examples
06/02/2022 13:17:35 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:17:35 - INFO - __main__ - ['sad']
06/02/2022 13:17:35 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:17:35 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:17:35 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:17:37 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:17:41 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:17:41 - INFO - __main__ - task name: emo
06/02/2022 13:17:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:17:41 - INFO - __main__ - Starting training!
06/02/2022 13:17:42 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:18:25 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.4_8_predictions.txt
06/02/2022 13:18:25 - INFO - __main__ - Classification-F1 on test data: 0.3691
06/02/2022 13:18:26 - INFO - __main__ - prefix=emo_16_21, lr=0.4, bsz=8, dev_performance=0.747259118701244, test_performance=0.3691317128312531
06/02/2022 13:18:26 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.3, bsz=8 ...
06/02/2022 13:18:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:18:27 - INFO - __main__ - Printing 3 examples
06/02/2022 13:18:27 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:18:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:18:27 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:18:27 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:18:27 - INFO - __main__ - Printing 3 examples
06/02/2022 13:18:27 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:18:27 - INFO - __main__ - ['sad']
06/02/2022 13:18:27 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:18:27 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:18:27 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:18:32 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:18:32 - INFO - __main__ - task name: emo
06/02/2022 13:18:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:18:33 - INFO - __main__ - Starting training!
06/02/2022 13:18:34 - INFO - __main__ - Step 10 Global step 10 Train loss 6.49 on epoch=2
06/02/2022 13:18:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.62 on epoch=4
06/02/2022 13:18:36 - INFO - __main__ - Step 30 Global step 30 Train loss 2.70 on epoch=7
06/02/2022 13:18:38 - INFO - __main__ - Step 40 Global step 40 Train loss 1.80 on epoch=9
06/02/2022 13:18:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.55 on epoch=12
06/02/2022 13:18:39 - INFO - __main__ - Global step 50 Train loss 3.23 Classification-F1 0.10897435897435896 on epoch=12
06/02/2022 13:18:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10897435897435896 on epoch=12, global_step=50
06/02/2022 13:18:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.64 on epoch=14
06/02/2022 13:18:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.47 on epoch=17
06/02/2022 13:18:43 - INFO - __main__ - Step 80 Global step 80 Train loss 1.17 on epoch=19
06/02/2022 13:18:44 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=22
06/02/2022 13:18:46 - INFO - __main__ - Step 100 Global step 100 Train loss 1.18 on epoch=24
06/02/2022 13:18:46 - INFO - __main__ - Global step 100 Train loss 1.34 Classification-F1 0.22807017543859648 on epoch=24
06/02/2022 13:18:46 - INFO - __main__ - Saving model with best Classification-F1: 0.10897435897435896 -> 0.22807017543859648 on epoch=24, global_step=100
06/02/2022 13:18:47 - INFO - __main__ - Step 110 Global step 110 Train loss 1.22 on epoch=27
06/02/2022 13:18:49 - INFO - __main__ - Step 120 Global step 120 Train loss 1.12 on epoch=29
06/02/2022 13:18:50 - INFO - __main__ - Step 130 Global step 130 Train loss 1.15 on epoch=32
06/02/2022 13:18:51 - INFO - __main__ - Step 140 Global step 140 Train loss 1.11 on epoch=34
06/02/2022 13:18:52 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=37
06/02/2022 13:18:53 - INFO - __main__ - Global step 150 Train loss 1.13 Classification-F1 0.11805555555555555 on epoch=37
06/02/2022 13:18:54 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=39
06/02/2022 13:18:55 - INFO - __main__ - Step 170 Global step 170 Train loss 1.21 on epoch=42
06/02/2022 13:18:57 - INFO - __main__ - Step 180 Global step 180 Train loss 1.12 on epoch=44
06/02/2022 13:18:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.07 on epoch=47
06/02/2022 13:18:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.95 on epoch=49
06/02/2022 13:19:00 - INFO - __main__ - Global step 200 Train loss 1.07 Classification-F1 0.130952380952381 on epoch=49
06/02/2022 13:19:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=52
06/02/2022 13:19:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=54
06/02/2022 13:19:03 - INFO - __main__ - Step 230 Global step 230 Train loss 1.09 on epoch=57
06/02/2022 13:19:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=59
06/02/2022 13:19:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
06/02/2022 13:19:06 - INFO - __main__ - Global step 250 Train loss 0.99 Classification-F1 0.13034188034188032 on epoch=62
06/02/2022 13:19:07 - INFO - __main__ - Step 260 Global step 260 Train loss 1.22 on epoch=64
06/02/2022 13:19:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
06/02/2022 13:19:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.99 on epoch=69
06/02/2022 13:19:11 - INFO - __main__ - Step 290 Global step 290 Train loss 1.04 on epoch=72
06/02/2022 13:19:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.99 on epoch=74
06/02/2022 13:19:13 - INFO - __main__ - Global step 300 Train loss 1.03 Classification-F1 0.24221003134796243 on epoch=74
06/02/2022 13:19:13 - INFO - __main__ - Saving model with best Classification-F1: 0.22807017543859648 -> 0.24221003134796243 on epoch=74, global_step=300
06/02/2022 13:19:14 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
06/02/2022 13:19:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=79
06/02/2022 13:19:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.98 on epoch=82
06/02/2022 13:19:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=84
06/02/2022 13:19:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=87
06/02/2022 13:19:20 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.11732186732186733 on epoch=87
06/02/2022 13:19:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.99 on epoch=89
06/02/2022 13:19:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=92
06/02/2022 13:19:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.82 on epoch=94
06/02/2022 13:19:24 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
06/02/2022 13:19:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.92 on epoch=99
06/02/2022 13:19:26 - INFO - __main__ - Global step 400 Train loss 0.94 Classification-F1 0.29630376344086023 on epoch=99
06/02/2022 13:19:26 - INFO - __main__ - Saving model with best Classification-F1: 0.24221003134796243 -> 0.29630376344086023 on epoch=99, global_step=400
06/02/2022 13:19:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.86 on epoch=102
06/02/2022 13:19:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
06/02/2022 13:19:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
06/02/2022 13:19:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.92 on epoch=109
06/02/2022 13:19:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=112
06/02/2022 13:19:33 - INFO - __main__ - Global step 450 Train loss 0.87 Classification-F1 0.23214285714285715 on epoch=112
06/02/2022 13:19:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.90 on epoch=114
06/02/2022 13:19:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.84 on epoch=117
06/02/2022 13:19:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=119
06/02/2022 13:19:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
06/02/2022 13:19:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=124
06/02/2022 13:19:40 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.2705821415211074 on epoch=124
06/02/2022 13:19:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=127
06/02/2022 13:19:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=129
06/02/2022 13:19:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.73 on epoch=132
06/02/2022 13:19:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.85 on epoch=134
06/02/2022 13:19:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=137
06/02/2022 13:19:46 - INFO - __main__ - Global step 550 Train loss 0.80 Classification-F1 0.4477747909199522 on epoch=137
06/02/2022 13:19:46 - INFO - __main__ - Saving model with best Classification-F1: 0.29630376344086023 -> 0.4477747909199522 on epoch=137, global_step=550
06/02/2022 13:19:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.76 on epoch=139
06/02/2022 13:19:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.77 on epoch=142
06/02/2022 13:19:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=144
06/02/2022 13:19:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.76 on epoch=147
06/02/2022 13:19:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=149
06/02/2022 13:19:53 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.3820409982174688 on epoch=149
06/02/2022 13:19:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=152
06/02/2022 13:19:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.69 on epoch=154
06/02/2022 13:19:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.77 on epoch=157
06/02/2022 13:19:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=159
06/02/2022 13:19:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
06/02/2022 13:20:00 - INFO - __main__ - Global step 650 Train loss 0.71 Classification-F1 0.37881231671554255 on epoch=162
06/02/2022 13:20:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=164
06/02/2022 13:20:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
06/02/2022 13:20:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=169
06/02/2022 13:20:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
06/02/2022 13:20:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.59 on epoch=174
06/02/2022 13:20:06 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.4722222222222222 on epoch=174
06/02/2022 13:20:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4477747909199522 -> 0.4722222222222222 on epoch=174, global_step=700
06/02/2022 13:20:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.65 on epoch=177
06/02/2022 13:20:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.60 on epoch=179
06/02/2022 13:20:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=182
06/02/2022 13:20:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=184
06/02/2022 13:20:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=187
06/02/2022 13:20:13 - INFO - __main__ - Global step 750 Train loss 0.59 Classification-F1 0.47088392740566654 on epoch=187
06/02/2022 13:20:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=189
06/02/2022 13:20:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=192
06/02/2022 13:20:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.48 on epoch=194
06/02/2022 13:20:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.60 on epoch=197
06/02/2022 13:20:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=199
06/02/2022 13:20:20 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.5361111111111111 on epoch=199
06/02/2022 13:20:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4722222222222222 -> 0.5361111111111111 on epoch=199, global_step=800
06/02/2022 13:20:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=202
06/02/2022 13:20:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=204
06/02/2022 13:20:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=207
06/02/2022 13:20:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=209
06/02/2022 13:20:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=212
06/02/2022 13:20:26 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.5037748982651531 on epoch=212
06/02/2022 13:20:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=214
06/02/2022 13:20:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=217
06/02/2022 13:20:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=219
06/02/2022 13:20:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=222
06/02/2022 13:20:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=224
06/02/2022 13:20:33 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.5589893852277754 on epoch=224
06/02/2022 13:20:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5361111111111111 -> 0.5589893852277754 on epoch=224, global_step=900
06/02/2022 13:20:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=227
06/02/2022 13:20:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=229
06/02/2022 13:20:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=232
06/02/2022 13:20:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=234
06/02/2022 13:20:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=237
06/02/2022 13:20:40 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.49413484692122467 on epoch=237
06/02/2022 13:20:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=239
06/02/2022 13:20:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=242
06/02/2022 13:20:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=244
06/02/2022 13:20:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=247
06/02/2022 13:20:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=249
06/02/2022 13:20:47 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.6201862495699113 on epoch=249
06/02/2022 13:20:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5589893852277754 -> 0.6201862495699113 on epoch=249, global_step=1000
06/02/2022 13:20:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=252
06/02/2022 13:20:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=254
06/02/2022 13:20:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=257
06/02/2022 13:20:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=259
06/02/2022 13:20:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=262
06/02/2022 13:20:53 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.6018687070444785 on epoch=262
06/02/2022 13:20:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=264
06/02/2022 13:20:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=267
06/02/2022 13:20:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=269
06/02/2022 13:20:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=272
06/02/2022 13:21:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=274
06/02/2022 13:21:00 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.6257352941176471 on epoch=274
06/02/2022 13:21:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6201862495699113 -> 0.6257352941176471 on epoch=274, global_step=1100
06/02/2022 13:21:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=277
06/02/2022 13:21:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=279
06/02/2022 13:21:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=282
06/02/2022 13:21:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=284
06/02/2022 13:21:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=287
06/02/2022 13:21:07 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.5287258521395607 on epoch=287
06/02/2022 13:21:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
06/02/2022 13:21:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=292
06/02/2022 13:21:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=294
06/02/2022 13:21:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=297
06/02/2022 13:21:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=299
06/02/2022 13:21:14 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.6077922077922079 on epoch=299
06/02/2022 13:21:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=302
06/02/2022 13:21:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=304
06/02/2022 13:21:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=307
06/02/2022 13:21:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=309
06/02/2022 13:21:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=312
06/02/2022 13:21:21 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.6606636500754148 on epoch=312
06/02/2022 13:21:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6257352941176471 -> 0.6606636500754148 on epoch=312, global_step=1250
06/02/2022 13:21:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=314
06/02/2022 13:21:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=317
06/02/2022 13:21:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=319
06/02/2022 13:21:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=322
06/02/2022 13:21:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=324
06/02/2022 13:21:27 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.6024393754656913 on epoch=324
06/02/2022 13:21:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=327
06/02/2022 13:21:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=329
06/02/2022 13:21:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
06/02/2022 13:21:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=334
06/02/2022 13:21:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=337
06/02/2022 13:21:34 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.621600765079026 on epoch=337
06/02/2022 13:21:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=339
06/02/2022 13:21:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=342
06/02/2022 13:21:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
06/02/2022 13:21:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=347
06/02/2022 13:21:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=349
06/02/2022 13:21:41 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.6760308028134854 on epoch=349
06/02/2022 13:21:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6606636500754148 -> 0.6760308028134854 on epoch=349, global_step=1400
06/02/2022 13:21:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
06/02/2022 13:21:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=354
06/02/2022 13:21:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=357
06/02/2022 13:21:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=359
06/02/2022 13:21:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=362
06/02/2022 13:21:47 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.6808442982456141 on epoch=362
06/02/2022 13:21:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6760308028134854 -> 0.6808442982456141 on epoch=362, global_step=1450
06/02/2022 13:21:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
06/02/2022 13:21:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=367
06/02/2022 13:21:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
06/02/2022 13:21:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=372
06/02/2022 13:21:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=374
06/02/2022 13:21:54 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.6626195945271148 on epoch=374
06/02/2022 13:21:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=377
06/02/2022 13:21:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=379
06/02/2022 13:21:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
06/02/2022 13:21:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=384
06/02/2022 13:22:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
06/02/2022 13:22:01 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.6192754613807245 on epoch=387
06/02/2022 13:22:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=389
06/02/2022 13:22:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=392
06/02/2022 13:22:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=394
06/02/2022 13:22:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
06/02/2022 13:22:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.11 on epoch=399
06/02/2022 13:22:08 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.6332070707070707 on epoch=399
06/02/2022 13:22:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=402
06/02/2022 13:22:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=404
06/02/2022 13:22:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=407
06/02/2022 13:22:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=409
06/02/2022 13:22:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
06/02/2022 13:22:14 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7239583333333334 on epoch=412
06/02/2022 13:22:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6808442982456141 -> 0.7239583333333334 on epoch=412, global_step=1650
06/02/2022 13:22:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=414
06/02/2022 13:22:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=417
06/02/2022 13:22:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=419
06/02/2022 13:22:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=422
06/02/2022 13:22:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
06/02/2022 13:22:21 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7318014705882353 on epoch=424
06/02/2022 13:22:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7239583333333334 -> 0.7318014705882353 on epoch=424, global_step=1700
06/02/2022 13:22:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=427
06/02/2022 13:22:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=429
06/02/2022 13:22:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=432
06/02/2022 13:22:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=434
06/02/2022 13:22:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
06/02/2022 13:22:28 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.694246385422856 on epoch=437
06/02/2022 13:22:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
06/02/2022 13:22:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=442
06/02/2022 13:22:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=444
06/02/2022 13:22:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=447
06/02/2022 13:22:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
06/02/2022 13:22:35 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.6594957983193277 on epoch=449
06/02/2022 13:22:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=452
06/02/2022 13:22:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=454
06/02/2022 13:22:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=457
06/02/2022 13:22:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=459
06/02/2022 13:22:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=462
06/02/2022 13:22:41 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7286676286676287 on epoch=462
06/02/2022 13:22:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
06/02/2022 13:22:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=467
06/02/2022 13:22:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
06/02/2022 13:22:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=472
06/02/2022 13:22:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
06/02/2022 13:22:48 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7088960059548295 on epoch=474
06/02/2022 13:22:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
06/02/2022 13:22:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/02/2022 13:22:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=482
06/02/2022 13:22:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=484
06/02/2022 13:22:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/02/2022 13:22:55 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6976699770817418 on epoch=487
06/02/2022 13:22:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=489
06/02/2022 13:22:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=492
06/02/2022 13:22:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/02/2022 13:23:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
06/02/2022 13:23:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=499
06/02/2022 13:23:01 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.6902871621621621 on epoch=499
06/02/2022 13:23:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=502
06/02/2022 13:23:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=504
06/02/2022 13:23:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/02/2022 13:23:06 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
06/02/2022 13:23:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
06/02/2022 13:23:08 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6755443548387097 on epoch=512
06/02/2022 13:23:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=514
06/02/2022 13:23:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=517
06/02/2022 13:23:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/02/2022 13:23:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=522
06/02/2022 13:23:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/02/2022 13:23:15 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.6760014478764479 on epoch=524
06/02/2022 13:23:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=527
06/02/2022 13:23:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.02 on epoch=529
06/02/2022 13:23:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=532
06/02/2022 13:23:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=534
06/02/2022 13:23:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
06/02/2022 13:23:21 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.6771962074303406 on epoch=537
06/02/2022 13:23:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
06/02/2022 13:23:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
06/02/2022 13:23:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/02/2022 13:23:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=547
06/02/2022 13:23:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=549
06/02/2022 13:23:28 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.6948832417582417 on epoch=549
06/02/2022 13:23:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=552
06/02/2022 13:23:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/02/2022 13:23:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=557
06/02/2022 13:23:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/02/2022 13:23:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=562
06/02/2022 13:23:35 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.6560096153846153 on epoch=562
06/02/2022 13:23:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
06/02/2022 13:23:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/02/2022 13:23:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=569
06/02/2022 13:23:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=572
06/02/2022 13:23:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/02/2022 13:23:41 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.689236111111111 on epoch=574
06/02/2022 13:23:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
06/02/2022 13:23:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/02/2022 13:23:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/02/2022 13:23:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/02/2022 13:23:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 13:23:48 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7292541389315582 on epoch=587
06/02/2022 13:23:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=589
06/02/2022 13:23:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
06/02/2022 13:23:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=594
06/02/2022 13:23:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=597
06/02/2022 13:23:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/02/2022 13:23:55 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6559403153153154 on epoch=599
06/02/2022 13:23:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=602
06/02/2022 13:23:57 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=604
06/02/2022 13:23:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/02/2022 13:24:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/02/2022 13:24:01 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=612
06/02/2022 13:24:01 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.6647427788319629 on epoch=612
06/02/2022 13:24:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/02/2022 13:24:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/02/2022 13:24:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/02/2022 13:24:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/02/2022 13:24:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/02/2022 13:24:08 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.6760657299356989 on epoch=624
06/02/2022 13:24:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/02/2022 13:24:11 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/02/2022 13:24:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=632
06/02/2022 13:24:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=634
06/02/2022 13:24:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
06/02/2022 13:24:15 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6567669172932331 on epoch=637
06/02/2022 13:24:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=639
06/02/2022 13:24:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 13:24:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/02/2022 13:24:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=647
06/02/2022 13:24:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=649
06/02/2022 13:24:22 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.6526892821010468 on epoch=649
06/02/2022 13:24:23 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=652
06/02/2022 13:24:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
06/02/2022 13:24:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/02/2022 13:24:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=659
06/02/2022 13:24:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=662
06/02/2022 13:24:28 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.69998460591133 on epoch=662
06/02/2022 13:24:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/02/2022 13:24:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/02/2022 13:24:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=669
06/02/2022 13:24:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
06/02/2022 13:24:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=674
06/02/2022 13:24:35 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.661313954862342 on epoch=674
06/02/2022 13:24:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/02/2022 13:24:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
06/02/2022 13:24:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=682
06/02/2022 13:24:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
06/02/2022 13:24:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
06/02/2022 13:24:42 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.6563628740970072 on epoch=687
06/02/2022 13:24:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/02/2022 13:24:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/02/2022 13:24:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 13:24:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=697
06/02/2022 13:24:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
06/02/2022 13:24:48 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.6636029411764706 on epoch=699
06/02/2022 13:24:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
06/02/2022 13:24:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/02/2022 13:24:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/02/2022 13:24:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
06/02/2022 13:24:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/02/2022 13:24:55 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6692502645112078 on epoch=712
06/02/2022 13:24:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/02/2022 13:24:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/02/2022 13:24:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=719
06/02/2022 13:25:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=722
06/02/2022 13:25:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=724
06/02/2022 13:25:02 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6528875713658322 on epoch=724
06/02/2022 13:25:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/02/2022 13:25:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 13:25:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/02/2022 13:25:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
06/02/2022 13:25:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 13:25:08 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.6601033375226923 on epoch=737
06/02/2022 13:25:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
06/02/2022 13:25:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/02/2022 13:25:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
06/02/2022 13:25:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=747
06/02/2022 13:25:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/02/2022 13:25:15 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6161054102230572 on epoch=749
06/02/2022 13:25:15 - INFO - __main__ - save last model!
06/02/2022 13:25:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:25:15 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:25:15 - INFO - __main__ - Printing 3 examples
06/02/2022 13:25:15 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:25:15 - INFO - __main__ - ['others']
06/02/2022 13:25:15 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:25:15 - INFO - __main__ - ['others']
06/02/2022 13:25:15 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:25:15 - INFO - __main__ - ['others']
06/02/2022 13:25:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:25:15 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:25:15 - INFO - __main__ - Printing 3 examples
06/02/2022 13:25:15 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:25:15 - INFO - __main__ - ['sad']
06/02/2022 13:25:15 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:25:15 - INFO - __main__ - ['sad']
06/02/2022 13:25:15 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:25:15 - INFO - __main__ - ['sad']
06/02/2022 13:25:15 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:25:15 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:25:16 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:25:16 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:25:16 - INFO - __main__ - Printing 3 examples
06/02/2022 13:25:16 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:25:16 - INFO - __main__ - ['sad']
06/02/2022 13:25:16 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:25:16 - INFO - __main__ - ['sad']
06/02/2022 13:25:16 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:25:16 - INFO - __main__ - ['sad']
06/02/2022 13:25:16 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:25:16 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:25:16 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:25:17 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:25:22 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:25:22 - INFO - __main__ - task name: emo
06/02/2022 13:25:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:25:22 - INFO - __main__ - Starting training!
06/02/2022 13:25:22 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:26:06 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.3_8_predictions.txt
06/02/2022 13:26:06 - INFO - __main__ - Classification-F1 on test data: 0.2811
06/02/2022 13:26:06 - INFO - __main__ - prefix=emo_16_21, lr=0.3, bsz=8, dev_performance=0.7318014705882353, test_performance=0.28111361443384264
06/02/2022 13:26:06 - INFO - __main__ - Running ... prefix=emo_16_21, lr=0.2, bsz=8 ...
06/02/2022 13:26:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:26:07 - INFO - __main__ - Printing 3 examples
06/02/2022 13:26:07 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:26:07 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:26:07 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:26:07 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:26:07 - INFO - __main__ - Printing 3 examples
06/02/2022 13:26:07 - INFO - __main__ -  [emo] i am good i'm doing great what are u doing feeling lonely
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ -  [emo] what about nonveg non veg food is also not allowed in canteens egg is though so sad
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ -  [emo] you wiollbe hre on monday sadly yes i work everyday but thursday sadly  whaynyou say
06/02/2022 13:26:07 - INFO - __main__ - ['sad']
06/02/2022 13:26:07 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:26:07 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:26:07 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:26:13 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:26:13 - INFO - __main__ - task name: emo
06/02/2022 13:26:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:26:13 - INFO - __main__ - Starting training!
06/02/2022 13:26:14 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=2
06/02/2022 13:26:15 - INFO - __main__ - Step 20 Global step 20 Train loss 5.46 on epoch=4
06/02/2022 13:26:17 - INFO - __main__ - Step 30 Global step 30 Train loss 4.14 on epoch=7
06/02/2022 13:26:18 - INFO - __main__ - Step 40 Global step 40 Train loss 3.02 on epoch=9
06/02/2022 13:26:19 - INFO - __main__ - Step 50 Global step 50 Train loss 2.57 on epoch=12
06/02/2022 13:26:20 - INFO - __main__ - Global step 50 Train loss 4.50 Classification-F1 0.1 on epoch=12
06/02/2022 13:26:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
06/02/2022 13:26:21 - INFO - __main__ - Step 60 Global step 60 Train loss 2.03 on epoch=14
06/02/2022 13:26:22 - INFO - __main__ - Step 70 Global step 70 Train loss 1.83 on epoch=17
06/02/2022 13:26:23 - INFO - __main__ - Step 80 Global step 80 Train loss 1.55 on epoch=19
06/02/2022 13:26:25 - INFO - __main__ - Step 90 Global step 90 Train loss 1.78 on epoch=22
06/02/2022 13:26:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.36 on epoch=24
06/02/2022 13:26:26 - INFO - __main__ - Global step 100 Train loss 1.71 Classification-F1 0.14560439560439564 on epoch=24
06/02/2022 13:26:26 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14560439560439564 on epoch=24, global_step=100
06/02/2022 13:26:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.39 on epoch=27
06/02/2022 13:26:29 - INFO - __main__ - Step 120 Global step 120 Train loss 1.43 on epoch=29
06/02/2022 13:26:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.39 on epoch=32
06/02/2022 13:26:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.08 on epoch=34
06/02/2022 13:26:32 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=37
06/02/2022 13:26:33 - INFO - __main__ - Global step 150 Train loss 1.27 Classification-F1 0.038461538461538464 on epoch=37
06/02/2022 13:26:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.19 on epoch=39
06/02/2022 13:26:35 - INFO - __main__ - Step 170 Global step 170 Train loss 1.24 on epoch=42
06/02/2022 13:26:37 - INFO - __main__ - Step 180 Global step 180 Train loss 1.16 on epoch=44
06/02/2022 13:26:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=47
06/02/2022 13:26:39 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
06/02/2022 13:26:40 - INFO - __main__ - Global step 200 Train loss 1.16 Classification-F1 0.12945998071359693 on epoch=49
06/02/2022 13:26:41 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
06/02/2022 13:26:42 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
06/02/2022 13:26:43 - INFO - __main__ - Step 230 Global step 230 Train loss 1.10 on epoch=57
06/02/2022 13:26:45 - INFO - __main__ - Step 240 Global step 240 Train loss 1.21 on epoch=59
06/02/2022 13:26:46 - INFO - __main__ - Step 250 Global step 250 Train loss 1.04 on epoch=62
06/02/2022 13:26:46 - INFO - __main__ - Global step 250 Train loss 1.11 Classification-F1 0.22731356693620847 on epoch=62
06/02/2022 13:26:46 - INFO - __main__ - Saving model with best Classification-F1: 0.14560439560439564 -> 0.22731356693620847 on epoch=62, global_step=250
06/02/2022 13:26:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
06/02/2022 13:26:49 - INFO - __main__ - Step 270 Global step 270 Train loss 1.05 on epoch=67
06/02/2022 13:26:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=69
06/02/2022 13:26:51 - INFO - __main__ - Step 290 Global step 290 Train loss 1.06 on epoch=72
06/02/2022 13:26:53 - INFO - __main__ - Step 300 Global step 300 Train loss 1.01 on epoch=74
06/02/2022 13:26:53 - INFO - __main__ - Global step 300 Train loss 1.02 Classification-F1 0.23541666666666666 on epoch=74
06/02/2022 13:26:53 - INFO - __main__ - Saving model with best Classification-F1: 0.22731356693620847 -> 0.23541666666666666 on epoch=74, global_step=300
06/02/2022 13:26:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.01 on epoch=77
06/02/2022 13:26:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
06/02/2022 13:26:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.92 on epoch=82
06/02/2022 13:26:58 - INFO - __main__ - Step 340 Global step 340 Train loss 1.07 on epoch=84
06/02/2022 13:26:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.04 on epoch=87
06/02/2022 13:27:00 - INFO - __main__ - Global step 350 Train loss 1.01 Classification-F1 0.2869298245614035 on epoch=87
06/02/2022 13:27:00 - INFO - __main__ - Saving model with best Classification-F1: 0.23541666666666666 -> 0.2869298245614035 on epoch=87, global_step=350
06/02/2022 13:27:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.93 on epoch=89
06/02/2022 13:27:02 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=92
06/02/2022 13:27:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.86 on epoch=94
06/02/2022 13:27:05 - INFO - __main__ - Step 390 Global step 390 Train loss 1.02 on epoch=97
06/02/2022 13:27:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.97 on epoch=99
06/02/2022 13:27:07 - INFO - __main__ - Global step 400 Train loss 0.95 Classification-F1 0.23494219419516535 on epoch=99
06/02/2022 13:27:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.97 on epoch=102
06/02/2022 13:27:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=104
06/02/2022 13:27:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=107
06/02/2022 13:27:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.98 on epoch=109
06/02/2022 13:27:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.81 on epoch=112
06/02/2022 13:27:13 - INFO - __main__ - Global step 450 Train loss 0.90 Classification-F1 0.21666666666666667 on epoch=112
06/02/2022 13:27:14 - INFO - __main__ - Step 460 Global step 460 Train loss 1.00 on epoch=114
06/02/2022 13:27:15 - INFO - __main__ - Step 470 Global step 470 Train loss 1.05 on epoch=117
06/02/2022 13:27:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.96 on epoch=119
06/02/2022 13:27:18 - INFO - __main__ - Step 490 Global step 490 Train loss 1.02 on epoch=122
06/02/2022 13:27:19 - INFO - __main__ - Step 500 Global step 500 Train loss 1.04 on epoch=124
06/02/2022 13:27:20 - INFO - __main__ - Global step 500 Train loss 1.01 Classification-F1 0.15675057208237989 on epoch=124
06/02/2022 13:27:21 - INFO - __main__ - Step 510 Global step 510 Train loss 1.01 on epoch=127
06/02/2022 13:27:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.99 on epoch=129
06/02/2022 13:27:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=132
06/02/2022 13:27:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.98 on epoch=134
06/02/2022 13:27:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.93 on epoch=137
06/02/2022 13:27:26 - INFO - __main__ - Global step 550 Train loss 0.94 Classification-F1 0.26363636363636367 on epoch=137
06/02/2022 13:27:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.96 on epoch=139
06/02/2022 13:27:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=142
06/02/2022 13:27:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.82 on epoch=144
06/02/2022 13:27:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.89 on epoch=147
06/02/2022 13:27:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.93 on epoch=149
06/02/2022 13:27:33 - INFO - __main__ - Global step 600 Train loss 0.89 Classification-F1 0.24678304787000435 on epoch=149
06/02/2022 13:27:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.89 on epoch=152
06/02/2022 13:27:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.94 on epoch=154
06/02/2022 13:27:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.97 on epoch=157
06/02/2022 13:27:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.89 on epoch=159
06/02/2022 13:27:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.89 on epoch=162
06/02/2022 13:27:39 - INFO - __main__ - Global step 650 Train loss 0.92 Classification-F1 0.3792610837438424 on epoch=162
06/02/2022 13:27:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2869298245614035 -> 0.3792610837438424 on epoch=162, global_step=650
06/02/2022 13:27:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.83 on epoch=164
06/02/2022 13:27:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.90 on epoch=167
06/02/2022 13:27:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
06/02/2022 13:27:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.87 on epoch=172
06/02/2022 13:27:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.82 on epoch=174
06/02/2022 13:27:46 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3309502122254237 on epoch=174
06/02/2022 13:27:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.76 on epoch=177
06/02/2022 13:27:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=179
06/02/2022 13:27:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.89 on epoch=182
06/02/2022 13:27:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.84 on epoch=184
06/02/2022 13:27:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.83 on epoch=187
06/02/2022 13:27:53 - INFO - __main__ - Global step 750 Train loss 0.85 Classification-F1 0.3787447603154737 on epoch=187
06/02/2022 13:27:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.76 on epoch=189
06/02/2022 13:27:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.79 on epoch=192
06/02/2022 13:27:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.82 on epoch=194
06/02/2022 13:27:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.80 on epoch=197
06/02/2022 13:27:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.67 on epoch=199
06/02/2022 13:27:59 - INFO - __main__ - Global step 800 Train loss 0.77 Classification-F1 0.27893374741200827 on epoch=199
06/02/2022 13:28:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.82 on epoch=202
06/02/2022 13:28:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.92 on epoch=204
06/02/2022 13:28:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.71 on epoch=207
06/02/2022 13:28:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.86 on epoch=209
06/02/2022 13:28:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=212
06/02/2022 13:28:06 - INFO - __main__ - Global step 850 Train loss 0.82 Classification-F1 0.36192185007974476 on epoch=212
06/02/2022 13:28:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.70 on epoch=214
06/02/2022 13:28:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=217
06/02/2022 13:28:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
06/02/2022 13:28:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
06/02/2022 13:28:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=224
06/02/2022 13:28:13 - INFO - __main__ - Global step 900 Train loss 0.72 Classification-F1 0.46370851370851374 on epoch=224
06/02/2022 13:28:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3792610837438424 -> 0.46370851370851374 on epoch=224, global_step=900
06/02/2022 13:28:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.66 on epoch=227
06/02/2022 13:28:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
06/02/2022 13:28:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=232
06/02/2022 13:28:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=234
06/02/2022 13:28:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.74 on epoch=237
06/02/2022 13:28:19 - INFO - __main__ - Global step 950 Train loss 0.71 Classification-F1 0.4506237006237006 on epoch=237
06/02/2022 13:28:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.70 on epoch=239
06/02/2022 13:28:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.74 on epoch=242
06/02/2022 13:28:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.68 on epoch=244
06/02/2022 13:28:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=247
06/02/2022 13:28:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.57 on epoch=249
06/02/2022 13:28:26 - INFO - __main__ - Global step 1000 Train loss 0.69 Classification-F1 0.5276679841897234 on epoch=249
06/02/2022 13:28:26 - INFO - __main__ - Saving model with best Classification-F1: 0.46370851370851374 -> 0.5276679841897234 on epoch=249, global_step=1000
06/02/2022 13:28:27 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=252
06/02/2022 13:28:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.57 on epoch=254
06/02/2022 13:28:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=257
06/02/2022 13:28:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=259
06/02/2022 13:28:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.67 on epoch=262
06/02/2022 13:28:32 - INFO - __main__ - Global step 1050 Train loss 0.59 Classification-F1 0.5618390405361229 on epoch=262
06/02/2022 13:28:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5276679841897234 -> 0.5618390405361229 on epoch=262, global_step=1050
06/02/2022 13:28:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.56 on epoch=264
06/02/2022 13:28:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=267
06/02/2022 13:28:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=269
06/02/2022 13:28:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=272
06/02/2022 13:28:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.57 on epoch=274
06/02/2022 13:28:39 - INFO - __main__ - Global step 1100 Train loss 0.54 Classification-F1 0.6030923315244395 on epoch=274
06/02/2022 13:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5618390405361229 -> 0.6030923315244395 on epoch=274, global_step=1100
06/02/2022 13:28:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=277
06/02/2022 13:28:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=279
06/02/2022 13:28:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.67 on epoch=282
06/02/2022 13:28:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=284
06/02/2022 13:28:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.62 on epoch=287
06/02/2022 13:28:46 - INFO - __main__ - Global step 1150 Train loss 0.54 Classification-F1 0.5199916937249868 on epoch=287
06/02/2022 13:28:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=289
06/02/2022 13:28:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.73 on epoch=292
06/02/2022 13:28:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=294
06/02/2022 13:28:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=297
06/02/2022 13:28:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=299
06/02/2022 13:28:52 - INFO - __main__ - Global step 1200 Train loss 0.57 Classification-F1 0.5130638052305402 on epoch=299
06/02/2022 13:28:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=302
06/02/2022 13:28:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=304
06/02/2022 13:28:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.61 on epoch=307
06/02/2022 13:28:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=309
06/02/2022 13:28:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.63 on epoch=312
06/02/2022 13:28:59 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.5557865251363704 on epoch=312
06/02/2022 13:29:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=314
06/02/2022 13:29:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=317
06/02/2022 13:29:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.53 on epoch=319
06/02/2022 13:29:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=322
06/02/2022 13:29:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=324
06/02/2022 13:29:05 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.5515512265512266 on epoch=324
06/02/2022 13:29:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=327
06/02/2022 13:29:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=329
06/02/2022 13:29:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=332
06/02/2022 13:29:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=334
06/02/2022 13:29:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=337
06/02/2022 13:29:12 - INFO - __main__ - Global step 1350 Train loss 0.45 Classification-F1 0.6035714285714286 on epoch=337
06/02/2022 13:29:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6030923315244395 -> 0.6035714285714286 on epoch=337, global_step=1350
06/02/2022 13:29:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=339
06/02/2022 13:29:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=342
06/02/2022 13:29:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=344
06/02/2022 13:29:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=347
06/02/2022 13:29:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=349
06/02/2022 13:29:19 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.5599028303117256 on epoch=349
06/02/2022 13:29:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=352
06/02/2022 13:29:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=354
06/02/2022 13:29:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
06/02/2022 13:29:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=359
06/02/2022 13:29:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=362
06/02/2022 13:29:25 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.6783771929824561 on epoch=362
06/02/2022 13:29:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6035714285714286 -> 0.6783771929824561 on epoch=362, global_step=1450
06/02/2022 13:29:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=364
06/02/2022 13:29:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=367
06/02/2022 13:29:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=369
06/02/2022 13:29:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=372
06/02/2022 13:29:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=374
06/02/2022 13:29:32 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.6264666516279419 on epoch=374
06/02/2022 13:29:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=377
06/02/2022 13:29:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=379
06/02/2022 13:29:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.28 on epoch=382
06/02/2022 13:29:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=384
06/02/2022 13:29:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=387
06/02/2022 13:29:39 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.5499923092698934 on epoch=387
06/02/2022 13:29:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=389
06/02/2022 13:29:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=392
06/02/2022 13:29:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=394
06/02/2022 13:29:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=397
06/02/2022 13:29:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=399
06/02/2022 13:29:46 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.6374418773612321 on epoch=399
06/02/2022 13:29:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
06/02/2022 13:29:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=404
06/02/2022 13:29:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=407
06/02/2022 13:29:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=409
06/02/2022 13:29:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=412
06/02/2022 13:29:52 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.6865543394777266 on epoch=412
06/02/2022 13:29:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6783771929824561 -> 0.6865543394777266 on epoch=412, global_step=1650
06/02/2022 13:29:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=414
06/02/2022 13:29:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=417
06/02/2022 13:29:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=419
06/02/2022 13:29:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=422
06/02/2022 13:29:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=424
06/02/2022 13:29:59 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.6807706093189965 on epoch=424
06/02/2022 13:30:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=427
06/02/2022 13:30:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=429
06/02/2022 13:30:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=432
06/02/2022 13:30:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=434
06/02/2022 13:30:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=437
06/02/2022 13:30:06 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.69869379748412 on epoch=437
06/02/2022 13:30:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6865543394777266 -> 0.69869379748412 on epoch=437, global_step=1750
06/02/2022 13:30:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=439
06/02/2022 13:30:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=442
06/02/2022 13:30:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=444
06/02/2022 13:30:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=447
06/02/2022 13:30:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=449
06/02/2022 13:30:13 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7323967086834734 on epoch=449
06/02/2022 13:30:13 - INFO - __main__ - Saving model with best Classification-F1: 0.69869379748412 -> 0.7323967086834734 on epoch=449, global_step=1800
06/02/2022 13:30:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=452
06/02/2022 13:30:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=454
06/02/2022 13:30:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=457
06/02/2022 13:30:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=459
06/02/2022 13:30:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=462
06/02/2022 13:30:19 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.699378667927055 on epoch=462
06/02/2022 13:30:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
06/02/2022 13:30:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=467
06/02/2022 13:30:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=469
06/02/2022 13:30:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=472
06/02/2022 13:30:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=474
06/02/2022 13:30:26 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.6544069729553601 on epoch=474
06/02/2022 13:30:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=477
06/02/2022 13:30:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=479
06/02/2022 13:30:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=482
06/02/2022 13:30:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=484
06/02/2022 13:30:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=487
06/02/2022 13:30:32 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7362745098039216 on epoch=487
06/02/2022 13:30:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7323967086834734 -> 0.7362745098039216 on epoch=487, global_step=1950
06/02/2022 13:30:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=489
06/02/2022 13:30:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=492
06/02/2022 13:30:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=494
06/02/2022 13:30:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=497
06/02/2022 13:30:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
06/02/2022 13:30:39 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.7323967086834734 on epoch=499
06/02/2022 13:30:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.28 on epoch=502
06/02/2022 13:30:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=504
06/02/2022 13:30:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=507
06/02/2022 13:30:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
06/02/2022 13:30:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=512
06/02/2022 13:30:46 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.701461038961039 on epoch=512
06/02/2022 13:30:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=514
06/02/2022 13:30:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
06/02/2022 13:30:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=519
06/02/2022 13:30:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
06/02/2022 13:30:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=524
06/02/2022 13:30:52 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.6421919243342856 on epoch=524
06/02/2022 13:30:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=527
06/02/2022 13:30:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/02/2022 13:30:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=532
06/02/2022 13:30:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
06/02/2022 13:30:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
06/02/2022 13:30:59 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.7521124519763358 on epoch=537
06/02/2022 13:30:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7362745098039216 -> 0.7521124519763358 on epoch=537, global_step=2150
06/02/2022 13:31:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=539
06/02/2022 13:31:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
06/02/2022 13:31:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=544
06/02/2022 13:31:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
06/02/2022 13:31:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=549
06/02/2022 13:31:06 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.7338848039215686 on epoch=549
06/02/2022 13:31:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=552
06/02/2022 13:31:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/02/2022 13:31:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
06/02/2022 13:31:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=559
06/02/2022 13:31:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=562
06/02/2022 13:31:12 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.7001002506265666 on epoch=562
06/02/2022 13:31:14 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=564
06/02/2022 13:31:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=567
06/02/2022 13:31:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=569
06/02/2022 13:31:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=572
06/02/2022 13:31:19 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=574
06/02/2022 13:31:19 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.7347069597069598 on epoch=574
06/02/2022 13:31:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=577
06/02/2022 13:31:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=579
06/02/2022 13:31:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=582
06/02/2022 13:31:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/02/2022 13:31:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=587
06/02/2022 13:31:26 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.7166666666666667 on epoch=587
06/02/2022 13:31:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
06/02/2022 13:31:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=592
06/02/2022 13:31:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=594
06/02/2022 13:31:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=597
06/02/2022 13:31:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
06/02/2022 13:31:32 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.6688218390804598 on epoch=599
06/02/2022 13:31:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=602
06/02/2022 13:31:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=604
06/02/2022 13:31:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
06/02/2022 13:31:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
06/02/2022 13:31:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=612
06/02/2022 13:31:39 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7632962862564381 on epoch=612
06/02/2022 13:31:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7521124519763358 -> 0.7632962862564381 on epoch=612, global_step=2450
06/02/2022 13:31:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=614
06/02/2022 13:31:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/02/2022 13:31:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
06/02/2022 13:31:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=622
06/02/2022 13:31:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=624
06/02/2022 13:31:46 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.6891403718989926 on epoch=624
06/02/2022 13:31:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/02/2022 13:31:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=629
06/02/2022 13:31:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=632
06/02/2022 13:31:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/02/2022 13:31:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
06/02/2022 13:31:52 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.7520289097875305 on epoch=637
06/02/2022 13:31:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=639
06/02/2022 13:31:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=642
06/02/2022 13:31:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=644
06/02/2022 13:31:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=647
06/02/2022 13:31:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
06/02/2022 13:31:59 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.733629477678421 on epoch=649
06/02/2022 13:32:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/02/2022 13:32:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=654
06/02/2022 13:32:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=657
06/02/2022 13:32:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/02/2022 13:32:05 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=662
06/02/2022 13:32:06 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.6739794967381174 on epoch=662
06/02/2022 13:32:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=664
06/02/2022 13:32:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=667
06/02/2022 13:32:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
06/02/2022 13:32:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=672
06/02/2022 13:32:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
06/02/2022 13:32:12 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7500960061443933 on epoch=674
06/02/2022 13:32:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=677
06/02/2022 13:32:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=679
06/02/2022 13:32:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
06/02/2022 13:32:17 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
06/02/2022 13:32:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=687
06/02/2022 13:32:19 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7027346592287169 on epoch=687
06/02/2022 13:32:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=689
06/02/2022 13:32:22 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
06/02/2022 13:32:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=694
06/02/2022 13:32:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
06/02/2022 13:32:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=699
06/02/2022 13:32:26 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7328951514435387 on epoch=699
06/02/2022 13:32:27 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=702
06/02/2022 13:32:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=704
06/02/2022 13:32:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=707
06/02/2022 13:32:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=709
06/02/2022 13:32:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=712
06/02/2022 13:32:33 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7473169191919192 on epoch=712
06/02/2022 13:32:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/02/2022 13:32:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=717
06/02/2022 13:32:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 13:32:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=722
06/02/2022 13:32:39 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
06/02/2022 13:32:39 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7178811354457906 on epoch=724
06/02/2022 13:32:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/02/2022 13:32:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=729
06/02/2022 13:32:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=732
06/02/2022 13:32:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=734
06/02/2022 13:32:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
06/02/2022 13:32:46 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7317879253363125 on epoch=737
06/02/2022 13:32:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
06/02/2022 13:32:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=742
06/02/2022 13:32:50 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=744
06/02/2022 13:32:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
06/02/2022 13:32:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/02/2022 13:32:53 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7636849606613806 on epoch=749
06/02/2022 13:32:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7632962862564381 -> 0.7636849606613806 on epoch=749, global_step=3000
06/02/2022 13:32:53 - INFO - __main__ - save last model!
06/02/2022 13:32:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:32:53 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:32:53 - INFO - __main__ - Printing 3 examples
06/02/2022 13:32:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:32:53 - INFO - __main__ - ['others']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:32:53 - INFO - __main__ - ['others']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:32:53 - INFO - __main__ - ['others']
06/02/2022 13:32:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:32:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:32:53 - INFO - __main__ - Printing 3 examples
06/02/2022 13:32:53 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:32:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:32:53 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:32:53 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:32:53 - INFO - __main__ - Printing 3 examples
06/02/2022 13:32:53 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:32:53 - INFO - __main__ - ['happy']
06/02/2022 13:32:53 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:32:53 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:32:53 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:32:55 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:32:59 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:32:59 - INFO - __main__ - task name: emo
06/02/2022 13:33:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:33:00 - INFO - __main__ - Starting training!
06/02/2022 13:33:00 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:33:43 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_21_0.2_8_predictions.txt
06/02/2022 13:33:43 - INFO - __main__ - Classification-F1 on test data: 0.4638
06/02/2022 13:33:44 - INFO - __main__ - prefix=emo_16_21, lr=0.2, bsz=8, dev_performance=0.7636849606613806, test_performance=0.4638090184615515
06/02/2022 13:33:44 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.5, bsz=8 ...
06/02/2022 13:33:44 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:33:44 - INFO - __main__ - Printing 3 examples
06/02/2022 13:33:44 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:33:44 - INFO - __main__ - ['happy']
06/02/2022 13:33:44 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:33:44 - INFO - __main__ - ['happy']
06/02/2022 13:33:44 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:33:44 - INFO - __main__ - ['happy']
06/02/2022 13:33:44 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:33:44 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:33:45 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:33:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:33:45 - INFO - __main__ - Printing 3 examples
06/02/2022 13:33:45 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:33:45 - INFO - __main__ - ['happy']
06/02/2022 13:33:45 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:33:45 - INFO - __main__ - ['happy']
06/02/2022 13:33:45 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:33:45 - INFO - __main__ - ['happy']
06/02/2022 13:33:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:33:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:33:45 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:33:50 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:33:50 - INFO - __main__ - task name: emo
06/02/2022 13:33:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:33:50 - INFO - __main__ - Starting training!
06/02/2022 13:33:52 - INFO - __main__ - Step 10 Global step 10 Train loss 5.88 on epoch=2
06/02/2022 13:33:53 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=4
06/02/2022 13:33:54 - INFO - __main__ - Step 30 Global step 30 Train loss 1.93 on epoch=7
06/02/2022 13:33:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.47 on epoch=9
06/02/2022 13:33:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=12
06/02/2022 13:33:57 - INFO - __main__ - Global step 50 Train loss 2.66 Classification-F1 0.13067758749069247 on epoch=12
06/02/2022 13:33:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=12, global_step=50
06/02/2022 13:33:58 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=14
06/02/2022 13:33:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=17
06/02/2022 13:34:01 - INFO - __main__ - Step 80 Global step 80 Train loss 1.18 on epoch=19
06/02/2022 13:34:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.33 on epoch=22
06/02/2022 13:34:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.68 on epoch=24
06/02/2022 13:34:04 - INFO - __main__ - Global step 100 Train loss 1.33 Classification-F1 0.16138763197586728 on epoch=24
06/02/2022 13:34:04 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.16138763197586728 on epoch=24, global_step=100
06/02/2022 13:34:05 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=27
06/02/2022 13:34:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=29
06/02/2022 13:34:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.00 on epoch=32
06/02/2022 13:34:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=34
06/02/2022 13:34:10 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=37
06/02/2022 13:34:10 - INFO - __main__ - Global step 150 Train loss 1.07 Classification-F1 0.21377995642701525 on epoch=37
06/02/2022 13:34:10 - INFO - __main__ - Saving model with best Classification-F1: 0.16138763197586728 -> 0.21377995642701525 on epoch=37, global_step=150
06/02/2022 13:34:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.99 on epoch=39
06/02/2022 13:34:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=42
06/02/2022 13:34:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=44
06/02/2022 13:34:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.11 on epoch=47
06/02/2022 13:34:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=49
06/02/2022 13:34:17 - INFO - __main__ - Global step 200 Train loss 0.98 Classification-F1 0.1 on epoch=49
06/02/2022 13:34:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=52
06/02/2022 13:34:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=54
06/02/2022 13:34:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
06/02/2022 13:34:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.98 on epoch=59
06/02/2022 13:34:23 - INFO - __main__ - Step 250 Global step 250 Train loss 1.00 on epoch=62
06/02/2022 13:34:24 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.21584440227703988 on epoch=62
06/02/2022 13:34:24 - INFO - __main__ - Saving model with best Classification-F1: 0.21377995642701525 -> 0.21584440227703988 on epoch=62, global_step=250
06/02/2022 13:34:25 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=64
06/02/2022 13:34:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.97 on epoch=67
06/02/2022 13:34:27 - INFO - __main__ - Step 280 Global step 280 Train loss 1.02 on epoch=69
06/02/2022 13:34:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.00 on epoch=72
06/02/2022 13:34:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.92 on epoch=74
06/02/2022 13:34:30 - INFO - __main__ - Global step 300 Train loss 0.99 Classification-F1 0.34285516093229745 on epoch=74
06/02/2022 13:34:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21584440227703988 -> 0.34285516093229745 on epoch=74, global_step=300
06/02/2022 13:34:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=77
06/02/2022 13:34:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=79
06/02/2022 13:34:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
06/02/2022 13:34:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=84
06/02/2022 13:34:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.84 on epoch=87
06/02/2022 13:34:37 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.18284347231715653 on epoch=87
06/02/2022 13:34:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.93 on epoch=89
06/02/2022 13:34:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.89 on epoch=92
06/02/2022 13:34:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
06/02/2022 13:34:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.93 on epoch=97
06/02/2022 13:34:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.99 on epoch=99
06/02/2022 13:34:44 - INFO - __main__ - Global step 400 Train loss 0.93 Classification-F1 0.13067758749069247 on epoch=99
06/02/2022 13:34:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.98 on epoch=102
06/02/2022 13:34:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=104
06/02/2022 13:34:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
06/02/2022 13:34:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=109
06/02/2022 13:34:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.91 on epoch=112
06/02/2022 13:34:50 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.23866366366366365 on epoch=112
06/02/2022 13:34:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.79 on epoch=114
06/02/2022 13:34:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.88 on epoch=117
06/02/2022 13:34:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=119
06/02/2022 13:34:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=122
06/02/2022 13:34:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
06/02/2022 13:34:57 - INFO - __main__ - Global step 500 Train loss 0.85 Classification-F1 0.1888634241575418 on epoch=124
06/02/2022 13:34:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=127
06/02/2022 13:35:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.89 on epoch=129
06/02/2022 13:35:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.85 on epoch=132
06/02/2022 13:35:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=134
06/02/2022 13:35:03 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
06/02/2022 13:35:04 - INFO - __main__ - Global step 550 Train loss 0.81 Classification-F1 0.4373265066929862 on epoch=137
06/02/2022 13:35:04 - INFO - __main__ - Saving model with best Classification-F1: 0.34285516093229745 -> 0.4373265066929862 on epoch=137, global_step=550
06/02/2022 13:35:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=139
06/02/2022 13:35:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.82 on epoch=142
06/02/2022 13:35:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=144
06/02/2022 13:35:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=147
06/02/2022 13:35:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=149
06/02/2022 13:35:10 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.5844961240310077 on epoch=149
06/02/2022 13:35:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4373265066929862 -> 0.5844961240310077 on epoch=149, global_step=600
06/02/2022 13:35:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=152
06/02/2022 13:35:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.79 on epoch=154
06/02/2022 13:35:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.66 on epoch=157
06/02/2022 13:35:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.71 on epoch=159
06/02/2022 13:35:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=162
06/02/2022 13:35:17 - INFO - __main__ - Global step 650 Train loss 0.73 Classification-F1 0.39413937511763597 on epoch=162
06/02/2022 13:35:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.78 on epoch=164
06/02/2022 13:35:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=167
06/02/2022 13:35:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.85 on epoch=169
06/02/2022 13:35:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.61 on epoch=172
06/02/2022 13:35:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=174
06/02/2022 13:35:24 - INFO - __main__ - Global step 700 Train loss 0.73 Classification-F1 0.4137254901960784 on epoch=174
06/02/2022 13:35:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=177
06/02/2022 13:35:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.69 on epoch=179
06/02/2022 13:35:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=182
06/02/2022 13:35:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.67 on epoch=184
06/02/2022 13:35:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.65 on epoch=187
06/02/2022 13:35:30 - INFO - __main__ - Global step 750 Train loss 0.68 Classification-F1 0.4680930930930931 on epoch=187
06/02/2022 13:35:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=189
06/02/2022 13:35:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.60 on epoch=192
06/02/2022 13:35:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.55 on epoch=194
06/02/2022 13:35:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=197
06/02/2022 13:35:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=199
06/02/2022 13:35:37 - INFO - __main__ - Global step 800 Train loss 0.59 Classification-F1 0.5797397047397048 on epoch=199
06/02/2022 13:35:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.62 on epoch=202
06/02/2022 13:35:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=204
06/02/2022 13:35:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=207
06/02/2022 13:35:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=209
06/02/2022 13:35:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=212
06/02/2022 13:35:43 - INFO - __main__ - Global step 850 Train loss 0.51 Classification-F1 0.5085125448028674 on epoch=212
06/02/2022 13:35:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=214
06/02/2022 13:35:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=217
06/02/2022 13:35:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.52 on epoch=219
06/02/2022 13:35:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=222
06/02/2022 13:35:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=224
06/02/2022 13:35:50 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.6032509157509158 on epoch=224
06/02/2022 13:35:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5844961240310077 -> 0.6032509157509158 on epoch=224, global_step=900
06/02/2022 13:35:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=227
06/02/2022 13:35:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=229
06/02/2022 13:35:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=232
06/02/2022 13:35:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=234
06/02/2022 13:35:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=237
06/02/2022 13:35:57 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.6228413163897035 on epoch=237
06/02/2022 13:35:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6032509157509158 -> 0.6228413163897035 on epoch=237, global_step=950
06/02/2022 13:35:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=239
06/02/2022 13:35:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=242
06/02/2022 13:36:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=244
06/02/2022 13:36:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=247
06/02/2022 13:36:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=249
06/02/2022 13:36:03 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.6896296935196365 on epoch=249
06/02/2022 13:36:03 - INFO - __main__ - Saving model with best Classification-F1: 0.6228413163897035 -> 0.6896296935196365 on epoch=249, global_step=1000
06/02/2022 13:36:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
06/02/2022 13:36:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=254
06/02/2022 13:36:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=257
06/02/2022 13:36:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.56 on epoch=259
06/02/2022 13:36:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.23 on epoch=262
06/02/2022 13:36:10 - INFO - __main__ - Global step 1050 Train loss 0.55 Classification-F1 0.221460282226098 on epoch=262
06/02/2022 13:36:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.61 on epoch=264
06/02/2022 13:36:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=267
06/02/2022 13:36:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=269
06/02/2022 13:36:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=272
06/02/2022 13:36:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=274
06/02/2022 13:36:17 - INFO - __main__ - Global step 1100 Train loss 0.72 Classification-F1 0.607843137254902 on epoch=274
06/02/2022 13:36:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=277
06/02/2022 13:36:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=279
06/02/2022 13:36:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=282
06/02/2022 13:36:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=284
06/02/2022 13:36:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=287
06/02/2022 13:36:23 - INFO - __main__ - Global step 1150 Train loss 0.55 Classification-F1 0.6142746522454386 on epoch=287
06/02/2022 13:36:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.54 on epoch=289
06/02/2022 13:36:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=292
06/02/2022 13:36:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=294
06/02/2022 13:36:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=297
06/02/2022 13:36:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=299
06/02/2022 13:36:30 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.67440424295263 on epoch=299
06/02/2022 13:36:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=302
06/02/2022 13:36:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=304
06/02/2022 13:36:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=307
06/02/2022 13:36:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=309
06/02/2022 13:36:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=312
06/02/2022 13:36:37 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.6802041605172535 on epoch=312
06/02/2022 13:36:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=314
06/02/2022 13:36:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=317
06/02/2022 13:36:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=319
06/02/2022 13:36:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=322
06/02/2022 13:36:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=324
06/02/2022 13:36:43 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.645170985060691 on epoch=324
06/02/2022 13:36:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=327
06/02/2022 13:36:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=329
06/02/2022 13:36:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=332
06/02/2022 13:36:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=334
06/02/2022 13:36:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=337
06/02/2022 13:36:50 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.6603708133971291 on epoch=337
06/02/2022 13:36:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=339
06/02/2022 13:36:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=342
06/02/2022 13:36:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=344
06/02/2022 13:36:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=347
06/02/2022 13:36:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=349
06/02/2022 13:36:56 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.6406508478081059 on epoch=349
06/02/2022 13:36:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=352
06/02/2022 13:36:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=354
06/02/2022 13:37:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=357
06/02/2022 13:37:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=359
06/02/2022 13:37:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=362
06/02/2022 13:37:03 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.6510501864817118 on epoch=362
06/02/2022 13:37:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=364
06/02/2022 13:37:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=367
06/02/2022 13:37:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=369
06/02/2022 13:37:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=372
06/02/2022 13:37:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=374
06/02/2022 13:37:10 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.6459401709401709 on epoch=374
06/02/2022 13:37:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=377
06/02/2022 13:37:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=379
06/02/2022 13:37:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=382
06/02/2022 13:37:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=384
06/02/2022 13:37:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.10 on epoch=387
06/02/2022 13:37:16 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.612196164269335 on epoch=387
06/02/2022 13:37:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=389
06/02/2022 13:37:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=392
06/02/2022 13:37:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=394
06/02/2022 13:37:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=397
06/02/2022 13:37:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=399
06/02/2022 13:37:23 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.6196476964769648 on epoch=399
06/02/2022 13:37:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=402
06/02/2022 13:37:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=404
06/02/2022 13:37:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.06 on epoch=407
06/02/2022 13:37:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=409
06/02/2022 13:37:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=412
06/02/2022 13:37:30 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.6568181818181819 on epoch=412
06/02/2022 13:37:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=414
06/02/2022 13:37:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
06/02/2022 13:37:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=419
06/02/2022 13:37:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=422
06/02/2022 13:37:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=424
06/02/2022 13:37:36 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.6423915130568356 on epoch=424
06/02/2022 13:37:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
06/02/2022 13:37:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=429
06/02/2022 13:37:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=432
06/02/2022 13:37:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=434
06/02/2022 13:37:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=437
06/02/2022 13:37:43 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.5861190452411864 on epoch=437
06/02/2022 13:37:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=439
06/02/2022 13:37:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=442
06/02/2022 13:37:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=444
06/02/2022 13:37:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=447
06/02/2022 13:37:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=449
06/02/2022 13:37:50 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.6572622779519331 on epoch=449
06/02/2022 13:37:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=452
06/02/2022 13:37:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=454
06/02/2022 13:37:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=457
06/02/2022 13:37:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=459
06/02/2022 13:37:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=462
06/02/2022 13:37:56 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.6172875615763547 on epoch=462
06/02/2022 13:37:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
06/02/2022 13:37:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
06/02/2022 13:38:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=469
06/02/2022 13:38:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=472
06/02/2022 13:38:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=474
06/02/2022 13:38:03 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.4875210138368033 on epoch=474
06/02/2022 13:38:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=477
06/02/2022 13:38:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=479
06/02/2022 13:38:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
06/02/2022 13:38:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
06/02/2022 13:38:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=487
06/02/2022 13:38:09 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.5786477036477036 on epoch=487
06/02/2022 13:38:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/02/2022 13:38:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=492
06/02/2022 13:38:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
06/02/2022 13:38:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=497
06/02/2022 13:38:16 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=499
06/02/2022 13:38:16 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.6410539215686275 on epoch=499
06/02/2022 13:38:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=502
06/02/2022 13:38:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=504
06/02/2022 13:38:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=507
06/02/2022 13:38:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=509
06/02/2022 13:38:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
06/02/2022 13:38:23 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.6252324612725058 on epoch=512
06/02/2022 13:38:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=514
06/02/2022 13:38:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=517
06/02/2022 13:38:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/02/2022 13:38:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=522
06/02/2022 13:38:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
06/02/2022 13:38:29 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.6364743837914569 on epoch=524
06/02/2022 13:38:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=527
06/02/2022 13:38:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=529
06/02/2022 13:38:33 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=532
06/02/2022 13:38:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
06/02/2022 13:38:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
06/02/2022 13:38:36 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.5630779130779131 on epoch=537
06/02/2022 13:38:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=539
06/02/2022 13:38:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
06/02/2022 13:38:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=544
06/02/2022 13:38:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=547
06/02/2022 13:38:42 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/02/2022 13:38:43 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.5444970016542597 on epoch=549
06/02/2022 13:38:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
06/02/2022 13:38:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/02/2022 13:38:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=557
06/02/2022 13:38:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=559
06/02/2022 13:38:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=562
06/02/2022 13:38:49 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.64609059490525 on epoch=562
06/02/2022 13:38:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=564
06/02/2022 13:38:52 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=567
06/02/2022 13:38:53 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=569
06/02/2022 13:38:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/02/2022 13:38:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
06/02/2022 13:38:56 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.5898119122257053 on epoch=574
06/02/2022 13:38:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
06/02/2022 13:38:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=579
06/02/2022 13:39:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=582
06/02/2022 13:39:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/02/2022 13:39:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=587
06/02/2022 13:39:03 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.6216201423097976 on epoch=587
06/02/2022 13:39:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
06/02/2022 13:39:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
06/02/2022 13:39:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
06/02/2022 13:39:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
06/02/2022 13:39:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=599
06/02/2022 13:39:09 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.6882927951239146 on epoch=599
06/02/2022 13:39:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/02/2022 13:39:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=604
06/02/2022 13:39:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.13 on epoch=607
06/02/2022 13:39:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=609
06/02/2022 13:39:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=612
06/02/2022 13:39:16 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.6523809523809524 on epoch=612
06/02/2022 13:39:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
06/02/2022 13:39:18 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=617
06/02/2022 13:39:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=619
06/02/2022 13:39:21 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
06/02/2022 13:39:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=624
06/02/2022 13:39:23 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6707987229677997 on epoch=624
06/02/2022 13:39:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=627
06/02/2022 13:39:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=629
06/02/2022 13:39:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/02/2022 13:39:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=634
06/02/2022 13:39:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=637
06/02/2022 13:39:29 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7080517202221056 on epoch=637
06/02/2022 13:39:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6896296935196365 -> 0.7080517202221056 on epoch=637, global_step=2550
06/02/2022 13:39:31 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=639
06/02/2022 13:39:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 13:39:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/02/2022 13:39:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
06/02/2022 13:39:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=649
06/02/2022 13:39:36 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.6416359307612747 on epoch=649
06/02/2022 13:39:37 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/02/2022 13:39:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/02/2022 13:39:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=657
06/02/2022 13:39:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/02/2022 13:39:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
06/02/2022 13:39:43 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.4630124777183601 on epoch=662
06/02/2022 13:39:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=664
06/02/2022 13:39:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/02/2022 13:39:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/02/2022 13:39:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=672
06/02/2022 13:39:49 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/02/2022 13:39:49 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.6892857142857143 on epoch=674
06/02/2022 13:39:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
06/02/2022 13:39:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=679
06/02/2022 13:39:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=682
06/02/2022 13:39:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=684
06/02/2022 13:39:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
06/02/2022 13:39:56 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.6595238095238094 on epoch=687
06/02/2022 13:39:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=689
06/02/2022 13:39:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=692
06/02/2022 13:40:00 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=694
06/02/2022 13:40:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/02/2022 13:40:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=699
06/02/2022 13:40:03 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.6901109257199357 on epoch=699
06/02/2022 13:40:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
06/02/2022 13:40:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=704
06/02/2022 13:40:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/02/2022 13:40:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=709
06/02/2022 13:40:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/02/2022 13:40:09 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.6156331498116488 on epoch=712
06/02/2022 13:40:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=714
06/02/2022 13:40:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/02/2022 13:40:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=719
06/02/2022 13:40:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=722
06/02/2022 13:40:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/02/2022 13:40:16 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.640180265654649 on epoch=724
06/02/2022 13:40:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 13:40:19 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=729
06/02/2022 13:40:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/02/2022 13:40:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/02/2022 13:40:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
06/02/2022 13:40:23 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7237067983438219 on epoch=737
06/02/2022 13:40:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7080517202221056 -> 0.7237067983438219 on epoch=737, global_step=2950
06/02/2022 13:40:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/02/2022 13:40:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/02/2022 13:40:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/02/2022 13:40:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/02/2022 13:40:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
06/02/2022 13:40:30 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.6538961038961039 on epoch=749
06/02/2022 13:40:30 - INFO - __main__ - save last model!
06/02/2022 13:40:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:40:30 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:40:30 - INFO - __main__ - Printing 3 examples
06/02/2022 13:40:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:40:30 - INFO - __main__ - ['others']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:40:30 - INFO - __main__ - ['others']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:40:30 - INFO - __main__ - ['others']
06/02/2022 13:40:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:40:30 - INFO - __main__ - Printing 3 examples
06/02/2022 13:40:30 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:40:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:40:30 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:40:30 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:40:30 - INFO - __main__ - Printing 3 examples
06/02/2022 13:40:30 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:40:30 - INFO - __main__ - ['happy']
06/02/2022 13:40:30 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:40:30 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:40:30 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:40:32 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:40:36 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:40:36 - INFO - __main__ - task name: emo
06/02/2022 13:40:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:40:36 - INFO - __main__ - Starting training!
06/02/2022 13:40:37 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:41:21 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.5_8_predictions.txt
06/02/2022 13:41:21 - INFO - __main__ - Classification-F1 on test data: 0.2699
06/02/2022 13:41:21 - INFO - __main__ - prefix=emo_16_42, lr=0.5, bsz=8, dev_performance=0.7237067983438219, test_performance=0.26986174430673965
06/02/2022 13:41:21 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.4, bsz=8 ...
06/02/2022 13:41:22 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:41:22 - INFO - __main__ - Printing 3 examples
06/02/2022 13:41:22 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:41:22 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:41:22 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:41:22 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:41:22 - INFO - __main__ - Printing 3 examples
06/02/2022 13:41:22 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:41:22 - INFO - __main__ - ['happy']
06/02/2022 13:41:22 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:41:22 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:41:22 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:41:28 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:41:28 - INFO - __main__ - task name: emo
06/02/2022 13:41:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:41:28 - INFO - __main__ - Starting training!
06/02/2022 13:41:30 - INFO - __main__ - Step 10 Global step 10 Train loss 6.15 on epoch=2
06/02/2022 13:41:31 - INFO - __main__ - Step 20 Global step 20 Train loss 3.40 on epoch=4
06/02/2022 13:41:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.57 on epoch=7
06/02/2022 13:41:33 - INFO - __main__ - Step 40 Global step 40 Train loss 2.31 on epoch=9
06/02/2022 13:41:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.67 on epoch=12
06/02/2022 13:41:35 - INFO - __main__ - Global step 50 Train loss 3.22 Classification-F1 0.1 on epoch=12
06/02/2022 13:41:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
06/02/2022 13:41:36 - INFO - __main__ - Step 60 Global step 60 Train loss 1.54 on epoch=14
06/02/2022 13:41:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.37 on epoch=17
06/02/2022 13:41:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.37 on epoch=19
06/02/2022 13:41:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.35 on epoch=22
06/02/2022 13:41:41 - INFO - __main__ - Step 100 Global step 100 Train loss 1.12 on epoch=24
06/02/2022 13:41:42 - INFO - __main__ - Global step 100 Train loss 1.35 Classification-F1 0.1 on epoch=24
06/02/2022 13:41:43 - INFO - __main__ - Step 110 Global step 110 Train loss 1.15 on epoch=27
06/02/2022 13:41:44 - INFO - __main__ - Step 120 Global step 120 Train loss 1.10 on epoch=29
06/02/2022 13:41:46 - INFO - __main__ - Step 130 Global step 130 Train loss 1.04 on epoch=32
06/02/2022 13:41:47 - INFO - __main__ - Step 140 Global step 140 Train loss 1.16 on epoch=34
06/02/2022 13:41:48 - INFO - __main__ - Step 150 Global step 150 Train loss 1.12 on epoch=37
06/02/2022 13:41:49 - INFO - __main__ - Global step 150 Train loss 1.11 Classification-F1 0.14583333333333334 on epoch=37
06/02/2022 13:41:49 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.14583333333333334 on epoch=37, global_step=150
06/02/2022 13:41:50 - INFO - __main__ - Step 160 Global step 160 Train loss 1.08 on epoch=39
06/02/2022 13:41:51 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
06/02/2022 13:41:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=44
06/02/2022 13:41:54 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
06/02/2022 13:41:55 - INFO - __main__ - Step 200 Global step 200 Train loss 1.11 on epoch=49
06/02/2022 13:41:56 - INFO - __main__ - Global step 200 Train loss 1.05 Classification-F1 0.1 on epoch=49
06/02/2022 13:41:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.12 on epoch=52
06/02/2022 13:41:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.06 on epoch=54
06/02/2022 13:41:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.99 on epoch=57
06/02/2022 13:42:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=59
06/02/2022 13:42:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.02 on epoch=62
06/02/2022 13:42:02 - INFO - __main__ - Global step 250 Train loss 1.03 Classification-F1 0.2095890410958904 on epoch=62
06/02/2022 13:42:02 - INFO - __main__ - Saving model with best Classification-F1: 0.14583333333333334 -> 0.2095890410958904 on epoch=62, global_step=250
06/02/2022 13:42:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.01 on epoch=64
06/02/2022 13:42:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=67
06/02/2022 13:42:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.94 on epoch=69
06/02/2022 13:42:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
06/02/2022 13:42:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.97 on epoch=74
06/02/2022 13:42:09 - INFO - __main__ - Global step 300 Train loss 0.94 Classification-F1 0.1081081081081081 on epoch=74
06/02/2022 13:42:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.95 on epoch=77
06/02/2022 13:42:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.89 on epoch=79
06/02/2022 13:42:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.00 on epoch=82
06/02/2022 13:42:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.86 on epoch=84
06/02/2022 13:42:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=87
06/02/2022 13:42:16 - INFO - __main__ - Global step 350 Train loss 0.93 Classification-F1 0.20526315789473687 on epoch=87
06/02/2022 13:42:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.95 on epoch=89
06/02/2022 13:42:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
06/02/2022 13:42:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.85 on epoch=94
06/02/2022 13:42:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
06/02/2022 13:42:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
06/02/2022 13:42:23 - INFO - __main__ - Global step 400 Train loss 0.89 Classification-F1 0.4794313369630974 on epoch=99
06/02/2022 13:42:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2095890410958904 -> 0.4794313369630974 on epoch=99, global_step=400
06/02/2022 13:42:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.92 on epoch=102
06/02/2022 13:42:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=104
06/02/2022 13:42:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.88 on epoch=107
06/02/2022 13:42:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=109
06/02/2022 13:42:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.85 on epoch=112
06/02/2022 13:42:30 - INFO - __main__ - Global step 450 Train loss 0.86 Classification-F1 0.40527950310559 on epoch=112
06/02/2022 13:42:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.82 on epoch=114
06/02/2022 13:42:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.92 on epoch=117
06/02/2022 13:42:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.89 on epoch=119
06/02/2022 13:42:35 - INFO - __main__ - Step 490 Global step 490 Train loss 1.65 on epoch=122
06/02/2022 13:42:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
06/02/2022 13:42:37 - INFO - __main__ - Global step 500 Train loss 1.03 Classification-F1 0.28399015732775323 on epoch=124
06/02/2022 13:42:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.71 on epoch=127
06/02/2022 13:42:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.72 on epoch=129
06/02/2022 13:42:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.79 on epoch=132
06/02/2022 13:42:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
06/02/2022 13:42:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=137
06/02/2022 13:42:43 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.5017009719839909 on epoch=137
06/02/2022 13:42:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4794313369630974 -> 0.5017009719839909 on epoch=137, global_step=550
06/02/2022 13:42:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
06/02/2022 13:42:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=142
06/02/2022 13:42:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.80 on epoch=144
06/02/2022 13:42:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=147
06/02/2022 13:42:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=149
06/02/2022 13:42:50 - INFO - __main__ - Global step 600 Train loss 0.75 Classification-F1 0.2627363184079602 on epoch=149
06/02/2022 13:42:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.73 on epoch=152
06/02/2022 13:42:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.66 on epoch=154
06/02/2022 13:42:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.72 on epoch=157
06/02/2022 13:42:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.64 on epoch=159
06/02/2022 13:42:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.73 on epoch=162
06/02/2022 13:42:57 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.4641267537493952 on epoch=162
06/02/2022 13:42:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.75 on epoch=164
06/02/2022 13:42:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
06/02/2022 13:43:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=169
06/02/2022 13:43:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=172
06/02/2022 13:43:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=174
06/02/2022 13:43:04 - INFO - __main__ - Global step 700 Train loss 0.68 Classification-F1 0.2639355742296918 on epoch=174
06/02/2022 13:43:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=177
06/02/2022 13:43:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.72 on epoch=179
06/02/2022 13:43:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=182
06/02/2022 13:43:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.61 on epoch=184
06/02/2022 13:43:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.64 on epoch=187
06/02/2022 13:43:11 - INFO - __main__ - Global step 750 Train loss 0.67 Classification-F1 0.5330759551590025 on epoch=187
06/02/2022 13:43:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5017009719839909 -> 0.5330759551590025 on epoch=187, global_step=750
06/02/2022 13:43:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.62 on epoch=189
06/02/2022 13:43:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.66 on epoch=192
06/02/2022 13:43:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.69 on epoch=194
06/02/2022 13:43:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.63 on epoch=197
06/02/2022 13:43:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.66 on epoch=199
06/02/2022 13:43:17 - INFO - __main__ - Global step 800 Train loss 0.65 Classification-F1 0.4752585377585377 on epoch=199
06/02/2022 13:43:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=202
06/02/2022 13:43:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=204
06/02/2022 13:43:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.57 on epoch=207
06/02/2022 13:43:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.69 on epoch=209
06/02/2022 13:43:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=212
06/02/2022 13:43:24 - INFO - __main__ - Global step 850 Train loss 0.63 Classification-F1 0.5604427254772877 on epoch=212
06/02/2022 13:43:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5330759551590025 -> 0.5604427254772877 on epoch=212, global_step=850
06/02/2022 13:43:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.60 on epoch=214
06/02/2022 13:43:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=217
06/02/2022 13:43:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=219
06/02/2022 13:43:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.59 on epoch=222
06/02/2022 13:43:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=224
06/02/2022 13:43:31 - INFO - __main__ - Global step 900 Train loss 0.55 Classification-F1 0.42327264617630356 on epoch=224
06/02/2022 13:43:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=227
06/02/2022 13:43:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=229
06/02/2022 13:43:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.60 on epoch=232
06/02/2022 13:43:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=234
06/02/2022 13:43:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=237
06/02/2022 13:43:38 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.4974381496120627 on epoch=237
06/02/2022 13:43:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=239
06/02/2022 13:43:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=242
06/02/2022 13:43:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=244
06/02/2022 13:43:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=247
06/02/2022 13:43:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=249
06/02/2022 13:43:44 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.6365309448770238 on epoch=249
06/02/2022 13:43:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5604427254772877 -> 0.6365309448770238 on epoch=249, global_step=1000
06/02/2022 13:43:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=252
06/02/2022 13:43:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=254
06/02/2022 13:43:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=257
06/02/2022 13:43:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=259
06/02/2022 13:43:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=262
06/02/2022 13:43:51 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.41901470848839273 on epoch=262
06/02/2022 13:43:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=264
06/02/2022 13:43:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=267
06/02/2022 13:43:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=269
06/02/2022 13:43:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=272
06/02/2022 13:43:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=274
06/02/2022 13:43:58 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.46111111111111114 on epoch=274
06/02/2022 13:43:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=277
06/02/2022 13:44:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.33 on epoch=279
06/02/2022 13:44:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=282
06/02/2022 13:44:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=284
06/02/2022 13:44:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=287
06/02/2022 13:44:06 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.4435854001071393 on epoch=287
06/02/2022 13:44:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=289
06/02/2022 13:44:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=292
06/02/2022 13:44:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=294
06/02/2022 13:44:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=297
06/02/2022 13:44:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=299
06/02/2022 13:44:13 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.616042291042291 on epoch=299
06/02/2022 13:44:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.34 on epoch=302
06/02/2022 13:44:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=304
06/02/2022 13:44:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=307
06/02/2022 13:44:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=309
06/02/2022 13:44:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=312
06/02/2022 13:44:20 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.5413913428619311 on epoch=312
06/02/2022 13:44:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=314
06/02/2022 13:44:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=317
06/02/2022 13:44:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=319
06/02/2022 13:44:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=322
06/02/2022 13:44:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=324
06/02/2022 13:44:28 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.5764695893158412 on epoch=324
06/02/2022 13:44:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=327
06/02/2022 13:44:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=329
06/02/2022 13:44:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=332
06/02/2022 13:44:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=334
06/02/2022 13:44:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=337
06/02/2022 13:44:35 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.6021241830065359 on epoch=337
06/02/2022 13:44:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=339
06/02/2022 13:44:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=342
06/02/2022 13:44:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=344
06/02/2022 13:44:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=347
06/02/2022 13:44:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=349
06/02/2022 13:44:41 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.5562306148513045 on epoch=349
06/02/2022 13:44:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=352
06/02/2022 13:44:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=354
06/02/2022 13:44:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=357
06/02/2022 13:44:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=359
06/02/2022 13:44:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=362
06/02/2022 13:44:48 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.5646223146223146 on epoch=362
06/02/2022 13:44:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=364
06/02/2022 13:44:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=367
06/02/2022 13:44:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=369
06/02/2022 13:44:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=372
06/02/2022 13:44:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=374
06/02/2022 13:44:55 - INFO - __main__ - Global step 1500 Train loss 0.08 Classification-F1 0.50899898464147 on epoch=374
06/02/2022 13:44:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=377
06/02/2022 13:44:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=379
06/02/2022 13:44:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
06/02/2022 13:45:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=384
06/02/2022 13:45:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=387
06/02/2022 13:45:01 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.601419878296146 on epoch=387
06/02/2022 13:45:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=389
06/02/2022 13:45:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=392
06/02/2022 13:45:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=394
06/02/2022 13:45:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=397
06/02/2022 13:45:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=399
06/02/2022 13:45:08 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.5275375375375375 on epoch=399
06/02/2022 13:45:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=402
06/02/2022 13:45:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=404
06/02/2022 13:45:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=407
06/02/2022 13:45:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=409
06/02/2022 13:45:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=412
06/02/2022 13:45:15 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.5587044534412955 on epoch=412
06/02/2022 13:45:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=414
06/02/2022 13:45:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=417
06/02/2022 13:45:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=419
06/02/2022 13:45:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=422
06/02/2022 13:45:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=424
06/02/2022 13:45:22 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5832246039142591 on epoch=424
06/02/2022 13:45:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=427
06/02/2022 13:45:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=429
06/02/2022 13:45:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=432
06/02/2022 13:45:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=434
06/02/2022 13:45:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=437
06/02/2022 13:45:28 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.6069325394228882 on epoch=437
06/02/2022 13:45:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=439
06/02/2022 13:45:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=442
06/02/2022 13:45:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=444
06/02/2022 13:45:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=447
06/02/2022 13:45:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=449
06/02/2022 13:45:35 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.5274020694752403 on epoch=449
06/02/2022 13:45:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=452
06/02/2022 13:45:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=454
06/02/2022 13:45:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=457
06/02/2022 13:45:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=459
06/02/2022 13:45:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=462
06/02/2022 13:45:42 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.6933458378691645 on epoch=462
06/02/2022 13:45:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6365309448770238 -> 0.6933458378691645 on epoch=462, global_step=1850
06/02/2022 13:45:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=464
06/02/2022 13:45:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=467
06/02/2022 13:45:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=469
06/02/2022 13:45:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/02/2022 13:45:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=474
06/02/2022 13:45:49 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5956439393939393 on epoch=474
06/02/2022 13:45:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=477
06/02/2022 13:45:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=479
06/02/2022 13:45:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=482
06/02/2022 13:45:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/02/2022 13:45:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=487
06/02/2022 13:45:56 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.6418708066781089 on epoch=487
06/02/2022 13:45:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=489
06/02/2022 13:45:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=492
06/02/2022 13:45:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=494
06/02/2022 13:46:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=497
06/02/2022 13:46:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=499
06/02/2022 13:46:02 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.5826539589442815 on epoch=499
06/02/2022 13:46:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
06/02/2022 13:46:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=504
06/02/2022 13:46:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=507
06/02/2022 13:46:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=509
06/02/2022 13:46:09 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
06/02/2022 13:46:10 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.5641403764581124 on epoch=512
06/02/2022 13:46:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=514
06/02/2022 13:46:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=517
06/02/2022 13:46:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=519
06/02/2022 13:46:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=522
06/02/2022 13:46:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=524
06/02/2022 13:46:16 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.5477281534985736 on epoch=524
06/02/2022 13:46:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/02/2022 13:46:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/02/2022 13:46:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=532
06/02/2022 13:46:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=534
06/02/2022 13:46:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=537
06/02/2022 13:46:23 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.5680295073768442 on epoch=537
06/02/2022 13:46:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.01 on epoch=539
06/02/2022 13:46:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=542
06/02/2022 13:46:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=544
06/02/2022 13:46:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=547
06/02/2022 13:46:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=549
06/02/2022 13:46:30 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6071560993696701 on epoch=549
06/02/2022 13:46:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=552
06/02/2022 13:46:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=554
06/02/2022 13:46:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=557
06/02/2022 13:46:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=559
06/02/2022 13:46:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=562
06/02/2022 13:46:36 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.5871194271194271 on epoch=562
06/02/2022 13:46:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=564
06/02/2022 13:46:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
06/02/2022 13:46:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=569
06/02/2022 13:46:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=572
06/02/2022 13:46:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=574
06/02/2022 13:46:43 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.5926501035196687 on epoch=574
06/02/2022 13:46:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=577
06/02/2022 13:46:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=579
06/02/2022 13:46:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=582
06/02/2022 13:46:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/02/2022 13:46:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/02/2022 13:46:50 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.6363899613899614 on epoch=587
06/02/2022 13:46:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.00 on epoch=589
06/02/2022 13:46:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.00 on epoch=592
06/02/2022 13:46:54 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=594
06/02/2022 13:46:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=597
06/02/2022 13:46:56 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=599
06/02/2022 13:46:57 - INFO - __main__ - Global step 2400 Train loss 0.01 Classification-F1 0.5343465891520811 on epoch=599
06/02/2022 13:46:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=602
06/02/2022 13:46:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=604
06/02/2022 13:47:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=607
06/02/2022 13:47:02 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=609
06/02/2022 13:47:03 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=612
06/02/2022 13:47:03 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.5893635231870527 on epoch=612
06/02/2022 13:47:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/02/2022 13:47:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=617
06/02/2022 13:47:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
06/02/2022 13:47:08 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=622
06/02/2022 13:47:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=624
06/02/2022 13:47:10 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.522070772070772 on epoch=624
06/02/2022 13:47:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/02/2022 13:47:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=629
06/02/2022 13:47:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 13:47:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=634
06/02/2022 13:47:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=637
06/02/2022 13:47:17 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.560014619883041 on epoch=637
06/02/2022 13:47:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=639
06/02/2022 13:47:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=642
06/02/2022 13:47:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/02/2022 13:47:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.01 on epoch=647
06/02/2022 13:47:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/02/2022 13:47:24 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.516798418972332 on epoch=649
06/02/2022 13:47:25 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=652
06/02/2022 13:47:26 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/02/2022 13:47:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=657
06/02/2022 13:47:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 13:47:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/02/2022 13:47:30 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.5222793335696561 on epoch=662
06/02/2022 13:47:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/02/2022 13:47:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=667
06/02/2022 13:47:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=669
06/02/2022 13:47:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/02/2022 13:47:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=674
06/02/2022 13:47:37 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5317721662549248 on epoch=674
06/02/2022 13:47:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.00 on epoch=677
06/02/2022 13:47:40 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/02/2022 13:47:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/02/2022 13:47:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/02/2022 13:47:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=687
06/02/2022 13:47:44 - INFO - __main__ - Global step 2750 Train loss 0.01 Classification-F1 0.5839482746503619 on epoch=687
06/02/2022 13:47:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
06/02/2022 13:47:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=692
06/02/2022 13:47:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 13:47:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=697
06/02/2022 13:47:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=699
06/02/2022 13:47:51 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.5738075213264449 on epoch=699
06/02/2022 13:47:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=702
06/02/2022 13:47:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/02/2022 13:47:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=707
06/02/2022 13:47:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/02/2022 13:47:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/02/2022 13:47:57 - INFO - __main__ - Global step 2850 Train loss 0.01 Classification-F1 0.56036866359447 on epoch=712
06/02/2022 13:47:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.00 on epoch=714
06/02/2022 13:48:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=717
06/02/2022 13:48:01 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/02/2022 13:48:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/02/2022 13:48:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 13:48:04 - INFO - __main__ - Global step 2900 Train loss 0.00 Classification-F1 0.5744397759103641 on epoch=724
06/02/2022 13:48:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 13:48:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
06/02/2022 13:48:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/02/2022 13:48:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/02/2022 13:48:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
06/02/2022 13:48:11 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5767067223963775 on epoch=737
06/02/2022 13:48:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
06/02/2022 13:48:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/02/2022 13:48:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/02/2022 13:48:16 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=747
06/02/2022 13:48:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=749
06/02/2022 13:48:17 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.5267319023569024 on epoch=749
06/02/2022 13:48:17 - INFO - __main__ - save last model!
06/02/2022 13:48:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:48:17 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:48:17 - INFO - __main__ - Printing 3 examples
06/02/2022 13:48:17 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:48:17 - INFO - __main__ - ['others']
06/02/2022 13:48:17 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:48:17 - INFO - __main__ - ['others']
06/02/2022 13:48:17 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:48:17 - INFO - __main__ - ['others']
06/02/2022 13:48:17 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:48:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:48:18 - INFO - __main__ - Printing 3 examples
06/02/2022 13:48:18 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:48:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:48:18 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:48:18 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:48:18 - INFO - __main__ - Printing 3 examples
06/02/2022 13:48:18 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:48:18 - INFO - __main__ - ['happy']
06/02/2022 13:48:18 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:48:18 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:48:18 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:48:20 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:48:24 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:48:24 - INFO - __main__ - task name: emo
06/02/2022 13:48:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:48:24 - INFO - __main__ - Starting training!
06/02/2022 13:48:25 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:49:08 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.4_8_predictions.txt
06/02/2022 13:49:08 - INFO - __main__ - Classification-F1 on test data: 0.3329
06/02/2022 13:49:08 - INFO - __main__ - prefix=emo_16_42, lr=0.4, bsz=8, dev_performance=0.6933458378691645, test_performance=0.33290717128840874
06/02/2022 13:49:08 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.3, bsz=8 ...
06/02/2022 13:49:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:49:09 - INFO - __main__ - Printing 3 examples
06/02/2022 13:49:09 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:49:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:49:09 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:49:09 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:49:09 - INFO - __main__ - Printing 3 examples
06/02/2022 13:49:09 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:49:09 - INFO - __main__ - ['happy']
06/02/2022 13:49:09 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:49:09 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:49:09 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:49:15 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:49:15 - INFO - __main__ - task name: emo
06/02/2022 13:49:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:49:15 - INFO - __main__ - Starting training!
06/02/2022 13:49:17 - INFO - __main__ - Step 10 Global step 10 Train loss 7.03 on epoch=2
06/02/2022 13:49:18 - INFO - __main__ - Step 20 Global step 20 Train loss 5.18 on epoch=4
06/02/2022 13:49:19 - INFO - __main__ - Step 30 Global step 30 Train loss 3.46 on epoch=7
06/02/2022 13:49:20 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
06/02/2022 13:49:22 - INFO - __main__ - Step 50 Global step 50 Train loss 1.93 on epoch=12
06/02/2022 13:49:22 - INFO - __main__ - Global step 50 Train loss 4.03 Classification-F1 0.2157754010695187 on epoch=12
06/02/2022 13:49:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2157754010695187 on epoch=12, global_step=50
06/02/2022 13:49:23 - INFO - __main__ - Step 60 Global step 60 Train loss 1.64 on epoch=14
06/02/2022 13:49:25 - INFO - __main__ - Step 70 Global step 70 Train loss 1.57 on epoch=17
06/02/2022 13:49:26 - INFO - __main__ - Step 80 Global step 80 Train loss 1.32 on epoch=19
06/02/2022 13:49:27 - INFO - __main__ - Step 90 Global step 90 Train loss 1.37 on epoch=22
06/02/2022 13:49:28 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=24
06/02/2022 13:49:29 - INFO - __main__ - Global step 100 Train loss 1.40 Classification-F1 0.13067758749069247 on epoch=24
06/02/2022 13:49:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.15 on epoch=27
06/02/2022 13:49:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.16 on epoch=29
06/02/2022 13:49:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.10 on epoch=32
06/02/2022 13:49:34 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=34
06/02/2022 13:49:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.07 on epoch=37
06/02/2022 13:49:36 - INFO - __main__ - Global step 150 Train loss 1.14 Classification-F1 0.26883780332056195 on epoch=37
06/02/2022 13:49:36 - INFO - __main__ - Saving model with best Classification-F1: 0.2157754010695187 -> 0.26883780332056195 on epoch=37, global_step=150
06/02/2022 13:49:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=39
06/02/2022 13:49:38 - INFO - __main__ - Step 170 Global step 170 Train loss 1.07 on epoch=42
06/02/2022 13:49:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=44
06/02/2022 13:49:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.96 on epoch=47
06/02/2022 13:49:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.09 on epoch=49
06/02/2022 13:49:42 - INFO - __main__ - Global step 200 Train loss 1.01 Classification-F1 0.0974025974025974 on epoch=49
06/02/2022 13:49:43 - INFO - __main__ - Step 210 Global step 210 Train loss 1.00 on epoch=52
06/02/2022 13:49:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.02 on epoch=54
06/02/2022 13:49:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.95 on epoch=57
06/02/2022 13:49:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.93 on epoch=59
06/02/2022 13:49:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.97 on epoch=62
06/02/2022 13:49:49 - INFO - __main__ - Global step 250 Train loss 0.97 Classification-F1 0.19722222222222224 on epoch=62
06/02/2022 13:49:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=64
06/02/2022 13:49:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=67
06/02/2022 13:49:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
06/02/2022 13:49:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.92 on epoch=72
06/02/2022 13:49:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
06/02/2022 13:49:55 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.2629541137583943 on epoch=74
06/02/2022 13:49:57 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
06/02/2022 13:49:58 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=79
06/02/2022 13:49:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=82
06/02/2022 13:50:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=84
06/02/2022 13:50:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=87
06/02/2022 13:50:02 - INFO - __main__ - Global step 350 Train loss 0.90 Classification-F1 0.14095238095238094 on epoch=87
06/02/2022 13:50:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=89
06/02/2022 13:50:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=92
06/02/2022 13:50:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.75 on epoch=94
06/02/2022 13:50:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=97
06/02/2022 13:50:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=99
06/02/2022 13:50:09 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.32608225108225114 on epoch=99
06/02/2022 13:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.26883780332056195 -> 0.32608225108225114 on epoch=99, global_step=400
06/02/2022 13:50:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=102
06/02/2022 13:50:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=104
06/02/2022 13:50:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=107
06/02/2022 13:50:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=109
06/02/2022 13:50:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.74 on epoch=112
06/02/2022 13:50:15 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.31043956043956045 on epoch=112
06/02/2022 13:50:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.75 on epoch=114
06/02/2022 13:50:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=117
06/02/2022 13:50:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.68 on epoch=119
06/02/2022 13:50:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=122
06/02/2022 13:50:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=124
06/02/2022 13:50:22 - INFO - __main__ - Global step 500 Train loss 0.71 Classification-F1 0.26627959927140255 on epoch=124
06/02/2022 13:50:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=127
06/02/2022 13:50:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=129
06/02/2022 13:50:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.58 on epoch=132
06/02/2022 13:50:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.68 on epoch=134
06/02/2022 13:50:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=137
06/02/2022 13:50:29 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.30159467963386727 on epoch=137
06/02/2022 13:50:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=139
06/02/2022 13:50:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=142
06/02/2022 13:50:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.71 on epoch=144
06/02/2022 13:50:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.70 on epoch=147
06/02/2022 13:50:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.84 on epoch=149
06/02/2022 13:50:35 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.4426691729323309 on epoch=149
06/02/2022 13:50:35 - INFO - __main__ - Saving model with best Classification-F1: 0.32608225108225114 -> 0.4426691729323309 on epoch=149, global_step=600
06/02/2022 13:50:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=152
06/02/2022 13:50:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=154
06/02/2022 13:50:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=157
06/02/2022 13:50:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=159
06/02/2022 13:50:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=162
06/02/2022 13:50:42 - INFO - __main__ - Global step 650 Train loss 0.62 Classification-F1 0.3836320926838168 on epoch=162
06/02/2022 13:50:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.72 on epoch=164
06/02/2022 13:50:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.67 on epoch=167
06/02/2022 13:50:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.79 on epoch=169
06/02/2022 13:50:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.70 on epoch=172
06/02/2022 13:50:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.73 on epoch=174
06/02/2022 13:50:48 - INFO - __main__ - Global step 700 Train loss 0.72 Classification-F1 0.38217787114845936 on epoch=174
06/02/2022 13:50:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=177
06/02/2022 13:50:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=179
06/02/2022 13:50:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.78 on epoch=182
06/02/2022 13:50:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.78 on epoch=184
06/02/2022 13:50:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.66 on epoch=187
06/02/2022 13:50:55 - INFO - __main__ - Global step 750 Train loss 0.69 Classification-F1 0.5050770308123249 on epoch=187
06/02/2022 13:50:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4426691729323309 -> 0.5050770308123249 on epoch=187, global_step=750
06/02/2022 13:50:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=189
06/02/2022 13:50:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.63 on epoch=192
06/02/2022 13:50:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=194
06/02/2022 13:51:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=197
06/02/2022 13:51:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.67 on epoch=199
06/02/2022 13:51:02 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.42704342273307794 on epoch=199
06/02/2022 13:51:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=202
06/02/2022 13:51:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.61 on epoch=204
06/02/2022 13:51:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.72 on epoch=207
06/02/2022 13:51:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.65 on epoch=209
06/02/2022 13:51:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=212
06/02/2022 13:51:08 - INFO - __main__ - Global step 850 Train loss 0.61 Classification-F1 0.43984171508715686 on epoch=212
06/02/2022 13:51:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.62 on epoch=214
06/02/2022 13:51:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.52 on epoch=217
06/02/2022 13:51:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=219
06/02/2022 13:51:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.61 on epoch=222
06/02/2022 13:51:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.63 on epoch=224
06/02/2022 13:51:15 - INFO - __main__ - Global step 900 Train loss 0.59 Classification-F1 0.4646551724137931 on epoch=224
06/02/2022 13:51:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.62 on epoch=227
06/02/2022 13:51:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.67 on epoch=229
06/02/2022 13:51:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=232
06/02/2022 13:51:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=234
06/02/2022 13:51:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=237
06/02/2022 13:51:21 - INFO - __main__ - Global step 950 Train loss 0.61 Classification-F1 0.4912903225806451 on epoch=237
06/02/2022 13:51:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=239
06/02/2022 13:51:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.53 on epoch=242
06/02/2022 13:51:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=244
06/02/2022 13:51:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=247
06/02/2022 13:51:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.50 on epoch=249
06/02/2022 13:51:28 - INFO - __main__ - Global step 1000 Train loss 0.51 Classification-F1 0.45020604395604397 on epoch=249
06/02/2022 13:51:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=252
06/02/2022 13:51:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=254
06/02/2022 13:51:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=257
06/02/2022 13:51:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.54 on epoch=259
06/02/2022 13:51:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=262
06/02/2022 13:51:35 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.44637422832797147 on epoch=262
06/02/2022 13:51:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=264
06/02/2022 13:51:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=267
06/02/2022 13:51:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
06/02/2022 13:51:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=272
06/02/2022 13:51:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=274
06/02/2022 13:51:41 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.46806491985950177 on epoch=274
06/02/2022 13:51:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=277
06/02/2022 13:51:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=279
06/02/2022 13:51:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=282
06/02/2022 13:51:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.54 on epoch=284
06/02/2022 13:51:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=287
06/02/2022 13:51:48 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.507010582010582 on epoch=287
06/02/2022 13:51:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5050770308123249 -> 0.507010582010582 on epoch=287, global_step=1150
06/02/2022 13:51:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=289
06/02/2022 13:51:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=292
06/02/2022 13:51:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=294
06/02/2022 13:51:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=297
06/02/2022 13:51:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=299
06/02/2022 13:51:54 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.511578947368421 on epoch=299
06/02/2022 13:51:55 - INFO - __main__ - Saving model with best Classification-F1: 0.507010582010582 -> 0.511578947368421 on epoch=299, global_step=1200
06/02/2022 13:51:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=302
06/02/2022 13:51:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=304
06/02/2022 13:51:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=307
06/02/2022 13:51:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=309
06/02/2022 13:52:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=312
06/02/2022 13:52:01 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.5916717980295567 on epoch=312
06/02/2022 13:52:01 - INFO - __main__ - Saving model with best Classification-F1: 0.511578947368421 -> 0.5916717980295567 on epoch=312, global_step=1250
06/02/2022 13:52:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=314
06/02/2022 13:52:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=317
06/02/2022 13:52:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=319
06/02/2022 13:52:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=322
06/02/2022 13:52:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=324
06/02/2022 13:52:08 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.5942930911680911 on epoch=324
06/02/2022 13:52:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5916717980295567 -> 0.5942930911680911 on epoch=324, global_step=1300
06/02/2022 13:52:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.31 on epoch=327
06/02/2022 13:52:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=329
06/02/2022 13:52:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=332
06/02/2022 13:52:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=334
06/02/2022 13:52:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=337
06/02/2022 13:52:14 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.47944518716577544 on epoch=337
06/02/2022 13:52:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=339
06/02/2022 13:52:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=342
06/02/2022 13:52:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=344
06/02/2022 13:52:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=347
06/02/2022 13:52:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=349
06/02/2022 13:52:21 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.6077233250620346 on epoch=349
06/02/2022 13:52:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5942930911680911 -> 0.6077233250620346 on epoch=349, global_step=1400
06/02/2022 13:52:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=352
06/02/2022 13:52:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=354
06/02/2022 13:52:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=357
06/02/2022 13:52:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=359
06/02/2022 13:52:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=362
06/02/2022 13:52:28 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.5862103174603175 on epoch=362
06/02/2022 13:52:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=364
06/02/2022 13:52:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=367
06/02/2022 13:52:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=369
06/02/2022 13:52:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=372
06/02/2022 13:52:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=374
06/02/2022 13:52:34 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.5623559907834101 on epoch=374
06/02/2022 13:52:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=377
06/02/2022 13:52:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
06/02/2022 13:52:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=382
06/02/2022 13:52:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=384
06/02/2022 13:52:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=387
06/02/2022 13:52:41 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.6122329059829059 on epoch=387
06/02/2022 13:52:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6077233250620346 -> 0.6122329059829059 on epoch=387, global_step=1550
06/02/2022 13:52:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=389
06/02/2022 13:52:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=392
06/02/2022 13:52:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=394
06/02/2022 13:52:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=397
06/02/2022 13:52:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=399
06/02/2022 13:52:48 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6313725490196078 on epoch=399
06/02/2022 13:52:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6122329059829059 -> 0.6313725490196078 on epoch=399, global_step=1600
06/02/2022 13:52:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=402
06/02/2022 13:52:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=404
06/02/2022 13:52:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=407
06/02/2022 13:52:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=409
06/02/2022 13:52:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=412
06/02/2022 13:52:54 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.5241403807899971 on epoch=412
06/02/2022 13:52:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=414
06/02/2022 13:52:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=417
06/02/2022 13:52:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
06/02/2022 13:52:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=422
06/02/2022 13:53:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=424
06/02/2022 13:53:01 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7028179458657767 on epoch=424
06/02/2022 13:53:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6313725490196078 -> 0.7028179458657767 on epoch=424, global_step=1700
06/02/2022 13:53:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=427
06/02/2022 13:53:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=429
06/02/2022 13:53:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=432
06/02/2022 13:53:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=434
06/02/2022 13:53:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=437
06/02/2022 13:53:08 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.6095947864240547 on epoch=437
06/02/2022 13:53:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=439
06/02/2022 13:53:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=442
06/02/2022 13:53:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
06/02/2022 13:53:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
06/02/2022 13:53:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=449
06/02/2022 13:53:15 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.6877314814814814 on epoch=449
06/02/2022 13:53:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=452
06/02/2022 13:53:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
06/02/2022 13:53:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=457
06/02/2022 13:53:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=459
06/02/2022 13:53:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=462
06/02/2022 13:53:21 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.6477990034866096 on epoch=462
06/02/2022 13:53:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=464
06/02/2022 13:53:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=467
06/02/2022 13:53:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=469
06/02/2022 13:53:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=472
06/02/2022 13:53:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=474
06/02/2022 13:53:28 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.6000663423264042 on epoch=474
06/02/2022 13:53:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=477
06/02/2022 13:53:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=479
06/02/2022 13:53:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=482
06/02/2022 13:53:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=484
06/02/2022 13:53:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=487
06/02/2022 13:53:35 - INFO - __main__ - Global step 1950 Train loss 0.06 Classification-F1 0.6462971954858365 on epoch=487
06/02/2022 13:53:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=489
06/02/2022 13:53:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=492
06/02/2022 13:53:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=494
06/02/2022 13:53:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=497
06/02/2022 13:53:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
06/02/2022 13:53:42 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.6352495543672014 on epoch=499
06/02/2022 13:53:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=502
06/02/2022 13:53:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=504
06/02/2022 13:53:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=507
06/02/2022 13:53:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
06/02/2022 13:53:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=512
06/02/2022 13:53:48 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.6705521767813203 on epoch=512
06/02/2022 13:53:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=514
06/02/2022 13:53:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=517
06/02/2022 13:53:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=519
06/02/2022 13:53:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=522
06/02/2022 13:53:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=524
06/02/2022 13:53:55 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.5895348837209302 on epoch=524
06/02/2022 13:53:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=527
06/02/2022 13:53:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=529
06/02/2022 13:53:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=532
06/02/2022 13:54:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=534
06/02/2022 13:54:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/02/2022 13:54:02 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.6251838235294118 on epoch=537
06/02/2022 13:54:03 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
06/02/2022 13:54:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=542
06/02/2022 13:54:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=544
06/02/2022 13:54:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=547
06/02/2022 13:54:08 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=549
06/02/2022 13:54:09 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.6366007834757835 on epoch=549
06/02/2022 13:54:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=552
06/02/2022 13:54:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=554
06/02/2022 13:54:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=557
06/02/2022 13:54:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/02/2022 13:54:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=562
06/02/2022 13:54:15 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.6464433416046318 on epoch=562
06/02/2022 13:54:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=564
06/02/2022 13:54:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=567
06/02/2022 13:54:19 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=569
06/02/2022 13:54:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/02/2022 13:54:22 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
06/02/2022 13:54:22 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.6706257104964002 on epoch=574
06/02/2022 13:54:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=577
06/02/2022 13:54:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
06/02/2022 13:54:26 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
06/02/2022 13:54:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=584
06/02/2022 13:54:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 13:54:29 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.6315826330532214 on epoch=587
06/02/2022 13:54:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=589
06/02/2022 13:54:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=592
06/02/2022 13:54:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=594
06/02/2022 13:54:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=597
06/02/2022 13:54:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=599
06/02/2022 13:54:35 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.6652802535155476 on epoch=599
06/02/2022 13:54:37 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/02/2022 13:54:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=604
06/02/2022 13:54:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=607
06/02/2022 13:54:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=609
06/02/2022 13:54:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=612
06/02/2022 13:54:42 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.6581696998722861 on epoch=612
06/02/2022 13:54:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=614
06/02/2022 13:54:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=617
06/02/2022 13:54:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=619
06/02/2022 13:54:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/02/2022 13:54:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/02/2022 13:54:49 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.6137065875664318 on epoch=624
06/02/2022 13:54:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=627
06/02/2022 13:54:51 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=629
06/02/2022 13:54:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 13:54:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/02/2022 13:54:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/02/2022 13:54:55 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.5661910669975186 on epoch=637
06/02/2022 13:54:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=639
06/02/2022 13:54:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 13:54:59 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=644
06/02/2022 13:55:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 13:55:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=649
06/02/2022 13:55:02 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.5488886242756211 on epoch=649
06/02/2022 13:55:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=652
06/02/2022 13:55:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/02/2022 13:55:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 13:55:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=659
06/02/2022 13:55:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=662
06/02/2022 13:55:09 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.599193470491645 on epoch=662
06/02/2022 13:55:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=664
06/02/2022 13:55:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=667
06/02/2022 13:55:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=669
06/02/2022 13:55:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=672
06/02/2022 13:55:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=674
06/02/2022 13:55:15 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6266789516789517 on epoch=674
06/02/2022 13:55:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=677
06/02/2022 13:55:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=679
06/02/2022 13:55:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/02/2022 13:55:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=684
06/02/2022 13:55:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=687
06/02/2022 13:55:22 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7149976565861453 on epoch=687
06/02/2022 13:55:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7028179458657767 -> 0.7149976565861453 on epoch=687, global_step=2750
06/02/2022 13:55:23 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=689
06/02/2022 13:55:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=692
06/02/2022 13:55:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=694
06/02/2022 13:55:27 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=697
06/02/2022 13:55:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=699
06/02/2022 13:55:29 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.6738815284178187 on epoch=699
06/02/2022 13:55:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=702
06/02/2022 13:55:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=704
06/02/2022 13:55:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=707
06/02/2022 13:55:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=709
06/02/2022 13:55:35 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=712
06/02/2022 13:55:35 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6754734848484848 on epoch=712
06/02/2022 13:55:37 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=714
06/02/2022 13:55:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=717
06/02/2022 13:55:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=719
06/02/2022 13:55:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
06/02/2022 13:55:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 13:55:42 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.6746700879765396 on epoch=724
06/02/2022 13:55:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
06/02/2022 13:55:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=729
06/02/2022 13:55:46 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=732
06/02/2022 13:55:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=734
06/02/2022 13:55:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
06/02/2022 13:55:49 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.662194011617186 on epoch=737
06/02/2022 13:55:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=739
06/02/2022 13:55:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/02/2022 13:55:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=744
06/02/2022 13:55:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
06/02/2022 13:55:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/02/2022 13:55:56 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.6388028740642755 on epoch=749
06/02/2022 13:55:56 - INFO - __main__ - save last model!
06/02/2022 13:55:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 13:55:56 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 13:55:56 - INFO - __main__ - Printing 3 examples
06/02/2022 13:55:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 13:55:56 - INFO - __main__ - ['others']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 13:55:56 - INFO - __main__ - ['others']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 13:55:56 - INFO - __main__ - ['others']
06/02/2022 13:55:56 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:55:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:55:56 - INFO - __main__ - Printing 3 examples
06/02/2022 13:55:56 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:55:56 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:55:56 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:55:56 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:55:56 - INFO - __main__ - Printing 3 examples
06/02/2022 13:55:56 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:55:56 - INFO - __main__ - ['happy']
06/02/2022 13:55:56 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:55:56 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:55:56 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:55:58 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:56:02 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:56:02 - INFO - __main__ - task name: emo
06/02/2022 13:56:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:56:03 - INFO - __main__ - Starting training!
06/02/2022 13:56:03 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 13:56:47 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.3_8_predictions.txt
06/02/2022 13:56:47 - INFO - __main__ - Classification-F1 on test data: 0.4317
06/02/2022 13:56:47 - INFO - __main__ - prefix=emo_16_42, lr=0.3, bsz=8, dev_performance=0.7149976565861453, test_performance=0.43165340428220306
06/02/2022 13:56:47 - INFO - __main__ - Running ... prefix=emo_16_42, lr=0.2, bsz=8 ...
06/02/2022 13:56:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:56:48 - INFO - __main__ - Printing 3 examples
06/02/2022 13:56:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:56:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:56:48 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 13:56:48 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 13:56:48 - INFO - __main__ - Printing 3 examples
06/02/2022 13:56:48 - INFO - __main__ -  [emo] i am happy i love u so much you  love me
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ -  [emo] yes because of shame to shame how and why are you saying shame i laughed because for the sentence you told shame to shame
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ -  [emo] excellent dvd fm 2 on a dvd everybody
06/02/2022 13:56:48 - INFO - __main__ - ['happy']
06/02/2022 13:56:48 - INFO - __main__ - Tokenizing Input ...
06/02/2022 13:56:48 - INFO - __main__ - Tokenizing Output ...
06/02/2022 13:56:48 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 13:56:53 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 13:56:53 - INFO - __main__ - task name: emo
06/02/2022 13:56:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 13:56:54 - INFO - __main__ - Starting training!
06/02/2022 13:56:55 - INFO - __main__ - Step 10 Global step 10 Train loss 7.33 on epoch=2
06/02/2022 13:56:56 - INFO - __main__ - Step 20 Global step 20 Train loss 5.75 on epoch=4
06/02/2022 13:56:57 - INFO - __main__ - Step 30 Global step 30 Train loss 4.16 on epoch=7
06/02/2022 13:56:59 - INFO - __main__ - Step 40 Global step 40 Train loss 3.01 on epoch=9
06/02/2022 13:57:00 - INFO - __main__ - Step 50 Global step 50 Train loss 2.49 on epoch=12
06/02/2022 13:57:00 - INFO - __main__ - Global step 50 Train loss 4.55 Classification-F1 0.23040411286465806 on epoch=12
06/02/2022 13:57:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23040411286465806 on epoch=12, global_step=50
06/02/2022 13:57:02 - INFO - __main__ - Step 60 Global step 60 Train loss 2.17 on epoch=14
06/02/2022 13:57:03 - INFO - __main__ - Step 70 Global step 70 Train loss 1.76 on epoch=17
06/02/2022 13:57:04 - INFO - __main__ - Step 80 Global step 80 Train loss 1.79 on epoch=19
06/02/2022 13:57:05 - INFO - __main__ - Step 90 Global step 90 Train loss 1.47 on epoch=22
06/02/2022 13:57:07 - INFO - __main__ - Step 100 Global step 100 Train loss 1.45 on epoch=24
06/02/2022 13:57:07 - INFO - __main__ - Global step 100 Train loss 1.73 Classification-F1 0.13197586726998492 on epoch=24
06/02/2022 13:57:08 - INFO - __main__ - Step 110 Global step 110 Train loss 1.50 on epoch=27
06/02/2022 13:57:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.48 on epoch=29
06/02/2022 13:57:11 - INFO - __main__ - Step 130 Global step 130 Train loss 1.43 on epoch=32
06/02/2022 13:57:12 - INFO - __main__ - Step 140 Global step 140 Train loss 1.23 on epoch=34
06/02/2022 13:57:13 - INFO - __main__ - Step 150 Global step 150 Train loss 1.38 on epoch=37
06/02/2022 13:57:14 - INFO - __main__ - Global step 150 Train loss 1.41 Classification-F1 0.125 on epoch=37
06/02/2022 13:57:15 - INFO - __main__ - Step 160 Global step 160 Train loss 1.37 on epoch=39
06/02/2022 13:57:16 - INFO - __main__ - Step 170 Global step 170 Train loss 1.26 on epoch=42
06/02/2022 13:57:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.22 on epoch=44
06/02/2022 13:57:19 - INFO - __main__ - Step 190 Global step 190 Train loss 1.08 on epoch=47
06/02/2022 13:57:20 - INFO - __main__ - Step 200 Global step 200 Train loss 1.24 on epoch=49
06/02/2022 13:57:20 - INFO - __main__ - Global step 200 Train loss 1.24 Classification-F1 0.13415384615384615 on epoch=49
06/02/2022 13:57:22 - INFO - __main__ - Step 210 Global step 210 Train loss 1.67 on epoch=52
06/02/2022 13:57:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.29 on epoch=54
06/02/2022 13:57:24 - INFO - __main__ - Step 230 Global step 230 Train loss 1.17 on epoch=57
06/02/2022 13:57:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.09 on epoch=59
06/02/2022 13:57:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.21 on epoch=62
06/02/2022 13:57:27 - INFO - __main__ - Global step 250 Train loss 1.29 Classification-F1 0.3362085769980506 on epoch=62
06/02/2022 13:57:27 - INFO - __main__ - Saving model with best Classification-F1: 0.23040411286465806 -> 0.3362085769980506 on epoch=62, global_step=250
06/02/2022 13:57:28 - INFO - __main__ - Step 260 Global step 260 Train loss 1.35 on epoch=64
06/02/2022 13:57:29 - INFO - __main__ - Step 270 Global step 270 Train loss 1.15 on epoch=67
06/02/2022 13:57:31 - INFO - __main__ - Step 280 Global step 280 Train loss 1.14 on epoch=69
06/02/2022 13:57:32 - INFO - __main__ - Step 290 Global step 290 Train loss 1.11 on epoch=72
06/02/2022 13:57:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.98 on epoch=74
06/02/2022 13:57:33 - INFO - __main__ - Global step 300 Train loss 1.14 Classification-F1 0.1581196581196581 on epoch=74
06/02/2022 13:57:35 - INFO - __main__ - Step 310 Global step 310 Train loss 1.02 on epoch=77
06/02/2022 13:57:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.05 on epoch=79
06/02/2022 13:57:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.05 on epoch=82
06/02/2022 13:57:38 - INFO - __main__ - Step 340 Global step 340 Train loss 1.08 on epoch=84
06/02/2022 13:57:40 - INFO - __main__ - Step 350 Global step 350 Train loss 1.07 on epoch=87
06/02/2022 13:57:40 - INFO - __main__ - Global step 350 Train loss 1.05 Classification-F1 0.1 on epoch=87
06/02/2022 13:57:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.12 on epoch=89
06/02/2022 13:57:43 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=92
06/02/2022 13:57:44 - INFO - __main__ - Step 380 Global step 380 Train loss 1.05 on epoch=94
06/02/2022 13:57:45 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=97
06/02/2022 13:57:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
06/02/2022 13:57:47 - INFO - __main__ - Global step 400 Train loss 1.02 Classification-F1 0.1 on epoch=99
06/02/2022 13:57:48 - INFO - __main__ - Step 410 Global step 410 Train loss 1.00 on epoch=102
06/02/2022 13:57:49 - INFO - __main__ - Step 420 Global step 420 Train loss 1.03 on epoch=104
06/02/2022 13:57:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=107
06/02/2022 13:57:52 - INFO - __main__ - Step 440 Global step 440 Train loss 1.04 on epoch=109
06/02/2022 13:57:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=112
06/02/2022 13:57:53 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.1 on epoch=112
06/02/2022 13:57:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.98 on epoch=114
06/02/2022 13:57:56 - INFO - __main__ - Step 470 Global step 470 Train loss 1.01 on epoch=117
06/02/2022 13:57:57 - INFO - __main__ - Step 480 Global step 480 Train loss 1.07 on epoch=119
06/02/2022 13:57:58 - INFO - __main__ - Step 490 Global step 490 Train loss 1.00 on epoch=122
06/02/2022 13:57:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.86 on epoch=124
06/02/2022 13:58:00 - INFO - __main__ - Global step 500 Train loss 0.98 Classification-F1 0.1 on epoch=124
06/02/2022 13:58:01 - INFO - __main__ - Step 510 Global step 510 Train loss 1.00 on epoch=127
06/02/2022 13:58:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.88 on epoch=129
06/02/2022 13:58:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
06/02/2022 13:58:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.86 on epoch=134
06/02/2022 13:58:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=137
06/02/2022 13:58:07 - INFO - __main__ - Global step 550 Train loss 0.89 Classification-F1 0.1860730593607306 on epoch=137
06/02/2022 13:58:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.92 on epoch=139
06/02/2022 13:58:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.89 on epoch=142
06/02/2022 13:58:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.95 on epoch=144
06/02/2022 13:58:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.93 on epoch=147
06/02/2022 13:58:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.85 on epoch=149
06/02/2022 13:58:13 - INFO - __main__ - Global step 600 Train loss 0.91 Classification-F1 0.18026315789473685 on epoch=149
06/02/2022 13:58:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.86 on epoch=152
06/02/2022 13:58:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.82 on epoch=154
06/02/2022 13:58:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.85 on epoch=157
06/02/2022 13:58:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=159
06/02/2022 13:58:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.79 on epoch=162
06/02/2022 13:58:20 - INFO - __main__ - Global step 650 Train loss 0.85 Classification-F1 0.30801619433198385 on epoch=162
06/02/2022 13:58:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=164
06/02/2022 13:58:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
06/02/2022 13:58:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.84 on epoch=169
06/02/2022 13:58:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.78 on epoch=172
06/02/2022 13:58:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.71 on epoch=174
06/02/2022 13:58:27 - INFO - __main__ - Global step 700 Train loss 0.81 Classification-F1 0.25869565217391305 on epoch=174
06/02/2022 13:58:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.90 on epoch=177
06/02/2022 13:58:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.81 on epoch=179
06/02/2022 13:58:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=182
06/02/2022 13:58:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.87 on epoch=184
06/02/2022 13:58:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.72 on epoch=187
06/02/2022 13:58:33 - INFO - __main__ - Global step 750 Train loss 0.81 Classification-F1 0.37473769168684423 on epoch=187
06/02/2022 13:58:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3362085769980506 -> 0.37473769168684423 on epoch=187, global_step=750
06/02/2022 13:58:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.80 on epoch=189
06/02/2022 13:58:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.73 on epoch=192
06/02/2022 13:58:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.75 on epoch=194
06/02/2022 13:58:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.72 on epoch=197
06/02/2022 13:58:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.82 on epoch=199
06/02/2022 13:58:40 - INFO - __main__ - Global step 800 Train loss 0.76 Classification-F1 0.44639730639730635 on epoch=199
06/02/2022 13:58:40 - INFO - __main__ - Saving model with best Classification-F1: 0.37473769168684423 -> 0.44639730639730635 on epoch=199, global_step=800
06/02/2022 13:58:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.78 on epoch=202
06/02/2022 13:58:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.70 on epoch=204
06/02/2022 13:58:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.68 on epoch=207
06/02/2022 13:58:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
06/02/2022 13:58:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.63 on epoch=212
06/02/2022 13:58:47 - INFO - __main__ - Global step 850 Train loss 0.71 Classification-F1 0.4748452696728559 on epoch=212
06/02/2022 13:58:47 - INFO - __main__ - Saving model with best Classification-F1: 0.44639730639730635 -> 0.4748452696728559 on epoch=212, global_step=850
06/02/2022 13:58:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.75 on epoch=214
06/02/2022 13:58:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=217
06/02/2022 13:58:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.76 on epoch=219
06/02/2022 13:58:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.60 on epoch=222
06/02/2022 13:58:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.72 on epoch=224
06/02/2022 13:58:53 - INFO - __main__ - Global step 900 Train loss 0.68 Classification-F1 0.28529411764705886 on epoch=224
06/02/2022 13:58:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.64 on epoch=227
06/02/2022 13:58:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
06/02/2022 13:58:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.68 on epoch=232
06/02/2022 13:58:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.64 on epoch=234
06/02/2022 13:58:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.66 on epoch=237
06/02/2022 13:59:00 - INFO - __main__ - Global step 950 Train loss 0.68 Classification-F1 0.4977671451355662 on epoch=237
06/02/2022 13:59:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4748452696728559 -> 0.4977671451355662 on epoch=237, global_step=950
06/02/2022 13:59:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.68 on epoch=239
06/02/2022 13:59:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.60 on epoch=242
06/02/2022 13:59:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.61 on epoch=244
06/02/2022 13:59:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=247
06/02/2022 13:59:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.63 on epoch=249
06/02/2022 13:59:07 - INFO - __main__ - Global step 1000 Train loss 0.65 Classification-F1 0.4481406355645707 on epoch=249
06/02/2022 13:59:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.62 on epoch=252
06/02/2022 13:59:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.55 on epoch=254
06/02/2022 13:59:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.56 on epoch=257
06/02/2022 13:59:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.62 on epoch=259
06/02/2022 13:59:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=262
06/02/2022 13:59:13 - INFO - __main__ - Global step 1050 Train loss 0.58 Classification-F1 0.4827403846153846 on epoch=262
06/02/2022 13:59:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.57 on epoch=264
06/02/2022 13:59:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.56 on epoch=267
06/02/2022 13:59:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.67 on epoch=269
06/02/2022 13:59:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=272
06/02/2022 13:59:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.62 on epoch=274
06/02/2022 13:59:20 - INFO - __main__ - Global step 1100 Train loss 0.58 Classification-F1 0.4570535714285714 on epoch=274
06/02/2022 13:59:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=277
06/02/2022 13:59:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.69 on epoch=279
06/02/2022 13:59:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.57 on epoch=282
06/02/2022 13:59:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.60 on epoch=284
06/02/2022 13:59:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=287
06/02/2022 13:59:27 - INFO - __main__ - Global step 1150 Train loss 0.57 Classification-F1 0.4894964953755546 on epoch=287
06/02/2022 13:59:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=289
06/02/2022 13:59:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=292
06/02/2022 13:59:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.54 on epoch=294
06/02/2022 13:59:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.57 on epoch=297
06/02/2022 13:59:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=299
06/02/2022 13:59:33 - INFO - __main__ - Global step 1200 Train loss 0.55 Classification-F1 0.4996124031007752 on epoch=299
06/02/2022 13:59:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4977671451355662 -> 0.4996124031007752 on epoch=299, global_step=1200
06/02/2022 13:59:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=302
06/02/2022 13:59:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.59 on epoch=304
06/02/2022 13:59:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=307
06/02/2022 13:59:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.51 on epoch=309
06/02/2022 13:59:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.64 on epoch=312
06/02/2022 13:59:40 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.4426688815060908 on epoch=312
06/02/2022 13:59:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.55 on epoch=314
06/02/2022 13:59:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.55 on epoch=317
06/02/2022 13:59:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.67 on epoch=319
06/02/2022 13:59:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=322
06/02/2022 13:59:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=324
06/02/2022 13:59:47 - INFO - __main__ - Global step 1300 Train loss 0.56 Classification-F1 0.4904729269520558 on epoch=324
06/02/2022 13:59:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=327
06/02/2022 13:59:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=329
06/02/2022 13:59:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=332
06/02/2022 13:59:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=334
06/02/2022 13:59:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=337
06/02/2022 13:59:53 - INFO - __main__ - Global step 1350 Train loss 0.49 Classification-F1 0.46131316663752403 on epoch=337
06/02/2022 13:59:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=339
06/02/2022 13:59:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=342
06/02/2022 13:59:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=344
06/02/2022 13:59:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=347
06/02/2022 13:59:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.68 on epoch=349
06/02/2022 14:00:00 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.50447261663286 on epoch=349
06/02/2022 14:00:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4996124031007752 -> 0.50447261663286 on epoch=349, global_step=1400
06/02/2022 14:00:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=352
06/02/2022 14:00:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.55 on epoch=354
06/02/2022 14:00:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=357
06/02/2022 14:00:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=359
06/02/2022 14:00:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=362
06/02/2022 14:00:07 - INFO - __main__ - Global step 1450 Train loss 0.50 Classification-F1 0.4360431235431236 on epoch=362
06/02/2022 14:00:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=364
06/02/2022 14:00:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=367
06/02/2022 14:00:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=369
06/02/2022 14:00:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=372
06/02/2022 14:00:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=374
06/02/2022 14:00:13 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.4460710821060058 on epoch=374
06/02/2022 14:00:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=377
06/02/2022 14:00:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=379
06/02/2022 14:00:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=382
06/02/2022 14:00:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=384
06/02/2022 14:00:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=387
06/02/2022 14:00:20 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.44002516586593454 on epoch=387
06/02/2022 14:00:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=389
06/02/2022 14:00:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=392
06/02/2022 14:00:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=394
06/02/2022 14:00:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=397
06/02/2022 14:00:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=399
06/02/2022 14:00:27 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.5808646276938959 on epoch=399
06/02/2022 14:00:27 - INFO - __main__ - Saving model with best Classification-F1: 0.50447261663286 -> 0.5808646276938959 on epoch=399, global_step=1600
06/02/2022 14:00:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=402
06/02/2022 14:00:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=404
06/02/2022 14:00:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=407
06/02/2022 14:00:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=409
06/02/2022 14:00:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=412
06/02/2022 14:00:33 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.5275133398011157 on epoch=412
06/02/2022 14:00:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=414
06/02/2022 14:00:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=417
06/02/2022 14:00:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=419
06/02/2022 14:00:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=422
06/02/2022 14:00:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=424
06/02/2022 14:00:40 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.5514652014652015 on epoch=424
06/02/2022 14:00:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=427
06/02/2022 14:00:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=429
06/02/2022 14:00:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=432
06/02/2022 14:00:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=434
06/02/2022 14:00:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=437
06/02/2022 14:00:47 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.6003366003366003 on epoch=437
06/02/2022 14:00:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5808646276938959 -> 0.6003366003366003 on epoch=437, global_step=1750
06/02/2022 14:00:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=439
06/02/2022 14:00:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=442
06/02/2022 14:00:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=444
06/02/2022 14:00:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=447
06/02/2022 14:00:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=449
06/02/2022 14:00:53 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.589957264957265 on epoch=449
06/02/2022 14:00:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=452
06/02/2022 14:00:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=454
06/02/2022 14:00:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=457
06/02/2022 14:00:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=459
06/02/2022 14:00:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=462
06/02/2022 14:01:00 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.5808333333333333 on epoch=462
06/02/2022 14:01:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=464
06/02/2022 14:01:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=467
06/02/2022 14:01:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=469
06/02/2022 14:01:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=472
06/02/2022 14:01:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=474
06/02/2022 14:01:07 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.5605098633115875 on epoch=474
06/02/2022 14:01:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=477
06/02/2022 14:01:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=479
06/02/2022 14:01:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=482
06/02/2022 14:01:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=484
06/02/2022 14:01:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=487
06/02/2022 14:01:13 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.5396161740558292 on epoch=487
06/02/2022 14:01:15 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=489
06/02/2022 14:01:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=492
06/02/2022 14:01:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=494
06/02/2022 14:01:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=497
06/02/2022 14:01:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=499
06/02/2022 14:01:20 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.5860820334504544 on epoch=499
06/02/2022 14:01:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=502
06/02/2022 14:01:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=504
06/02/2022 14:01:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=507
06/02/2022 14:01:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=509
06/02/2022 14:01:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=512
06/02/2022 14:01:26 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.5508520215416767 on epoch=512
06/02/2022 14:01:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=514
06/02/2022 14:01:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.32 on epoch=517
06/02/2022 14:01:30 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=519
06/02/2022 14:01:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=522
06/02/2022 14:01:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
06/02/2022 14:01:33 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.5910897635035566 on epoch=524
06/02/2022 14:01:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=527
06/02/2022 14:01:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=529
06/02/2022 14:01:37 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=532
06/02/2022 14:01:38 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=534
06/02/2022 14:01:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=537
06/02/2022 14:01:40 - INFO - __main__ - Global step 2150 Train loss 0.11 Classification-F1 0.5583287416120999 on epoch=537
06/02/2022 14:01:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=539
06/02/2022 14:01:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=542
06/02/2022 14:01:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=544
06/02/2022 14:01:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=547
06/02/2022 14:01:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=549
06/02/2022 14:01:46 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.5884646962233169 on epoch=549
06/02/2022 14:01:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=552
06/02/2022 14:01:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=554
06/02/2022 14:01:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=557
06/02/2022 14:01:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
06/02/2022 14:01:52 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=562
06/02/2022 14:01:53 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.5746871109774335 on epoch=562
06/02/2022 14:01:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=564
06/02/2022 14:01:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=567
06/02/2022 14:01:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=569
06/02/2022 14:01:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=572
06/02/2022 14:01:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=574
06/02/2022 14:02:00 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.5202442485051181 on epoch=574
06/02/2022 14:02:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
06/02/2022 14:02:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=579
06/02/2022 14:02:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=582
06/02/2022 14:02:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=584
06/02/2022 14:02:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 14:02:06 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.5845460845460846 on epoch=587
06/02/2022 14:02:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=589
06/02/2022 14:02:09 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=592
06/02/2022 14:02:10 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=594
06/02/2022 14:02:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=597
06/02/2022 14:02:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/02/2022 14:02:13 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.594176245210728 on epoch=599
06/02/2022 14:02:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/02/2022 14:02:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=604
06/02/2022 14:02:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=607
06/02/2022 14:02:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=609
06/02/2022 14:02:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=612
06/02/2022 14:02:20 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.5644059795436664 on epoch=612
06/02/2022 14:02:21 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/02/2022 14:02:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/02/2022 14:02:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=619
06/02/2022 14:02:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.12 on epoch=622
06/02/2022 14:02:26 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=624
06/02/2022 14:02:26 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.5903518641246552 on epoch=624
06/02/2022 14:02:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/02/2022 14:02:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=629
06/02/2022 14:02:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=632
06/02/2022 14:02:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/02/2022 14:02:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=637
06/02/2022 14:02:33 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.6098818507049875 on epoch=637
06/02/2022 14:02:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6003366003366003 -> 0.6098818507049875 on epoch=637, global_step=2550
06/02/2022 14:02:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=639
06/02/2022 14:02:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=642
06/02/2022 14:02:37 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/02/2022 14:02:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 14:02:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=649
06/02/2022 14:02:40 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.608893200743795 on epoch=649
06/02/2022 14:02:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=652
06/02/2022 14:02:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=654
06/02/2022 14:02:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 14:02:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=659
06/02/2022 14:02:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=662
06/02/2022 14:02:46 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.5660384331116038 on epoch=662
06/02/2022 14:02:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=664
06/02/2022 14:02:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/02/2022 14:02:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=669
06/02/2022 14:02:51 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=672
06/02/2022 14:02:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=674
06/02/2022 14:02:53 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.6122652063828534 on epoch=674
06/02/2022 14:02:53 - INFO - __main__ - Saving model with best Classification-F1: 0.6098818507049875 -> 0.6122652063828534 on epoch=674, global_step=2700
06/02/2022 14:02:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
06/02/2022 14:02:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=679
06/02/2022 14:02:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=682
06/02/2022 14:02:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=684
06/02/2022 14:02:59 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/02/2022 14:03:00 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.5404014262709915 on epoch=687
06/02/2022 14:03:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=689
06/02/2022 14:03:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=692
06/02/2022 14:03:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/02/2022 14:03:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=697
06/02/2022 14:03:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
06/02/2022 14:03:06 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.5916008248857085 on epoch=699
06/02/2022 14:03:07 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
06/02/2022 14:03:09 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=704
06/02/2022 14:03:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=707
06/02/2022 14:03:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=709
06/02/2022 14:03:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=712
06/02/2022 14:03:13 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5568943943943944 on epoch=712
06/02/2022 14:03:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=714
06/02/2022 14:03:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=717
06/02/2022 14:03:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=719
06/02/2022 14:03:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
06/02/2022 14:03:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=724
06/02/2022 14:03:20 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.628906955736224 on epoch=724
06/02/2022 14:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6122652063828534 -> 0.628906955736224 on epoch=724, global_step=2900
06/02/2022 14:03:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=727
06/02/2022 14:03:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=729
06/02/2022 14:03:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/02/2022 14:03:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=734
06/02/2022 14:03:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=737
06/02/2022 14:03:26 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.5808230293455423 on epoch=737
06/02/2022 14:03:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=739
06/02/2022 14:03:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=742
06/02/2022 14:03:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=744
06/02/2022 14:03:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/02/2022 14:03:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=749
06/02/2022 14:03:33 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.6287660256410257 on epoch=749
06/02/2022 14:03:33 - INFO - __main__ - save last model!
06/02/2022 14:03:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 14:03:33 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 14:03:33 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 14:03:33 - INFO - __main__ - ['others']
06/02/2022 14:03:33 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 14:03:33 - INFO - __main__ - ['others']
06/02/2022 14:03:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 14:03:33 - INFO - __main__ - ['others']
06/02/2022 14:03:33 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:03:34 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:34 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:34 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:03:34 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:03:34 - INFO - __main__ - Printing 3 examples
06/02/2022 14:03:34 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:03:34 - INFO - __main__ - ['others']
06/02/2022 14:03:34 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:03:34 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:34 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:03:35 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:03:40 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:03:40 - INFO - __main__ - task name: emo
06/02/2022 14:03:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:03:40 - INFO - __main__ - Starting training!
06/02/2022 14:03:41 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 14:04:24 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_42_0.2_8_predictions.txt
06/02/2022 14:04:24 - INFO - __main__ - Classification-F1 on test data: 0.3631
06/02/2022 14:04:24 - INFO - __main__ - prefix=emo_16_42, lr=0.2, bsz=8, dev_performance=0.628906955736224, test_performance=0.3631340472735185
06/02/2022 14:04:24 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.5, bsz=8 ...
06/02/2022 14:04:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:04:25 - INFO - __main__ - Printing 3 examples
06/02/2022 14:04:25 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:04:25 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:04:25 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:04:25 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:04:25 - INFO - __main__ - Printing 3 examples
06/02/2022 14:04:25 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:04:25 - INFO - __main__ - ['others']
06/02/2022 14:04:25 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:04:25 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:04:25 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:04:31 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:04:31 - INFO - __main__ - task name: emo
06/02/2022 14:04:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:04:31 - INFO - __main__ - Starting training!
06/02/2022 14:04:33 - INFO - __main__ - Step 10 Global step 10 Train loss 6.00 on epoch=2
06/02/2022 14:04:34 - INFO - __main__ - Step 20 Global step 20 Train loss 2.56 on epoch=4
06/02/2022 14:04:35 - INFO - __main__ - Step 30 Global step 30 Train loss 1.85 on epoch=7
06/02/2022 14:04:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.56 on epoch=9
06/02/2022 14:04:38 - INFO - __main__ - Step 50 Global step 50 Train loss 1.29 on epoch=12
06/02/2022 14:04:38 - INFO - __main__ - Global step 50 Train loss 2.65 Classification-F1 0.1 on epoch=12
06/02/2022 14:04:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=12, global_step=50
06/02/2022 14:04:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=14
06/02/2022 14:04:41 - INFO - __main__ - Step 70 Global step 70 Train loss 1.12 on epoch=17
06/02/2022 14:04:42 - INFO - __main__ - Step 80 Global step 80 Train loss 1.13 on epoch=19
06/02/2022 14:04:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=22
06/02/2022 14:04:45 - INFO - __main__ - Step 100 Global step 100 Train loss 1.15 on epoch=24
06/02/2022 14:04:45 - INFO - __main__ - Global step 100 Train loss 1.10 Classification-F1 0.23529411764705882 on epoch=24
06/02/2022 14:04:45 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.23529411764705882 on epoch=24, global_step=100
06/02/2022 14:04:46 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=27
06/02/2022 14:04:48 - INFO - __main__ - Step 120 Global step 120 Train loss 1.09 on epoch=29
06/02/2022 14:04:49 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=32
06/02/2022 14:04:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=34
06/02/2022 14:04:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=37
06/02/2022 14:04:52 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.1 on epoch=37
06/02/2022 14:04:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.01 on epoch=39
06/02/2022 14:04:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.97 on epoch=42
06/02/2022 14:04:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=44
06/02/2022 14:04:57 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=47
06/02/2022 14:04:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=49
06/02/2022 14:04:59 - INFO - __main__ - Global step 200 Train loss 0.97 Classification-F1 0.09493670886075949 on epoch=49
06/02/2022 14:05:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.97 on epoch=52
06/02/2022 14:05:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=54
06/02/2022 14:05:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.91 on epoch=57
06/02/2022 14:05:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=59
06/02/2022 14:05:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=62
06/02/2022 14:05:05 - INFO - __main__ - Global step 250 Train loss 0.93 Classification-F1 0.1 on epoch=62
06/02/2022 14:05:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=64
06/02/2022 14:05:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=67
06/02/2022 14:05:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.97 on epoch=69
06/02/2022 14:05:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=72
06/02/2022 14:05:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=74
06/02/2022 14:05:12 - INFO - __main__ - Global step 300 Train loss 0.89 Classification-F1 0.30243161094224924 on epoch=74
06/02/2022 14:05:12 - INFO - __main__ - Saving model with best Classification-F1: 0.23529411764705882 -> 0.30243161094224924 on epoch=74, global_step=300
06/02/2022 14:05:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.85 on epoch=77
06/02/2022 14:05:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=79
06/02/2022 14:05:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=82
06/02/2022 14:05:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=84
06/02/2022 14:05:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.85 on epoch=87
06/02/2022 14:05:19 - INFO - __main__ - Global step 350 Train loss 0.84 Classification-F1 0.25146713615023475 on epoch=87
06/02/2022 14:05:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=89
06/02/2022 14:05:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=92
06/02/2022 14:05:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.73 on epoch=94
06/02/2022 14:05:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=97
06/02/2022 14:05:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=99
06/02/2022 14:05:26 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.3693926846100759 on epoch=99
06/02/2022 14:05:26 - INFO - __main__ - Saving model with best Classification-F1: 0.30243161094224924 -> 0.3693926846100759 on epoch=99, global_step=400
06/02/2022 14:05:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=102
06/02/2022 14:05:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=104
06/02/2022 14:05:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=107
06/02/2022 14:05:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.73 on epoch=109
06/02/2022 14:05:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=112
06/02/2022 14:05:32 - INFO - __main__ - Global step 450 Train loss 0.75 Classification-F1 0.5256011730205279 on epoch=112
06/02/2022 14:05:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3693926846100759 -> 0.5256011730205279 on epoch=112, global_step=450
06/02/2022 14:05:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=114
06/02/2022 14:05:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=117
06/02/2022 14:05:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.65 on epoch=119
06/02/2022 14:05:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=122
06/02/2022 14:05:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=124
06/02/2022 14:05:39 - INFO - __main__ - Global step 500 Train loss 0.65 Classification-F1 0.43770146520146525 on epoch=124
06/02/2022 14:05:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.70 on epoch=127
06/02/2022 14:05:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=129
06/02/2022 14:05:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=132
06/02/2022 14:05:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.59 on epoch=134
06/02/2022 14:05:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=137
06/02/2022 14:05:46 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.6362503622138511 on epoch=137
06/02/2022 14:05:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5256011730205279 -> 0.6362503622138511 on epoch=137, global_step=550
06/02/2022 14:05:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=139
06/02/2022 14:05:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=142
06/02/2022 14:05:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=144
06/02/2022 14:05:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=147
06/02/2022 14:05:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=149
06/02/2022 14:05:53 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.5696218866950574 on epoch=149
06/02/2022 14:05:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=152
06/02/2022 14:05:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=154
06/02/2022 14:05:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=157
06/02/2022 14:05:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=159
06/02/2022 14:05:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=162
06/02/2022 14:06:00 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.5788206536845375 on epoch=162
06/02/2022 14:06:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=164
06/02/2022 14:06:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=167
06/02/2022 14:06:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=169
06/02/2022 14:06:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=172
06/02/2022 14:06:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=174
06/02/2022 14:06:06 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6240589198036006 on epoch=174
06/02/2022 14:06:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=177
06/02/2022 14:06:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=179
06/02/2022 14:06:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=182
06/02/2022 14:06:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=184
06/02/2022 14:06:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=187
06/02/2022 14:06:13 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.6534873188405798 on epoch=187
06/02/2022 14:06:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6362503622138511 -> 0.6534873188405798 on epoch=187, global_step=750
06/02/2022 14:06:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=189
06/02/2022 14:06:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=192
06/02/2022 14:06:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=194
06/02/2022 14:06:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=197
06/02/2022 14:06:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=199
06/02/2022 14:06:20 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.6971108326371485 on epoch=199
06/02/2022 14:06:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6534873188405798 -> 0.6971108326371485 on epoch=199, global_step=800
06/02/2022 14:06:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=202
06/02/2022 14:06:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=204
06/02/2022 14:06:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=207
06/02/2022 14:06:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.15 on epoch=209
06/02/2022 14:06:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=212
06/02/2022 14:06:27 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.6976453726453726 on epoch=212
06/02/2022 14:06:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6971108326371485 -> 0.6976453726453726 on epoch=212, global_step=850
06/02/2022 14:06:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=214
06/02/2022 14:06:30 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=217
06/02/2022 14:06:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.13 on epoch=219
06/02/2022 14:06:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.15 on epoch=222
06/02/2022 14:06:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=224
06/02/2022 14:06:34 - INFO - __main__ - Global step 900 Train loss 0.15 Classification-F1 0.7237191898703055 on epoch=224
06/02/2022 14:06:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6976453726453726 -> 0.7237191898703055 on epoch=224, global_step=900
06/02/2022 14:06:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=227
06/02/2022 14:06:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=229
06/02/2022 14:06:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=232
06/02/2022 14:06:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=234
06/02/2022 14:06:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=237
06/02/2022 14:06:41 - INFO - __main__ - Global step 950 Train loss 0.13 Classification-F1 0.6448323323323324 on epoch=237
06/02/2022 14:06:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=239
06/02/2022 14:06:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=242
06/02/2022 14:06:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=244
06/02/2022 14:06:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=247
06/02/2022 14:06:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=249
06/02/2022 14:06:47 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.681699586333207 on epoch=249
06/02/2022 14:06:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=252
06/02/2022 14:06:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=254
06/02/2022 14:06:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=257
06/02/2022 14:06:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=259
06/02/2022 14:06:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=262
06/02/2022 14:06:54 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.7413458245111471 on epoch=262
06/02/2022 14:06:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7237191898703055 -> 0.7413458245111471 on epoch=262, global_step=1050
06/02/2022 14:06:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.05 on epoch=264
06/02/2022 14:06:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=267
06/02/2022 14:06:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=269
06/02/2022 14:06:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=272
06/02/2022 14:07:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=274
06/02/2022 14:07:01 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.6960562240873698 on epoch=274
06/02/2022 14:07:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=277
06/02/2022 14:07:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.82 on epoch=279
06/02/2022 14:07:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=282
06/02/2022 14:07:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=284
06/02/2022 14:07:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=287
06/02/2022 14:07:08 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.7172799422799423 on epoch=287
06/02/2022 14:07:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=289
06/02/2022 14:07:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=292
06/02/2022 14:07:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=294
06/02/2022 14:07:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=297
06/02/2022 14:07:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=299
06/02/2022 14:07:15 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.725094696969697 on epoch=299
06/02/2022 14:07:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=302
06/02/2022 14:07:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.08 on epoch=304
06/02/2022 14:07:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=307
06/02/2022 14:07:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=309
06/02/2022 14:07:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=312
06/02/2022 14:07:22 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7251838235294118 on epoch=312
06/02/2022 14:07:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=314
06/02/2022 14:07:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=317
06/02/2022 14:07:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=319
06/02/2022 14:07:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=322
06/02/2022 14:07:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=324
06/02/2022 14:07:28 - INFO - __main__ - Global step 1300 Train loss 0.10 Classification-F1 0.7730654761904763 on epoch=324
06/02/2022 14:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7413458245111471 -> 0.7730654761904763 on epoch=324, global_step=1300
06/02/2022 14:07:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=327
06/02/2022 14:07:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=329
06/02/2022 14:07:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=332
06/02/2022 14:07:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=334
06/02/2022 14:07:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=337
06/02/2022 14:07:35 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7094438188188188 on epoch=337
06/02/2022 14:07:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=339
06/02/2022 14:07:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=342
06/02/2022 14:07:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=344
06/02/2022 14:07:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=347
06/02/2022 14:07:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=349
06/02/2022 14:07:42 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.6929050638728058 on epoch=349
06/02/2022 14:07:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=352
06/02/2022 14:07:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=354
06/02/2022 14:07:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=357
06/02/2022 14:07:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=359
06/02/2022 14:07:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=362
06/02/2022 14:07:49 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7074674317617865 on epoch=362
06/02/2022 14:07:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=364
06/02/2022 14:07:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=367
06/02/2022 14:07:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=369
06/02/2022 14:07:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=372
06/02/2022 14:07:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=374
06/02/2022 14:07:56 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.7343693693693694 on epoch=374
06/02/2022 14:07:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=377
06/02/2022 14:07:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=379
06/02/2022 14:07:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=382
06/02/2022 14:08:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=384
06/02/2022 14:08:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=387
06/02/2022 14:08:02 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.7226088056680162 on epoch=387
06/02/2022 14:08:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=389
06/02/2022 14:08:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=392
06/02/2022 14:08:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=394
06/02/2022 14:08:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=397
06/02/2022 14:08:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=399
06/02/2022 14:08:09 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.7359047202797202 on epoch=399
06/02/2022 14:08:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=402
06/02/2022 14:08:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=404
06/02/2022 14:08:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=407
06/02/2022 14:08:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=409
06/02/2022 14:08:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=412
06/02/2022 14:08:16 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.7080849141824751 on epoch=412
06/02/2022 14:08:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=414
06/02/2022 14:08:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=417
06/02/2022 14:08:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=419
06/02/2022 14:08:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=422
06/02/2022 14:08:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=424
06/02/2022 14:08:23 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7786585015237768 on epoch=424
06/02/2022 14:08:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7730654761904763 -> 0.7786585015237768 on epoch=424, global_step=1700
06/02/2022 14:08:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=427
06/02/2022 14:08:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=429
06/02/2022 14:08:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=432
06/02/2022 14:08:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=434
06/02/2022 14:08:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/02/2022 14:08:30 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.7598400650582814 on epoch=437
06/02/2022 14:08:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=439
06/02/2022 14:08:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
06/02/2022 14:08:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=444
06/02/2022 14:08:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=447
06/02/2022 14:08:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=449
06/02/2022 14:08:36 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.735356926799758 on epoch=449
06/02/2022 14:08:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=452
06/02/2022 14:08:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=454
06/02/2022 14:08:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=457
06/02/2022 14:08:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=459
06/02/2022 14:08:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=462
06/02/2022 14:08:43 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.730179403139555 on epoch=462
06/02/2022 14:08:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=464
06/02/2022 14:08:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=467
06/02/2022 14:08:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=469
06/02/2022 14:08:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=472
06/02/2022 14:08:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=474
06/02/2022 14:08:50 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.742189956297554 on epoch=474
06/02/2022 14:08:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=477
06/02/2022 14:08:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=479
06/02/2022 14:08:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=482
06/02/2022 14:08:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=484
06/02/2022 14:08:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=487
06/02/2022 14:08:57 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.7407874960057123 on epoch=487
06/02/2022 14:08:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=489
06/02/2022 14:09:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=492
06/02/2022 14:09:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=494
06/02/2022 14:09:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=497
06/02/2022 14:09:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=499
06/02/2022 14:09:04 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7222964943553178 on epoch=499
06/02/2022 14:09:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=502
06/02/2022 14:09:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=504
06/02/2022 14:09:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=507
06/02/2022 14:09:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=509
06/02/2022 14:09:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=512
06/02/2022 14:09:11 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7716310160427807 on epoch=512
06/02/2022 14:09:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/02/2022 14:09:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.02 on epoch=517
06/02/2022 14:09:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=519
06/02/2022 14:09:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=522
06/02/2022 14:09:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=524
06/02/2022 14:09:18 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7567353330220977 on epoch=524
06/02/2022 14:09:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=527
06/02/2022 14:09:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=529
06/02/2022 14:09:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=532
06/02/2022 14:09:23 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=534
06/02/2022 14:09:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=537
06/02/2022 14:09:24 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.729488416988417 on epoch=537
06/02/2022 14:09:26 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=539
06/02/2022 14:09:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=542
06/02/2022 14:09:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=544
06/02/2022 14:09:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=547
06/02/2022 14:09:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=549
06/02/2022 14:09:31 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.6863435110114428 on epoch=549
06/02/2022 14:09:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=552
06/02/2022 14:09:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=554
06/02/2022 14:09:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
06/02/2022 14:09:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/02/2022 14:09:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=562
06/02/2022 14:09:38 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7294976205277153 on epoch=562
06/02/2022 14:09:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/02/2022 14:09:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=567
06/02/2022 14:09:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=569
06/02/2022 14:09:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/02/2022 14:09:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=574
06/02/2022 14:09:45 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7078815397126592 on epoch=574
06/02/2022 14:09:46 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=577
06/02/2022 14:09:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=579
06/02/2022 14:09:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=582
06/02/2022 14:09:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=584
06/02/2022 14:09:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=587
06/02/2022 14:09:52 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7019628647214855 on epoch=587
06/02/2022 14:09:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=589
06/02/2022 14:09:54 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=592
06/02/2022 14:09:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=594
06/02/2022 14:09:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.00 on epoch=597
06/02/2022 14:09:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=599
06/02/2022 14:09:59 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7261042003985553 on epoch=599
06/02/2022 14:10:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=602
06/02/2022 14:10:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=604
06/02/2022 14:10:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=607
06/02/2022 14:10:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=609
06/02/2022 14:10:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/02/2022 14:10:05 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.741014391014391 on epoch=612
06/02/2022 14:10:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=614
06/02/2022 14:10:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=617
06/02/2022 14:10:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/02/2022 14:10:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=622
06/02/2022 14:10:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=624
06/02/2022 14:10:12 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7287562532344316 on epoch=624
06/02/2022 14:10:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=627
06/02/2022 14:10:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.00 on epoch=629
06/02/2022 14:10:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=632
06/02/2022 14:10:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=634
06/02/2022 14:10:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=637
06/02/2022 14:10:19 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7294372294372294 on epoch=637
06/02/2022 14:10:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 14:10:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/02/2022 14:10:23 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=644
06/02/2022 14:10:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.00 on epoch=647
06/02/2022 14:10:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=649
06/02/2022 14:10:26 - INFO - __main__ - Global step 2600 Train loss 0.01 Classification-F1 0.7011429879076938 on epoch=649
06/02/2022 14:10:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/02/2022 14:10:28 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.00 on epoch=654
06/02/2022 14:10:30 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=657
06/02/2022 14:10:31 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 14:10:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=662
06/02/2022 14:10:33 - INFO - __main__ - Global step 2650 Train loss 0.01 Classification-F1 0.7174789915966387 on epoch=662
06/02/2022 14:10:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.00 on epoch=664
06/02/2022 14:10:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=667
06/02/2022 14:10:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=669
06/02/2022 14:10:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=672
06/02/2022 14:10:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/02/2022 14:10:40 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7144940476190477 on epoch=674
06/02/2022 14:10:41 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
06/02/2022 14:10:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=679
06/02/2022 14:10:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=682
06/02/2022 14:10:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=684
06/02/2022 14:10:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=687
06/02/2022 14:10:46 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7431245225362872 on epoch=687
06/02/2022 14:10:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=689
06/02/2022 14:10:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/02/2022 14:10:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=694
06/02/2022 14:10:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=697
06/02/2022 14:10:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=699
06/02/2022 14:10:53 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7222612691362691 on epoch=699
06/02/2022 14:10:54 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=702
06/02/2022 14:10:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=704
06/02/2022 14:10:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/02/2022 14:10:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/02/2022 14:11:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=712
06/02/2022 14:11:00 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7607071830653589 on epoch=712
06/02/2022 14:11:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=714
06/02/2022 14:11:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/02/2022 14:11:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=719
06/02/2022 14:11:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.00 on epoch=722
06/02/2022 14:11:06 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
06/02/2022 14:11:07 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7327976190476191 on epoch=724
06/02/2022 14:11:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=727
06/02/2022 14:11:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=729
06/02/2022 14:11:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=732
06/02/2022 14:11:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/02/2022 14:11:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=737
06/02/2022 14:11:14 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7308712121212121 on epoch=737
06/02/2022 14:11:15 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=739
06/02/2022 14:11:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=742
06/02/2022 14:11:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/02/2022 14:11:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=747
06/02/2022 14:11:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=749
06/02/2022 14:11:21 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7497556207233625 on epoch=749
06/02/2022 14:11:21 - INFO - __main__ - save last model!
06/02/2022 14:11:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 14:11:21 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 14:11:21 - INFO - __main__ - Printing 3 examples
06/02/2022 14:11:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:11:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:11:21 - INFO - __main__ - Printing 3 examples
06/02/2022 14:11:21 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:11:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:11:21 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:11:21 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:11:21 - INFO - __main__ - Printing 3 examples
06/02/2022 14:11:21 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:11:21 - INFO - __main__ - ['others']
06/02/2022 14:11:21 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:11:21 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:11:21 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:11:23 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:11:27 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:11:27 - INFO - __main__ - task name: emo
06/02/2022 14:11:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:11:28 - INFO - __main__ - Starting training!
06/02/2022 14:11:28 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 14:12:11 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.5_8_predictions.txt
06/02/2022 14:12:11 - INFO - __main__ - Classification-F1 on test data: 0.5439
06/02/2022 14:12:11 - INFO - __main__ - prefix=emo_16_87, lr=0.5, bsz=8, dev_performance=0.7786585015237768, test_performance=0.5439438445845611
06/02/2022 14:12:11 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.4, bsz=8 ...
06/02/2022 14:12:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:12:12 - INFO - __main__ - Printing 3 examples
06/02/2022 14:12:12 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:12:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:12:12 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:12:12 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:12:12 - INFO - __main__ - Printing 3 examples
06/02/2022 14:12:12 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:12:12 - INFO - __main__ - ['others']
06/02/2022 14:12:12 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:12:12 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:12:12 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:12:18 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:12:18 - INFO - __main__ - task name: emo
06/02/2022 14:12:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:12:18 - INFO - __main__ - Starting training!
06/02/2022 14:12:19 - INFO - __main__ - Step 10 Global step 10 Train loss 7.25 on epoch=2
06/02/2022 14:12:21 - INFO - __main__ - Step 20 Global step 20 Train loss 4.56 on epoch=4
06/02/2022 14:12:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.37 on epoch=7
06/02/2022 14:12:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.73 on epoch=9
06/02/2022 14:12:24 - INFO - __main__ - Step 50 Global step 50 Train loss 1.50 on epoch=12
06/02/2022 14:12:25 - INFO - __main__ - Global step 50 Train loss 3.48 Classification-F1 0.16 on epoch=12
06/02/2022 14:12:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16 on epoch=12, global_step=50
06/02/2022 14:12:26 - INFO - __main__ - Step 60 Global step 60 Train loss 1.21 on epoch=14
06/02/2022 14:12:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.14 on epoch=17
06/02/2022 14:12:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.19 on epoch=19
06/02/2022 14:12:30 - INFO - __main__ - Step 90 Global step 90 Train loss 1.11 on epoch=22
06/02/2022 14:12:31 - INFO - __main__ - Step 100 Global step 100 Train loss 1.11 on epoch=24
06/02/2022 14:12:31 - INFO - __main__ - Global step 100 Train loss 1.15 Classification-F1 0.22863666014350945 on epoch=24
06/02/2022 14:12:31 - INFO - __main__ - Saving model with best Classification-F1: 0.16 -> 0.22863666014350945 on epoch=24, global_step=100
06/02/2022 14:12:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.10 on epoch=27
06/02/2022 14:12:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.10 on epoch=29
06/02/2022 14:12:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=32
06/02/2022 14:12:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.29 on epoch=34
06/02/2022 14:12:37 - INFO - __main__ - Step 150 Global step 150 Train loss 2.28 on epoch=37
06/02/2022 14:12:38 - INFO - __main__ - Global step 150 Train loss 1.33 Classification-F1 0.10126582278481013 on epoch=37
06/02/2022 14:12:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=39
06/02/2022 14:12:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=42
06/02/2022 14:12:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.00 on epoch=44
06/02/2022 14:12:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=47
06/02/2022 14:12:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=49
06/02/2022 14:12:44 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=49
06/02/2022 14:12:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=52
06/02/2022 14:12:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.01 on epoch=54
06/02/2022 14:12:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.92 on epoch=57
06/02/2022 14:12:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.96 on epoch=59
06/02/2022 14:12:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.95 on epoch=62
06/02/2022 14:12:51 - INFO - __main__ - Global step 250 Train loss 0.95 Classification-F1 0.10126582278481013 on epoch=62
06/02/2022 14:12:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=64
06/02/2022 14:12:53 - INFO - __main__ - Step 270 Global step 270 Train loss 1.01 on epoch=67
06/02/2022 14:12:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.95 on epoch=69
06/02/2022 14:12:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=72
06/02/2022 14:12:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=74
06/02/2022 14:12:58 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.13067758749069247 on epoch=74
06/02/2022 14:12:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=77
06/02/2022 14:13:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.94 on epoch=79
06/02/2022 14:13:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=82
06/02/2022 14:13:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.97 on epoch=84
06/02/2022 14:13:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=87
06/02/2022 14:13:04 - INFO - __main__ - Global step 350 Train loss 0.94 Classification-F1 0.2413283248081841 on epoch=87
06/02/2022 14:13:04 - INFO - __main__ - Saving model with best Classification-F1: 0.22863666014350945 -> 0.2413283248081841 on epoch=87, global_step=350
06/02/2022 14:13:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.89 on epoch=89
06/02/2022 14:13:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=92
06/02/2022 14:13:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=94
06/02/2022 14:13:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
06/02/2022 14:13:10 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=99
06/02/2022 14:13:11 - INFO - __main__ - Global step 400 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=99
06/02/2022 14:13:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.89 on epoch=102
06/02/2022 14:13:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.94 on epoch=104
06/02/2022 14:13:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=107
06/02/2022 14:13:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=109
06/02/2022 14:13:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.98 on epoch=112
06/02/2022 14:13:17 - INFO - __main__ - Global step 450 Train loss 0.92 Classification-F1 0.13067758749069247 on epoch=112
06/02/2022 14:13:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.91 on epoch=114
06/02/2022 14:13:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.99 on epoch=117
06/02/2022 14:13:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.91 on epoch=119
06/02/2022 14:13:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=122
06/02/2022 14:13:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=124
06/02/2022 14:13:24 - INFO - __main__ - Global step 500 Train loss 0.93 Classification-F1 0.13067758749069247 on epoch=124
06/02/2022 14:13:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.87 on epoch=127
06/02/2022 14:13:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=129
06/02/2022 14:13:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.93 on epoch=132
06/02/2022 14:13:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.88 on epoch=134
06/02/2022 14:13:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.84 on epoch=137
06/02/2022 14:13:31 - INFO - __main__ - Global step 550 Train loss 0.87 Classification-F1 0.10126582278481013 on epoch=137
06/02/2022 14:13:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.84 on epoch=139
06/02/2022 14:13:33 - INFO - __main__ - Step 570 Global step 570 Train loss 1.26 on epoch=142
06/02/2022 14:13:34 - INFO - __main__ - Step 580 Global step 580 Train loss 2.14 on epoch=144
06/02/2022 14:13:35 - INFO - __main__ - Step 590 Global step 590 Train loss 1.22 on epoch=147
06/02/2022 14:13:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.80 on epoch=149
06/02/2022 14:13:37 - INFO - __main__ - Global step 600 Train loss 1.25 Classification-F1 0.38642597058684447 on epoch=149
06/02/2022 14:13:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2413283248081841 -> 0.38642597058684447 on epoch=149, global_step=600
06/02/2022 14:13:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.83 on epoch=152
06/02/2022 14:13:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.86 on epoch=154
06/02/2022 14:13:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.83 on epoch=157
06/02/2022 14:13:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.93 on epoch=159
06/02/2022 14:13:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.86 on epoch=162
06/02/2022 14:13:44 - INFO - __main__ - Global step 650 Train loss 0.86 Classification-F1 0.37962962962962965 on epoch=162
06/02/2022 14:13:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=164
06/02/2022 14:13:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.86 on epoch=167
06/02/2022 14:13:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.88 on epoch=169
06/02/2022 14:13:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
06/02/2022 14:13:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.88 on epoch=174
06/02/2022 14:13:50 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.2911686413695913 on epoch=174
06/02/2022 14:13:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.77 on epoch=177
06/02/2022 14:13:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.76 on epoch=179
06/02/2022 14:13:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.90 on epoch=182
06/02/2022 14:13:55 - INFO - __main__ - Step 740 Global step 740 Train loss 1.06 on epoch=184
06/02/2022 14:13:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.87 on epoch=187
06/02/2022 14:13:57 - INFO - __main__ - Global step 750 Train loss 0.87 Classification-F1 0.28257853257853255 on epoch=187
06/02/2022 14:13:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.79 on epoch=189
06/02/2022 14:13:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.84 on epoch=192
06/02/2022 14:14:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=194
06/02/2022 14:14:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.85 on epoch=197
06/02/2022 14:14:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
06/02/2022 14:14:04 - INFO - __main__ - Global step 800 Train loss 0.84 Classification-F1 0.3539840249366842 on epoch=199
06/02/2022 14:14:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
06/02/2022 14:14:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.87 on epoch=204
06/02/2022 14:14:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.79 on epoch=207
06/02/2022 14:14:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.89 on epoch=209
06/02/2022 14:14:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.85 on epoch=212
06/02/2022 14:14:11 - INFO - __main__ - Global step 850 Train loss 0.86 Classification-F1 0.21405228758169936 on epoch=212
06/02/2022 14:14:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.84 on epoch=214
06/02/2022 14:14:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.83 on epoch=217
06/02/2022 14:14:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.80 on epoch=219
06/02/2022 14:14:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=222
06/02/2022 14:14:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.84 on epoch=224
06/02/2022 14:14:18 - INFO - __main__ - Global step 900 Train loss 0.83 Classification-F1 0.2984826932195353 on epoch=224
06/02/2022 14:14:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.84 on epoch=227
06/02/2022 14:14:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.76 on epoch=229
06/02/2022 14:14:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.89 on epoch=232
06/02/2022 14:14:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.83 on epoch=234
06/02/2022 14:14:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.77 on epoch=237
06/02/2022 14:14:25 - INFO - __main__ - Global step 950 Train loss 0.82 Classification-F1 0.19143413367942896 on epoch=237
06/02/2022 14:14:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.91 on epoch=239
06/02/2022 14:14:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=242
06/02/2022 14:14:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.80 on epoch=244
06/02/2022 14:14:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.91 on epoch=247
06/02/2022 14:14:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.90 on epoch=249
06/02/2022 14:14:32 - INFO - __main__ - Global step 1000 Train loss 0.86 Classification-F1 0.36345523329129886 on epoch=249
06/02/2022 14:14:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.85 on epoch=252
06/02/2022 14:14:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.88 on epoch=254
06/02/2022 14:14:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.92 on epoch=257
06/02/2022 14:14:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.79 on epoch=259
06/02/2022 14:14:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.80 on epoch=262
06/02/2022 14:14:39 - INFO - __main__ - Global step 1050 Train loss 0.85 Classification-F1 0.4168293404318249 on epoch=262
06/02/2022 14:14:39 - INFO - __main__ - Saving model with best Classification-F1: 0.38642597058684447 -> 0.4168293404318249 on epoch=262, global_step=1050
06/02/2022 14:14:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.86 on epoch=264
06/02/2022 14:14:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.78 on epoch=267
06/02/2022 14:14:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.83 on epoch=269
06/02/2022 14:14:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.84 on epoch=272
06/02/2022 14:14:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.78 on epoch=274
06/02/2022 14:14:45 - INFO - __main__ - Global step 1100 Train loss 0.82 Classification-F1 0.4325233720180529 on epoch=274
06/02/2022 14:14:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4168293404318249 -> 0.4325233720180529 on epoch=274, global_step=1100
06/02/2022 14:14:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.75 on epoch=277
06/02/2022 14:14:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.83 on epoch=279
06/02/2022 14:14:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.84 on epoch=282
06/02/2022 14:14:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.77 on epoch=284
06/02/2022 14:14:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.77 on epoch=287
06/02/2022 14:14:52 - INFO - __main__ - Global step 1150 Train loss 0.79 Classification-F1 0.407051282051282 on epoch=287
06/02/2022 14:14:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.79 on epoch=289
06/02/2022 14:14:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.79 on epoch=292
06/02/2022 14:14:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.76 on epoch=294
06/02/2022 14:14:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.80 on epoch=297
06/02/2022 14:14:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.78 on epoch=299
06/02/2022 14:14:59 - INFO - __main__ - Global step 1200 Train loss 0.78 Classification-F1 0.27847782258064513 on epoch=299
06/02/2022 14:15:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=302
06/02/2022 14:15:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.75 on epoch=304
06/02/2022 14:15:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.89 on epoch=307
06/02/2022 14:15:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.78 on epoch=309
06/02/2022 14:15:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.78 on epoch=312
06/02/2022 14:15:06 - INFO - __main__ - Global step 1250 Train loss 0.82 Classification-F1 0.43126047520457456 on epoch=312
06/02/2022 14:15:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.83 on epoch=314
06/02/2022 14:15:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.78 on epoch=317
06/02/2022 14:15:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=319
06/02/2022 14:15:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.90 on epoch=322
06/02/2022 14:15:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.71 on epoch=324
06/02/2022 14:15:13 - INFO - __main__ - Global step 1300 Train loss 0.79 Classification-F1 0.3192307692307692 on epoch=324
06/02/2022 14:15:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.72 on epoch=327
06/02/2022 14:15:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=329
06/02/2022 14:15:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.75 on epoch=332
06/02/2022 14:15:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.81 on epoch=334
06/02/2022 14:15:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.81 on epoch=337
06/02/2022 14:15:20 - INFO - __main__ - Global step 1350 Train loss 0.77 Classification-F1 0.44395477368782277 on epoch=337
06/02/2022 14:15:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4325233720180529 -> 0.44395477368782277 on epoch=337, global_step=1350
06/02/2022 14:15:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.76 on epoch=339
06/02/2022 14:15:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.79 on epoch=342
06/02/2022 14:15:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.69 on epoch=344
06/02/2022 14:15:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.77 on epoch=347
06/02/2022 14:15:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.76 on epoch=349
06/02/2022 14:15:27 - INFO - __main__ - Global step 1400 Train loss 0.76 Classification-F1 0.5084768053562889 on epoch=349
06/02/2022 14:15:27 - INFO - __main__ - Saving model with best Classification-F1: 0.44395477368782277 -> 0.5084768053562889 on epoch=349, global_step=1400
06/02/2022 14:15:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.80 on epoch=352
06/02/2022 14:15:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.81 on epoch=354
06/02/2022 14:15:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.67 on epoch=357
06/02/2022 14:15:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.74 on epoch=359
06/02/2022 14:15:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=362
06/02/2022 14:15:34 - INFO - __main__ - Global step 1450 Train loss 0.75 Classification-F1 0.5543263043263044 on epoch=362
06/02/2022 14:15:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5084768053562889 -> 0.5543263043263044 on epoch=362, global_step=1450
06/02/2022 14:15:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.67 on epoch=364
06/02/2022 14:15:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.69 on epoch=367
06/02/2022 14:15:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.74 on epoch=369
06/02/2022 14:15:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.82 on epoch=372
06/02/2022 14:15:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.68 on epoch=374
06/02/2022 14:15:41 - INFO - __main__ - Global step 1500 Train loss 0.72 Classification-F1 0.44948979591836735 on epoch=374
06/02/2022 14:15:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.65 on epoch=377
06/02/2022 14:15:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.72 on epoch=379
06/02/2022 14:15:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.63 on epoch=382
06/02/2022 14:15:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.63 on epoch=384
06/02/2022 14:15:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.68 on epoch=387
06/02/2022 14:15:48 - INFO - __main__ - Global step 1550 Train loss 0.66 Classification-F1 0.6259834368530021 on epoch=387
06/02/2022 14:15:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5543263043263044 -> 0.6259834368530021 on epoch=387, global_step=1550
06/02/2022 14:15:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.64 on epoch=389
06/02/2022 14:15:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.71 on epoch=392
06/02/2022 14:15:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.72 on epoch=394
06/02/2022 14:15:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.74 on epoch=397
06/02/2022 14:15:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.62 on epoch=399
06/02/2022 14:15:55 - INFO - __main__ - Global step 1600 Train loss 0.69 Classification-F1 0.4357371794871795 on epoch=399
06/02/2022 14:15:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.63 on epoch=402
06/02/2022 14:15:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.62 on epoch=404
06/02/2022 14:15:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.60 on epoch=407
06/02/2022 14:16:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.69 on epoch=409
06/02/2022 14:16:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.60 on epoch=412
06/02/2022 14:16:02 - INFO - __main__ - Global step 1650 Train loss 0.63 Classification-F1 0.6327606177606178 on epoch=412
06/02/2022 14:16:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6259834368530021 -> 0.6327606177606178 on epoch=412, global_step=1650
06/02/2022 14:16:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.62 on epoch=414
06/02/2022 14:16:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.60 on epoch=417
06/02/2022 14:16:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=419
06/02/2022 14:16:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=422
06/02/2022 14:16:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.63 on epoch=424
06/02/2022 14:16:09 - INFO - __main__ - Global step 1700 Train loss 0.61 Classification-F1 0.4947163733500942 on epoch=424
06/02/2022 14:16:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.53 on epoch=427
06/02/2022 14:16:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=429
06/02/2022 14:16:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=432
06/02/2022 14:16:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=434
06/02/2022 14:16:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=437
06/02/2022 14:16:16 - INFO - __main__ - Global step 1750 Train loss 0.51 Classification-F1 0.6671040778986805 on epoch=437
06/02/2022 14:16:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6327606177606178 -> 0.6671040778986805 on epoch=437, global_step=1750
06/02/2022 14:16:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=439
06/02/2022 14:16:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=442
06/02/2022 14:16:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=444
06/02/2022 14:16:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.54 on epoch=447
06/02/2022 14:16:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=449
06/02/2022 14:16:23 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.528545588778147 on epoch=449
06/02/2022 14:16:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.53 on epoch=452
06/02/2022 14:16:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.60 on epoch=454
06/02/2022 14:16:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.45 on epoch=457
06/02/2022 14:16:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=459
06/02/2022 14:16:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=462
06/02/2022 14:16:30 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.6643907563025211 on epoch=462
06/02/2022 14:16:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=464
06/02/2022 14:16:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=467
06/02/2022 14:16:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=469
06/02/2022 14:16:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=472
06/02/2022 14:16:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=474
06/02/2022 14:16:37 - INFO - __main__ - Global step 1900 Train loss 0.45 Classification-F1 0.4240777338603426 on epoch=474
06/02/2022 14:16:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=477
06/02/2022 14:16:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.49 on epoch=479
06/02/2022 14:16:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=482
06/02/2022 14:16:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=484
06/02/2022 14:16:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=487
06/02/2022 14:16:43 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.6006599609540786 on epoch=487
06/02/2022 14:16:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=489
06/02/2022 14:16:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=492
06/02/2022 14:16:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=494
06/02/2022 14:16:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=497
06/02/2022 14:16:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=499
06/02/2022 14:16:50 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.6193924539512775 on epoch=499
06/02/2022 14:16:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=502
06/02/2022 14:16:53 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.36 on epoch=504
06/02/2022 14:16:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=507
06/02/2022 14:16:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=509
06/02/2022 14:16:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=512
06/02/2022 14:16:57 - INFO - __main__ - Global step 2050 Train loss 0.28 Classification-F1 0.7395988420181968 on epoch=512
06/02/2022 14:16:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6671040778986805 -> 0.7395988420181968 on epoch=512, global_step=2050
06/02/2022 14:16:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=514
06/02/2022 14:17:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=517
06/02/2022 14:17:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=519
06/02/2022 14:17:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.34 on epoch=522
06/02/2022 14:17:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=524
06/02/2022 14:17:04 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.6911196911196911 on epoch=524
06/02/2022 14:17:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=527
06/02/2022 14:17:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=529
06/02/2022 14:17:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=532
06/02/2022 14:17:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=534
06/02/2022 14:17:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=537
06/02/2022 14:17:11 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.5780812324929971 on epoch=537
06/02/2022 14:17:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=539
06/02/2022 14:17:14 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=542
06/02/2022 14:17:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=544
06/02/2022 14:17:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=547
06/02/2022 14:17:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=549
06/02/2022 14:17:18 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.5815820150268066 on epoch=549
06/02/2022 14:17:20 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=552
06/02/2022 14:17:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=554
06/02/2022 14:17:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=557
06/02/2022 14:17:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=559
06/02/2022 14:17:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.22 on epoch=562
06/02/2022 14:17:25 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.6918529060369668 on epoch=562
06/02/2022 14:17:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=564
06/02/2022 14:17:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=567
06/02/2022 14:17:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=569
06/02/2022 14:17:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=572
06/02/2022 14:17:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=574
06/02/2022 14:17:32 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.5646213073538655 on epoch=574
06/02/2022 14:17:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=577
06/02/2022 14:17:35 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=579
06/02/2022 14:17:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=582
06/02/2022 14:17:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=584
06/02/2022 14:17:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=587
06/02/2022 14:17:39 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.7080126787557128 on epoch=587
06/02/2022 14:17:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=589
06/02/2022 14:17:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=592
06/02/2022 14:17:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=594
06/02/2022 14:17:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=597
06/02/2022 14:17:46 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=599
06/02/2022 14:17:46 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.7292178542178541 on epoch=599
06/02/2022 14:17:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=602
06/02/2022 14:17:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=604
06/02/2022 14:17:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=607
06/02/2022 14:17:52 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=609
06/02/2022 14:17:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=612
06/02/2022 14:17:53 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7457208927797163 on epoch=612
06/02/2022 14:17:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7395988420181968 -> 0.7457208927797163 on epoch=612, global_step=2450
06/02/2022 14:17:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=614
06/02/2022 14:17:56 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=617
06/02/2022 14:17:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=619
06/02/2022 14:17:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
06/02/2022 14:18:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=624
06/02/2022 14:18:00 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.7292690417690417 on epoch=624
06/02/2022 14:18:02 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=627
06/02/2022 14:18:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=629
06/02/2022 14:18:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=632
06/02/2022 14:18:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=634
06/02/2022 14:18:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=637
06/02/2022 14:18:07 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.7446969696969697 on epoch=637
06/02/2022 14:18:09 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=639
06/02/2022 14:18:10 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=642
06/02/2022 14:18:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=644
06/02/2022 14:18:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=647
06/02/2022 14:18:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=649
06/02/2022 14:18:14 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7442067736185383 on epoch=649
06/02/2022 14:18:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=652
06/02/2022 14:18:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=654
06/02/2022 14:18:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=657
06/02/2022 14:18:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=659
06/02/2022 14:18:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=662
06/02/2022 14:18:21 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.7044750914719955 on epoch=662
06/02/2022 14:18:23 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=664
06/02/2022 14:18:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=667
06/02/2022 14:18:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=669
06/02/2022 14:18:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=672
06/02/2022 14:18:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=674
06/02/2022 14:18:28 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.7217503217503218 on epoch=674
06/02/2022 14:18:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=677
06/02/2022 14:18:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.22 on epoch=679
06/02/2022 14:18:32 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=682
06/02/2022 14:18:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=684
06/02/2022 14:18:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=687
06/02/2022 14:18:35 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.7318295739348372 on epoch=687
06/02/2022 14:18:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=689
06/02/2022 14:18:38 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=692
06/02/2022 14:18:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=694
06/02/2022 14:18:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=697
06/02/2022 14:18:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=699
06/02/2022 14:18:43 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.7187028657616894 on epoch=699
06/02/2022 14:18:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=702
06/02/2022 14:18:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=704
06/02/2022 14:18:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=707
06/02/2022 14:18:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=709
06/02/2022 14:18:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
06/02/2022 14:18:50 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6897692091190544 on epoch=712
06/02/2022 14:18:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
06/02/2022 14:18:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=717
06/02/2022 14:18:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=719
06/02/2022 14:18:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.10 on epoch=722
06/02/2022 14:18:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=724
06/02/2022 14:18:57 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7292178542178541 on epoch=724
06/02/2022 14:18:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=727
06/02/2022 14:18:59 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=729
06/02/2022 14:19:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=732
06/02/2022 14:19:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
06/02/2022 14:19:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=737
06/02/2022 14:19:04 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7062712062712062 on epoch=737
06/02/2022 14:19:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=739
06/02/2022 14:19:06 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=742
06/02/2022 14:19:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=744
06/02/2022 14:19:09 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=747
06/02/2022 14:19:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=749
06/02/2022 14:19:11 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7217503217503218 on epoch=749
06/02/2022 14:19:11 - INFO - __main__ - save last model!
06/02/2022 14:19:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 14:19:11 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 14:19:11 - INFO - __main__ - Printing 3 examples
06/02/2022 14:19:11 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:19:11 - INFO - __main__ - Printing 3 examples
06/02/2022 14:19:11 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:19:11 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:19:11 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:19:11 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:19:11 - INFO - __main__ - Printing 3 examples
06/02/2022 14:19:11 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:19:11 - INFO - __main__ - ['others']
06/02/2022 14:19:11 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:19:11 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:19:11 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:19:13 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:19:17 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:19:17 - INFO - __main__ - task name: emo
06/02/2022 14:19:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:19:18 - INFO - __main__ - Starting training!
06/02/2022 14:19:18 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 14:20:01 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.4_8_predictions.txt
06/02/2022 14:20:01 - INFO - __main__ - Classification-F1 on test data: 0.1554
06/02/2022 14:20:01 - INFO - __main__ - prefix=emo_16_87, lr=0.4, bsz=8, dev_performance=0.7457208927797163, test_performance=0.15542408855152712
06/02/2022 14:20:01 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.3, bsz=8 ...
06/02/2022 14:20:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:20:02 - INFO - __main__ - Printing 3 examples
06/02/2022 14:20:02 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:20:02 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:20:02 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:20:02 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:20:02 - INFO - __main__ - Printing 3 examples
06/02/2022 14:20:02 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:20:02 - INFO - __main__ - ['others']
06/02/2022 14:20:02 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:20:02 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:20:02 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:20:09 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:20:09 - INFO - __main__ - task name: emo
06/02/2022 14:20:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:20:09 - INFO - __main__ - Starting training!
06/02/2022 14:20:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.36 on epoch=2
06/02/2022 14:20:12 - INFO - __main__ - Step 20 Global step 20 Train loss 5.29 on epoch=4
06/02/2022 14:20:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=7
06/02/2022 14:20:14 - INFO - __main__ - Step 40 Global step 40 Train loss 2.14 on epoch=9
06/02/2022 14:20:15 - INFO - __main__ - Step 50 Global step 50 Train loss 2.09 on epoch=12
06/02/2022 14:20:16 - INFO - __main__ - Global step 50 Train loss 3.93 Classification-F1 0.15702087286527516 on epoch=12
06/02/2022 14:20:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15702087286527516 on epoch=12, global_step=50
06/02/2022 14:20:17 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=14
06/02/2022 14:20:18 - INFO - __main__ - Step 70 Global step 70 Train loss 1.67 on epoch=17
06/02/2022 14:20:20 - INFO - __main__ - Step 80 Global step 80 Train loss 1.39 on epoch=19
06/02/2022 14:20:21 - INFO - __main__ - Step 90 Global step 90 Train loss 2.63 on epoch=22
06/02/2022 14:20:22 - INFO - __main__ - Step 100 Global step 100 Train loss 2.89 on epoch=24
06/02/2022 14:20:23 - INFO - __main__ - Global step 100 Train loss 2.04 Classification-F1 0.1 on epoch=24
06/02/2022 14:20:24 - INFO - __main__ - Step 110 Global step 110 Train loss 1.42 on epoch=27
06/02/2022 14:20:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.79 on epoch=29
06/02/2022 14:20:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.35 on epoch=32
06/02/2022 14:20:27 - INFO - __main__ - Step 140 Global step 140 Train loss 1.34 on epoch=34
06/02/2022 14:20:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.37 on epoch=37
06/02/2022 14:20:29 - INFO - __main__ - Global step 150 Train loss 1.45 Classification-F1 0.12368421052631579 on epoch=37
06/02/2022 14:20:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.21 on epoch=39
06/02/2022 14:20:32 - INFO - __main__ - Step 170 Global step 170 Train loss 1.25 on epoch=42
06/02/2022 14:20:33 - INFO - __main__ - Step 180 Global step 180 Train loss 1.20 on epoch=44
06/02/2022 14:20:34 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=47
06/02/2022 14:20:35 - INFO - __main__ - Step 200 Global step 200 Train loss 1.19 on epoch=49
06/02/2022 14:20:36 - INFO - __main__ - Global step 200 Train loss 1.19 Classification-F1 0.1 on epoch=49
06/02/2022 14:20:37 - INFO - __main__ - Step 210 Global step 210 Train loss 1.15 on epoch=52
06/02/2022 14:20:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.21 on epoch=54
06/02/2022 14:20:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.16 on epoch=57
06/02/2022 14:20:41 - INFO - __main__ - Step 240 Global step 240 Train loss 1.30 on epoch=59
06/02/2022 14:20:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.10 on epoch=62
06/02/2022 14:20:43 - INFO - __main__ - Global step 250 Train loss 1.18 Classification-F1 0.1457326892109501 on epoch=62
06/02/2022 14:20:44 - INFO - __main__ - Step 260 Global step 260 Train loss 1.06 on epoch=64
06/02/2022 14:20:45 - INFO - __main__ - Step 270 Global step 270 Train loss 1.23 on epoch=67
06/02/2022 14:20:46 - INFO - __main__ - Step 280 Global step 280 Train loss 1.24 on epoch=69
06/02/2022 14:20:48 - INFO - __main__ - Step 290 Global step 290 Train loss 1.08 on epoch=72
06/02/2022 14:20:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.24 on epoch=74
06/02/2022 14:20:49 - INFO - __main__ - Global step 300 Train loss 1.17 Classification-F1 0.1796875 on epoch=74
06/02/2022 14:20:49 - INFO - __main__ - Saving model with best Classification-F1: 0.15702087286527516 -> 0.1796875 on epoch=74, global_step=300
06/02/2022 14:20:51 - INFO - __main__ - Step 310 Global step 310 Train loss 1.11 on epoch=77
06/02/2022 14:20:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=79
06/02/2022 14:20:53 - INFO - __main__ - Step 330 Global step 330 Train loss 1.07 on epoch=82
06/02/2022 14:20:54 - INFO - __main__ - Step 340 Global step 340 Train loss 1.06 on epoch=84
06/02/2022 14:20:56 - INFO - __main__ - Step 350 Global step 350 Train loss 1.08 on epoch=87
06/02/2022 14:20:56 - INFO - __main__ - Global step 350 Train loss 1.06 Classification-F1 0.267551133222775 on epoch=87
06/02/2022 14:20:56 - INFO - __main__ - Saving model with best Classification-F1: 0.1796875 -> 0.267551133222775 on epoch=87, global_step=350
06/02/2022 14:20:57 - INFO - __main__ - Step 360 Global step 360 Train loss 1.12 on epoch=89
06/02/2022 14:20:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=92
06/02/2022 14:21:00 - INFO - __main__ - Step 380 Global step 380 Train loss 1.03 on epoch=94
06/02/2022 14:21:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.03 on epoch=97
06/02/2022 14:21:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.98 on epoch=99
06/02/2022 14:21:03 - INFO - __main__ - Global step 400 Train loss 1.06 Classification-F1 0.1640625 on epoch=99
06/02/2022 14:21:04 - INFO - __main__ - Step 410 Global step 410 Train loss 1.10 on epoch=102
06/02/2022 14:21:05 - INFO - __main__ - Step 420 Global step 420 Train loss 1.02 on epoch=104
06/02/2022 14:21:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.96 on epoch=107
06/02/2022 14:21:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.91 on epoch=109
06/02/2022 14:21:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=112
06/02/2022 14:21:09 - INFO - __main__ - Global step 450 Train loss 0.98 Classification-F1 0.1808979236547279 on epoch=112
06/02/2022 14:21:11 - INFO - __main__ - Step 460 Global step 460 Train loss 1.05 on epoch=114
06/02/2022 14:21:12 - INFO - __main__ - Step 470 Global step 470 Train loss 1.11 on epoch=117
06/02/2022 14:21:13 - INFO - __main__ - Step 480 Global step 480 Train loss 1.01 on epoch=119
06/02/2022 14:21:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=122
06/02/2022 14:21:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.89 on epoch=124
06/02/2022 14:21:16 - INFO - __main__ - Global step 500 Train loss 1.01 Classification-F1 0.15 on epoch=124
06/02/2022 14:21:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=127
06/02/2022 14:21:19 - INFO - __main__ - Step 520 Global step 520 Train loss 1.01 on epoch=129
06/02/2022 14:21:20 - INFO - __main__ - Step 530 Global step 530 Train loss 1.06 on epoch=132
06/02/2022 14:21:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.99 on epoch=134
06/02/2022 14:21:22 - INFO - __main__ - Step 550 Global step 550 Train loss 1.14 on epoch=137
06/02/2022 14:21:23 - INFO - __main__ - Global step 550 Train loss 1.03 Classification-F1 0.3299220272904484 on epoch=137
06/02/2022 14:21:23 - INFO - __main__ - Saving model with best Classification-F1: 0.267551133222775 -> 0.3299220272904484 on epoch=137, global_step=550
06/02/2022 14:21:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.87 on epoch=139
06/02/2022 14:21:25 - INFO - __main__ - Step 570 Global step 570 Train loss 1.01 on epoch=142
06/02/2022 14:21:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=144
06/02/2022 14:21:28 - INFO - __main__ - Step 590 Global step 590 Train loss 1.02 on epoch=147
06/02/2022 14:21:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.97 on epoch=149
06/02/2022 14:21:29 - INFO - __main__ - Global step 600 Train loss 0.96 Classification-F1 0.21124551971326164 on epoch=149
06/02/2022 14:21:31 - INFO - __main__ - Step 610 Global step 610 Train loss 1.02 on epoch=152
06/02/2022 14:21:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.90 on epoch=154
06/02/2022 14:21:33 - INFO - __main__ - Step 630 Global step 630 Train loss 1.01 on epoch=157
06/02/2022 14:21:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.90 on epoch=159
06/02/2022 14:21:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.91 on epoch=162
06/02/2022 14:21:36 - INFO - __main__ - Global step 650 Train loss 0.95 Classification-F1 0.22306058164237053 on epoch=162
06/02/2022 14:21:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.86 on epoch=164
06/02/2022 14:21:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.95 on epoch=167
06/02/2022 14:21:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=169
06/02/2022 14:21:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.83 on epoch=172
06/02/2022 14:21:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.92 on epoch=174
06/02/2022 14:21:43 - INFO - __main__ - Global step 700 Train loss 0.90 Classification-F1 0.13067758749069247 on epoch=174
06/02/2022 14:21:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.89 on epoch=177
06/02/2022 14:21:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.87 on epoch=179
06/02/2022 14:21:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.87 on epoch=182
06/02/2022 14:21:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=184
06/02/2022 14:21:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.81 on epoch=187
06/02/2022 14:21:49 - INFO - __main__ - Global step 750 Train loss 0.86 Classification-F1 0.26006991944064445 on epoch=187
06/02/2022 14:21:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.94 on epoch=189
06/02/2022 14:21:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.92 on epoch=192
06/02/2022 14:21:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.88 on epoch=194
06/02/2022 14:21:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.92 on epoch=197
06/02/2022 14:21:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.89 on epoch=199
06/02/2022 14:21:56 - INFO - __main__ - Global step 800 Train loss 0.91 Classification-F1 0.17234848484848483 on epoch=199
06/02/2022 14:21:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.88 on epoch=202
06/02/2022 14:21:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=204
06/02/2022 14:22:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.87 on epoch=207
06/02/2022 14:22:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.79 on epoch=209
06/02/2022 14:22:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.86 on epoch=212
06/02/2022 14:22:03 - INFO - __main__ - Global step 850 Train loss 0.84 Classification-F1 0.3576189096328186 on epoch=212
06/02/2022 14:22:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3299220272904484 -> 0.3576189096328186 on epoch=212, global_step=850
06/02/2022 14:22:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.87 on epoch=214
06/02/2022 14:22:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.92 on epoch=217
06/02/2022 14:22:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
06/02/2022 14:22:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.85 on epoch=222
06/02/2022 14:22:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.79 on epoch=224
06/02/2022 14:22:10 - INFO - __main__ - Global step 900 Train loss 0.85 Classification-F1 0.2861111111111111 on epoch=224
06/02/2022 14:22:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=227
06/02/2022 14:22:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.85 on epoch=229
06/02/2022 14:22:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.70 on epoch=232
06/02/2022 14:22:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=234
06/02/2022 14:22:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.73 on epoch=237
06/02/2022 14:22:17 - INFO - __main__ - Global step 950 Train loss 0.77 Classification-F1 0.4451149425287356 on epoch=237
06/02/2022 14:22:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3576189096328186 -> 0.4451149425287356 on epoch=237, global_step=950
06/02/2022 14:22:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.78 on epoch=239
06/02/2022 14:22:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.79 on epoch=242
06/02/2022 14:22:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.64 on epoch=244
06/02/2022 14:22:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.82 on epoch=247
06/02/2022 14:22:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.80 on epoch=249
06/02/2022 14:22:23 - INFO - __main__ - Global step 1000 Train loss 0.77 Classification-F1 0.3641998844304812 on epoch=249
06/02/2022 14:22:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=252
06/02/2022 14:22:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.66 on epoch=254
06/02/2022 14:22:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.70 on epoch=257
06/02/2022 14:22:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.73 on epoch=259
06/02/2022 14:22:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.76 on epoch=262
06/02/2022 14:22:30 - INFO - __main__ - Global step 1050 Train loss 0.72 Classification-F1 0.2702508908230621 on epoch=262
06/02/2022 14:22:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.72 on epoch=264
06/02/2022 14:22:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.67 on epoch=267
06/02/2022 14:22:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=269
06/02/2022 14:22:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.78 on epoch=272
06/02/2022 14:22:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=274
06/02/2022 14:22:37 - INFO - __main__ - Global step 1100 Train loss 0.71 Classification-F1 0.5492424242424242 on epoch=274
06/02/2022 14:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4451149425287356 -> 0.5492424242424242 on epoch=274, global_step=1100
06/02/2022 14:22:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.58 on epoch=277
06/02/2022 14:22:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.56 on epoch=279
06/02/2022 14:22:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=282
06/02/2022 14:22:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.57 on epoch=284
06/02/2022 14:22:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=287
06/02/2022 14:22:44 - INFO - __main__ - Global step 1150 Train loss 0.59 Classification-F1 0.5808479532163742 on epoch=287
06/02/2022 14:22:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5492424242424242 -> 0.5808479532163742 on epoch=287, global_step=1150
06/02/2022 14:22:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.56 on epoch=289
06/02/2022 14:22:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.57 on epoch=292
06/02/2022 14:22:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.56 on epoch=294
06/02/2022 14:22:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=297
06/02/2022 14:22:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=299
06/02/2022 14:22:50 - INFO - __main__ - Global step 1200 Train loss 0.53 Classification-F1 0.6105462519936204 on epoch=299
06/02/2022 14:22:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5808479532163742 -> 0.6105462519936204 on epoch=299, global_step=1200
06/02/2022 14:22:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=302
06/02/2022 14:22:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=304
06/02/2022 14:22:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=307
06/02/2022 14:22:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=309
06/02/2022 14:22:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=312
06/02/2022 14:22:57 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.6128102615358987 on epoch=312
06/02/2022 14:22:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6105462519936204 -> 0.6128102615358987 on epoch=312, global_step=1250
06/02/2022 14:22:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=314
06/02/2022 14:23:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=317
06/02/2022 14:23:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=319
06/02/2022 14:23:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=322
06/02/2022 14:23:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=324
06/02/2022 14:23:04 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.6492436974789917 on epoch=324
06/02/2022 14:23:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6128102615358987 -> 0.6492436974789917 on epoch=324, global_step=1300
06/02/2022 14:23:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=327
06/02/2022 14:23:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=329
06/02/2022 14:23:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.47 on epoch=332
06/02/2022 14:23:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=334
06/02/2022 14:23:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=337
06/02/2022 14:23:11 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.6534199522102747 on epoch=337
06/02/2022 14:23:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6492436974789917 -> 0.6534199522102747 on epoch=337, global_step=1350
06/02/2022 14:23:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=339
06/02/2022 14:23:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=342
06/02/2022 14:23:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=344
06/02/2022 14:23:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=347
06/02/2022 14:23:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=349
06/02/2022 14:23:17 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.638344634089315 on epoch=349
06/02/2022 14:23:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=352
06/02/2022 14:23:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=354
06/02/2022 14:23:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
06/02/2022 14:23:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=359
06/02/2022 14:23:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=362
06/02/2022 14:23:24 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.737296494355318 on epoch=362
06/02/2022 14:23:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6534199522102747 -> 0.737296494355318 on epoch=362, global_step=1450
06/02/2022 14:23:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=364
06/02/2022 14:23:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=367
06/02/2022 14:23:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=369
06/02/2022 14:23:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=372
06/02/2022 14:23:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=374
06/02/2022 14:23:31 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.6859806859806861 on epoch=374
06/02/2022 14:23:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=377
06/02/2022 14:23:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=379
06/02/2022 14:23:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=382
06/02/2022 14:23:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=384
06/02/2022 14:23:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=387
06/02/2022 14:23:38 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.6804061784897024 on epoch=387
06/02/2022 14:23:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=389
06/02/2022 14:23:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=392
06/02/2022 14:23:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=394
06/02/2022 14:23:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=397
06/02/2022 14:23:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=399
06/02/2022 14:23:44 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.6937181746964356 on epoch=399
06/02/2022 14:23:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=402
06/02/2022 14:23:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=404
06/02/2022 14:23:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=407
06/02/2022 14:23:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=409
06/02/2022 14:23:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=412
06/02/2022 14:23:51 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.689766081871345 on epoch=412
06/02/2022 14:23:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=414
06/02/2022 14:23:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=417
06/02/2022 14:23:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
06/02/2022 14:23:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=422
06/02/2022 14:23:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=424
06/02/2022 14:23:58 - INFO - __main__ - Global step 1700 Train loss 0.16 Classification-F1 0.7751277272968041 on epoch=424
06/02/2022 14:23:58 - INFO - __main__ - Saving model with best Classification-F1: 0.737296494355318 -> 0.7751277272968041 on epoch=424, global_step=1700
06/02/2022 14:23:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=427
06/02/2022 14:24:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=429
06/02/2022 14:24:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=432
06/02/2022 14:24:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=434
06/02/2022 14:24:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=437
06/02/2022 14:24:05 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7170412391000627 on epoch=437
06/02/2022 14:24:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=439
06/02/2022 14:24:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=442
06/02/2022 14:24:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=444
06/02/2022 14:24:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=447
06/02/2022 14:24:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=449
06/02/2022 14:24:12 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7457062677650913 on epoch=449
06/02/2022 14:24:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=452
06/02/2022 14:24:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
06/02/2022 14:24:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=457
06/02/2022 14:24:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=459
06/02/2022 14:24:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=462
06/02/2022 14:24:18 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7518551587301587 on epoch=462
06/02/2022 14:24:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=464
06/02/2022 14:24:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=467
06/02/2022 14:24:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=469
06/02/2022 14:24:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=472
06/02/2022 14:24:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=474
06/02/2022 14:24:25 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.7515697515697515 on epoch=474
06/02/2022 14:24:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=477
06/02/2022 14:24:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=479
06/02/2022 14:24:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=482
06/02/2022 14:24:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=484
06/02/2022 14:24:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=487
06/02/2022 14:24:32 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.724159126671912 on epoch=487
06/02/2022 14:24:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=489
06/02/2022 14:24:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=492
06/02/2022 14:24:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=494
06/02/2022 14:24:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=497
06/02/2022 14:24:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=499
06/02/2022 14:24:39 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7371970527341373 on epoch=499
06/02/2022 14:24:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=502
06/02/2022 14:24:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=504
06/02/2022 14:24:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=507
06/02/2022 14:24:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=509
06/02/2022 14:24:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
06/02/2022 14:24:45 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.6777177993210601 on epoch=512
06/02/2022 14:24:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=514
06/02/2022 14:24:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=517
06/02/2022 14:24:49 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=519
06/02/2022 14:24:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=522
06/02/2022 14:24:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=524
06/02/2022 14:24:52 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7220740178742603 on epoch=524
06/02/2022 14:24:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=527
06/02/2022 14:24:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=529
06/02/2022 14:24:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=532
06/02/2022 14:24:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=534
06/02/2022 14:24:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=537
06/02/2022 14:24:59 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7096495472454551 on epoch=537
06/02/2022 14:25:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=539
06/02/2022 14:25:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=542
06/02/2022 14:25:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=544
06/02/2022 14:25:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=547
06/02/2022 14:25:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=549
06/02/2022 14:25:05 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7059698995182866 on epoch=549
06/02/2022 14:25:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=552
06/02/2022 14:25:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=554
06/02/2022 14:25:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=557
06/02/2022 14:25:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=559
06/02/2022 14:25:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=562
06/02/2022 14:25:12 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.7364182364182363 on epoch=562
06/02/2022 14:25:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=564
06/02/2022 14:25:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=567
06/02/2022 14:25:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=569
06/02/2022 14:25:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=572
06/02/2022 14:25:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=574
06/02/2022 14:25:19 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.709022449559534 on epoch=574
06/02/2022 14:25:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=577
06/02/2022 14:25:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=579
06/02/2022 14:25:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=582
06/02/2022 14:25:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=584
06/02/2022 14:25:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=587
06/02/2022 14:25:26 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7235946420729029 on epoch=587
06/02/2022 14:25:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=589
06/02/2022 14:25:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=592
06/02/2022 14:25:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=594
06/02/2022 14:25:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=597
06/02/2022 14:25:32 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=599
06/02/2022 14:25:32 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.717296494355318 on epoch=599
06/02/2022 14:25:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=602
06/02/2022 14:25:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=604
06/02/2022 14:25:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=607
06/02/2022 14:25:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
06/02/2022 14:25:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=612
06/02/2022 14:25:39 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7450980392156864 on epoch=612
06/02/2022 14:25:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=614
06/02/2022 14:25:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=617
06/02/2022 14:25:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=619
06/02/2022 14:25:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=622
06/02/2022 14:25:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/02/2022 14:25:46 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7673611111111112 on epoch=624
06/02/2022 14:25:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=627
06/02/2022 14:25:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=629
06/02/2022 14:25:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=632
06/02/2022 14:25:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=634
06/02/2022 14:25:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=637
06/02/2022 14:25:53 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7096495472454551 on epoch=637
06/02/2022 14:25:54 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=639
06/02/2022 14:25:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=642
06/02/2022 14:25:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=644
06/02/2022 14:25:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=647
06/02/2022 14:25:59 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/02/2022 14:26:00 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7591431556948799 on epoch=649
06/02/2022 14:26:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=652
06/02/2022 14:26:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=654
06/02/2022 14:26:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=657
06/02/2022 14:26:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=659
06/02/2022 14:26:06 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=662
06/02/2022 14:26:07 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7589361831689198 on epoch=662
06/02/2022 14:26:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=664
06/02/2022 14:26:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=667
06/02/2022 14:26:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=669
06/02/2022 14:26:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=672
06/02/2022 14:26:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=674
06/02/2022 14:26:13 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7588638650930086 on epoch=674
06/02/2022 14:26:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=677
06/02/2022 14:26:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=679
06/02/2022 14:26:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=682
06/02/2022 14:26:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=684
06/02/2022 14:26:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=687
06/02/2022 14:26:20 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7595444117183248 on epoch=687
06/02/2022 14:26:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=689
06/02/2022 14:26:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=692
06/02/2022 14:26:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=694
06/02/2022 14:26:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/02/2022 14:26:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=699
06/02/2022 14:26:27 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7303030303030303 on epoch=699
06/02/2022 14:26:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=702
06/02/2022 14:26:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=704
06/02/2022 14:26:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/02/2022 14:26:32 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=709
06/02/2022 14:26:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.00 on epoch=712
06/02/2022 14:26:34 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7512554112554113 on epoch=712
06/02/2022 14:26:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=714
06/02/2022 14:26:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=717
06/02/2022 14:26:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=719
06/02/2022 14:26:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=722
06/02/2022 14:26:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=724
06/02/2022 14:26:40 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.731764705882353 on epoch=724
06/02/2022 14:26:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=727
06/02/2022 14:26:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=729
06/02/2022 14:26:44 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=732
06/02/2022 14:26:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=734
06/02/2022 14:26:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=737
06/02/2022 14:26:47 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7311853832442069 on epoch=737
06/02/2022 14:26:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=739
06/02/2022 14:26:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=742
06/02/2022 14:26:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/02/2022 14:26:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=747
06/02/2022 14:26:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=749
06/02/2022 14:26:54 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7075155369273016 on epoch=749
06/02/2022 14:26:54 - INFO - __main__ - save last model!
06/02/2022 14:26:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 14:26:54 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 14:26:54 - INFO - __main__ - Printing 3 examples
06/02/2022 14:26:54 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:26:54 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:26:54 - INFO - __main__ - Printing 3 examples
06/02/2022 14:26:54 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:26:54 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:26:54 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:26:54 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:26:54 - INFO - __main__ - Printing 3 examples
06/02/2022 14:26:54 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:26:54 - INFO - __main__ - ['others']
06/02/2022 14:26:54 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:26:54 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:26:54 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:26:56 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:27:00 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:27:00 - INFO - __main__ - task name: emo
06/02/2022 14:27:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:27:01 - INFO - __main__ - Starting training!
06/02/2022 14:27:01 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 14:27:44 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.3_8_predictions.txt
06/02/2022 14:27:44 - INFO - __main__ - Classification-F1 on test data: 0.4074
06/02/2022 14:27:44 - INFO - __main__ - prefix=emo_16_87, lr=0.3, bsz=8, dev_performance=0.7751277272968041, test_performance=0.4074195859958414
06/02/2022 14:27:44 - INFO - __main__ - Running ... prefix=emo_16_87, lr=0.2, bsz=8 ...
06/02/2022 14:27:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:27:45 - INFO - __main__ - Printing 3 examples
06/02/2022 14:27:45 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:27:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:27:45 - INFO - __main__ - Loaded 64 examples from train data
06/02/2022 14:27:45 - INFO - __main__ - Start tokenizing ... 64 instances
06/02/2022 14:27:45 - INFO - __main__ - Printing 3 examples
06/02/2022 14:27:45 - INFO - __main__ -  [emo] you 5050 hahaha not even close haha slightlysmilingface yas
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ -  [emo] punjabi movie as a punjabi this is my answer too you are giving diplomatic ans
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ -  [emo] for exaple what kind of music do you listen to rap music for example eminem
06/02/2022 14:27:45 - INFO - __main__ - ['others']
06/02/2022 14:27:45 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:27:45 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:27:45 - INFO - __main__ - Loaded 64 examples from dev data
06/02/2022 14:27:51 - INFO - __main__ - try to initialize prompt embeddings
06/02/2022 14:27:51 - INFO - __main__ - task name: emo
06/02/2022 14:27:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/02/2022 14:27:52 - INFO - __main__ - Starting training!
06/02/2022 14:27:53 - INFO - __main__ - Step 10 Global step 10 Train loss 7.26 on epoch=2
06/02/2022 14:27:55 - INFO - __main__ - Step 20 Global step 20 Train loss 5.00 on epoch=4
06/02/2022 14:27:56 - INFO - __main__ - Step 30 Global step 30 Train loss 3.37 on epoch=7
06/02/2022 14:27:57 - INFO - __main__ - Step 40 Global step 40 Train loss 2.56 on epoch=9
06/02/2022 14:27:58 - INFO - __main__ - Step 50 Global step 50 Train loss 2.16 on epoch=12
06/02/2022 14:27:59 - INFO - __main__ - Global step 50 Train loss 4.07 Classification-F1 0.20782188214039002 on epoch=12
06/02/2022 14:27:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20782188214039002 on epoch=12, global_step=50
06/02/2022 14:28:00 - INFO - __main__ - Step 60 Global step 60 Train loss 1.85 on epoch=14
06/02/2022 14:28:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.75 on epoch=17
06/02/2022 14:28:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.61 on epoch=19
06/02/2022 14:28:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.66 on epoch=22
06/02/2022 14:28:05 - INFO - __main__ - Step 100 Global step 100 Train loss 1.55 on epoch=24
06/02/2022 14:28:05 - INFO - __main__ - Global step 100 Train loss 1.69 Classification-F1 0.2306338028169014 on epoch=24
06/02/2022 14:28:05 - INFO - __main__ - Saving model with best Classification-F1: 0.20782188214039002 -> 0.2306338028169014 on epoch=24, global_step=100
06/02/2022 14:28:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.38 on epoch=27
06/02/2022 14:28:08 - INFO - __main__ - Step 120 Global step 120 Train loss 1.42 on epoch=29
06/02/2022 14:28:09 - INFO - __main__ - Step 130 Global step 130 Train loss 1.38 on epoch=32
06/02/2022 14:28:10 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=34
06/02/2022 14:28:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.33 on epoch=37
06/02/2022 14:28:12 - INFO - __main__ - Global step 150 Train loss 1.34 Classification-F1 0.13347763347763347 on epoch=37
06/02/2022 14:28:13 - INFO - __main__ - Step 160 Global step 160 Train loss 1.18 on epoch=39
06/02/2022 14:28:15 - INFO - __main__ - Step 170 Global step 170 Train loss 1.10 on epoch=42
06/02/2022 14:28:16 - INFO - __main__ - Step 180 Global step 180 Train loss 1.15 on epoch=44
06/02/2022 14:28:17 - INFO - __main__ - Step 190 Global step 190 Train loss 1.17 on epoch=47
06/02/2022 14:28:18 - INFO - __main__ - Step 200 Global step 200 Train loss 1.06 on epoch=49
06/02/2022 14:28:19 - INFO - __main__ - Global step 200 Train loss 1.13 Classification-F1 0.3577061259286323 on epoch=49
06/02/2022 14:28:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2306338028169014 -> 0.3577061259286323 on epoch=49, global_step=200
06/02/2022 14:28:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=52
06/02/2022 14:28:21 - INFO - __main__ - Step 220 Global step 220 Train loss 1.04 on epoch=54
06/02/2022 14:28:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.03 on epoch=57
06/02/2022 14:28:24 - INFO - __main__ - Step 240 Global step 240 Train loss 1.04 on epoch=59
06/02/2022 14:28:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.07 on epoch=62
06/02/2022 14:28:26 - INFO - __main__ - Global step 250 Train loss 1.07 Classification-F1 0.4106176414120187 on epoch=62
06/02/2022 14:28:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3577061259286323 -> 0.4106176414120187 on epoch=62, global_step=250
06/02/2022 14:28:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.04 on epoch=64
06/02/2022 14:28:28 - INFO - __main__ - Step 270 Global step 270 Train loss 1.09 on epoch=67
06/02/2022 14:28:29 - INFO - __main__ - Step 280 Global step 280 Train loss 1.05 on epoch=69
06/02/2022 14:28:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.97 on epoch=72
06/02/2022 14:28:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=74
06/02/2022 14:28:32 - INFO - __main__ - Global step 300 Train loss 0.99 Classification-F1 0.42694805194805197 on epoch=74
06/02/2022 14:28:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4106176414120187 -> 0.42694805194805197 on epoch=74, global_step=300
06/02/2022 14:28:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.86 on epoch=77
06/02/2022 14:28:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=79
06/02/2022 14:28:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.94 on epoch=82
06/02/2022 14:28:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.91 on epoch=84
06/02/2022 14:28:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=87
06/02/2022 14:28:39 - INFO - __main__ - Global step 350 Train loss 0.92 Classification-F1 0.4998961183743792 on epoch=87
06/02/2022 14:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.42694805194805197 -> 0.4998961183743792 on epoch=87, global_step=350
06/02/2022 14:28:40 - INFO - __main__ - Step 360 Global step 360 Train loss 1.01 on epoch=89
06/02/2022 14:28:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.91 on epoch=92
06/02/2022 14:28:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.87 on epoch=94
06/02/2022 14:28:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.89 on epoch=97
06/02/2022 14:28:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=99
06/02/2022 14:28:46 - INFO - __main__ - Global step 400 Train loss 0.91 Classification-F1 0.32757021727609964 on epoch=99
06/02/2022 14:28:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.90 on epoch=102
06/02/2022 14:28:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.98 on epoch=104
06/02/2022 14:28:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=107
06/02/2022 14:28:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.98 on epoch=109
06/02/2022 14:28:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.94 on epoch=112
06/02/2022 14:28:53 - INFO - __main__ - Global step 450 Train loss 0.93 Classification-F1 0.411144883485309 on epoch=112
06/02/2022 14:28:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=114
06/02/2022 14:28:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=117
06/02/2022 14:28:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.88 on epoch=119
06/02/2022 14:28:58 - INFO - __main__ - Step 490 Global step 490 Train loss 1.08 on epoch=122
06/02/2022 14:28:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.78 on epoch=124
06/02/2022 14:28:59 - INFO - __main__ - Global step 500 Train loss 0.89 Classification-F1 0.43686868686868685 on epoch=124
06/02/2022 14:29:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=127
06/02/2022 14:29:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=129
06/02/2022 14:29:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.90 on epoch=132
06/02/2022 14:29:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=134
06/02/2022 14:29:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.87 on epoch=137
06/02/2022 14:29:06 - INFO - __main__ - Global step 550 Train loss 0.86 Classification-F1 0.5263736263736264 on epoch=137
06/02/2022 14:29:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4998961183743792 -> 0.5263736263736264 on epoch=137, global_step=550
06/02/2022 14:29:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.82 on epoch=139
06/02/2022 14:29:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.91 on epoch=142
06/02/2022 14:29:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.84 on epoch=144
06/02/2022 14:29:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.85 on epoch=147
06/02/2022 14:29:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.88 on epoch=149
06/02/2022 14:29:13 - INFO - __main__ - Global step 600 Train loss 0.86 Classification-F1 0.42036474164133736 on epoch=149
06/02/2022 14:29:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.85 on epoch=152
06/02/2022 14:29:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=154
06/02/2022 14:29:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.86 on epoch=157
06/02/2022 14:29:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.84 on epoch=159
06/02/2022 14:29:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.81 on epoch=162
06/02/2022 14:29:20 - INFO - __main__ - Global step 650 Train loss 0.83 Classification-F1 0.5959549411162315 on epoch=162
06/02/2022 14:29:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5263736263736264 -> 0.5959549411162315 on epoch=162, global_step=650
06/02/2022 14:29:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.92 on epoch=164
06/02/2022 14:29:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.84 on epoch=167
06/02/2022 14:29:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.80 on epoch=169
06/02/2022 14:29:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.82 on epoch=172
06/02/2022 14:29:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.86 on epoch=174
06/02/2022 14:29:26 - INFO - __main__ - Global step 700 Train loss 0.85 Classification-F1 0.3660749751737835 on epoch=174
06/02/2022 14:29:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.81 on epoch=177
06/02/2022 14:29:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.79 on epoch=179
06/02/2022 14:29:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.77 on epoch=182
06/02/2022 14:29:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.85 on epoch=184
06/02/2022 14:29:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=187
06/02/2022 14:29:33 - INFO - __main__ - Global step 750 Train loss 0.79 Classification-F1 0.5951576576576576 on epoch=187
06/02/2022 14:29:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.79 on epoch=189
06/02/2022 14:29:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.85 on epoch=192
06/02/2022 14:29:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=194
06/02/2022 14:29:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.75 on epoch=197
06/02/2022 14:29:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.85 on epoch=199
06/02/2022 14:29:40 - INFO - __main__ - Global step 800 Train loss 0.78 Classification-F1 0.4294349128540305 on epoch=199
06/02/2022 14:29:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=202
06/02/2022 14:29:42 - INFO - __main__ - Step 820 Global step 820 Train loss 0.74 on epoch=204
06/02/2022 14:29:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.82 on epoch=207
06/02/2022 14:29:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.87 on epoch=209
06/02/2022 14:29:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.78 on epoch=212
06/02/2022 14:29:46 - INFO - __main__ - Global step 850 Train loss 0.77 Classification-F1 0.5567890701431074 on epoch=212
06/02/2022 14:29:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=214
06/02/2022 14:29:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=217
06/02/2022 14:29:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.83 on epoch=219
06/02/2022 14:29:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.73 on epoch=222
06/02/2022 14:29:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.71 on epoch=224
06/02/2022 14:29:53 - INFO - __main__ - Global step 900 Train loss 0.75 Classification-F1 0.4935483870967742 on epoch=224
06/02/2022 14:29:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.67 on epoch=227
06/02/2022 14:29:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.68 on epoch=229
06/02/2022 14:29:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=232
06/02/2022 14:29:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.72 on epoch=234
06/02/2022 14:30:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=237
06/02/2022 14:30:00 - INFO - __main__ - Global step 950 Train loss 0.70 Classification-F1 0.6682961460446248 on epoch=237
06/02/2022 14:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5959549411162315 -> 0.6682961460446248 on epoch=237, global_step=950
06/02/2022 14:30:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.57 on epoch=239
06/02/2022 14:30:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.69 on epoch=242
06/02/2022 14:30:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.67 on epoch=244
06/02/2022 14:30:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.60 on epoch=247
06/02/2022 14:30:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=249
06/02/2022 14:30:07 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.5476312741222209 on epoch=249
06/02/2022 14:30:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=252
06/02/2022 14:30:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.64 on epoch=254
06/02/2022 14:30:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=257
06/02/2022 14:30:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.57 on epoch=259
06/02/2022 14:30:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=262
06/02/2022 14:30:14 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.5992052635007491 on epoch=262
06/02/2022 14:30:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=264
06/02/2022 14:30:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.56 on epoch=267
06/02/2022 14:30:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=269
06/02/2022 14:30:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.59 on epoch=272
06/02/2022 14:30:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=274
06/02/2022 14:30:20 - INFO - __main__ - Global step 1100 Train loss 0.55 Classification-F1 0.5926767676767677 on epoch=274
06/02/2022 14:30:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=277
06/02/2022 14:30:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=279
06/02/2022 14:30:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=282
06/02/2022 14:30:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=284
06/02/2022 14:30:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=287
06/02/2022 14:30:27 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.6741119431643625 on epoch=287
06/02/2022 14:30:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6682961460446248 -> 0.6741119431643625 on epoch=287, global_step=1150
06/02/2022 14:30:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=289
06/02/2022 14:30:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=292
06/02/2022 14:30:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=294
06/02/2022 14:30:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=297
06/02/2022 14:30:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=299
06/02/2022 14:30:34 - INFO - __main__ - Global step 1200 Train loss 0.46 Classification-F1 0.5976049159348661 on epoch=299
06/02/2022 14:30:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=302
06/02/2022 14:30:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=304
06/02/2022 14:30:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=307
06/02/2022 14:30:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=309
06/02/2022 14:30:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=312
06/02/2022 14:30:41 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.71829086473299 on epoch=312
06/02/2022 14:30:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6741119431643625 -> 0.71829086473299 on epoch=312, global_step=1250
06/02/2022 14:30:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=314
06/02/2022 14:30:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=317
06/02/2022 14:30:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=319
06/02/2022 14:30:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=322
06/02/2022 14:30:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=324
06/02/2022 14:30:47 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.6756823659696437 on epoch=324
06/02/2022 14:30:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=327
06/02/2022 14:30:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=329
06/02/2022 14:30:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=332
06/02/2022 14:30:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=334
06/02/2022 14:30:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=337
06/02/2022 14:30:54 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.6813431824342641 on epoch=337
06/02/2022 14:30:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=339
06/02/2022 14:30:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=342
06/02/2022 14:30:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=344
06/02/2022 14:30:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=347
06/02/2022 14:31:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=349
06/02/2022 14:31:01 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.6535557184750733 on epoch=349
06/02/2022 14:31:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=352
06/02/2022 14:31:03 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=354
06/02/2022 14:31:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=357
06/02/2022 14:31:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=359
06/02/2022 14:31:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=362
06/02/2022 14:31:08 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.6860906862745099 on epoch=362
06/02/2022 14:31:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=364
06/02/2022 14:31:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=367
06/02/2022 14:31:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=369
06/02/2022 14:31:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=372
06/02/2022 14:31:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=374
06/02/2022 14:31:14 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.7019327731092437 on epoch=374
06/02/2022 14:31:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=377
06/02/2022 14:31:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=379
06/02/2022 14:31:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=382
06/02/2022 14:31:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=384
06/02/2022 14:31:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=387
06/02/2022 14:31:21 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.6752822341057635 on epoch=387
06/02/2022 14:31:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=389
06/02/2022 14:31:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=392
06/02/2022 14:31:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=394
06/02/2022 14:31:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=397
06/02/2022 14:31:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=399
06/02/2022 14:31:28 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.6736479207067443 on epoch=399
06/02/2022 14:31:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=402
06/02/2022 14:31:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=404
06/02/2022 14:31:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=407
06/02/2022 14:31:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=409
06/02/2022 14:31:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=412
06/02/2022 14:31:35 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.6859106314848313 on epoch=412
06/02/2022 14:31:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=414
06/02/2022 14:31:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=417
06/02/2022 14:31:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=419
06/02/2022 14:31:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=422
06/02/2022 14:31:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=424
06/02/2022 14:31:41 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7572979512317748 on epoch=424
06/02/2022 14:31:41 - INFO - __main__ - Saving model with best Classification-F1: 0.71829086473299 -> 0.7572979512317748 on epoch=424, global_step=1700
06/02/2022 14:31:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=427
06/02/2022 14:31:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=429
06/02/2022 14:31:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=432
06/02/2022 14:31:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=434
06/02/2022 14:31:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=437
06/02/2022 14:31:48 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.7012364903885817 on epoch=437
06/02/2022 14:31:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=439
06/02/2022 14:31:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=442
06/02/2022 14:31:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=444
06/02/2022 14:31:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=447
06/02/2022 14:31:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=449
06/02/2022 14:31:55 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.6809207553989338 on epoch=449
06/02/2022 14:31:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=452
06/02/2022 14:31:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=454
06/02/2022 14:31:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=457
06/02/2022 14:32:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=459
06/02/2022 14:32:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=462
06/02/2022 14:32:02 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7431607744107743 on epoch=462
06/02/2022 14:32:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=464
06/02/2022 14:32:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=467
06/02/2022 14:32:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=469
06/02/2022 14:32:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=472
06/02/2022 14:32:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=474
06/02/2022 14:32:08 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.6795093795093795 on epoch=474
06/02/2022 14:32:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=477
06/02/2022 14:32:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=479
06/02/2022 14:32:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=482
06/02/2022 14:32:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=484
06/02/2022 14:32:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=487
06/02/2022 14:32:15 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7524480095068331 on epoch=487
06/02/2022 14:32:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=489
06/02/2022 14:32:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=492
06/02/2022 14:32:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=494
06/02/2022 14:32:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=497
06/02/2022 14:32:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=499
06/02/2022 14:32:22 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.716017316017316 on epoch=499
06/02/2022 14:32:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=502
06/02/2022 14:32:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=504
06/02/2022 14:32:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=507
06/02/2022 14:32:27 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=509
06/02/2022 14:32:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=512
06/02/2022 14:32:28 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.6656318827371459 on epoch=512
06/02/2022 14:32:30 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=514
06/02/2022 14:32:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=517
06/02/2022 14:32:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=519
06/02/2022 14:32:33 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=522
06/02/2022 14:32:35 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=524
06/02/2022 14:32:35 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.6837218337218337 on epoch=524
06/02/2022 14:32:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=527
06/02/2022 14:32:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=529
06/02/2022 14:32:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=532
06/02/2022 14:32:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=534
06/02/2022 14:32:42 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=537
06/02/2022 14:32:42 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7397937710437711 on epoch=537
06/02/2022 14:32:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=539
06/02/2022 14:32:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=542
06/02/2022 14:32:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=544
06/02/2022 14:32:48 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=547
06/02/2022 14:32:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=549
06/02/2022 14:32:50 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7450472942800308 on epoch=549
06/02/2022 14:32:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=552
06/02/2022 14:32:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=554
06/02/2022 14:32:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=557
06/02/2022 14:32:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=559
06/02/2022 14:32:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=562
06/02/2022 14:32:56 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7450472942800308 on epoch=562
06/02/2022 14:32:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=564
06/02/2022 14:32:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=567
06/02/2022 14:33:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=569
06/02/2022 14:33:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=572
06/02/2022 14:33:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=574
06/02/2022 14:33:03 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7315656565656565 on epoch=574
06/02/2022 14:33:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=577
06/02/2022 14:33:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=579
06/02/2022 14:33:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=582
06/02/2022 14:33:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=584
06/02/2022 14:33:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=587
06/02/2022 14:33:10 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.7177421271538919 on epoch=587
06/02/2022 14:33:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=589
06/02/2022 14:33:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=592
06/02/2022 14:33:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=594
06/02/2022 14:33:15 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=597
06/02/2022 14:33:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=599
06/02/2022 14:33:17 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7704650673400674 on epoch=599
06/02/2022 14:33:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7572979512317748 -> 0.7704650673400674 on epoch=599, global_step=2400
06/02/2022 14:33:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=602
06/02/2022 14:33:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=604
06/02/2022 14:33:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=607
06/02/2022 14:33:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=609
06/02/2022 14:33:23 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=612
06/02/2022 14:33:24 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.743462451595058 on epoch=612
06/02/2022 14:33:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=614
06/02/2022 14:33:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=617
06/02/2022 14:33:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=619
06/02/2022 14:33:29 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=622
06/02/2022 14:33:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=624
06/02/2022 14:33:30 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7526455026455026 on epoch=624
06/02/2022 14:33:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=627
06/02/2022 14:33:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=629
06/02/2022 14:33:34 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=632
06/02/2022 14:33:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=634
06/02/2022 14:33:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=637
06/02/2022 14:33:37 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7313886776802377 on epoch=637
06/02/2022 14:33:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=639
06/02/2022 14:33:40 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=642
06/02/2022 14:33:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=644
06/02/2022 14:33:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=647
06/02/2022 14:33:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=649
06/02/2022 14:33:44 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7512955182072829 on epoch=649
06/02/2022 14:33:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=652
06/02/2022 14:33:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=654
06/02/2022 14:33:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=657
06/02/2022 14:33:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=659
06/02/2022 14:33:50 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=662
06/02/2022 14:33:51 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.7450472942800308 on epoch=662
06/02/2022 14:33:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=664
06/02/2022 14:33:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=667
06/02/2022 14:33:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=669
06/02/2022 14:33:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=672
06/02/2022 14:33:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=674
06/02/2022 14:33:57 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7182800982800983 on epoch=674
06/02/2022 14:33:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=677
06/02/2022 14:34:00 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=679
06/02/2022 14:34:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=682
06/02/2022 14:34:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=684
06/02/2022 14:34:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=687
06/02/2022 14:34:04 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7031106148753209 on epoch=687
06/02/2022 14:34:05 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=689
06/02/2022 14:34:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=692
06/02/2022 14:34:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=694
06/02/2022 14:34:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=697
06/02/2022 14:34:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=699
06/02/2022 14:34:11 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7524480095068331 on epoch=699
06/02/2022 14:34:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=702
06/02/2022 14:34:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=704
06/02/2022 14:34:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=707
06/02/2022 14:34:16 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=709
06/02/2022 14:34:17 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=712
06/02/2022 14:34:18 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7383640324816796 on epoch=712
06/02/2022 14:34:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=714
06/02/2022 14:34:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=717
06/02/2022 14:34:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=719
06/02/2022 14:34:23 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=722
06/02/2022 14:34:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=724
06/02/2022 14:34:24 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.6939679320003863 on epoch=724
06/02/2022 14:34:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=727
06/02/2022 14:34:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=729
06/02/2022 14:34:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=732
06/02/2022 14:34:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=734
06/02/2022 14:34:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=737
06/02/2022 14:34:31 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7660014005602241 on epoch=737
06/02/2022 14:34:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=739
06/02/2022 14:34:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=742
06/02/2022 14:34:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=744
06/02/2022 14:34:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=747
06/02/2022 14:34:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=749
06/02/2022 14:34:38 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.7520424836601307 on epoch=749
06/02/2022 14:34:38 - INFO - __main__ - save last model!
06/02/2022 14:34:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/02/2022 14:34:38 - INFO - __main__ - Start tokenizing ... 5509 instances
06/02/2022 14:34:38 - INFO - __main__ - Printing 3 examples
06/02/2022 14:34:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/02/2022 14:34:38 - INFO - __main__ - ['others']
06/02/2022 14:34:38 - INFO - __main__ -  [emo] what you like very little things ok
06/02/2022 14:34:38 - INFO - __main__ - ['others']
06/02/2022 14:34:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/02/2022 14:34:38 - INFO - __main__ - ['others']
06/02/2022 14:34:38 - INFO - __main__ - Tokenizing Input ...
06/02/2022 14:34:40 - INFO - __main__ - Tokenizing Output ...
06/02/2022 14:34:45 - INFO - __main__ - Loaded 5509 examples from test data
06/02/2022 14:35:28 - INFO - __main__ - Saved prediction in models/T5-base-cls2cls/singletask-emo/emo_16_87_0.2_8_predictions.txt
06/02/2022 14:35:28 - INFO - __main__ - Classification-F1 on test data: 0.4131
06/02/2022 14:35:28 - INFO - __main__ - prefix=emo_16_87, lr=0.2, bsz=8, dev_performance=0.7704650673400674, test_performance=0.41314744943622894
