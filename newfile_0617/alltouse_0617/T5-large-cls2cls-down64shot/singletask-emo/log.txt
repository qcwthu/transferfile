05/23/2022 10:25:32 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/23/2022 10:25:32 - INFO - __main__ - models/T5-large-cls2cls-down64shot/singletask-emo
05/23/2022 10:25:32 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/23/2022 10:25:32 - INFO - __main__ - models/T5-large-cls2cls-down64shot/singletask-emo
05/23/2022 10:25:33 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/23/2022 10:25:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/23/2022 10:25:34 - INFO - __main__ - args.device: cuda:0
05/23/2022 10:25:34 - INFO - __main__ - Using 2 gpus
05/23/2022 10:25:34 - INFO - __main__ - args.device: cuda:1
05/23/2022 10:25:34 - INFO - __main__ - Using 2 gpus
05/23/2022 10:25:34 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/23/2022 10:25:34 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/23/2022 10:25:38 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.5, bsz=8 ...
05/23/2022 10:25:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:25:39 - INFO - __main__ - Printing 3 examples
05/23/2022 10:25:39 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:25:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:25:39 - INFO - __main__ - Printing 3 examples
05/23/2022 10:25:39 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:25:39 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 10:25:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:25:39 - INFO - __main__ - Printing 3 examples
05/23/2022 10:25:39 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:25:39 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 10:25:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:25:39 - INFO - __main__ - Printing 3 examples
05/23/2022 10:25:39 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 10:25:39 - INFO - __main__ - ['others']
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:25:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:25:40 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 10:25:40 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 10:25:57 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 10:25:57 - INFO - __main__ - task name: emo
05/23/2022 10:25:58 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 10:25:58 - INFO - __main__ - task name: emo
05/23/2022 10:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 10:25:58 - INFO - __main__ - Starting training!
05/23/2022 10:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 10:25:58 - INFO - __main__ - Starting training!
05/23/2022 10:26:02 - INFO - __main__ - Step 10 Global step 10 Train loss 6.23 on epoch=0
05/23/2022 10:26:04 - INFO - __main__ - Step 20 Global step 20 Train loss 2.42 on epoch=1
05/23/2022 10:26:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.24 on epoch=1
05/23/2022 10:26:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.09 on epoch=2
05/23/2022 10:26:11 - INFO - __main__ - Step 50 Global step 50 Train loss 1.08 on epoch=3
05/23/2022 10:26:15 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.10448634297799578 on epoch=3
05/23/2022 10:26:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10448634297799578 on epoch=3, global_step=50
05/23/2022 10:26:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=3
05/23/2022 10:26:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=4
05/23/2022 10:26:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=4
05/23/2022 10:26:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=5
05/23/2022 10:26:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=6
05/23/2022 10:26:31 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.15002923976608187 on epoch=6
05/23/2022 10:26:31 - INFO - __main__ - Saving model with best Classification-F1: 0.10448634297799578 -> 0.15002923976608187 on epoch=6, global_step=100
05/23/2022 10:26:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.84 on epoch=6
05/23/2022 10:26:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.82 on epoch=7
05/23/2022 10:26:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.91 on epoch=8
05/23/2022 10:26:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=8
05/23/2022 10:26:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.93 on epoch=9
05/23/2022 10:26:46 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.10031347962382445 on epoch=9
05/23/2022 10:26:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=9
05/23/2022 10:26:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.94 on epoch=10
05/23/2022 10:26:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=11
05/23/2022 10:26:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=11
05/23/2022 10:26:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=12
05/23/2022 10:27:02 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.1917964529475321 on epoch=12
05/23/2022 10:27:02 - INFO - __main__ - Saving model with best Classification-F1: 0.15002923976608187 -> 0.1917964529475321 on epoch=12, global_step=200
05/23/2022 10:27:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=13
05/23/2022 10:27:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=13
05/23/2022 10:27:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=14
05/23/2022 10:27:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=14
05/23/2022 10:27:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
05/23/2022 10:27:18 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.27296642087147194 on epoch=15
05/23/2022 10:27:18 - INFO - __main__ - Saving model with best Classification-F1: 0.1917964529475321 -> 0.27296642087147194 on epoch=15, global_step=250
05/23/2022 10:27:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=16
05/23/2022 10:27:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=16
05/23/2022 10:27:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.87 on epoch=17
05/23/2022 10:27:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=18
05/23/2022 10:27:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=18
05/23/2022 10:27:34 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.2992733862244308 on epoch=18
05/23/2022 10:27:34 - INFO - __main__ - Saving model with best Classification-F1: 0.27296642087147194 -> 0.2992733862244308 on epoch=18, global_step=300
05/23/2022 10:27:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
05/23/2022 10:27:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.80 on epoch=19
05/23/2022 10:27:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=20
05/23/2022 10:27:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=21
05/23/2022 10:27:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=21
05/23/2022 10:27:49 - INFO - __main__ - Global step 350 Train loss 0.74 Classification-F1 0.3996612974317011 on epoch=21
05/23/2022 10:27:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2992733862244308 -> 0.3996612974317011 on epoch=21, global_step=350
05/23/2022 10:27:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=22
05/23/2022 10:27:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=23
05/23/2022 10:27:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.65 on epoch=23
05/23/2022 10:27:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=24
05/23/2022 10:28:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=24
05/23/2022 10:28:05 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.5286399016520099 on epoch=24
05/23/2022 10:28:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3996612974317011 -> 0.5286399016520099 on epoch=24, global_step=400
05/23/2022 10:28:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=25
05/23/2022 10:28:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=26
05/23/2022 10:28:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=26
05/23/2022 10:28:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=27
05/23/2022 10:28:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=28
05/23/2022 10:28:21 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.43956679894179895 on epoch=28
05/23/2022 10:28:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=28
05/23/2022 10:28:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=29
05/23/2022 10:28:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=29
05/23/2022 10:28:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=30
05/23/2022 10:28:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
05/23/2022 10:28:37 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.5842008066708964 on epoch=31
05/23/2022 10:28:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5286399016520099 -> 0.5842008066708964 on epoch=31, global_step=500
05/23/2022 10:28:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=31
05/23/2022 10:28:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=32
05/23/2022 10:28:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=33
05/23/2022 10:28:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/23/2022 10:28:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/23/2022 10:28:53 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.7030901949009181 on epoch=34
05/23/2022 10:28:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5842008066708964 -> 0.7030901949009181 on epoch=34, global_step=550
05/23/2022 10:28:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=34
05/23/2022 10:28:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
05/23/2022 10:29:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=36
05/23/2022 10:29:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/23/2022 10:29:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=37
05/23/2022 10:29:09 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.7245457898336364 on epoch=37
05/23/2022 10:29:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7030901949009181 -> 0.7245457898336364 on epoch=37, global_step=600
05/23/2022 10:29:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=38
05/23/2022 10:29:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=38
05/23/2022 10:29:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=39
05/23/2022 10:29:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=39
05/23/2022 10:29:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/23/2022 10:29:25 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.6916013028116278 on epoch=40
05/23/2022 10:29:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=41
05/23/2022 10:29:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=41
05/23/2022 10:29:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=42
05/23/2022 10:29:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=43
05/23/2022 10:29:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=43
05/23/2022 10:29:41 - INFO - __main__ - Global step 700 Train loss 0.24 Classification-F1 0.6918156660169461 on epoch=43
05/23/2022 10:29:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=44
05/23/2022 10:29:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=44
05/23/2022 10:29:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=45
05/23/2022 10:29:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=46
05/23/2022 10:29:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=46
05/23/2022 10:29:56 - INFO - __main__ - Global step 750 Train loss 0.25 Classification-F1 0.761795254303689 on epoch=46
05/23/2022 10:29:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7245457898336364 -> 0.761795254303689 on epoch=46, global_step=750
05/23/2022 10:29:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=47
05/23/2022 10:30:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.11 on epoch=48
05/23/2022 10:30:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.08 on epoch=48
05/23/2022 10:30:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=49
05/23/2022 10:30:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/23/2022 10:30:12 - INFO - __main__ - Global step 800 Train loss 0.16 Classification-F1 0.727637686092425 on epoch=49
05/23/2022 10:30:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=50
05/23/2022 10:30:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=51
05/23/2022 10:30:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=51
05/23/2022 10:30:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=52
05/23/2022 10:30:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/23/2022 10:30:28 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.7190635535379584 on epoch=53
05/23/2022 10:30:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=53
05/23/2022 10:30:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.10 on epoch=54
05/23/2022 10:30:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=54
05/23/2022 10:30:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=55
05/23/2022 10:30:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=56
05/23/2022 10:30:44 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.6944961665982105 on epoch=56
05/23/2022 10:30:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=56
05/23/2022 10:30:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=57
05/23/2022 10:30:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=58
05/23/2022 10:30:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=58
05/23/2022 10:30:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=59
05/23/2022 10:30:59 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.7590991387747112 on epoch=59
05/23/2022 10:31:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=59
05/23/2022 10:31:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.13 on epoch=60
05/23/2022 10:31:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=61
05/23/2022 10:31:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=61
05/23/2022 10:31:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=62
05/23/2022 10:31:15 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.7509518192326909 on epoch=62
05/23/2022 10:31:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=63
05/23/2022 10:31:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=63
05/23/2022 10:31:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=64
05/23/2022 10:31:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=64
05/23/2022 10:31:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=65
05/23/2022 10:31:31 - INFO - __main__ - Global step 1050 Train loss 0.13 Classification-F1 0.7916446972972403 on epoch=65
05/23/2022 10:31:31 - INFO - __main__ - Saving model with best Classification-F1: 0.761795254303689 -> 0.7916446972972403 on epoch=65, global_step=1050
05/23/2022 10:31:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=66
05/23/2022 10:31:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.09 on epoch=66
05/23/2022 10:31:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=67
05/23/2022 10:31:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=68
05/23/2022 10:31:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=68
05/23/2022 10:31:47 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.7556073935839331 on epoch=68
05/23/2022 10:31:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=69
05/23/2022 10:31:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=69
05/23/2022 10:31:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=70
05/23/2022 10:31:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=71
05/23/2022 10:32:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=71
05/23/2022 10:32:04 - INFO - __main__ - Global step 1150 Train loss 0.09 Classification-F1 0.7629704445880916 on epoch=71
05/23/2022 10:32:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=72
05/23/2022 10:32:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=73
05/23/2022 10:32:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=73
05/23/2022 10:32:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=74
05/23/2022 10:32:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=74
05/23/2022 10:32:20 - INFO - __main__ - Global step 1200 Train loss 0.09 Classification-F1 0.7878062127421759 on epoch=74
05/23/2022 10:32:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=75
05/23/2022 10:32:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=76
05/23/2022 10:32:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.10 on epoch=76
05/23/2022 10:32:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/23/2022 10:32:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.05 on epoch=78
05/23/2022 10:32:36 - INFO - __main__ - Global step 1250 Train loss 0.10 Classification-F1 0.7299168037423072 on epoch=78
05/23/2022 10:32:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/23/2022 10:32:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=79
05/23/2022 10:32:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=79
05/23/2022 10:32:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=80
05/23/2022 10:32:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=81
05/23/2022 10:32:51 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.752541604104104 on epoch=81
05/23/2022 10:32:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=81
05/23/2022 10:32:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=82
05/23/2022 10:32:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=83
05/23/2022 10:33:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=83
05/23/2022 10:33:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=84
05/23/2022 10:33:08 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7504099571221121 on epoch=84
05/23/2022 10:33:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=84
05/23/2022 10:33:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.09 on epoch=85
05/23/2022 10:33:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=86
05/23/2022 10:33:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=86
05/23/2022 10:33:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=87
05/23/2022 10:33:23 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.7262084909625894 on epoch=87
05/23/2022 10:33:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=88
05/23/2022 10:33:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.09 on epoch=88
05/23/2022 10:33:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.04 on epoch=89
05/23/2022 10:33:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=89
05/23/2022 10:33:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=90
05/23/2022 10:33:39 - INFO - __main__ - Global step 1450 Train loss 0.06 Classification-F1 0.7883170667353555 on epoch=90
05/23/2022 10:33:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=91
05/23/2022 10:33:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=91
05/23/2022 10:33:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=92
05/23/2022 10:33:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=93
05/23/2022 10:33:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=93
05/23/2022 10:33:55 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.7970380746807809 on epoch=93
05/23/2022 10:33:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7916446972972403 -> 0.7970380746807809 on epoch=93, global_step=1500
05/23/2022 10:33:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=94
05/23/2022 10:34:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=94
05/23/2022 10:34:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=95
05/23/2022 10:34:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=96
05/23/2022 10:34:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=96
05/23/2022 10:34:11 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7822596209427716 on epoch=96
05/23/2022 10:34:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=97
05/23/2022 10:34:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=98
05/23/2022 10:34:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=98
05/23/2022 10:34:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=99
05/23/2022 10:34:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=99
05/23/2022 10:34:27 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.7756132439407242 on epoch=99
05/23/2022 10:34:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=100
05/23/2022 10:34:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=101
05/23/2022 10:34:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=101
05/23/2022 10:34:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=102
05/23/2022 10:34:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=103
05/23/2022 10:34:43 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7906579934087186 on epoch=103
05/23/2022 10:34:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=103
05/23/2022 10:34:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=104
05/23/2022 10:34:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=104
05/23/2022 10:34:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=105
05/23/2022 10:34:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=106
05/23/2022 10:34:58 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7766929673809061 on epoch=106
05/23/2022 10:35:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=106
05/23/2022 10:35:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=107
05/23/2022 10:35:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=108
05/23/2022 10:35:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=108
05/23/2022 10:35:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=109
05/23/2022 10:35:14 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7735777243589743 on epoch=109
05/23/2022 10:35:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=109
05/23/2022 10:35:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=110
05/23/2022 10:35:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=111
05/23/2022 10:35:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=111
05/23/2022 10:35:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=112
05/23/2022 10:35:30 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7791649368420267 on epoch=112
05/23/2022 10:35:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=113
05/23/2022 10:35:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=113
05/23/2022 10:35:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=114
05/23/2022 10:35:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=114
05/23/2022 10:35:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=115
05/23/2022 10:35:46 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.7615125028375196 on epoch=115
05/23/2022 10:35:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=116
05/23/2022 10:35:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=116
05/23/2022 10:35:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=117
05/23/2022 10:35:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=118
05/23/2022 10:35:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=118
05/23/2022 10:36:02 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.7599390261154968 on epoch=118
05/23/2022 10:36:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=119
05/23/2022 10:36:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=119
05/23/2022 10:36:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=120
05/23/2022 10:36:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=121
05/23/2022 10:36:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=121
05/23/2022 10:36:18 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7912646939424346 on epoch=121
05/23/2022 10:36:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=122
05/23/2022 10:36:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=123
05/23/2022 10:36:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=123
05/23/2022 10:36:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=124
05/23/2022 10:36:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=124
05/23/2022 10:36:34 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.7693029534218059 on epoch=124
05/23/2022 10:36:37 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/23/2022 10:36:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/23/2022 10:36:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=126
05/23/2022 10:36:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=127
05/23/2022 10:36:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=128
05/23/2022 10:36:50 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7910222352478286 on epoch=128
05/23/2022 10:36:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.01 on epoch=128
05/23/2022 10:36:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=129
05/23/2022 10:36:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=129
05/23/2022 10:37:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=130
05/23/2022 10:37:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=131
05/23/2022 10:37:07 - INFO - __main__ - Global step 2100 Train loss 0.03 Classification-F1 0.7981187728423894 on epoch=131
05/23/2022 10:37:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7970380746807809 -> 0.7981187728423894 on epoch=131, global_step=2100
05/23/2022 10:37:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=131
05/23/2022 10:37:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.01 on epoch=132
05/23/2022 10:37:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.02 on epoch=133
05/23/2022 10:37:17 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=133
05/23/2022 10:37:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=134
05/23/2022 10:37:23 - INFO - __main__ - Global step 2150 Train loss 0.02 Classification-F1 0.764907897901306 on epoch=134
05/23/2022 10:37:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=134
05/23/2022 10:37:28 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/23/2022 10:37:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=136
05/23/2022 10:37:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=136
05/23/2022 10:37:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=137
05/23/2022 10:37:39 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.7882098267622462 on epoch=137
05/23/2022 10:37:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=138
05/23/2022 10:37:44 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=138
05/23/2022 10:37:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=139
05/23/2022 10:37:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=139
05/23/2022 10:37:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
05/23/2022 10:37:55 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.7764006216939859 on epoch=140
05/23/2022 10:37:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=141
05/23/2022 10:38:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=141
05/23/2022 10:38:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.00 on epoch=142
05/23/2022 10:38:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=143
05/23/2022 10:38:07 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=143
05/23/2022 10:38:11 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.7596048932297068 on epoch=143
05/23/2022 10:38:14 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=144
05/23/2022 10:38:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=144
05/23/2022 10:38:19 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/23/2022 10:38:21 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=146
05/23/2022 10:38:24 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=146
05/23/2022 10:38:27 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7758480522019128 on epoch=146
05/23/2022 10:38:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
05/23/2022 10:38:32 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=148
05/23/2022 10:38:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/23/2022 10:38:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=149
05/23/2022 10:38:40 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/23/2022 10:38:43 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7765029929998309 on epoch=149
05/23/2022 10:38:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=150
05/23/2022 10:38:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=151
05/23/2022 10:38:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.01 on epoch=151
05/23/2022 10:38:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=152
05/23/2022 10:38:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.02 on epoch=153
05/23/2022 10:38:59 - INFO - __main__ - Global step 2450 Train loss 0.01 Classification-F1 0.8022064752125402 on epoch=153
05/23/2022 10:38:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7981187728423894 -> 0.8022064752125402 on epoch=153, global_step=2450
05/23/2022 10:39:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=153
05/23/2022 10:39:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=154
05/23/2022 10:39:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=154
05/23/2022 10:39:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.00 on epoch=155
05/23/2022 10:39:11 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
05/23/2022 10:39:15 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.7773520569326025 on epoch=156
05/23/2022 10:39:17 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=156
05/23/2022 10:39:20 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=157
05/23/2022 10:39:22 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/23/2022 10:39:25 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/23/2022 10:39:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=159
05/23/2022 10:39:31 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7622829666217632 on epoch=159
05/23/2022 10:39:33 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=159
05/23/2022 10:39:35 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.01 on epoch=160
05/23/2022 10:39:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/23/2022 10:39:40 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
05/23/2022 10:39:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=162
05/23/2022 10:39:46 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.765084434665927 on epoch=162
05/23/2022 10:39:49 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
05/23/2022 10:39:51 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=163
05/23/2022 10:39:54 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.00 on epoch=164
05/23/2022 10:39:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=164
05/23/2022 10:39:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/23/2022 10:40:02 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7729847885974368 on epoch=165
05/23/2022 10:40:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
05/23/2022 10:40:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=166
05/23/2022 10:40:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=167
05/23/2022 10:40:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=168
05/23/2022 10:40:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=168
05/23/2022 10:40:18 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7533465237236763 on epoch=168
05/23/2022 10:40:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/23/2022 10:40:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=169
05/23/2022 10:40:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
05/23/2022 10:40:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/23/2022 10:40:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=171
05/23/2022 10:40:34 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7771144278606965 on epoch=171
05/23/2022 10:40:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
05/23/2022 10:40:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=173
05/23/2022 10:40:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=173
05/23/2022 10:40:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/23/2022 10:40:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/23/2022 10:40:50 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7637525964079517 on epoch=174
05/23/2022 10:40:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=175
05/23/2022 10:40:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/23/2022 10:40:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=176
05/23/2022 10:40:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
05/23/2022 10:41:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/23/2022 10:41:05 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7692669172932332 on epoch=178
05/23/2022 10:41:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/23/2022 10:41:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=179
05/23/2022 10:41:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=179
05/23/2022 10:41:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/23/2022 10:41:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=181
05/23/2022 10:41:21 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7774440597177086 on epoch=181
05/23/2022 10:41:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=181
05/23/2022 10:41:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/23/2022 10:41:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=183
05/23/2022 10:41:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
05/23/2022 10:41:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 10:41:37 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7876774749242866 on epoch=184
05/23/2022 10:41:40 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/23/2022 10:41:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
05/23/2022 10:41:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=186
05/23/2022 10:41:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/23/2022 10:41:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/23/2022 10:41:51 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:41:51 - INFO - __main__ - Printing 3 examples
05/23/2022 10:41:51 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:41:51 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:41:51 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 10:41:51 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:41:51 - INFO - __main__ - Printing 3 examples
05/23/2022 10:41:51 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 10:41:51 - INFO - __main__ - ['others']
05/23/2022 10:41:51 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:41:51 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:41:52 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 10:41:53 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7876804894888912 on epoch=187
05/23/2022 10:41:53 - INFO - __main__ - save last model!
05/23/2022 10:41:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 10:41:53 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 10:41:53 - INFO - __main__ - Printing 3 examples
05/23/2022 10:41:53 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 10:41:53 - INFO - __main__ - ['others']
05/23/2022 10:41:53 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 10:41:53 - INFO - __main__ - ['others']
05/23/2022 10:41:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 10:41:53 - INFO - __main__ - ['others']
05/23/2022 10:41:53 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:41:56 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:42:01 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 10:42:10 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 10:42:10 - INFO - __main__ - task name: emo
05/23/2022 10:42:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 10:42:11 - INFO - __main__ - Starting training!
05/23/2022 10:43:19 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_100_0.5_8_predictions.txt
05/23/2022 10:43:19 - INFO - __main__ - Classification-F1 on test data: 0.5738
05/23/2022 10:43:19 - INFO - __main__ - prefix=emo_64_100, lr=0.5, bsz=8, dev_performance=0.8022064752125402, test_performance=0.5738011310126594
05/23/2022 10:43:19 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.4, bsz=8 ...
05/23/2022 10:43:20 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:43:20 - INFO - __main__ - Printing 3 examples
05/23/2022 10:43:20 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 10:43:20 - INFO - __main__ - ['others']
05/23/2022 10:43:20 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 10:43:20 - INFO - __main__ - ['others']
05/23/2022 10:43:20 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 10:43:20 - INFO - __main__ - ['others']
05/23/2022 10:43:20 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:43:20 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:43:21 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 10:43:21 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:43:21 - INFO - __main__ - Printing 3 examples
05/23/2022 10:43:21 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 10:43:21 - INFO - __main__ - ['others']
05/23/2022 10:43:21 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 10:43:21 - INFO - __main__ - ['others']
05/23/2022 10:43:21 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 10:43:21 - INFO - __main__ - ['others']
05/23/2022 10:43:21 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:43:21 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:43:21 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 10:43:36 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 10:43:36 - INFO - __main__ - task name: emo
05/23/2022 10:43:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 10:43:37 - INFO - __main__ - Starting training!
05/23/2022 10:43:40 - INFO - __main__ - Step 10 Global step 10 Train loss 7.01 on epoch=0
05/23/2022 10:43:42 - INFO - __main__ - Step 20 Global step 20 Train loss 3.36 on epoch=1
05/23/2022 10:43:44 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=1
05/23/2022 10:43:47 - INFO - __main__ - Step 40 Global step 40 Train loss 1.27 on epoch=2
05/23/2022 10:43:49 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=3
05/23/2022 10:43:53 - INFO - __main__ - Global step 50 Train loss 2.84 Classification-F1 0.11544984655144566 on epoch=3
05/23/2022 10:43:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11544984655144566 on epoch=3, global_step=50
05/23/2022 10:43:55 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=3
05/23/2022 10:43:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=4
05/23/2022 10:44:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
05/23/2022 10:44:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=5
05/23/2022 10:44:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=6
05/23/2022 10:44:09 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.16966568338249755 on epoch=6
05/23/2022 10:44:09 - INFO - __main__ - Saving model with best Classification-F1: 0.11544984655144566 -> 0.16966568338249755 on epoch=6, global_step=100
05/23/2022 10:44:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=6
05/23/2022 10:44:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=7
05/23/2022 10:44:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.94 on epoch=8
05/23/2022 10:44:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=8
05/23/2022 10:44:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=9
05/23/2022 10:44:24 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 10:44:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=9
05/23/2022 10:44:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=10
05/23/2022 10:44:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=11
05/23/2022 10:44:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=11
05/23/2022 10:44:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=12
05/23/2022 10:44:40 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.17763050702314775 on epoch=12
05/23/2022 10:44:40 - INFO - __main__ - Saving model with best Classification-F1: 0.16966568338249755 -> 0.17763050702314775 on epoch=12, global_step=200
05/23/2022 10:44:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=13
05/23/2022 10:44:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=13
05/23/2022 10:44:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=14
05/23/2022 10:44:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.80 on epoch=14
05/23/2022 10:44:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=15
05/23/2022 10:44:56 - INFO - __main__ - Global step 250 Train loss 0.81 Classification-F1 0.3901789810338116 on epoch=15
05/23/2022 10:44:56 - INFO - __main__ - Saving model with best Classification-F1: 0.17763050702314775 -> 0.3901789810338116 on epoch=15, global_step=250
05/23/2022 10:44:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=16
05/23/2022 10:45:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=16
05/23/2022 10:45:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=17
05/23/2022 10:45:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=18
05/23/2022 10:45:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=18
05/23/2022 10:45:11 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.1980968905780936 on epoch=18
05/23/2022 10:45:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
05/23/2022 10:45:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.79 on epoch=19
05/23/2022 10:45:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=20
05/23/2022 10:45:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=21
05/23/2022 10:45:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.62 on epoch=21
05/23/2022 10:45:27 - INFO - __main__ - Global step 350 Train loss 0.73 Classification-F1 0.15385337894816226 on epoch=21
05/23/2022 10:45:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=22
05/23/2022 10:45:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.75 on epoch=23
05/23/2022 10:45:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=23
05/23/2022 10:45:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=24
05/23/2022 10:45:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=24
05/23/2022 10:45:43 - INFO - __main__ - Global step 400 Train loss 0.73 Classification-F1 0.38277013040111374 on epoch=24
05/23/2022 10:45:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=25
05/23/2022 10:45:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.62 on epoch=26
05/23/2022 10:45:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=26
05/23/2022 10:45:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=27
05/23/2022 10:45:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=28
05/23/2022 10:45:58 - INFO - __main__ - Global step 450 Train loss 0.62 Classification-F1 0.49293831927251464 on epoch=28
05/23/2022 10:45:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3901789810338116 -> 0.49293831927251464 on epoch=28, global_step=450
05/23/2022 10:46:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=28
05/23/2022 10:46:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=29
05/23/2022 10:46:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=29
05/23/2022 10:46:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=30
05/23/2022 10:46:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=31
05/23/2022 10:46:14 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.5738644705637121 on epoch=31
05/23/2022 10:46:14 - INFO - __main__ - Saving model with best Classification-F1: 0.49293831927251464 -> 0.5738644705637121 on epoch=31, global_step=500
05/23/2022 10:46:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=31
05/23/2022 10:46:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=32
05/23/2022 10:46:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=33
05/23/2022 10:46:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=33
05/23/2022 10:46:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=34
05/23/2022 10:46:30 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.5768336361399724 on epoch=34
05/23/2022 10:46:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5738644705637121 -> 0.5768336361399724 on epoch=34, global_step=550
05/23/2022 10:46:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/23/2022 10:46:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=35
05/23/2022 10:46:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=36
05/23/2022 10:46:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
05/23/2022 10:46:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=37
05/23/2022 10:46:45 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.6691311275568481 on epoch=37
05/23/2022 10:46:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5768336361399724 -> 0.6691311275568481 on epoch=37, global_step=600
05/23/2022 10:46:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/23/2022 10:46:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=38
05/23/2022 10:46:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
05/23/2022 10:46:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=39
05/23/2022 10:46:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=40
05/23/2022 10:47:01 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5695802951149802 on epoch=40
05/23/2022 10:47:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
05/23/2022 10:47:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/23/2022 10:47:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=42
05/23/2022 10:47:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=43
05/23/2022 10:47:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
05/23/2022 10:47:17 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6743382071150366 on epoch=43
05/23/2022 10:47:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6691311275568481 -> 0.6743382071150366 on epoch=43, global_step=700
05/23/2022 10:47:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=44
05/23/2022 10:47:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=44
05/23/2022 10:47:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/23/2022 10:47:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=46
05/23/2022 10:47:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=46
05/23/2022 10:47:33 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.7072843822843823 on epoch=46
05/23/2022 10:47:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6743382071150366 -> 0.7072843822843823 on epoch=46, global_step=750
05/23/2022 10:47:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.30 on epoch=47
05/23/2022 10:47:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=48
05/23/2022 10:47:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/23/2022 10:47:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=49
05/23/2022 10:47:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=49
05/23/2022 10:47:49 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.7123213236832826 on epoch=49
05/23/2022 10:47:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7072843822843823 -> 0.7123213236832826 on epoch=49, global_step=800
05/23/2022 10:47:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=50
05/23/2022 10:47:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=51
05/23/2022 10:47:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=51
05/23/2022 10:47:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=52
05/23/2022 10:48:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=53
05/23/2022 10:48:05 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.7309001385088341 on epoch=53
05/23/2022 10:48:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7123213236832826 -> 0.7309001385088341 on epoch=53, global_step=850
05/23/2022 10:48:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=53
05/23/2022 10:48:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=54
05/23/2022 10:48:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/23/2022 10:48:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/23/2022 10:48:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/23/2022 10:48:21 - INFO - __main__ - Global step 900 Train loss 0.24 Classification-F1 0.7311336324494219 on epoch=56
05/23/2022 10:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7309001385088341 -> 0.7311336324494219 on epoch=56, global_step=900
05/23/2022 10:48:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=56
05/23/2022 10:48:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=57
05/23/2022 10:48:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/23/2022 10:48:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=58
05/23/2022 10:48:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=59
05/23/2022 10:48:37 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.712809952075803 on epoch=59
05/23/2022 10:48:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/23/2022 10:48:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/23/2022 10:48:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/23/2022 10:48:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=61
05/23/2022 10:48:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=62
05/23/2022 10:48:53 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.7485241066364152 on epoch=62
05/23/2022 10:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7311336324494219 -> 0.7485241066364152 on epoch=62, global_step=1000
05/23/2022 10:48:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=63
05/23/2022 10:48:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=63
05/23/2022 10:49:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=64
05/23/2022 10:49:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/23/2022 10:49:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=65
05/23/2022 10:49:10 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.7572503700641577 on epoch=65
05/23/2022 10:49:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7485241066364152 -> 0.7572503700641577 on epoch=65, global_step=1050
05/23/2022 10:49:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=66
05/23/2022 10:49:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=66
05/23/2022 10:49:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
05/23/2022 10:49:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=68
05/23/2022 10:49:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/23/2022 10:49:26 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7289861434515403 on epoch=68
05/23/2022 10:49:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/23/2022 10:49:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=69
05/23/2022 10:49:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/23/2022 10:49:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=71
05/23/2022 10:49:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=71
05/23/2022 10:49:43 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.7131275361808662 on epoch=71
05/23/2022 10:49:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=72
05/23/2022 10:49:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=73
05/23/2022 10:49:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.17 on epoch=73
05/23/2022 10:49:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=74
05/23/2022 10:49:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=74
05/23/2022 10:49:59 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.7104361842976553 on epoch=74
05/23/2022 10:50:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=75
05/23/2022 10:50:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=76
05/23/2022 10:50:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=76
05/23/2022 10:50:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=77
05/23/2022 10:50:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=78
05/23/2022 10:50:15 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.749298687881087 on epoch=78
05/23/2022 10:50:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=78
05/23/2022 10:50:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=79
05/23/2022 10:50:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=79
05/23/2022 10:50:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/23/2022 10:50:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=81
05/23/2022 10:50:31 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.7491708441468783 on epoch=81
05/23/2022 10:50:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=81
05/23/2022 10:50:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=82
05/23/2022 10:50:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=83
05/23/2022 10:50:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=83
05/23/2022 10:50:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=84
05/23/2022 10:50:47 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.7592790651614181 on epoch=84
05/23/2022 10:50:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7572503700641577 -> 0.7592790651614181 on epoch=84, global_step=1350
05/23/2022 10:50:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=84
05/23/2022 10:50:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=85
05/23/2022 10:50:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=86
05/23/2022 10:50:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=86
05/23/2022 10:51:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=87
05/23/2022 10:51:04 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.7375981822091915 on epoch=87
05/23/2022 10:51:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
05/23/2022 10:51:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=88
05/23/2022 10:51:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=89
05/23/2022 10:51:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=89
05/23/2022 10:51:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=90
05/23/2022 10:51:19 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7485761420652703 on epoch=90
05/23/2022 10:51:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=91
05/23/2022 10:51:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=91
05/23/2022 10:51:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=92
05/23/2022 10:51:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/23/2022 10:51:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/23/2022 10:51:36 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7534108484957998 on epoch=93
05/23/2022 10:51:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=94
05/23/2022 10:51:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=94
05/23/2022 10:51:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=95
05/23/2022 10:51:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=96
05/23/2022 10:51:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=96
05/23/2022 10:51:52 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.7366747624529423 on epoch=96
05/23/2022 10:51:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=97
05/23/2022 10:51:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=98
05/23/2022 10:51:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=98
05/23/2022 10:52:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=99
05/23/2022 10:52:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=99
05/23/2022 10:52:08 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.766338352317054 on epoch=99
05/23/2022 10:52:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7592790651614181 -> 0.766338352317054 on epoch=99, global_step=1600
05/23/2022 10:52:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=100
05/23/2022 10:52:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=101
05/23/2022 10:52:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=101
05/23/2022 10:52:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=102
05/23/2022 10:52:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=103
05/23/2022 10:52:24 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.7514992293680818 on epoch=103
05/23/2022 10:52:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=103
05/23/2022 10:52:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=104
05/23/2022 10:52:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=104
05/23/2022 10:52:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=105
05/23/2022 10:52:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/23/2022 10:52:40 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7439596142927163 on epoch=106
05/23/2022 10:52:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=106
05/23/2022 10:52:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=107
05/23/2022 10:52:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=108
05/23/2022 10:52:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=108
05/23/2022 10:52:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=109
05/23/2022 10:52:56 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.7554804576904797 on epoch=109
05/23/2022 10:52:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=109
05/23/2022 10:53:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=110
05/23/2022 10:53:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=111
05/23/2022 10:53:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=111
05/23/2022 10:53:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=112
05/23/2022 10:53:11 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7717752826906961 on epoch=112
05/23/2022 10:53:12 - INFO - __main__ - Saving model with best Classification-F1: 0.766338352317054 -> 0.7717752826906961 on epoch=112, global_step=1800
05/23/2022 10:53:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=113
05/23/2022 10:53:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=113
05/23/2022 10:53:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=114
05/23/2022 10:53:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=114
05/23/2022 10:53:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=115
05/23/2022 10:53:28 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.7568681849221246 on epoch=115
05/23/2022 10:53:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=116
05/23/2022 10:53:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=116
05/23/2022 10:53:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=117
05/23/2022 10:53:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=118
05/23/2022 10:53:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=118
05/23/2022 10:53:44 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7468457069488454 on epoch=118
05/23/2022 10:53:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=119
05/23/2022 10:53:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=119
05/23/2022 10:53:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=120
05/23/2022 10:53:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=121
05/23/2022 10:53:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/23/2022 10:54:00 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7499480859832875 on epoch=121
05/23/2022 10:54:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=122
05/23/2022 10:54:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=123
05/23/2022 10:54:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=123
05/23/2022 10:54:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=124
05/23/2022 10:54:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=124
05/23/2022 10:54:16 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.7545139356539696 on epoch=124
05/23/2022 10:54:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=125
05/23/2022 10:54:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/23/2022 10:54:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=126
05/23/2022 10:54:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=127
05/23/2022 10:54:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=128
05/23/2022 10:54:31 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.7766086750512448 on epoch=128
05/23/2022 10:54:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7717752826906961 -> 0.7766086750512448 on epoch=128, global_step=2050
05/23/2022 10:54:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=128
05/23/2022 10:54:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/23/2022 10:54:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/23/2022 10:54:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.03 on epoch=130
05/23/2022 10:54:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.07 on epoch=131
05/23/2022 10:54:47 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.776491726296959 on epoch=131
05/23/2022 10:54:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=131
05/23/2022 10:54:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=132
05/23/2022 10:54:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=133
05/23/2022 10:54:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.01 on epoch=133
05/23/2022 10:55:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=134
05/23/2022 10:55:03 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.7686574331510059 on epoch=134
05/23/2022 10:55:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=134
05/23/2022 10:55:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/23/2022 10:55:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/23/2022 10:55:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=136
05/23/2022 10:55:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.03 on epoch=137
05/23/2022 10:55:19 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7546450859273693 on epoch=137
05/23/2022 10:55:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/23/2022 10:55:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=138
05/23/2022 10:55:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=139
05/23/2022 10:55:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=139
05/23/2022 10:55:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=140
05/23/2022 10:55:35 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7617210925787106 on epoch=140
05/23/2022 10:55:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/23/2022 10:55:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=141
05/23/2022 10:55:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=142
05/23/2022 10:55:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=143
05/23/2022 10:55:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=143
05/23/2022 10:55:51 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.772670234479539 on epoch=143
05/23/2022 10:55:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=144
05/23/2022 10:55:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/23/2022 10:55:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=145
05/23/2022 10:56:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=146
05/23/2022 10:56:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=146
05/23/2022 10:56:06 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7523332694974485 on epoch=146
05/23/2022 10:56:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
05/23/2022 10:56:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/23/2022 10:56:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/23/2022 10:56:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=149
05/23/2022 10:56:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
05/23/2022 10:56:23 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.762135823749096 on epoch=149
05/23/2022 10:56:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=150
05/23/2022 10:56:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=151
05/23/2022 10:56:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/23/2022 10:56:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=152
05/23/2022 10:56:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/23/2022 10:56:38 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7414630239708241 on epoch=153
05/23/2022 10:56:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/23/2022 10:56:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/23/2022 10:56:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=154
05/23/2022 10:56:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/23/2022 10:56:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
05/23/2022 10:56:55 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7717187114673657 on epoch=156
05/23/2022 10:56:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
05/23/2022 10:57:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/23/2022 10:57:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=158
05/23/2022 10:57:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/23/2022 10:57:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/23/2022 10:57:11 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7498096573730834 on epoch=159
05/23/2022 10:57:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
05/23/2022 10:57:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/23/2022 10:57:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/23/2022 10:57:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=161
05/23/2022 10:57:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=162
05/23/2022 10:57:27 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7536228740020826 on epoch=162
05/23/2022 10:57:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=163
05/23/2022 10:57:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/23/2022 10:57:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=164
05/23/2022 10:57:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=164
05/23/2022 10:57:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/23/2022 10:57:43 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7625679875679876 on epoch=165
05/23/2022 10:57:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=166
05/23/2022 10:57:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=166
05/23/2022 10:57:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/23/2022 10:57:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
05/23/2022 10:57:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/23/2022 10:57:59 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7590942948546019 on epoch=168
05/23/2022 10:58:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
05/23/2022 10:58:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=169
05/23/2022 10:58:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/23/2022 10:58:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/23/2022 10:58:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=171
05/23/2022 10:58:15 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7745570731951558 on epoch=171
05/23/2022 10:58:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/23/2022 10:58:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=173
05/23/2022 10:58:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=173
05/23/2022 10:58:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
05/23/2022 10:58:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=174
05/23/2022 10:58:32 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7767533907573377 on epoch=174
05/23/2022 10:58:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7766086750512448 -> 0.7767533907573377 on epoch=174, global_step=2800
05/23/2022 10:58:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=175
05/23/2022 10:58:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/23/2022 10:58:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=176
05/23/2022 10:58:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=177
05/23/2022 10:58:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/23/2022 10:58:48 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7751972060403018 on epoch=178
05/23/2022 10:58:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=178
05/23/2022 10:58:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/23/2022 10:58:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/23/2022 10:58:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
05/23/2022 10:59:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
05/23/2022 10:59:04 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.7794371423174713 on epoch=181
05/23/2022 10:59:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7767533907573377 -> 0.7794371423174713 on epoch=181, global_step=2900
05/23/2022 10:59:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/23/2022 10:59:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=182
05/23/2022 10:59:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/23/2022 10:59:14 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
05/23/2022 10:59:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 10:59:20 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7887060357373411 on epoch=184
05/23/2022 10:59:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7794371423174713 -> 0.7887060357373411 on epoch=184, global_step=2950
05/23/2022 10:59:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=184
05/23/2022 10:59:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=185
05/23/2022 10:59:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/23/2022 10:59:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/23/2022 10:59:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/23/2022 10:59:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:59:34 - INFO - __main__ - Printing 3 examples
05/23/2022 10:59:34 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:59:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:59:34 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 10:59:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 10:59:34 - INFO - __main__ - Printing 3 examples
05/23/2022 10:59:34 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 10:59:34 - INFO - __main__ - ['others']
05/23/2022 10:59:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:59:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:59:34 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 10:59:36 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7672144642111568 on epoch=187
05/23/2022 10:59:36 - INFO - __main__ - save last model!
05/23/2022 10:59:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 10:59:36 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 10:59:36 - INFO - __main__ - Printing 3 examples
05/23/2022 10:59:36 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 10:59:36 - INFO - __main__ - ['others']
05/23/2022 10:59:36 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 10:59:36 - INFO - __main__ - ['others']
05/23/2022 10:59:36 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 10:59:36 - INFO - __main__ - ['others']
05/23/2022 10:59:36 - INFO - __main__ - Tokenizing Input ...
05/23/2022 10:59:38 - INFO - __main__ - Tokenizing Output ...
05/23/2022 10:59:44 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 10:59:49 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 10:59:49 - INFO - __main__ - task name: emo
05/23/2022 10:59:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 10:59:50 - INFO - __main__ - Starting training!
05/23/2022 11:00:56 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_100_0.4_8_predictions.txt
05/23/2022 11:00:56 - INFO - __main__ - Classification-F1 on test data: 0.5454
05/23/2022 11:00:57 - INFO - __main__ - prefix=emo_64_100, lr=0.4, bsz=8, dev_performance=0.7887060357373411, test_performance=0.5454121337144812
05/23/2022 11:00:57 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.3, bsz=8 ...
05/23/2022 11:00:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:00:57 - INFO - __main__ - Printing 3 examples
05/23/2022 11:00:57 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 11:00:57 - INFO - __main__ - ['others']
05/23/2022 11:00:57 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 11:00:57 - INFO - __main__ - ['others']
05/23/2022 11:00:57 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 11:00:57 - INFO - __main__ - ['others']
05/23/2022 11:00:57 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:00:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:00:58 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:00:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:00:58 - INFO - __main__ - Printing 3 examples
05/23/2022 11:00:58 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 11:00:58 - INFO - __main__ - ['others']
05/23/2022 11:00:58 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 11:00:58 - INFO - __main__ - ['others']
05/23/2022 11:00:58 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 11:00:58 - INFO - __main__ - ['others']
05/23/2022 11:00:58 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:00:58 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:00:58 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:01:14 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:01:14 - INFO - __main__ - task name: emo
05/23/2022 11:01:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:01:14 - INFO - __main__ - Starting training!
05/23/2022 11:01:17 - INFO - __main__ - Step 10 Global step 10 Train loss 7.24 on epoch=0
05/23/2022 11:01:20 - INFO - __main__ - Step 20 Global step 20 Train loss 4.49 on epoch=1
05/23/2022 11:01:22 - INFO - __main__ - Step 30 Global step 30 Train loss 2.27 on epoch=1
05/23/2022 11:01:25 - INFO - __main__ - Step 40 Global step 40 Train loss 1.47 on epoch=2
05/23/2022 11:01:28 - INFO - __main__ - Step 50 Global step 50 Train loss 1.27 on epoch=3
05/23/2022 11:01:31 - INFO - __main__ - Global step 50 Train loss 3.35 Classification-F1 0.2410787771898883 on epoch=3
05/23/2022 11:01:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2410787771898883 on epoch=3, global_step=50
05/23/2022 11:01:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=3
05/23/2022 11:01:37 - INFO - __main__ - Step 70 Global step 70 Train loss 1.16 on epoch=4
05/23/2022 11:01:39 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=4
05/23/2022 11:01:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
05/23/2022 11:01:44 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=6
05/23/2022 11:01:48 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.1269482151835093 on epoch=6
05/23/2022 11:01:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=6
05/23/2022 11:01:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=7
05/23/2022 11:01:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.99 on epoch=8
05/23/2022 11:01:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.96 on epoch=8
05/23/2022 11:02:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/23/2022 11:02:04 - INFO - __main__ - Global step 150 Train loss 0.95 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 11:02:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=9
05/23/2022 11:02:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=10
05/23/2022 11:02:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/23/2022 11:02:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=11
05/23/2022 11:02:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.87 on epoch=12
05/23/2022 11:02:20 - INFO - __main__ - Global step 200 Train loss 0.92 Classification-F1 0.15417690417690416 on epoch=12
05/23/2022 11:02:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=13
05/23/2022 11:02:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.94 on epoch=13
05/23/2022 11:02:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=14
05/23/2022 11:02:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=14
05/23/2022 11:02:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=15
05/23/2022 11:02:36 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.20181335816999 on epoch=15
05/23/2022 11:02:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=16
05/23/2022 11:02:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=16
05/23/2022 11:02:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.80 on epoch=17
05/23/2022 11:02:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=18
05/23/2022 11:02:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=18
05/23/2022 11:02:52 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.2096432848809619 on epoch=18
05/23/2022 11:02:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.87 on epoch=19
05/23/2022 11:02:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=19
05/23/2022 11:02:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.80 on epoch=20
05/23/2022 11:03:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=21
05/23/2022 11:03:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=21
05/23/2022 11:03:07 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.25730127438226136 on epoch=21
05/23/2022 11:03:07 - INFO - __main__ - Saving model with best Classification-F1: 0.2410787771898883 -> 0.25730127438226136 on epoch=21, global_step=350
05/23/2022 11:03:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.88 on epoch=22
05/23/2022 11:03:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=23
05/23/2022 11:03:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.75 on epoch=23
05/23/2022 11:03:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=24
05/23/2022 11:03:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.72 on epoch=24
05/23/2022 11:03:23 - INFO - __main__ - Global step 400 Train loss 0.77 Classification-F1 0.36006633193012444 on epoch=24
05/23/2022 11:03:23 - INFO - __main__ - Saving model with best Classification-F1: 0.25730127438226136 -> 0.36006633193012444 on epoch=24, global_step=400
05/23/2022 11:03:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.73 on epoch=25
05/23/2022 11:03:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.71 on epoch=26
05/23/2022 11:03:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.75 on epoch=26
05/23/2022 11:03:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.82 on epoch=27
05/23/2022 11:03:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=28
05/23/2022 11:03:39 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.4183671076085937 on epoch=28
05/23/2022 11:03:39 - INFO - __main__ - Saving model with best Classification-F1: 0.36006633193012444 -> 0.4183671076085937 on epoch=28, global_step=450
05/23/2022 11:03:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.77 on epoch=28
05/23/2022 11:03:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=29
05/23/2022 11:03:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.72 on epoch=29
05/23/2022 11:03:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.69 on epoch=30
05/23/2022 11:03:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.66 on epoch=31
05/23/2022 11:03:55 - INFO - __main__ - Global step 500 Train loss 0.71 Classification-F1 0.49156232656232657 on epoch=31
05/23/2022 11:03:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4183671076085937 -> 0.49156232656232657 on epoch=31, global_step=500
05/23/2022 11:03:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.65 on epoch=31
05/23/2022 11:04:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=32
05/23/2022 11:04:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.62 on epoch=33
05/23/2022 11:04:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.61 on epoch=33
05/23/2022 11:04:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.70 on epoch=34
05/23/2022 11:04:10 - INFO - __main__ - Global step 550 Train loss 0.66 Classification-F1 0.547199243522773 on epoch=34
05/23/2022 11:04:10 - INFO - __main__ - Saving model with best Classification-F1: 0.49156232656232657 -> 0.547199243522773 on epoch=34, global_step=550
05/23/2022 11:04:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.65 on epoch=34
05/23/2022 11:04:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.61 on epoch=35
05/23/2022 11:04:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=36
05/23/2022 11:04:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.62 on epoch=36
05/23/2022 11:04:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=37
05/23/2022 11:04:26 - INFO - __main__ - Global step 600 Train loss 0.60 Classification-F1 0.5974561553635072 on epoch=37
05/23/2022 11:04:26 - INFO - __main__ - Saving model with best Classification-F1: 0.547199243522773 -> 0.5974561553635072 on epoch=37, global_step=600
05/23/2022 11:04:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.54 on epoch=38
05/23/2022 11:04:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=38
05/23/2022 11:04:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=39
05/23/2022 11:04:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=39
05/23/2022 11:04:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=40
05/23/2022 11:04:42 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.5925443646912576 on epoch=40
05/23/2022 11:04:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=41
05/23/2022 11:04:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=41
05/23/2022 11:04:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=42
05/23/2022 11:04:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/23/2022 11:04:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=43
05/23/2022 11:04:58 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.6142345038522175 on epoch=43
05/23/2022 11:04:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5974561553635072 -> 0.6142345038522175 on epoch=43, global_step=700
05/23/2022 11:05:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=44
05/23/2022 11:05:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
05/23/2022 11:05:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/23/2022 11:05:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/23/2022 11:05:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=46
05/23/2022 11:05:14 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.6918149119818788 on epoch=46
05/23/2022 11:05:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6142345038522175 -> 0.6918149119818788 on epoch=46, global_step=750
05/23/2022 11:05:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=47
05/23/2022 11:05:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/23/2022 11:05:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=48
05/23/2022 11:05:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=49
05/23/2022 11:05:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=49
05/23/2022 11:05:30 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.7151765990792153 on epoch=49
05/23/2022 11:05:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6918149119818788 -> 0.7151765990792153 on epoch=49, global_step=800
05/23/2022 11:05:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=50
05/23/2022 11:05:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=51
05/23/2022 11:05:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
05/23/2022 11:05:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/23/2022 11:05:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
05/23/2022 11:05:45 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.6874072991963526 on epoch=53
05/23/2022 11:05:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=53
05/23/2022 11:05:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=54
05/23/2022 11:05:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=54
05/23/2022 11:05:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=55
05/23/2022 11:05:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/23/2022 11:06:01 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7258601240200186 on epoch=56
05/23/2022 11:06:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7151765990792153 -> 0.7258601240200186 on epoch=56, global_step=900
05/23/2022 11:06:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=56
05/23/2022 11:06:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
05/23/2022 11:06:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/23/2022 11:06:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
05/23/2022 11:06:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
05/23/2022 11:06:17 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7366081431290239 on epoch=59
05/23/2022 11:06:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7258601240200186 -> 0.7366081431290239 on epoch=59, global_step=950
05/23/2022 11:06:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=59
05/23/2022 11:06:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=60
05/23/2022 11:06:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
05/23/2022 11:06:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=61
05/23/2022 11:06:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=62
05/23/2022 11:06:33 - INFO - __main__ - Global step 1000 Train loss 0.27 Classification-F1 0.7458310042609513 on epoch=62
05/23/2022 11:06:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7366081431290239 -> 0.7458310042609513 on epoch=62, global_step=1000
05/23/2022 11:06:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/23/2022 11:06:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=63
05/23/2022 11:06:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/23/2022 11:06:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=64
05/23/2022 11:06:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/23/2022 11:06:49 - INFO - __main__ - Global step 1050 Train loss 0.24 Classification-F1 0.7869665189646684 on epoch=65
05/23/2022 11:06:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7458310042609513 -> 0.7869665189646684 on epoch=65, global_step=1050
05/23/2022 11:06:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/23/2022 11:06:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=66
05/23/2022 11:06:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=67
05/23/2022 11:06:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/23/2022 11:07:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=68
05/23/2022 11:07:05 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.7648252533687576 on epoch=68
05/23/2022 11:07:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/23/2022 11:07:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/23/2022 11:07:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/23/2022 11:07:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=71
05/23/2022 11:07:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
05/23/2022 11:07:21 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.7585782273282273 on epoch=71
05/23/2022 11:07:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=72
05/23/2022 11:07:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=73
05/23/2022 11:07:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=73
05/23/2022 11:07:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=74
05/23/2022 11:07:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
05/23/2022 11:07:37 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7578055418109847 on epoch=74
05/23/2022 11:07:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=75
05/23/2022 11:07:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/23/2022 11:07:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=76
05/23/2022 11:07:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=77
05/23/2022 11:07:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
05/23/2022 11:07:53 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.7754459072414598 on epoch=78
05/23/2022 11:07:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/23/2022 11:07:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=79
05/23/2022 11:08:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=79
05/23/2022 11:08:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/23/2022 11:08:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=81
05/23/2022 11:08:09 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.7906496854888226 on epoch=81
05/23/2022 11:08:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7869665189646684 -> 0.7906496854888226 on epoch=81, global_step=1300
05/23/2022 11:08:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=81
05/23/2022 11:08:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=82
05/23/2022 11:08:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/23/2022 11:08:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=83
05/23/2022 11:08:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=84
05/23/2022 11:08:25 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.7731506177297384 on epoch=84
05/23/2022 11:08:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=84
05/23/2022 11:08:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=85
05/23/2022 11:08:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=86
05/23/2022 11:08:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=86
05/23/2022 11:08:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=87
05/23/2022 11:08:41 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.7523457393429711 on epoch=87
05/23/2022 11:08:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/23/2022 11:08:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=88
05/23/2022 11:08:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/23/2022 11:08:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=89
05/23/2022 11:08:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=90
05/23/2022 11:08:57 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.768945258420082 on epoch=90
05/23/2022 11:09:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=91
05/23/2022 11:09:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
05/23/2022 11:09:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=92
05/23/2022 11:09:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=93
05/23/2022 11:09:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=93
05/23/2022 11:09:13 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.7903747704255655 on epoch=93
05/23/2022 11:09:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=94
05/23/2022 11:09:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=94
05/23/2022 11:09:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.08 on epoch=95
05/23/2022 11:09:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=96
05/23/2022 11:09:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=96
05/23/2022 11:09:29 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.7789841178538331 on epoch=96
05/23/2022 11:09:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=97
05/23/2022 11:09:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=98
05/23/2022 11:09:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=98
05/23/2022 11:09:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/23/2022 11:09:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=99
05/23/2022 11:09:45 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7907033213299205 on epoch=99
05/23/2022 11:09:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7906496854888226 -> 0.7907033213299205 on epoch=99, global_step=1600
05/23/2022 11:09:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=100
05/23/2022 11:09:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=101
05/23/2022 11:09:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=101
05/23/2022 11:09:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=102
05/23/2022 11:09:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.21 on epoch=103
05/23/2022 11:10:01 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.7673964022884886 on epoch=103
05/23/2022 11:10:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=103
05/23/2022 11:10:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=104
05/23/2022 11:10:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/23/2022 11:10:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=105
05/23/2022 11:10:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=106
05/23/2022 11:10:17 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.7795575039979833 on epoch=106
05/23/2022 11:10:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=106
05/23/2022 11:10:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=107
05/23/2022 11:10:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=108
05/23/2022 11:10:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=108
05/23/2022 11:10:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/23/2022 11:10:33 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7590208225351414 on epoch=109
05/23/2022 11:10:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=109
05/23/2022 11:10:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=110
05/23/2022 11:10:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/23/2022 11:10:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/23/2022 11:10:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=112
05/23/2022 11:10:48 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.774131745961794 on epoch=112
05/23/2022 11:10:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=113
05/23/2022 11:10:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=113
05/23/2022 11:10:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=114
05/23/2022 11:10:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=114
05/23/2022 11:11:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=115
05/23/2022 11:11:04 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7481277744619262 on epoch=115
05/23/2022 11:11:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=116
05/23/2022 11:11:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=116
05/23/2022 11:11:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=117
05/23/2022 11:11:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=118
05/23/2022 11:11:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=118
05/23/2022 11:11:20 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7633113024517832 on epoch=118
05/23/2022 11:11:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/23/2022 11:11:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/23/2022 11:11:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/23/2022 11:11:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=121
05/23/2022 11:11:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=121
05/23/2022 11:11:36 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7745127770609381 on epoch=121
05/23/2022 11:11:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=122
05/23/2022 11:11:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/23/2022 11:11:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=123
05/23/2022 11:11:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/23/2022 11:11:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/23/2022 11:11:52 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7728775058159397 on epoch=124
05/23/2022 11:11:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/23/2022 11:11:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/23/2022 11:12:00 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=126
05/23/2022 11:12:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=127
05/23/2022 11:12:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=128
05/23/2022 11:12:08 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.7869733211393322 on epoch=128
05/23/2022 11:12:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
05/23/2022 11:12:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=129
05/23/2022 11:12:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=129
05/23/2022 11:12:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=130
05/23/2022 11:12:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/23/2022 11:12:24 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7735283635724373 on epoch=131
05/23/2022 11:12:27 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=131
05/23/2022 11:12:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=132
05/23/2022 11:12:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=133
05/23/2022 11:12:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=133
05/23/2022 11:12:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=134
05/23/2022 11:12:40 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.7791862527716186 on epoch=134
05/23/2022 11:12:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/23/2022 11:12:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=135
05/23/2022 11:12:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=136
05/23/2022 11:12:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=136
05/23/2022 11:12:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=137
05/23/2022 11:12:56 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.7509100006924345 on epoch=137
05/23/2022 11:12:59 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/23/2022 11:13:01 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=138
05/23/2022 11:13:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=139
05/23/2022 11:13:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
05/23/2022 11:13:09 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
05/23/2022 11:13:12 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7651909393288703 on epoch=140
05/23/2022 11:13:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=141
05/23/2022 11:13:17 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
05/23/2022 11:13:20 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=142
05/23/2022 11:13:22 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/23/2022 11:13:25 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=143
05/23/2022 11:13:28 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7666324786324785 on epoch=143
05/23/2022 11:13:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
05/23/2022 11:13:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=144
05/23/2022 11:13:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/23/2022 11:13:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/23/2022 11:13:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/23/2022 11:13:44 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.789459892964469 on epoch=146
05/23/2022 11:13:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=147
05/23/2022 11:13:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.05 on epoch=148
05/23/2022 11:13:52 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=148
05/23/2022 11:13:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/23/2022 11:13:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=149
05/23/2022 11:14:00 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.789460474417442 on epoch=149
05/23/2022 11:14:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
05/23/2022 11:14:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=151
05/23/2022 11:14:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=151
05/23/2022 11:14:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=152
05/23/2022 11:14:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/23/2022 11:14:16 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7898296153607158 on epoch=153
05/23/2022 11:14:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=153
05/23/2022 11:14:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=154
05/23/2022 11:14:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=154
05/23/2022 11:14:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
05/23/2022 11:14:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
05/23/2022 11:14:32 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.7882226868293427 on epoch=156
05/23/2022 11:14:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/23/2022 11:14:37 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/23/2022 11:14:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/23/2022 11:14:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/23/2022 11:14:45 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
05/23/2022 11:14:48 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.7691457701381867 on epoch=159
05/23/2022 11:14:50 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
05/23/2022 11:14:53 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=160
05/23/2022 11:14:55 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/23/2022 11:14:58 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=161
05/23/2022 11:15:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
05/23/2022 11:15:04 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.7613792373304906 on epoch=162
05/23/2022 11:15:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/23/2022 11:15:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/23/2022 11:15:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=164
05/23/2022 11:15:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/23/2022 11:15:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/23/2022 11:15:20 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7707244151545956 on epoch=165
05/23/2022 11:15:22 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
05/23/2022 11:15:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
05/23/2022 11:15:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
05/23/2022 11:15:30 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/23/2022 11:15:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/23/2022 11:15:36 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7659072811580064 on epoch=168
05/23/2022 11:15:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=169
05/23/2022 11:15:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=169
05/23/2022 11:15:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=170
05/23/2022 11:15:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
05/23/2022 11:15:48 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=171
05/23/2022 11:15:52 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7663991381015917 on epoch=171
05/23/2022 11:15:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
05/23/2022 11:15:57 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=173
05/23/2022 11:15:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
05/23/2022 11:16:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=174
05/23/2022 11:16:05 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=174
05/23/2022 11:16:08 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7592943362412394 on epoch=174
05/23/2022 11:16:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/23/2022 11:16:13 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/23/2022 11:16:15 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=176
05/23/2022 11:16:18 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
05/23/2022 11:16:21 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/23/2022 11:16:24 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7873876677833788 on epoch=178
05/23/2022 11:16:27 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=178
05/23/2022 11:16:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/23/2022 11:16:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=179
05/23/2022 11:16:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/23/2022 11:16:36 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=181
05/23/2022 11:16:40 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7771929824561403 on epoch=181
05/23/2022 11:16:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/23/2022 11:16:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=182
05/23/2022 11:16:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/23/2022 11:16:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
05/23/2022 11:16:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/23/2022 11:16:56 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.775696284649024 on epoch=184
05/23/2022 11:16:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
05/23/2022 11:17:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=185
05/23/2022 11:17:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/23/2022 11:17:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=186
05/23/2022 11:17:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
05/23/2022 11:17:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:17:10 - INFO - __main__ - Printing 3 examples
05/23/2022 11:17:10 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:17:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:17:10 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:17:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:17:10 - INFO - __main__ - Printing 3 examples
05/23/2022 11:17:10 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 11:17:10 - INFO - __main__ - ['others']
05/23/2022 11:17:10 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:17:10 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:17:11 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:17:12 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7595375114159164 on epoch=187
05/23/2022 11:17:12 - INFO - __main__ - save last model!
05/23/2022 11:17:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:17:12 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:17:12 - INFO - __main__ - Printing 3 examples
05/23/2022 11:17:12 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:17:12 - INFO - __main__ - ['others']
05/23/2022 11:17:12 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:17:12 - INFO - __main__ - ['others']
05/23/2022 11:17:12 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:17:12 - INFO - __main__ - ['others']
05/23/2022 11:17:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:17:14 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:17:20 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:17:29 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:17:29 - INFO - __main__ - task name: emo
05/23/2022 11:17:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:17:30 - INFO - __main__ - Starting training!
05/23/2022 11:18:35 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_100_0.3_8_predictions.txt
05/23/2022 11:18:35 - INFO - __main__ - Classification-F1 on test data: 0.5953
05/23/2022 11:18:36 - INFO - __main__ - prefix=emo_64_100, lr=0.3, bsz=8, dev_performance=0.7907033213299205, test_performance=0.5953193750473667
05/23/2022 11:18:36 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.2, bsz=8 ...
05/23/2022 11:18:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:18:37 - INFO - __main__ - Printing 3 examples
05/23/2022 11:18:37 - INFO - __main__ -  [emo] how cause yes am listening
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:18:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:18:37 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:18:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:18:37 - INFO - __main__ - Printing 3 examples
05/23/2022 11:18:37 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/23/2022 11:18:37 - INFO - __main__ - ['others']
05/23/2022 11:18:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:18:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:18:38 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:18:53 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:18:53 - INFO - __main__ - task name: emo
05/23/2022 11:18:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:18:54 - INFO - __main__ - Starting training!
05/23/2022 11:18:57 - INFO - __main__ - Step 10 Global step 10 Train loss 7.64 on epoch=0
05/23/2022 11:18:59 - INFO - __main__ - Step 20 Global step 20 Train loss 5.23 on epoch=1
05/23/2022 11:19:02 - INFO - __main__ - Step 30 Global step 30 Train loss 3.16 on epoch=1
05/23/2022 11:19:04 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=2
05/23/2022 11:19:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.71 on epoch=3
05/23/2022 11:19:10 - INFO - __main__ - Global step 50 Train loss 3.99 Classification-F1 0.12047163362952837 on epoch=3
05/23/2022 11:19:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12047163362952837 on epoch=3, global_step=50
05/23/2022 11:19:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.35 on epoch=3
05/23/2022 11:19:15 - INFO - __main__ - Step 70 Global step 70 Train loss 1.23 on epoch=4
05/23/2022 11:19:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.06 on epoch=4
05/23/2022 11:19:20 - INFO - __main__ - Step 90 Global step 90 Train loss 1.14 on epoch=5
05/23/2022 11:19:22 - INFO - __main__ - Step 100 Global step 100 Train loss 1.12 on epoch=6
05/23/2022 11:19:26 - INFO - __main__ - Global step 100 Train loss 1.18 Classification-F1 0.20806659356170246 on epoch=6
05/23/2022 11:19:26 - INFO - __main__ - Saving model with best Classification-F1: 0.12047163362952837 -> 0.20806659356170246 on epoch=6, global_step=100
05/23/2022 11:19:28 - INFO - __main__ - Step 110 Global step 110 Train loss 1.15 on epoch=6
05/23/2022 11:19:31 - INFO - __main__ - Step 120 Global step 120 Train loss 1.01 on epoch=7
05/23/2022 11:19:33 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=8
05/23/2022 11:19:36 - INFO - __main__ - Step 140 Global step 140 Train loss 1.01 on epoch=8
05/23/2022 11:19:38 - INFO - __main__ - Step 150 Global step 150 Train loss 1.00 on epoch=9
05/23/2022 11:19:41 - INFO - __main__ - Global step 150 Train loss 1.04 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 11:19:44 - INFO - __main__ - Step 160 Global step 160 Train loss 1.06 on epoch=9
05/23/2022 11:19:46 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=10
05/23/2022 11:19:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.99 on epoch=11
05/23/2022 11:19:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=11
05/23/2022 11:19:54 - INFO - __main__ - Step 200 Global step 200 Train loss 1.00 on epoch=12
05/23/2022 11:19:57 - INFO - __main__ - Global step 200 Train loss 1.01 Classification-F1 0.11578044596912522 on epoch=12
05/23/2022 11:20:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=13
05/23/2022 11:20:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=13
05/23/2022 11:20:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=14
05/23/2022 11:20:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=14
05/23/2022 11:20:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.93 on epoch=15
05/23/2022 11:20:13 - INFO - __main__ - Global step 250 Train loss 0.90 Classification-F1 0.2271551276370134 on epoch=15
05/23/2022 11:20:13 - INFO - __main__ - Saving model with best Classification-F1: 0.20806659356170246 -> 0.2271551276370134 on epoch=15, global_step=250
05/23/2022 11:20:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.85 on epoch=16
05/23/2022 11:20:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.91 on epoch=16
05/23/2022 11:20:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=17
05/23/2022 11:20:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=18
05/23/2022 11:20:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.83 on epoch=18
05/23/2022 11:20:29 - INFO - __main__ - Global step 300 Train loss 0.86 Classification-F1 0.1547911547911548 on epoch=18
05/23/2022 11:20:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=19
05/23/2022 11:20:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=19
05/23/2022 11:20:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.88 on epoch=20
05/23/2022 11:20:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.88 on epoch=21
05/23/2022 11:20:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.86 on epoch=21
05/23/2022 11:20:44 - INFO - __main__ - Global step 350 Train loss 0.88 Classification-F1 0.1710453386542512 on epoch=21
05/23/2022 11:20:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.79 on epoch=22
05/23/2022 11:20:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.90 on epoch=23
05/23/2022 11:20:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=23
05/23/2022 11:20:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=24
05/23/2022 11:20:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=24
05/23/2022 11:21:00 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.19210051836826683 on epoch=24
05/23/2022 11:21:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.95 on epoch=25
05/23/2022 11:21:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.83 on epoch=26
05/23/2022 11:21:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.81 on epoch=26
05/23/2022 11:21:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=27
05/23/2022 11:21:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.80 on epoch=28
05/23/2022 11:21:16 - INFO - __main__ - Global step 450 Train loss 0.84 Classification-F1 0.1658106386367256 on epoch=28
05/23/2022 11:21:19 - INFO - __main__ - Step 460 Global step 460 Train loss 0.81 on epoch=28
05/23/2022 11:21:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.86 on epoch=29
05/23/2022 11:21:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=29
05/23/2022 11:21:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=30
05/23/2022 11:21:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.79 on epoch=31
05/23/2022 11:21:32 - INFO - __main__ - Global step 500 Train loss 0.82 Classification-F1 0.2789606935891766 on epoch=31
05/23/2022 11:21:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2271551276370134 -> 0.2789606935891766 on epoch=31, global_step=500
05/23/2022 11:21:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=31
05/23/2022 11:21:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=32
05/23/2022 11:21:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.83 on epoch=33
05/23/2022 11:21:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=33
05/23/2022 11:21:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.74 on epoch=34
05/23/2022 11:21:48 - INFO - __main__ - Global step 550 Train loss 0.79 Classification-F1 0.3752031011677504 on epoch=34
05/23/2022 11:21:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2789606935891766 -> 0.3752031011677504 on epoch=34, global_step=550
05/23/2022 11:21:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=34
05/23/2022 11:21:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=35
05/23/2022 11:21:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.74 on epoch=36
05/23/2022 11:21:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=36
05/23/2022 11:22:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.70 on epoch=37
05/23/2022 11:22:04 - INFO - __main__ - Global step 600 Train loss 0.73 Classification-F1 0.41206814959502697 on epoch=37
05/23/2022 11:22:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3752031011677504 -> 0.41206814959502697 on epoch=37, global_step=600
05/23/2022 11:22:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=38
05/23/2022 11:22:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=38
05/23/2022 11:22:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.73 on epoch=39
05/23/2022 11:22:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=39
05/23/2022 11:22:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.70 on epoch=40
05/23/2022 11:22:20 - INFO - __main__ - Global step 650 Train loss 0.72 Classification-F1 0.41031789469217494 on epoch=40
05/23/2022 11:22:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=41
05/23/2022 11:22:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=41
05/23/2022 11:22:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.69 on epoch=42
05/23/2022 11:22:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.67 on epoch=43
05/23/2022 11:22:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.66 on epoch=43
05/23/2022 11:22:36 - INFO - __main__ - Global step 700 Train loss 0.70 Classification-F1 0.3010905848063905 on epoch=43
05/23/2022 11:22:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=44
05/23/2022 11:22:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.69 on epoch=44
05/23/2022 11:22:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.71 on epoch=45
05/23/2022 11:22:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.69 on epoch=46
05/23/2022 11:22:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=46
05/23/2022 11:22:52 - INFO - __main__ - Global step 750 Train loss 0.67 Classification-F1 0.47960158283154364 on epoch=46
05/23/2022 11:22:52 - INFO - __main__ - Saving model with best Classification-F1: 0.41206814959502697 -> 0.47960158283154364 on epoch=46, global_step=750
05/23/2022 11:22:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=47
05/23/2022 11:22:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.65 on epoch=48
05/23/2022 11:22:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.64 on epoch=48
05/23/2022 11:23:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.61 on epoch=49
05/23/2022 11:23:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.65 on epoch=49
05/23/2022 11:23:08 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.5075216667158313 on epoch=49
05/23/2022 11:23:08 - INFO - __main__ - Saving model with best Classification-F1: 0.47960158283154364 -> 0.5075216667158313 on epoch=49, global_step=800
05/23/2022 11:23:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.55 on epoch=50
05/23/2022 11:23:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=51
05/23/2022 11:23:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.63 on epoch=51
05/23/2022 11:23:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.62 on epoch=52
05/23/2022 11:23:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.65 on epoch=53
05/23/2022 11:23:24 - INFO - __main__ - Global step 850 Train loss 0.62 Classification-F1 0.4354462344883765 on epoch=53
05/23/2022 11:23:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.65 on epoch=53
05/23/2022 11:23:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.64 on epoch=54
05/23/2022 11:23:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=54
05/23/2022 11:23:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.61 on epoch=55
05/23/2022 11:23:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=56
05/23/2022 11:23:40 - INFO - __main__ - Global step 900 Train loss 0.61 Classification-F1 0.5662700019278967 on epoch=56
05/23/2022 11:23:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5075216667158313 -> 0.5662700019278967 on epoch=56, global_step=900
05/23/2022 11:23:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.62 on epoch=56
05/23/2022 11:23:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=57
05/23/2022 11:23:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.51 on epoch=58
05/23/2022 11:23:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=58
05/23/2022 11:23:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.56 on epoch=59
05/23/2022 11:23:56 - INFO - __main__ - Global step 950 Train loss 0.56 Classification-F1 0.5230968098900004 on epoch=59
05/23/2022 11:23:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=59
05/23/2022 11:24:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=60
05/23/2022 11:24:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=61
05/23/2022 11:24:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.56 on epoch=61
05/23/2022 11:24:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=62
05/23/2022 11:24:11 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.5429460408953826 on epoch=62
05/23/2022 11:24:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.61 on epoch=63
05/23/2022 11:24:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.54 on epoch=63
05/23/2022 11:24:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=64
05/23/2022 11:24:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.63 on epoch=64
05/23/2022 11:24:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=65
05/23/2022 11:24:27 - INFO - __main__ - Global step 1050 Train loss 0.58 Classification-F1 0.44210699990915453 on epoch=65
05/23/2022 11:24:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=66
05/23/2022 11:24:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.54 on epoch=66
05/23/2022 11:24:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.57 on epoch=67
05/23/2022 11:24:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=68
05/23/2022 11:24:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=68
05/23/2022 11:24:43 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.5690760183591509 on epoch=68
05/23/2022 11:24:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5662700019278967 -> 0.5690760183591509 on epoch=68, global_step=1100
05/23/2022 11:24:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=69
05/23/2022 11:24:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.53 on epoch=69
05/23/2022 11:24:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=70
05/23/2022 11:24:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=71
05/23/2022 11:24:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=71
05/23/2022 11:24:59 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.44526927143329353 on epoch=71
05/23/2022 11:25:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=72
05/23/2022 11:25:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=73
05/23/2022 11:25:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=73
05/23/2022 11:25:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.49 on epoch=74
05/23/2022 11:25:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.55 on epoch=74
05/23/2022 11:25:16 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.6024724334385473 on epoch=74
05/23/2022 11:25:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5690760183591509 -> 0.6024724334385473 on epoch=74, global_step=1200
05/23/2022 11:25:18 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=75
05/23/2022 11:25:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=76
05/23/2022 11:25:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=76
05/23/2022 11:25:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/23/2022 11:25:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=78
05/23/2022 11:25:32 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.5427155172413793 on epoch=78
05/23/2022 11:25:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=78
05/23/2022 11:25:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=79
05/23/2022 11:25:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=79
05/23/2022 11:25:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=80
05/23/2022 11:25:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
05/23/2022 11:25:48 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.6211125403005675 on epoch=81
05/23/2022 11:25:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6024724334385473 -> 0.6211125403005675 on epoch=81, global_step=1300
05/23/2022 11:25:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=81
05/23/2022 11:25:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
05/23/2022 11:25:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.39 on epoch=83
05/23/2022 11:25:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=83
05/23/2022 11:26:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=84
05/23/2022 11:26:04 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.6072831164035816 on epoch=84
05/23/2022 11:26:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=84
05/23/2022 11:26:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.35 on epoch=85
05/23/2022 11:26:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=86
05/23/2022 11:26:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=86
05/23/2022 11:26:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=87
05/23/2022 11:26:20 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.6539148351648352 on epoch=87
05/23/2022 11:26:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6211125403005675 -> 0.6539148351648352 on epoch=87, global_step=1400
05/23/2022 11:26:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=88
05/23/2022 11:26:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=88
05/23/2022 11:26:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=89
05/23/2022 11:26:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=89
05/23/2022 11:26:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=90
05/23/2022 11:26:36 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.6220000824348447 on epoch=90
05/23/2022 11:26:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=91
05/23/2022 11:26:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=91
05/23/2022 11:26:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=92
05/23/2022 11:26:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=93
05/23/2022 11:26:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.28 on epoch=93
05/23/2022 11:26:52 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.5933396295309049 on epoch=93
05/23/2022 11:26:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
05/23/2022 11:26:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=94
05/23/2022 11:27:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=95
05/23/2022 11:27:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=96
05/23/2022 11:27:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=96
05/23/2022 11:27:08 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.6542432195975503 on epoch=96
05/23/2022 11:27:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6539148351648352 -> 0.6542432195975503 on epoch=96, global_step=1550
05/23/2022 11:27:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=97
05/23/2022 11:27:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=98
05/23/2022 11:27:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=98
05/23/2022 11:27:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=99
05/23/2022 11:27:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.34 on epoch=99
05/23/2022 11:27:24 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.7006023691696988 on epoch=99
05/23/2022 11:27:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6542432195975503 -> 0.7006023691696988 on epoch=99, global_step=1600
05/23/2022 11:27:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/23/2022 11:27:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
05/23/2022 11:27:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
05/23/2022 11:27:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=102
05/23/2022 11:27:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=103
05/23/2022 11:27:40 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.6422355459037548 on epoch=103
05/23/2022 11:27:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=103
05/23/2022 11:27:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=104
05/23/2022 11:27:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.35 on epoch=104
05/23/2022 11:27:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
05/23/2022 11:27:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=106
05/23/2022 11:27:56 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.7065318376559562 on epoch=106
05/23/2022 11:27:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7006023691696988 -> 0.7065318376559562 on epoch=106, global_step=1700
05/23/2022 11:27:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=106
05/23/2022 11:28:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=107
05/23/2022 11:28:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=108
05/23/2022 11:28:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
05/23/2022 11:28:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=109
05/23/2022 11:28:12 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7091715912983899 on epoch=109
05/23/2022 11:28:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7065318376559562 -> 0.7091715912983899 on epoch=109, global_step=1750
05/23/2022 11:28:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
05/23/2022 11:28:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=110
05/23/2022 11:28:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=111
05/23/2022 11:28:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=111
05/23/2022 11:28:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=112
05/23/2022 11:28:28 - INFO - __main__ - Global step 1800 Train loss 0.28 Classification-F1 0.6507894958955075 on epoch=112
05/23/2022 11:28:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=113
05/23/2022 11:28:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/23/2022 11:28:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=114
05/23/2022 11:28:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=114
05/23/2022 11:28:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=115
05/23/2022 11:28:44 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.7027243762697488 on epoch=115
05/23/2022 11:28:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=116
05/23/2022 11:28:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=116
05/23/2022 11:28:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=117
05/23/2022 11:28:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=118
05/23/2022 11:28:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=118
05/23/2022 11:29:00 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.6917031059924397 on epoch=118
05/23/2022 11:29:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
05/23/2022 11:29:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=119
05/23/2022 11:29:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=120
05/23/2022 11:29:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=121
05/23/2022 11:29:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=121
05/23/2022 11:29:16 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.6876571398279413 on epoch=121
05/23/2022 11:29:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/23/2022 11:29:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=123
05/23/2022 11:29:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=123
05/23/2022 11:29:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/23/2022 11:29:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=124
05/23/2022 11:29:32 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.7024706247743551 on epoch=124
05/23/2022 11:29:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
05/23/2022 11:29:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/23/2022 11:29:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=126
05/23/2022 11:29:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
05/23/2022 11:29:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.30 on epoch=128
05/23/2022 11:29:47 - INFO - __main__ - Global step 2050 Train loss 0.20 Classification-F1 0.6755348686033811 on epoch=128
05/23/2022 11:29:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/23/2022 11:29:52 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/23/2022 11:29:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/23/2022 11:29:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/23/2022 11:30:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/23/2022 11:30:03 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.6897321191379417 on epoch=131
05/23/2022 11:30:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=131
05/23/2022 11:30:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
05/23/2022 11:30:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=133
05/23/2022 11:30:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=133
05/23/2022 11:30:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.29 on epoch=134
05/23/2022 11:30:19 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.6979217404753313 on epoch=134
05/23/2022 11:30:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
05/23/2022 11:30:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/23/2022 11:30:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
05/23/2022 11:30:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
05/23/2022 11:30:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/23/2022 11:30:35 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.705057417381006 on epoch=137
05/23/2022 11:30:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=138
05/23/2022 11:30:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=138
05/23/2022 11:30:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
05/23/2022 11:30:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/23/2022 11:30:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.24 on epoch=140
05/23/2022 11:30:51 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.6995651976975066 on epoch=140
05/23/2022 11:30:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=141
05/23/2022 11:30:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
05/23/2022 11:30:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
05/23/2022 11:31:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/23/2022 11:31:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/23/2022 11:31:07 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.6450347681451312 on epoch=143
05/23/2022 11:31:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=144
05/23/2022 11:31:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/23/2022 11:31:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/23/2022 11:31:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=146
05/23/2022 11:31:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=146
05/23/2022 11:31:23 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.6831602253461806 on epoch=146
05/23/2022 11:31:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
05/23/2022 11:31:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/23/2022 11:31:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/23/2022 11:31:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
05/23/2022 11:31:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.24 on epoch=149
05/23/2022 11:31:39 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7028968718499009 on epoch=149
05/23/2022 11:31:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
05/23/2022 11:31:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=151
05/23/2022 11:31:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=151
05/23/2022 11:31:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
05/23/2022 11:31:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/23/2022 11:31:55 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.6912442770010913 on epoch=153
05/23/2022 11:31:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=153
05/23/2022 11:32:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/23/2022 11:32:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
05/23/2022 11:32:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=155
05/23/2022 11:32:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/23/2022 11:32:11 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.7241107470264325 on epoch=156
05/23/2022 11:32:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7091715912983899 -> 0.7241107470264325 on epoch=156, global_step=2500
05/23/2022 11:32:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/23/2022 11:32:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
05/23/2022 11:32:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=158
05/23/2022 11:32:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/23/2022 11:32:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/23/2022 11:32:27 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7217405337282546 on epoch=159
05/23/2022 11:32:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
05/23/2022 11:32:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/23/2022 11:32:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/23/2022 11:32:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=161
05/23/2022 11:32:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=162
05/23/2022 11:32:43 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.6912776544860386 on epoch=162
05/23/2022 11:32:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/23/2022 11:32:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=163
05/23/2022 11:32:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/23/2022 11:32:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/23/2022 11:32:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
05/23/2022 11:32:59 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.7268584155570457 on epoch=165
05/23/2022 11:32:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7241107470264325 -> 0.7268584155570457 on epoch=165, global_step=2650
05/23/2022 11:33:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=166
05/23/2022 11:33:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/23/2022 11:33:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
05/23/2022 11:33:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
05/23/2022 11:33:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/23/2022 11:33:15 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.7147112041329651 on epoch=168
05/23/2022 11:33:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/23/2022 11:33:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/23/2022 11:33:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/23/2022 11:33:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=171
05/23/2022 11:33:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/23/2022 11:33:31 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.6698203995843567 on epoch=171
05/23/2022 11:33:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=172
05/23/2022 11:33:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/23/2022 11:33:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
05/23/2022 11:33:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/23/2022 11:33:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/23/2022 11:33:47 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.7091440978575287 on epoch=174
05/23/2022 11:33:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=175
05/23/2022 11:33:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
05/23/2022 11:33:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/23/2022 11:33:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
05/23/2022 11:34:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/23/2022 11:34:03 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.6693351929475997 on epoch=178
05/23/2022 11:34:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
05/23/2022 11:34:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
05/23/2022 11:34:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/23/2022 11:34:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/23/2022 11:34:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/23/2022 11:34:19 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.731104713848277 on epoch=181
05/23/2022 11:34:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7268584155570457 -> 0.731104713848277 on epoch=181, global_step=2900
05/23/2022 11:34:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/23/2022 11:34:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=182
05/23/2022 11:34:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.17 on epoch=183
05/23/2022 11:34:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/23/2022 11:34:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=184
05/23/2022 11:34:36 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.7466719884813728 on epoch=184
05/23/2022 11:34:36 - INFO - __main__ - Saving model with best Classification-F1: 0.731104713848277 -> 0.7466719884813728 on epoch=184, global_step=2950
05/23/2022 11:34:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=184
05/23/2022 11:34:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=185
05/23/2022 11:34:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=186
05/23/2022 11:34:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/23/2022 11:34:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/23/2022 11:34:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:34:49 - INFO - __main__ - Printing 3 examples
05/23/2022 11:34:49 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 11:34:49 - INFO - __main__ - ['others']
05/23/2022 11:34:49 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 11:34:49 - INFO - __main__ - ['others']
05/23/2022 11:34:49 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 11:34:49 - INFO - __main__ - ['others']
05/23/2022 11:34:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:34:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:34:50 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:34:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:34:50 - INFO - __main__ - Printing 3 examples
05/23/2022 11:34:50 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 11:34:50 - INFO - __main__ - ['others']
05/23/2022 11:34:50 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 11:34:50 - INFO - __main__ - ['others']
05/23/2022 11:34:50 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 11:34:50 - INFO - __main__ - ['others']
05/23/2022 11:34:50 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:34:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:34:50 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:34:52 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.7513291235230046 on epoch=187
05/23/2022 11:34:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7466719884813728 -> 0.7513291235230046 on epoch=187, global_step=3000
05/23/2022 11:34:52 - INFO - __main__ - save last model!
05/23/2022 11:34:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:34:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:34:52 - INFO - __main__ - Printing 3 examples
05/23/2022 11:34:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:34:52 - INFO - __main__ - ['others']
05/23/2022 11:34:52 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:34:52 - INFO - __main__ - ['others']
05/23/2022 11:34:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:34:52 - INFO - __main__ - ['others']
05/23/2022 11:34:52 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:34:54 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:34:59 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:35:05 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:35:05 - INFO - __main__ - task name: emo
05/23/2022 11:35:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:35:06 - INFO - __main__ - Starting training!
05/23/2022 11:36:10 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_100_0.2_8_predictions.txt
05/23/2022 11:36:10 - INFO - __main__ - Classification-F1 on test data: 0.5240
05/23/2022 11:36:11 - INFO - __main__ - prefix=emo_64_100, lr=0.2, bsz=8, dev_performance=0.7513291235230046, test_performance=0.5240320558396215
05/23/2022 11:36:11 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.5, bsz=8 ...
05/23/2022 11:36:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:36:12 - INFO - __main__ - Printing 3 examples
05/23/2022 11:36:12 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:36:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:36:12 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:36:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:36:12 - INFO - __main__ - Printing 3 examples
05/23/2022 11:36:12 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 11:36:12 - INFO - __main__ - ['others']
05/23/2022 11:36:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:36:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:36:12 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:36:31 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:36:31 - INFO - __main__ - task name: emo
05/23/2022 11:36:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:36:32 - INFO - __main__ - Starting training!
05/23/2022 11:36:35 - INFO - __main__ - Step 10 Global step 10 Train loss 6.22 on epoch=0
05/23/2022 11:36:37 - INFO - __main__ - Step 20 Global step 20 Train loss 2.30 on epoch=1
05/23/2022 11:36:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=1
05/23/2022 11:36:42 - INFO - __main__ - Step 40 Global step 40 Train loss 1.15 on epoch=2
05/23/2022 11:36:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=3
05/23/2022 11:36:48 - INFO - __main__ - Global step 50 Train loss 2.38 Classification-F1 0.1610026674442213 on epoch=3
05/23/2022 11:36:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1610026674442213 on epoch=3, global_step=50
05/23/2022 11:36:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.96 on epoch=3
05/23/2022 11:36:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=4
05/23/2022 11:36:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=4
05/23/2022 11:36:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=5
05/23/2022 11:37:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=6
05/23/2022 11:37:04 - INFO - __main__ - Global step 100 Train loss 0.97 Classification-F1 0.10820468839336764 on epoch=6
05/23/2022 11:37:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=6
05/23/2022 11:37:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=7
05/23/2022 11:37:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=8
05/23/2022 11:37:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=8
05/23/2022 11:37:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=9
05/23/2022 11:37:20 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.1 on epoch=9
05/23/2022 11:37:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
05/23/2022 11:37:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=10
05/23/2022 11:37:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.87 on epoch=11
05/23/2022 11:37:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=11
05/23/2022 11:37:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=12
05/23/2022 11:37:36 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.11463715044156686 on epoch=12
05/23/2022 11:37:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=13
05/23/2022 11:37:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.80 on epoch=13
05/23/2022 11:37:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=14
05/23/2022 11:37:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=14
05/23/2022 11:37:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=15
05/23/2022 11:37:52 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.17351511543703985 on epoch=15
05/23/2022 11:37:52 - INFO - __main__ - Saving model with best Classification-F1: 0.1610026674442213 -> 0.17351511543703985 on epoch=15, global_step=250
05/23/2022 11:37:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=16
05/23/2022 11:37:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=16
05/23/2022 11:38:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=17
05/23/2022 11:38:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.73 on epoch=18
05/23/2022 11:38:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=18
05/23/2022 11:38:08 - INFO - __main__ - Global step 300 Train loss 0.78 Classification-F1 0.33603326835079034 on epoch=18
05/23/2022 11:38:08 - INFO - __main__ - Saving model with best Classification-F1: 0.17351511543703985 -> 0.33603326835079034 on epoch=18, global_step=300
05/23/2022 11:38:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=19
05/23/2022 11:38:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=19
05/23/2022 11:38:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=20
05/23/2022 11:38:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.68 on epoch=21
05/23/2022 11:38:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.68 on epoch=21
05/23/2022 11:38:24 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.35043740322701966 on epoch=21
05/23/2022 11:38:24 - INFO - __main__ - Saving model with best Classification-F1: 0.33603326835079034 -> 0.35043740322701966 on epoch=21, global_step=350
05/23/2022 11:38:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.78 on epoch=22
05/23/2022 11:38:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.76 on epoch=23
05/23/2022 11:38:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.69 on epoch=23
05/23/2022 11:38:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=24
05/23/2022 11:38:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.71 on epoch=24
05/23/2022 11:38:40 - INFO - __main__ - Global step 400 Train loss 0.74 Classification-F1 0.3014913657770801 on epoch=24
05/23/2022 11:38:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=25
05/23/2022 11:38:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=26
05/23/2022 11:38:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=26
05/23/2022 11:38:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=27
05/23/2022 11:38:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.73 on epoch=28
05/23/2022 11:38:56 - INFO - __main__ - Global step 450 Train loss 0.70 Classification-F1 0.4479201306703703 on epoch=28
05/23/2022 11:38:56 - INFO - __main__ - Saving model with best Classification-F1: 0.35043740322701966 -> 0.4479201306703703 on epoch=28, global_step=450
05/23/2022 11:38:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.59 on epoch=28
05/23/2022 11:39:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.69 on epoch=29
05/23/2022 11:39:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.64 on epoch=29
05/23/2022 11:39:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=30
05/23/2022 11:39:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.61 on epoch=31
05/23/2022 11:39:12 - INFO - __main__ - Global step 500 Train loss 0.63 Classification-F1 0.6664713306309978 on epoch=31
05/23/2022 11:39:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4479201306703703 -> 0.6664713306309978 on epoch=31, global_step=500
05/23/2022 11:39:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=31
05/23/2022 11:39:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=32
05/23/2022 11:39:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=33
05/23/2022 11:39:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=33
05/23/2022 11:39:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=34
05/23/2022 11:39:28 - INFO - __main__ - Global step 550 Train loss 0.59 Classification-F1 0.6429107019342469 on epoch=34
05/23/2022 11:39:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.65 on epoch=34
05/23/2022 11:39:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=35
05/23/2022 11:39:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=36
05/23/2022 11:39:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=36
05/23/2022 11:39:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=37
05/23/2022 11:39:45 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.6625649295676543 on epoch=37
05/23/2022 11:39:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=38
05/23/2022 11:39:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=38
05/23/2022 11:39:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=39
05/23/2022 11:39:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=39
05/23/2022 11:39:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=40
05/23/2022 11:40:00 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.7202502948090328 on epoch=40
05/23/2022 11:40:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6664713306309978 -> 0.7202502948090328 on epoch=40, global_step=650
05/23/2022 11:40:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=41
05/23/2022 11:40:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=41
05/23/2022 11:40:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=42
05/23/2022 11:40:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/23/2022 11:40:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=43
05/23/2022 11:40:16 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.7269793711716898 on epoch=43
05/23/2022 11:40:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7202502948090328 -> 0.7269793711716898 on epoch=43, global_step=700
05/23/2022 11:40:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/23/2022 11:40:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=44
05/23/2022 11:40:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=45
05/23/2022 11:40:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=46
05/23/2022 11:40:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/23/2022 11:40:32 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.764864480572598 on epoch=46
05/23/2022 11:40:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7269793711716898 -> 0.764864480572598 on epoch=46, global_step=750
05/23/2022 11:40:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/23/2022 11:40:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
05/23/2022 11:40:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=48
05/23/2022 11:40:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=49
05/23/2022 11:40:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=49
05/23/2022 11:40:48 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.7553286844881383 on epoch=49
05/23/2022 11:40:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=50
05/23/2022 11:40:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=51
05/23/2022 11:40:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
05/23/2022 11:40:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/23/2022 11:41:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=53
05/23/2022 11:41:04 - INFO - __main__ - Global step 850 Train loss 0.31 Classification-F1 0.7291701279397055 on epoch=53
05/23/2022 11:41:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=53
05/23/2022 11:41:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/23/2022 11:41:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=54
05/23/2022 11:41:14 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=55
05/23/2022 11:41:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/23/2022 11:41:20 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.7057097563368011 on epoch=56
05/23/2022 11:41:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=56
05/23/2022 11:41:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/23/2022 11:41:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=58
05/23/2022 11:41:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=58
05/23/2022 11:41:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=59
05/23/2022 11:41:36 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.7897398814598303 on epoch=59
05/23/2022 11:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.764864480572598 -> 0.7897398814598303 on epoch=59, global_step=950
05/23/2022 11:41:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=59
05/23/2022 11:41:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=60
05/23/2022 11:41:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=61
05/23/2022 11:41:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=61
05/23/2022 11:41:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/23/2022 11:41:53 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.7601234896943851 on epoch=62
05/23/2022 11:41:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=63
05/23/2022 11:41:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=63
05/23/2022 11:42:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/23/2022 11:42:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=64
05/23/2022 11:42:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=65
05/23/2022 11:42:09 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.7949116603989368 on epoch=65
05/23/2022 11:42:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7897398814598303 -> 0.7949116603989368 on epoch=65, global_step=1050
05/23/2022 11:42:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=66
05/23/2022 11:42:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=66
05/23/2022 11:42:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/23/2022 11:42:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/23/2022 11:42:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=68
05/23/2022 11:42:25 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.788856876708203 on epoch=68
05/23/2022 11:42:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=69
05/23/2022 11:42:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=69
05/23/2022 11:42:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=70
05/23/2022 11:42:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=71
05/23/2022 11:42:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=71
05/23/2022 11:42:41 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.7495237696996729 on epoch=71
05/23/2022 11:42:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=72
05/23/2022 11:42:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=73
05/23/2022 11:42:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/23/2022 11:42:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=74
05/23/2022 11:42:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=74
05/23/2022 11:42:57 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7607822531735575 on epoch=74
05/23/2022 11:43:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.11 on epoch=75
05/23/2022 11:43:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/23/2022 11:43:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=76
05/23/2022 11:43:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=77
05/23/2022 11:43:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=78
05/23/2022 11:43:14 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.7855897105772979 on epoch=78
05/23/2022 11:43:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=78
05/23/2022 11:43:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=79
05/23/2022 11:43:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=79
05/23/2022 11:43:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=80
05/23/2022 11:43:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=81
05/23/2022 11:43:30 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.7938229914313073 on epoch=81
05/23/2022 11:43:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.08 on epoch=81
05/23/2022 11:43:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=82
05/23/2022 11:43:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=83
05/23/2022 11:43:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=83
05/23/2022 11:43:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=84
05/23/2022 11:43:46 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7796821189200869 on epoch=84
05/23/2022 11:43:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=84
05/23/2022 11:43:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=85
05/23/2022 11:43:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=86
05/23/2022 11:43:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=86
05/23/2022 11:43:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=87
05/23/2022 11:44:02 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.781210229842523 on epoch=87
05/23/2022 11:44:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/23/2022 11:44:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=88
05/23/2022 11:44:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=89
05/23/2022 11:44:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=89
05/23/2022 11:44:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=90
05/23/2022 11:44:18 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.7609219727838525 on epoch=90
05/23/2022 11:44:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=91
05/23/2022 11:44:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=91
05/23/2022 11:44:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=92
05/23/2022 11:44:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=93
05/23/2022 11:44:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=93
05/23/2022 11:44:34 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.7859434135539465 on epoch=93
05/23/2022 11:44:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=94
05/23/2022 11:44:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/23/2022 11:44:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=95
05/23/2022 11:44:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=96
05/23/2022 11:44:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=96
05/23/2022 11:44:50 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.780093555093555 on epoch=96
05/23/2022 11:44:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=97
05/23/2022 11:44:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=98
05/23/2022 11:44:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=98
05/23/2022 11:45:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=99
05/23/2022 11:45:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=99
05/23/2022 11:45:06 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.7728581183757477 on epoch=99
05/23/2022 11:45:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=100
05/23/2022 11:45:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=101
05/23/2022 11:45:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=101
05/23/2022 11:45:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=102
05/23/2022 11:45:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=103
05/23/2022 11:45:23 - INFO - __main__ - Global step 1650 Train loss 0.07 Classification-F1 0.7556621417780052 on epoch=103
05/23/2022 11:45:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=103
05/23/2022 11:45:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=104
05/23/2022 11:45:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/23/2022 11:45:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=105
05/23/2022 11:45:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/23/2022 11:45:39 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.7644144443359271 on epoch=106
05/23/2022 11:45:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=106
05/23/2022 11:45:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=107
05/23/2022 11:45:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=108
05/23/2022 11:45:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=108
05/23/2022 11:45:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=109
05/23/2022 11:45:55 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.7945834288713545 on epoch=109
05/23/2022 11:45:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=109
05/23/2022 11:46:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=110
05/23/2022 11:46:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
05/23/2022 11:46:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=111
05/23/2022 11:46:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=112
05/23/2022 11:46:11 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.7693222397449784 on epoch=112
05/23/2022 11:46:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=113
05/23/2022 11:46:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=113
05/23/2022 11:46:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=114
05/23/2022 11:46:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=114
05/23/2022 11:46:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=115
05/23/2022 11:46:27 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7779719787195454 on epoch=115
05/23/2022 11:46:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=116
05/23/2022 11:46:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=116
05/23/2022 11:46:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=117
05/23/2022 11:46:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=118
05/23/2022 11:46:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/23/2022 11:46:43 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.7782130886463717 on epoch=118
05/23/2022 11:46:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=119
05/23/2022 11:46:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/23/2022 11:46:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=120
05/23/2022 11:46:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=121
05/23/2022 11:46:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/23/2022 11:46:59 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.7719530213195618 on epoch=121
05/23/2022 11:47:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=122
05/23/2022 11:47:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=123
05/23/2022 11:47:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=123
05/23/2022 11:47:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=124
05/23/2022 11:47:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=124
05/23/2022 11:47:15 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.8021042189225914 on epoch=124
05/23/2022 11:47:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7949116603989368 -> 0.8021042189225914 on epoch=124, global_step=2000
05/23/2022 11:47:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=125
05/23/2022 11:47:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=126
05/23/2022 11:47:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=126
05/23/2022 11:47:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=127
05/23/2022 11:47:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=128
05/23/2022 11:47:32 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7959506842699232 on epoch=128
05/23/2022 11:47:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/23/2022 11:47:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/23/2022 11:47:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.01 on epoch=129
05/23/2022 11:47:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=130
05/23/2022 11:47:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=131
05/23/2022 11:47:48 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7862341566255899 on epoch=131
05/23/2022 11:47:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=131
05/23/2022 11:47:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=132
05/23/2022 11:47:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=133
05/23/2022 11:47:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=133
05/23/2022 11:48:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=134
05/23/2022 11:48:04 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.7790383132848886 on epoch=134
05/23/2022 11:48:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=134
05/23/2022 11:48:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=135
05/23/2022 11:48:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=136
05/23/2022 11:48:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=136
05/23/2022 11:48:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=137
05/23/2022 11:48:20 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.7847808729219578 on epoch=137
05/23/2022 11:48:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=138
05/23/2022 11:48:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=138
05/23/2022 11:48:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/23/2022 11:48:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.01 on epoch=139
05/23/2022 11:48:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=140
05/23/2022 11:48:36 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.76423477671049 on epoch=140
05/23/2022 11:48:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/23/2022 11:48:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=141
05/23/2022 11:48:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=142
05/23/2022 11:48:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=143
05/23/2022 11:48:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.02 on epoch=143
05/23/2022 11:48:52 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.8024472096566295 on epoch=143
05/23/2022 11:48:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8021042189225914 -> 0.8024472096566295 on epoch=143, global_step=2300
05/23/2022 11:48:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=144
05/23/2022 11:48:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=144
05/23/2022 11:49:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=145
05/23/2022 11:49:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
05/23/2022 11:49:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/23/2022 11:49:08 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.8079339847954381 on epoch=146
05/23/2022 11:49:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8024472096566295 -> 0.8079339847954381 on epoch=146, global_step=2350
05/23/2022 11:49:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.01 on epoch=147
05/23/2022 11:49:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/23/2022 11:49:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.01 on epoch=148
05/23/2022 11:49:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=149
05/23/2022 11:49:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=149
05/23/2022 11:49:24 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.8164905258002393 on epoch=149
05/23/2022 11:49:24 - INFO - __main__ - Saving model with best Classification-F1: 0.8079339847954381 -> 0.8164905258002393 on epoch=149, global_step=2400
05/23/2022 11:49:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=150
05/23/2022 11:49:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/23/2022 11:49:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=151
05/23/2022 11:49:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=152
05/23/2022 11:49:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.00 on epoch=153
05/23/2022 11:49:40 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.8060272259503237 on epoch=153
05/23/2022 11:49:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/23/2022 11:49:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/23/2022 11:49:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=154
05/23/2022 11:49:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/23/2022 11:49:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=156
05/23/2022 11:49:56 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7893050877718069 on epoch=156
05/23/2022 11:49:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
05/23/2022 11:50:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=157
05/23/2022 11:50:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/23/2022 11:50:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=158
05/23/2022 11:50:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
05/23/2022 11:50:12 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.8152530360494962 on epoch=159
05/23/2022 11:50:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/23/2022 11:50:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=160
05/23/2022 11:50:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
05/23/2022 11:50:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=161
05/23/2022 11:50:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.01 on epoch=162
05/23/2022 11:50:28 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7873497684826286 on epoch=162
05/23/2022 11:50:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=163
05/23/2022 11:50:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/23/2022 11:50:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=164
05/23/2022 11:50:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
05/23/2022 11:50:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=165
05/23/2022 11:50:44 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.8146801775125749 on epoch=165
05/23/2022 11:50:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/23/2022 11:50:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/23/2022 11:50:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/23/2022 11:50:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=168
05/23/2022 11:50:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.00 on epoch=168
05/23/2022 11:51:00 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.8104362782551032 on epoch=168
05/23/2022 11:51:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/23/2022 11:51:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
05/23/2022 11:51:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/23/2022 11:51:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/23/2022 11:51:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=171
05/23/2022 11:51:16 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.8201586085027119 on epoch=171
05/23/2022 11:51:16 - INFO - __main__ - Saving model with best Classification-F1: 0.8164905258002393 -> 0.8201586085027119 on epoch=171, global_step=2750
05/23/2022 11:51:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/23/2022 11:51:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=173
05/23/2022 11:51:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/23/2022 11:51:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/23/2022 11:51:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/23/2022 11:51:32 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.7904872909213869 on epoch=174
05/23/2022 11:51:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=175
05/23/2022 11:51:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/23/2022 11:51:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=176
05/23/2022 11:51:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
05/23/2022 11:51:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/23/2022 11:51:49 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7846126341195611 on epoch=178
05/23/2022 11:51:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/23/2022 11:51:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/23/2022 11:51:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/23/2022 11:51:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
05/23/2022 11:52:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
05/23/2022 11:52:05 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.8179497250589159 on epoch=181
05/23/2022 11:52:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/23/2022 11:52:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/23/2022 11:52:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/23/2022 11:52:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
05/23/2022 11:52:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/23/2022 11:52:21 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.8072580458542007 on epoch=184
05/23/2022 11:52:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/23/2022 11:52:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=185
05/23/2022 11:52:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=186
05/23/2022 11:52:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=186
05/23/2022 11:52:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=187
05/23/2022 11:52:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:52:35 - INFO - __main__ - Printing 3 examples
05/23/2022 11:52:35 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:52:35 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:52:35 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:52:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:52:35 - INFO - __main__ - Printing 3 examples
05/23/2022 11:52:35 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 11:52:35 - INFO - __main__ - ['others']
05/23/2022 11:52:35 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:52:35 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:52:35 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:52:37 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.8070795673936003 on epoch=187
05/23/2022 11:52:37 - INFO - __main__ - save last model!
05/23/2022 11:52:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 11:52:37 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 11:52:37 - INFO - __main__ - Printing 3 examples
05/23/2022 11:52:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 11:52:37 - INFO - __main__ - ['others']
05/23/2022 11:52:37 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 11:52:37 - INFO - __main__ - ['others']
05/23/2022 11:52:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 11:52:37 - INFO - __main__ - ['others']
05/23/2022 11:52:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:52:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:52:45 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 11:52:50 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:52:50 - INFO - __main__ - task name: emo
05/23/2022 11:52:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:52:51 - INFO - __main__ - Starting training!
05/23/2022 11:53:59 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_13_0.5_8_predictions.txt
05/23/2022 11:53:59 - INFO - __main__ - Classification-F1 on test data: 0.3560
05/23/2022 11:53:59 - INFO - __main__ - prefix=emo_64_13, lr=0.5, bsz=8, dev_performance=0.8201586085027119, test_performance=0.3559798623347532
05/23/2022 11:53:59 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.4, bsz=8 ...
05/23/2022 11:54:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:54:00 - INFO - __main__ - Printing 3 examples
05/23/2022 11:54:00 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 11:54:00 - INFO - __main__ - ['others']
05/23/2022 11:54:00 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 11:54:00 - INFO - __main__ - ['others']
05/23/2022 11:54:00 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 11:54:00 - INFO - __main__ - ['others']
05/23/2022 11:54:00 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:54:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:54:01 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 11:54:01 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 11:54:01 - INFO - __main__ - Printing 3 examples
05/23/2022 11:54:01 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 11:54:01 - INFO - __main__ - ['others']
05/23/2022 11:54:01 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 11:54:01 - INFO - __main__ - ['others']
05/23/2022 11:54:01 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 11:54:01 - INFO - __main__ - ['others']
05/23/2022 11:54:01 - INFO - __main__ - Tokenizing Input ...
05/23/2022 11:54:01 - INFO - __main__ - Tokenizing Output ...
05/23/2022 11:54:01 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 11:54:20 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 11:54:20 - INFO - __main__ - task name: emo
05/23/2022 11:54:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 11:54:21 - INFO - __main__ - Starting training!
05/23/2022 11:54:24 - INFO - __main__ - Step 10 Global step 10 Train loss 6.46 on epoch=0
05/23/2022 11:54:26 - INFO - __main__ - Step 20 Global step 20 Train loss 2.72 on epoch=1
05/23/2022 11:54:29 - INFO - __main__ - Step 30 Global step 30 Train loss 1.54 on epoch=1
05/23/2022 11:54:31 - INFO - __main__ - Step 40 Global step 40 Train loss 1.20 on epoch=2
05/23/2022 11:54:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.24 on epoch=3
05/23/2022 11:54:37 - INFO - __main__ - Global step 50 Train loss 2.63 Classification-F1 0.13021618903971846 on epoch=3
05/23/2022 11:54:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13021618903971846 on epoch=3, global_step=50
05/23/2022 11:54:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=3
05/23/2022 11:54:42 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=4
05/23/2022 11:54:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
05/23/2022 11:54:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
05/23/2022 11:54:50 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=6
05/23/2022 11:54:53 - INFO - __main__ - Global step 100 Train loss 1.00 Classification-F1 0.16899235878026136 on epoch=6
05/23/2022 11:54:53 - INFO - __main__ - Saving model with best Classification-F1: 0.13021618903971846 -> 0.16899235878026136 on epoch=6, global_step=100
05/23/2022 11:54:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=6
05/23/2022 11:54:58 - INFO - __main__ - Step 120 Global step 120 Train loss 1.02 on epoch=7
05/23/2022 11:55:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.90 on epoch=8
05/23/2022 11:55:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=8
05/23/2022 11:55:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=9
05/23/2022 11:55:09 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.1 on epoch=9
05/23/2022 11:55:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=9
05/23/2022 11:55:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.96 on epoch=10
05/23/2022 11:55:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=11
05/23/2022 11:55:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=11
05/23/2022 11:55:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=12
05/23/2022 11:55:25 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.21351083143711588 on epoch=12
05/23/2022 11:55:25 - INFO - __main__ - Saving model with best Classification-F1: 0.16899235878026136 -> 0.21351083143711588 on epoch=12, global_step=200
05/23/2022 11:55:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=13
05/23/2022 11:55:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=13
05/23/2022 11:55:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.79 on epoch=14
05/23/2022 11:55:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=14
05/23/2022 11:55:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=15
05/23/2022 11:55:41 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.28845393773254113 on epoch=15
05/23/2022 11:55:41 - INFO - __main__ - Saving model with best Classification-F1: 0.21351083143711588 -> 0.28845393773254113 on epoch=15, global_step=250
05/23/2022 11:55:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=16
05/23/2022 11:55:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.75 on epoch=16
05/23/2022 11:55:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=17
05/23/2022 11:55:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=18
05/23/2022 11:55:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=18
05/23/2022 11:55:57 - INFO - __main__ - Global step 300 Train loss 0.82 Classification-F1 0.34390723243637206 on epoch=18
05/23/2022 11:55:57 - INFO - __main__ - Saving model with best Classification-F1: 0.28845393773254113 -> 0.34390723243637206 on epoch=18, global_step=300
05/23/2022 11:55:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=19
05/23/2022 11:56:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.77 on epoch=19
05/23/2022 11:56:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.82 on epoch=20
05/23/2022 11:56:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=21
05/23/2022 11:56:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=21
05/23/2022 11:56:13 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.27110744688084004 on epoch=21
05/23/2022 11:56:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=22
05/23/2022 11:56:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.74 on epoch=23
05/23/2022 11:56:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=23
05/23/2022 11:56:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.74 on epoch=24
05/23/2022 11:56:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=24
05/23/2022 11:56:28 - INFO - __main__ - Global step 400 Train loss 0.76 Classification-F1 0.3080816272081332 on epoch=24
05/23/2022 11:56:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=25
05/23/2022 11:56:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=26
05/23/2022 11:56:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=26
05/23/2022 11:56:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=27
05/23/2022 11:56:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=28
05/23/2022 11:56:44 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.40287008104018446 on epoch=28
05/23/2022 11:56:44 - INFO - __main__ - Saving model with best Classification-F1: 0.34390723243637206 -> 0.40287008104018446 on epoch=28, global_step=450
05/23/2022 11:56:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=28
05/23/2022 11:56:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=29
05/23/2022 11:56:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.68 on epoch=29
05/23/2022 11:56:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=30
05/23/2022 11:56:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=31
05/23/2022 11:57:00 - INFO - __main__ - Global step 500 Train loss 0.68 Classification-F1 0.4813224851981952 on epoch=31
05/23/2022 11:57:00 - INFO - __main__ - Saving model with best Classification-F1: 0.40287008104018446 -> 0.4813224851981952 on epoch=31, global_step=500
05/23/2022 11:57:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=31
05/23/2022 11:57:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=32
05/23/2022 11:57:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.71 on epoch=33
05/23/2022 11:57:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=33
05/23/2022 11:57:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=34
05/23/2022 11:57:16 - INFO - __main__ - Global step 550 Train loss 0.59 Classification-F1 0.6554034664316081 on epoch=34
05/23/2022 11:57:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4813224851981952 -> 0.6554034664316081 on epoch=34, global_step=550
05/23/2022 11:57:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=34
05/23/2022 11:57:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=35
05/23/2022 11:57:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=36
05/23/2022 11:57:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=36
05/23/2022 11:57:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=37
05/23/2022 11:57:32 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.4838836104707594 on epoch=37
05/23/2022 11:57:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=38
05/23/2022 11:57:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=38
05/23/2022 11:57:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=39
05/23/2022 11:57:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=39
05/23/2022 11:57:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=40
05/23/2022 11:57:48 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.5491821253267036 on epoch=40
05/23/2022 11:57:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=41
05/23/2022 11:57:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
05/23/2022 11:57:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.51 on epoch=42
05/23/2022 11:57:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=43
05/23/2022 11:58:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=43
05/23/2022 11:58:04 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.6296158796158795 on epoch=43
05/23/2022 11:58:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=44
05/23/2022 11:58:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/23/2022 11:58:11 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=45
05/23/2022 11:58:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
05/23/2022 11:58:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=46
05/23/2022 11:58:20 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.6570920816132947 on epoch=46
05/23/2022 11:58:20 - INFO - __main__ - Saving model with best Classification-F1: 0.6554034664316081 -> 0.6570920816132947 on epoch=46, global_step=750
05/23/2022 11:58:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=47
05/23/2022 11:58:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
05/23/2022 11:58:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
05/23/2022 11:58:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=49
05/23/2022 11:58:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/23/2022 11:58:36 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.7513349301073087 on epoch=49
05/23/2022 11:58:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6570920816132947 -> 0.7513349301073087 on epoch=49, global_step=800
05/23/2022 11:58:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=50
05/23/2022 11:58:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
05/23/2022 11:58:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=51
05/23/2022 11:58:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/23/2022 11:58:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/23/2022 11:58:52 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.7445015797328277 on epoch=53
05/23/2022 11:58:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/23/2022 11:58:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=54
05/23/2022 11:58:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
05/23/2022 11:59:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=55
05/23/2022 11:59:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
05/23/2022 11:59:08 - INFO - __main__ - Global step 900 Train loss 0.32 Classification-F1 0.7614355738526278 on epoch=56
05/23/2022 11:59:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7513349301073087 -> 0.7614355738526278 on epoch=56, global_step=900
05/23/2022 11:59:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/23/2022 11:59:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
05/23/2022 11:59:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
05/23/2022 11:59:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
05/23/2022 11:59:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=59
05/23/2022 11:59:24 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.7571551521371908 on epoch=59
05/23/2022 11:59:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=59
05/23/2022 11:59:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
05/23/2022 11:59:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/23/2022 11:59:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=61
05/23/2022 11:59:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=62
05/23/2022 11:59:40 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.7636561444122013 on epoch=62
05/23/2022 11:59:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7614355738526278 -> 0.7636561444122013 on epoch=62, global_step=1000
05/23/2022 11:59:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=63
05/23/2022 11:59:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=63
05/23/2022 11:59:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=64
05/23/2022 11:59:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/23/2022 11:59:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=65
05/23/2022 11:59:56 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.6865664724069797 on epoch=65
05/23/2022 11:59:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=66
05/23/2022 12:00:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=66
05/23/2022 12:00:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=67
05/23/2022 12:00:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=68
05/23/2022 12:00:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=68
05/23/2022 12:00:12 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.7841986733866296 on epoch=68
05/23/2022 12:00:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7636561444122013 -> 0.7841986733866296 on epoch=68, global_step=1100
05/23/2022 12:00:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=69
05/23/2022 12:00:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=69
05/23/2022 12:00:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=70
05/23/2022 12:00:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/23/2022 12:00:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/23/2022 12:00:28 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.6895691005409798 on epoch=71
05/23/2022 12:00:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=72
05/23/2022 12:00:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=73
05/23/2022 12:00:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
05/23/2022 12:00:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/23/2022 12:00:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=74
05/23/2022 12:00:43 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7802162024815155 on epoch=74
05/23/2022 12:00:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
05/23/2022 12:00:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
05/23/2022 12:00:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=76
05/23/2022 12:00:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/23/2022 12:00:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=78
05/23/2022 12:00:59 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.7783235914140367 on epoch=78
05/23/2022 12:01:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/23/2022 12:01:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=79
05/23/2022 12:01:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=79
05/23/2022 12:01:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/23/2022 12:01:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=81
05/23/2022 12:01:15 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.7894358411318817 on epoch=81
05/23/2022 12:01:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7841986733866296 -> 0.7894358411318817 on epoch=81, global_step=1300
05/23/2022 12:01:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/23/2022 12:01:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/23/2022 12:01:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/23/2022 12:01:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/23/2022 12:01:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=84
05/23/2022 12:01:31 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.8064811634354019 on epoch=84
05/23/2022 12:01:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7894358411318817 -> 0.8064811634354019 on epoch=84, global_step=1350
05/23/2022 12:01:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/23/2022 12:01:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
05/23/2022 12:01:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=86
05/23/2022 12:01:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
05/23/2022 12:01:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/23/2022 12:01:47 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.7580830331769005 on epoch=87
05/23/2022 12:01:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=88
05/23/2022 12:01:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=88
05/23/2022 12:01:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/23/2022 12:01:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=89
05/23/2022 12:02:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=90
05/23/2022 12:02:03 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.8102359852544081 on epoch=90
05/23/2022 12:02:03 - INFO - __main__ - Saving model with best Classification-F1: 0.8064811634354019 -> 0.8102359852544081 on epoch=90, global_step=1450
05/23/2022 12:02:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.15 on epoch=91
05/23/2022 12:02:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=91
05/23/2022 12:02:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=92
05/23/2022 12:02:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/23/2022 12:02:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=93
05/23/2022 12:02:19 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.7686428234373439 on epoch=93
05/23/2022 12:02:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=94
05/23/2022 12:02:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=94
05/23/2022 12:02:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=95
05/23/2022 12:02:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
05/23/2022 12:02:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=96
05/23/2022 12:02:35 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.7492227371948595 on epoch=96
05/23/2022 12:02:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=97
05/23/2022 12:02:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=98
05/23/2022 12:02:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/23/2022 12:02:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/23/2022 12:02:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/23/2022 12:02:51 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.7831385903822229 on epoch=99
05/23/2022 12:02:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=100
05/23/2022 12:02:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=101
05/23/2022 12:02:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=101
05/23/2022 12:03:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=102
05/23/2022 12:03:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=103
05/23/2022 12:03:07 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.8062174135167318 on epoch=103
05/23/2022 12:03:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=103
05/23/2022 12:03:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=104
05/23/2022 12:03:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=104
05/23/2022 12:03:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
05/23/2022 12:03:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/23/2022 12:03:23 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.8018346873237874 on epoch=106
05/23/2022 12:03:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=106
05/23/2022 12:03:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=107
05/23/2022 12:03:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=108
05/23/2022 12:03:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=108
05/23/2022 12:03:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/23/2022 12:03:39 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.7929847796913327 on epoch=109
05/23/2022 12:03:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/23/2022 12:03:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/23/2022 12:03:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=111
05/23/2022 12:03:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=111
05/23/2022 12:03:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
05/23/2022 12:03:55 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.7753119482355728 on epoch=112
05/23/2022 12:03:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=113
05/23/2022 12:04:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=113
05/23/2022 12:04:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/23/2022 12:04:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=114
05/23/2022 12:04:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=115
05/23/2022 12:04:11 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.77145987556509 on epoch=115
05/23/2022 12:04:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=116
05/23/2022 12:04:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=116
05/23/2022 12:04:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=117
05/23/2022 12:04:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=118
05/23/2022 12:04:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
05/23/2022 12:04:27 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.8047682864981662 on epoch=118
05/23/2022 12:04:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=119
05/23/2022 12:04:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=119
05/23/2022 12:04:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=120
05/23/2022 12:04:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=121
05/23/2022 12:04:40 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/23/2022 12:04:43 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.799636825533133 on epoch=121
05/23/2022 12:04:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=122
05/23/2022 12:04:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/23/2022 12:04:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
05/23/2022 12:04:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/23/2022 12:04:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=124
05/23/2022 12:04:59 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.8090040014049995 on epoch=124
05/23/2022 12:05:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=125
05/23/2022 12:05:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/23/2022 12:05:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=126
05/23/2022 12:05:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=127
05/23/2022 12:05:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=128
05/23/2022 12:05:15 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.7792644193326638 on epoch=128
05/23/2022 12:05:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=128
05/23/2022 12:05:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=129
05/23/2022 12:05:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=129
05/23/2022 12:05:25 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=130
05/23/2022 12:05:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
05/23/2022 12:05:31 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7306132264374225 on epoch=131
05/23/2022 12:05:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.02 on epoch=131
05/23/2022 12:05:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=132
05/23/2022 12:05:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=133
05/23/2022 12:05:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=133
05/23/2022 12:05:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=134
05/23/2022 12:05:47 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.789266927532957 on epoch=134
05/23/2022 12:05:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=134
05/23/2022 12:05:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=135
05/23/2022 12:05:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/23/2022 12:05:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/23/2022 12:05:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=137
05/23/2022 12:06:03 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7784044558127724 on epoch=137
05/23/2022 12:06:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=138
05/23/2022 12:06:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/23/2022 12:06:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=139
05/23/2022 12:06:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=139
05/23/2022 12:06:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/23/2022 12:06:19 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.78752223834884 on epoch=140
05/23/2022 12:06:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/23/2022 12:06:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
05/23/2022 12:06:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/23/2022 12:06:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=143
05/23/2022 12:06:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/23/2022 12:06:35 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.7929924793648656 on epoch=143
05/23/2022 12:06:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=144
05/23/2022 12:06:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=144
05/23/2022 12:06:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=145
05/23/2022 12:06:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=146
05/23/2022 12:06:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/23/2022 12:06:51 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.7677097479274176 on epoch=146
05/23/2022 12:06:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
05/23/2022 12:06:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=148
05/23/2022 12:06:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=148
05/23/2022 12:07:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=149
05/23/2022 12:07:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/23/2022 12:07:07 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7720315398886827 on epoch=149
05/23/2022 12:07:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/23/2022 12:07:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
05/23/2022 12:07:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=151
05/23/2022 12:07:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
05/23/2022 12:07:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=153
05/23/2022 12:07:23 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.8123187022367093 on epoch=153
05/23/2022 12:07:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8102359852544081 -> 0.8123187022367093 on epoch=153, global_step=2450
05/23/2022 12:07:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=153
05/23/2022 12:07:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/23/2022 12:07:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=154
05/23/2022 12:07:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=155
05/23/2022 12:07:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
05/23/2022 12:07:38 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7826425460434798 on epoch=156
05/23/2022 12:07:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/23/2022 12:07:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=157
05/23/2022 12:07:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/23/2022 12:07:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/23/2022 12:07:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
05/23/2022 12:07:54 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7877149018058109 on epoch=159
05/23/2022 12:07:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/23/2022 12:07:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/23/2022 12:08:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
05/23/2022 12:08:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/23/2022 12:08:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/23/2022 12:08:10 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.8115022405728819 on epoch=162
05/23/2022 12:08:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=163
05/23/2022 12:08:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=163
05/23/2022 12:08:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=164
05/23/2022 12:08:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/23/2022 12:08:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/23/2022 12:08:26 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.811804596926385 on epoch=165
05/23/2022 12:08:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
05/23/2022 12:08:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/23/2022 12:08:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/23/2022 12:08:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=168
05/23/2022 12:08:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/23/2022 12:08:42 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.8114719266109127 on epoch=168
05/23/2022 12:08:45 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/23/2022 12:08:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/23/2022 12:08:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
05/23/2022 12:08:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/23/2022 12:08:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/23/2022 12:08:58 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.8073018547291211 on epoch=171
05/23/2022 12:09:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/23/2022 12:09:03 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
05/23/2022 12:09:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/23/2022 12:09:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=174
05/23/2022 12:09:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=174
05/23/2022 12:09:14 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8199832281641891 on epoch=174
05/23/2022 12:09:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8123187022367093 -> 0.8199832281641891 on epoch=174, global_step=2800
05/23/2022 12:09:17 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/23/2022 12:09:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
05/23/2022 12:09:22 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/23/2022 12:09:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
05/23/2022 12:09:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/23/2022 12:09:30 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.8130295763389288 on epoch=178
05/23/2022 12:09:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/23/2022 12:09:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/23/2022 12:09:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/23/2022 12:09:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/23/2022 12:09:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
05/23/2022 12:09:46 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7889503023431595 on epoch=181
05/23/2022 12:09:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/23/2022 12:09:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/23/2022 12:09:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/23/2022 12:09:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/23/2022 12:09:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
05/23/2022 12:10:03 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7880773561530083 on epoch=184
05/23/2022 12:10:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/23/2022 12:10:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
05/23/2022 12:10:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/23/2022 12:10:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/23/2022 12:10:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/23/2022 12:10:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:10:17 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:17 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:10:17 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:10:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:10:17 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:17 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 12:10:17 - INFO - __main__ - ['others']
05/23/2022 12:10:17 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:17 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:10:17 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:10:19 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7924673704516473 on epoch=187
05/23/2022 12:10:19 - INFO - __main__ - save last model!
05/23/2022 12:10:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 12:10:19 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 12:10:19 - INFO - __main__ - Printing 3 examples
05/23/2022 12:10:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 12:10:19 - INFO - __main__ - ['others']
05/23/2022 12:10:19 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 12:10:19 - INFO - __main__ - ['others']
05/23/2022 12:10:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 12:10:19 - INFO - __main__ - ['others']
05/23/2022 12:10:19 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:10:21 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:10:26 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 12:10:32 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:10:32 - INFO - __main__ - task name: emo
05/23/2022 12:10:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:10:32 - INFO - __main__ - Starting training!
05/23/2022 12:11:41 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_13_0.4_8_predictions.txt
05/23/2022 12:11:41 - INFO - __main__ - Classification-F1 on test data: 0.5038
05/23/2022 12:11:41 - INFO - __main__ - prefix=emo_64_13, lr=0.4, bsz=8, dev_performance=0.8199832281641891, test_performance=0.5037886018002186
05/23/2022 12:11:41 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.3, bsz=8 ...
05/23/2022 12:11:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:11:42 - INFO - __main__ - Printing 3 examples
05/23/2022 12:11:42 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:11:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:11:42 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:11:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:11:42 - INFO - __main__ - Printing 3 examples
05/23/2022 12:11:42 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 12:11:42 - INFO - __main__ - ['others']
05/23/2022 12:11:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:11:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:11:43 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:12:02 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:12:02 - INFO - __main__ - task name: emo
05/23/2022 12:12:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:12:03 - INFO - __main__ - Starting training!
05/23/2022 12:12:06 - INFO - __main__ - Step 10 Global step 10 Train loss 6.78 on epoch=0
05/23/2022 12:12:08 - INFO - __main__ - Step 20 Global step 20 Train loss 3.61 on epoch=1
05/23/2022 12:12:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.87 on epoch=1
05/23/2022 12:12:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.41 on epoch=2
05/23/2022 12:12:15 - INFO - __main__ - Step 50 Global step 50 Train loss 1.10 on epoch=3
05/23/2022 12:12:19 - INFO - __main__ - Global step 50 Train loss 2.95 Classification-F1 0.127614709851552 on epoch=3
05/23/2022 12:12:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.127614709851552 on epoch=3, global_step=50
05/23/2022 12:12:21 - INFO - __main__ - Step 60 Global step 60 Train loss 1.02 on epoch=3
05/23/2022 12:12:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.97 on epoch=4
05/23/2022 12:12:26 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=4
05/23/2022 12:12:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=5
05/23/2022 12:12:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.99 on epoch=6
05/23/2022 12:12:35 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.3378308703235663 on epoch=6
05/23/2022 12:12:35 - INFO - __main__ - Saving model with best Classification-F1: 0.127614709851552 -> 0.3378308703235663 on epoch=6, global_step=100
05/23/2022 12:12:37 - INFO - __main__ - Step 110 Global step 110 Train loss 1.04 on epoch=6
05/23/2022 12:12:40 - INFO - __main__ - Step 120 Global step 120 Train loss 1.02 on epoch=7
05/23/2022 12:12:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=8
05/23/2022 12:12:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=8
05/23/2022 12:12:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/23/2022 12:12:51 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 12:12:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=9
05/23/2022 12:12:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.80 on epoch=10
05/23/2022 12:12:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=11
05/23/2022 12:13:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=11
05/23/2022 12:13:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=12
05/23/2022 12:13:06 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.14994873547505128 on epoch=12
05/23/2022 12:13:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.93 on epoch=13
05/23/2022 12:13:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=13
05/23/2022 12:13:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.96 on epoch=14
05/23/2022 12:13:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=14
05/23/2022 12:13:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=15
05/23/2022 12:13:22 - INFO - __main__ - Global step 250 Train loss 0.88 Classification-F1 0.11228875222125531 on epoch=15
05/23/2022 12:13:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=16
05/23/2022 12:13:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=16
05/23/2022 12:13:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.81 on epoch=17
05/23/2022 12:13:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=18
05/23/2022 12:13:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.84 on epoch=18
05/23/2022 12:13:38 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.15133983806679643 on epoch=18
05/23/2022 12:13:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.92 on epoch=19
05/23/2022 12:13:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.78 on epoch=19
05/23/2022 12:13:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=20
05/23/2022 12:13:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=21
05/23/2022 12:13:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.75 on epoch=21
05/23/2022 12:13:54 - INFO - __main__ - Global step 350 Train loss 0.81 Classification-F1 0.12408843229738753 on epoch=21
05/23/2022 12:13:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=22
05/23/2022 12:13:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=23
05/23/2022 12:14:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=23
05/23/2022 12:14:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=24
05/23/2022 12:14:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=24
05/23/2022 12:14:10 - INFO - __main__ - Global step 400 Train loss 0.79 Classification-F1 0.28277629412840677 on epoch=24
05/23/2022 12:14:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.73 on epoch=25
05/23/2022 12:14:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.74 on epoch=26
05/23/2022 12:14:18 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=26
05/23/2022 12:14:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.79 on epoch=27
05/23/2022 12:14:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.76 on epoch=28
05/23/2022 12:14:26 - INFO - __main__ - Global step 450 Train loss 0.75 Classification-F1 0.1893725664532222 on epoch=28
05/23/2022 12:14:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.77 on epoch=28
05/23/2022 12:14:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.76 on epoch=29
05/23/2022 12:14:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=29
05/23/2022 12:14:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=30
05/23/2022 12:14:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.75 on epoch=31
05/23/2022 12:14:42 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.5410598096173288 on epoch=31
05/23/2022 12:14:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3378308703235663 -> 0.5410598096173288 on epoch=31, global_step=500
05/23/2022 12:14:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=31
05/23/2022 12:14:47 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=32
05/23/2022 12:14:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.73 on epoch=33
05/23/2022 12:14:52 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=33
05/23/2022 12:14:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.78 on epoch=34
05/23/2022 12:14:58 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.3406593406593406 on epoch=34
05/23/2022 12:15:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.75 on epoch=34
05/23/2022 12:15:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=35
05/23/2022 12:15:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.69 on epoch=36
05/23/2022 12:15:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.66 on epoch=36
05/23/2022 12:15:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=37
05/23/2022 12:15:14 - INFO - __main__ - Global step 600 Train loss 0.69 Classification-F1 0.47042125411715313 on epoch=37
05/23/2022 12:15:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=38
05/23/2022 12:15:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.66 on epoch=38
05/23/2022 12:15:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.64 on epoch=39
05/23/2022 12:15:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.58 on epoch=39
05/23/2022 12:15:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.59 on epoch=40
05/23/2022 12:15:30 - INFO - __main__ - Global step 650 Train loss 0.61 Classification-F1 0.63305605404525 on epoch=40
05/23/2022 12:15:30 - INFO - __main__ - Saving model with best Classification-F1: 0.5410598096173288 -> 0.63305605404525 on epoch=40, global_step=650
05/23/2022 12:15:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.62 on epoch=41
05/23/2022 12:15:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.55 on epoch=41
05/23/2022 12:15:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=42
05/23/2022 12:15:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=43
05/23/2022 12:15:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=43
05/23/2022 12:15:45 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.402490435144273 on epoch=43
05/23/2022 12:15:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=44
05/23/2022 12:15:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=44
05/23/2022 12:15:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=45
05/23/2022 12:15:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=46
05/23/2022 12:15:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=46
05/23/2022 12:16:01 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.4772316488061128 on epoch=46
05/23/2022 12:16:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=47
05/23/2022 12:16:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=48
05/23/2022 12:16:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=48
05/23/2022 12:16:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=49
05/23/2022 12:16:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=49
05/23/2022 12:16:17 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.665857294334738 on epoch=49
05/23/2022 12:16:17 - INFO - __main__ - Saving model with best Classification-F1: 0.63305605404525 -> 0.665857294334738 on epoch=49, global_step=800
05/23/2022 12:16:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=50
05/23/2022 12:16:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=51
05/23/2022 12:16:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=51
05/23/2022 12:16:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=52
05/23/2022 12:16:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
05/23/2022 12:16:34 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.7042715342715826 on epoch=53
05/23/2022 12:16:34 - INFO - __main__ - Saving model with best Classification-F1: 0.665857294334738 -> 0.7042715342715826 on epoch=53, global_step=850
05/23/2022 12:16:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=53
05/23/2022 12:16:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
05/23/2022 12:16:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=54
05/23/2022 12:16:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=55
05/23/2022 12:16:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
05/23/2022 12:16:49 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.6691714643637721 on epoch=56
05/23/2022 12:16:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=56
05/23/2022 12:16:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
05/23/2022 12:16:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
05/23/2022 12:17:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=58
05/23/2022 12:17:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=59
05/23/2022 12:17:05 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.7490662931839402 on epoch=59
05/23/2022 12:17:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7042715342715826 -> 0.7490662931839402 on epoch=59, global_step=950
05/23/2022 12:17:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=59
05/23/2022 12:17:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
05/23/2022 12:17:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=61
05/23/2022 12:17:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=61
05/23/2022 12:17:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/23/2022 12:17:21 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.7279932024305318 on epoch=62
05/23/2022 12:17:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
05/23/2022 12:17:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=63
05/23/2022 12:17:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=64
05/23/2022 12:17:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=64
05/23/2022 12:17:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
05/23/2022 12:17:38 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7176726781989939 on epoch=65
05/23/2022 12:17:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
05/23/2022 12:17:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.32 on epoch=66
05/23/2022 12:17:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
05/23/2022 12:17:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=68
05/23/2022 12:17:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=68
05/23/2022 12:17:54 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.7062494801449211 on epoch=68
05/23/2022 12:17:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=69
05/23/2022 12:17:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=69
05/23/2022 12:18:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/23/2022 12:18:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
05/23/2022 12:18:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=71
05/23/2022 12:18:10 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.7268039953857087 on epoch=71
05/23/2022 12:18:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=72
05/23/2022 12:18:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=73
05/23/2022 12:18:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=73
05/23/2022 12:18:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/23/2022 12:18:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=74
05/23/2022 12:18:26 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.6894686907020873 on epoch=74
05/23/2022 12:18:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/23/2022 12:18:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=76
05/23/2022 12:18:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
05/23/2022 12:18:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
05/23/2022 12:18:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
05/23/2022 12:18:42 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7201229271415633 on epoch=78
05/23/2022 12:18:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=78
05/23/2022 12:18:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=79
05/23/2022 12:18:49 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=79
05/23/2022 12:18:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/23/2022 12:18:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=81
05/23/2022 12:18:58 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.7403328229279791 on epoch=81
05/23/2022 12:19:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=81
05/23/2022 12:19:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/23/2022 12:19:05 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=83
05/23/2022 12:19:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/23/2022 12:19:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=84
05/23/2022 12:19:14 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7580641177010308 on epoch=84
05/23/2022 12:19:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7490662931839402 -> 0.7580641177010308 on epoch=84, global_step=1350
05/23/2022 12:19:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/23/2022 12:19:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=85
05/23/2022 12:19:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=86
05/23/2022 12:19:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
05/23/2022 12:19:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=87
05/23/2022 12:19:30 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.7730410215908273 on epoch=87
05/23/2022 12:19:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7580641177010308 -> 0.7730410215908273 on epoch=87, global_step=1400
05/23/2022 12:19:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/23/2022 12:19:35 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/23/2022 12:19:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=89
05/23/2022 12:19:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=89
05/23/2022 12:19:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=90
05/23/2022 12:19:46 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.786123513971118 on epoch=90
05/23/2022 12:19:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7730410215908273 -> 0.786123513971118 on epoch=90, global_step=1450
05/23/2022 12:19:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=91
05/23/2022 12:19:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=91
05/23/2022 12:19:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=92
05/23/2022 12:19:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/23/2022 12:19:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=93
05/23/2022 12:20:01 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.7843170869444955 on epoch=93
05/23/2022 12:20:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/23/2022 12:20:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=94
05/23/2022 12:20:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=95
05/23/2022 12:20:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.26 on epoch=96
05/23/2022 12:20:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=96
05/23/2022 12:20:17 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.7943426374679792 on epoch=96
05/23/2022 12:20:17 - INFO - __main__ - Saving model with best Classification-F1: 0.786123513971118 -> 0.7943426374679792 on epoch=96, global_step=1550
05/23/2022 12:20:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=97
05/23/2022 12:20:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=98
05/23/2022 12:20:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=98
05/23/2022 12:20:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/23/2022 12:20:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/23/2022 12:20:34 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.7843714649083777 on epoch=99
05/23/2022 12:20:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=100
05/23/2022 12:20:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=101
05/23/2022 12:20:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
05/23/2022 12:20:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
05/23/2022 12:20:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/23/2022 12:20:50 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.7355154832682923 on epoch=103
05/23/2022 12:20:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/23/2022 12:20:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
05/23/2022 12:20:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=104
05/23/2022 12:21:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=105
05/23/2022 12:21:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
05/23/2022 12:21:06 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.6622731965414892 on epoch=106
05/23/2022 12:21:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
05/23/2022 12:21:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
05/23/2022 12:21:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/23/2022 12:21:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=108
05/23/2022 12:21:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=109
05/23/2022 12:21:22 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.8040392706479718 on epoch=109
05/23/2022 12:21:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7943426374679792 -> 0.8040392706479718 on epoch=109, global_step=1750
05/23/2022 12:21:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
05/23/2022 12:21:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=110
05/23/2022 12:21:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/23/2022 12:21:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
05/23/2022 12:21:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=112
05/23/2022 12:21:38 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.787990942934386 on epoch=112
05/23/2022 12:21:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
05/23/2022 12:21:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
05/23/2022 12:21:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/23/2022 12:21:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=114
05/23/2022 12:21:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=115
05/23/2022 12:21:54 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.7907144320820019 on epoch=115
05/23/2022 12:21:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=116
05/23/2022 12:21:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=116
05/23/2022 12:22:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=117
05/23/2022 12:22:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=118
05/23/2022 12:22:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
05/23/2022 12:22:10 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.7533591294210307 on epoch=118
05/23/2022 12:22:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=119
05/23/2022 12:22:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.05 on epoch=119
05/23/2022 12:22:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=120
05/23/2022 12:22:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=121
05/23/2022 12:22:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=121
05/23/2022 12:22:26 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.8155286034156348 on epoch=121
05/23/2022 12:22:26 - INFO - __main__ - Saving model with best Classification-F1: 0.8040392706479718 -> 0.8155286034156348 on epoch=121, global_step=1950
05/23/2022 12:22:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=122
05/23/2022 12:22:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=123
05/23/2022 12:22:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=123
05/23/2022 12:22:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=124
05/23/2022 12:22:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=124
05/23/2022 12:22:42 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.8145879882728689 on epoch=124
05/23/2022 12:22:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/23/2022 12:22:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=126
05/23/2022 12:22:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=126
05/23/2022 12:22:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=127
05/23/2022 12:22:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
05/23/2022 12:22:58 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.7962070649884949 on epoch=128
05/23/2022 12:23:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/23/2022 12:23:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=129
05/23/2022 12:23:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/23/2022 12:23:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=130
05/23/2022 12:23:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/23/2022 12:23:15 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7920356642591923 on epoch=131
05/23/2022 12:23:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=131
05/23/2022 12:23:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=132
05/23/2022 12:23:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=133
05/23/2022 12:23:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=133
05/23/2022 12:23:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/23/2022 12:23:31 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7829235700646693 on epoch=134
05/23/2022 12:23:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/23/2022 12:23:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=135
05/23/2022 12:23:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=136
05/23/2022 12:23:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/23/2022 12:23:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/23/2022 12:23:47 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7932795755743907 on epoch=137
05/23/2022 12:23:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/23/2022 12:23:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=138
05/23/2022 12:23:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=139
05/23/2022 12:23:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=139
05/23/2022 12:23:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/23/2022 12:24:03 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.7696550341661639 on epoch=140
05/23/2022 12:24:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/23/2022 12:24:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/23/2022 12:24:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=142
05/23/2022 12:24:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/23/2022 12:24:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=143
05/23/2022 12:24:19 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.7934658795123912 on epoch=143
05/23/2022 12:24:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/23/2022 12:24:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=144
05/23/2022 12:24:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=145
05/23/2022 12:24:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/23/2022 12:24:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/23/2022 12:24:35 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.8044237846683677 on epoch=146
05/23/2022 12:24:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=147
05/23/2022 12:24:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=148
05/23/2022 12:24:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
05/23/2022 12:24:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=149
05/23/2022 12:24:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=149
05/23/2022 12:24:51 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.805165534694907 on epoch=149
05/23/2022 12:24:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/23/2022 12:24:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
05/23/2022 12:24:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/23/2022 12:25:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
05/23/2022 12:25:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/23/2022 12:25:07 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.7926255958929227 on epoch=153
05/23/2022 12:25:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=153
05/23/2022 12:25:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=154
05/23/2022 12:25:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=154
05/23/2022 12:25:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/23/2022 12:25:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=156
05/23/2022 12:25:23 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7991353212031447 on epoch=156
05/23/2022 12:25:25 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=156
05/23/2022 12:25:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=157
05/23/2022 12:25:30 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=158
05/23/2022 12:25:33 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=158
05/23/2022 12:25:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/23/2022 12:25:39 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.8098914828329512 on epoch=159
05/23/2022 12:25:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
05/23/2022 12:25:44 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=160
05/23/2022 12:25:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/23/2022 12:25:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
05/23/2022 12:25:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/23/2022 12:25:55 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.8273613396610234 on epoch=162
05/23/2022 12:25:55 - INFO - __main__ - Saving model with best Classification-F1: 0.8155286034156348 -> 0.8273613396610234 on epoch=162, global_step=2600
05/23/2022 12:25:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=163
05/23/2022 12:26:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=163
05/23/2022 12:26:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/23/2022 12:26:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=164
05/23/2022 12:26:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=165
05/23/2022 12:26:11 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7912362497475606 on epoch=165
05/23/2022 12:26:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
05/23/2022 12:26:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/23/2022 12:26:18 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
05/23/2022 12:26:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/23/2022 12:26:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=168
05/23/2022 12:26:26 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.7828712139089797 on epoch=168
05/23/2022 12:26:29 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=169
05/23/2022 12:26:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/23/2022 12:26:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
05/23/2022 12:26:36 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/23/2022 12:26:39 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/23/2022 12:26:42 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.8042876002158496 on epoch=171
05/23/2022 12:26:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
05/23/2022 12:26:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/23/2022 12:26:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/23/2022 12:26:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=174
05/23/2022 12:26:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/23/2022 12:26:58 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8113319814282574 on epoch=174
05/23/2022 12:27:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/23/2022 12:27:03 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/23/2022 12:27:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/23/2022 12:27:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/23/2022 12:27:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
05/23/2022 12:27:14 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7938494717227619 on epoch=178
05/23/2022 12:27:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=178
05/23/2022 12:27:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
05/23/2022 12:27:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/23/2022 12:27:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/23/2022 12:27:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/23/2022 12:27:30 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.7781599117316919 on epoch=181
05/23/2022 12:27:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
05/23/2022 12:27:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/23/2022 12:27:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/23/2022 12:27:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
05/23/2022 12:27:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 12:27:46 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.8132669619387343 on epoch=184
05/23/2022 12:27:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/23/2022 12:27:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/23/2022 12:27:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/23/2022 12:27:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/23/2022 12:27:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
05/23/2022 12:27:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:27:59 - INFO - __main__ - Printing 3 examples
05/23/2022 12:27:59 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 12:27:59 - INFO - __main__ - ['others']
05/23/2022 12:27:59 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 12:27:59 - INFO - __main__ - ['others']
05/23/2022 12:27:59 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 12:27:59 - INFO - __main__ - ['others']
05/23/2022 12:27:59 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:27:59 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:28:00 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:28:00 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:28:00 - INFO - __main__ - Printing 3 examples
05/23/2022 12:28:00 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 12:28:00 - INFO - __main__ - ['others']
05/23/2022 12:28:00 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 12:28:00 - INFO - __main__ - ['others']
05/23/2022 12:28:00 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 12:28:00 - INFO - __main__ - ['others']
05/23/2022 12:28:00 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:28:00 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:28:00 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:28:02 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7988751365648898 on epoch=187
05/23/2022 12:28:02 - INFO - __main__ - save last model!
05/23/2022 12:28:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 12:28:02 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 12:28:02 - INFO - __main__ - Printing 3 examples
05/23/2022 12:28:02 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 12:28:02 - INFO - __main__ - ['others']
05/23/2022 12:28:02 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 12:28:02 - INFO - __main__ - ['others']
05/23/2022 12:28:02 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 12:28:02 - INFO - __main__ - ['others']
05/23/2022 12:28:02 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:28:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:28:09 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 12:28:18 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:28:18 - INFO - __main__ - task name: emo
05/23/2022 12:28:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:28:19 - INFO - __main__ - Starting training!
05/23/2022 12:29:25 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_13_0.3_8_predictions.txt
05/23/2022 12:29:25 - INFO - __main__ - Classification-F1 on test data: 0.5706
05/23/2022 12:29:26 - INFO - __main__ - prefix=emo_64_13, lr=0.3, bsz=8, dev_performance=0.8273613396610234, test_performance=0.5705965483511881
05/23/2022 12:29:26 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.2, bsz=8 ...
05/23/2022 12:29:26 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:29:26 - INFO - __main__ - Printing 3 examples
05/23/2022 12:29:26 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/23/2022 12:29:26 - INFO - __main__ - ['others']
05/23/2022 12:29:26 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/23/2022 12:29:26 - INFO - __main__ - ['others']
05/23/2022 12:29:26 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/23/2022 12:29:26 - INFO - __main__ - ['others']
05/23/2022 12:29:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:29:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:29:27 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:29:27 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:29:27 - INFO - __main__ - Printing 3 examples
05/23/2022 12:29:27 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/23/2022 12:29:27 - INFO - __main__ - ['others']
05/23/2022 12:29:27 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/23/2022 12:29:27 - INFO - __main__ - ['others']
05/23/2022 12:29:27 - INFO - __main__ -  [emo] why because you don't want to i want to
05/23/2022 12:29:27 - INFO - __main__ - ['others']
05/23/2022 12:29:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:29:27 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:29:27 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:29:43 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:29:43 - INFO - __main__ - task name: emo
05/23/2022 12:29:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:29:44 - INFO - __main__ - Starting training!
05/23/2022 12:29:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.56 on epoch=0
05/23/2022 12:29:49 - INFO - __main__ - Step 20 Global step 20 Train loss 5.51 on epoch=1
05/23/2022 12:29:52 - INFO - __main__ - Step 30 Global step 30 Train loss 3.33 on epoch=1
05/23/2022 12:29:54 - INFO - __main__ - Step 40 Global step 40 Train loss 2.19 on epoch=2
05/23/2022 12:29:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.55 on epoch=3
05/23/2022 12:30:00 - INFO - __main__ - Global step 50 Train loss 4.03 Classification-F1 0.11734888019134593 on epoch=3
05/23/2022 12:30:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11734888019134593 on epoch=3, global_step=50
05/23/2022 12:30:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.23 on epoch=3
05/23/2022 12:30:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.19 on epoch=4
05/23/2022 12:30:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.15 on epoch=4
05/23/2022 12:30:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=5
05/23/2022 12:30:12 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=6
05/23/2022 12:30:16 - INFO - __main__ - Global step 100 Train loss 1.12 Classification-F1 0.13063186813186814 on epoch=6
05/23/2022 12:30:16 - INFO - __main__ - Saving model with best Classification-F1: 0.11734888019134593 -> 0.13063186813186814 on epoch=6, global_step=100
05/23/2022 12:30:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=6
05/23/2022 12:30:21 - INFO - __main__ - Step 120 Global step 120 Train loss 1.05 on epoch=7
05/23/2022 12:30:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.95 on epoch=8
05/23/2022 12:30:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.99 on epoch=8
05/23/2022 12:30:28 - INFO - __main__ - Step 150 Global step 150 Train loss 1.03 on epoch=9
05/23/2022 12:30:32 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.17191049522944118 on epoch=9
05/23/2022 12:30:32 - INFO - __main__ - Saving model with best Classification-F1: 0.13063186813186814 -> 0.17191049522944118 on epoch=9, global_step=150
05/23/2022 12:30:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=9
05/23/2022 12:30:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=10
05/23/2022 12:30:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/23/2022 12:30:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.91 on epoch=11
05/23/2022 12:30:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=12
05/23/2022 12:30:47 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.2680521178655507 on epoch=12
05/23/2022 12:30:47 - INFO - __main__ - Saving model with best Classification-F1: 0.17191049522944118 -> 0.2680521178655507 on epoch=12, global_step=200
05/23/2022 12:30:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=13
05/23/2022 12:30:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=13
05/23/2022 12:30:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=14
05/23/2022 12:30:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.97 on epoch=14
05/23/2022 12:31:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=15
05/23/2022 12:31:03 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.18054387646634526 on epoch=15
05/23/2022 12:31:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.92 on epoch=16
05/23/2022 12:31:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/23/2022 12:31:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=17
05/23/2022 12:31:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=18
05/23/2022 12:31:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=18
05/23/2022 12:31:19 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.10031347962382445 on epoch=18
05/23/2022 12:31:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.90 on epoch=19
05/23/2022 12:31:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.87 on epoch=19
05/23/2022 12:31:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=20
05/23/2022 12:31:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=21
05/23/2022 12:31:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.87 on epoch=21
05/23/2022 12:31:34 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.14058841891680973 on epoch=21
05/23/2022 12:31:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.87 on epoch=22
05/23/2022 12:31:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=23
05/23/2022 12:31:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.85 on epoch=23
05/23/2022 12:31:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=24
05/23/2022 12:31:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=24
05/23/2022 12:31:50 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.218919158224199 on epoch=24
05/23/2022 12:31:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.81 on epoch=25
05/23/2022 12:31:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=26
05/23/2022 12:31:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.79 on epoch=26
05/23/2022 12:32:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=27
05/23/2022 12:32:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.88 on epoch=28
05/23/2022 12:32:06 - INFO - __main__ - Global step 450 Train loss 0.82 Classification-F1 0.21281021732226307 on epoch=28
05/23/2022 12:32:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=28
05/23/2022 12:32:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=29
05/23/2022 12:32:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.83 on epoch=29
05/23/2022 12:32:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.81 on epoch=30
05/23/2022 12:32:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.83 on epoch=31
05/23/2022 12:32:22 - INFO - __main__ - Global step 500 Train loss 0.83 Classification-F1 0.3781272088391595 on epoch=31
05/23/2022 12:32:22 - INFO - __main__ - Saving model with best Classification-F1: 0.2680521178655507 -> 0.3781272088391595 on epoch=31, global_step=500
05/23/2022 12:32:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=31
05/23/2022 12:32:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=32
05/23/2022 12:32:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.78 on epoch=33
05/23/2022 12:32:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=33
05/23/2022 12:32:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.79 on epoch=34
05/23/2022 12:32:38 - INFO - __main__ - Global step 550 Train loss 0.79 Classification-F1 0.28713924963924964 on epoch=34
05/23/2022 12:32:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=34
05/23/2022 12:32:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.72 on epoch=35
05/23/2022 12:32:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.85 on epoch=36
05/23/2022 12:32:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.82 on epoch=36
05/23/2022 12:32:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=37
05/23/2022 12:32:54 - INFO - __main__ - Global step 600 Train loss 0.80 Classification-F1 0.3328642774130729 on epoch=37
05/23/2022 12:32:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.78 on epoch=38
05/23/2022 12:32:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.77 on epoch=38
05/23/2022 12:33:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.76 on epoch=39
05/23/2022 12:33:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.72 on epoch=39
05/23/2022 12:33:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.77 on epoch=40
05/23/2022 12:33:10 - INFO - __main__ - Global step 650 Train loss 0.76 Classification-F1 0.33198254483570144 on epoch=40
05/23/2022 12:33:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=41
05/23/2022 12:33:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.72 on epoch=41
05/23/2022 12:33:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.77 on epoch=42
05/23/2022 12:33:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.76 on epoch=43
05/23/2022 12:33:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.73 on epoch=43
05/23/2022 12:33:26 - INFO - __main__ - Global step 700 Train loss 0.75 Classification-F1 0.4021197921463879 on epoch=43
05/23/2022 12:33:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3781272088391595 -> 0.4021197921463879 on epoch=43, global_step=700
05/23/2022 12:33:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.79 on epoch=44
05/23/2022 12:33:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.75 on epoch=44
05/23/2022 12:33:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.70 on epoch=45
05/23/2022 12:33:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.68 on epoch=46
05/23/2022 12:33:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.70 on epoch=46
05/23/2022 12:33:41 - INFO - __main__ - Global step 750 Train loss 0.72 Classification-F1 0.2973106522561924 on epoch=46
05/23/2022 12:33:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.74 on epoch=47
05/23/2022 12:33:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.68 on epoch=48
05/23/2022 12:33:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.69 on epoch=48
05/23/2022 12:33:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.76 on epoch=49
05/23/2022 12:33:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.64 on epoch=49
05/23/2022 12:33:57 - INFO - __main__ - Global step 800 Train loss 0.70 Classification-F1 0.30907811647111255 on epoch=49
05/23/2022 12:34:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.74 on epoch=50
05/23/2022 12:34:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=51
05/23/2022 12:34:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.75 on epoch=51
05/23/2022 12:34:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.60 on epoch=52
05/23/2022 12:34:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.67 on epoch=53
05/23/2022 12:34:13 - INFO - __main__ - Global step 850 Train loss 0.69 Classification-F1 0.45233482036608563 on epoch=53
05/23/2022 12:34:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4021197921463879 -> 0.45233482036608563 on epoch=53, global_step=850
05/23/2022 12:34:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.61 on epoch=53
05/23/2022 12:34:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.70 on epoch=54
05/23/2022 12:34:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=54
05/23/2022 12:34:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=55
05/23/2022 12:34:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=56
05/23/2022 12:34:29 - INFO - __main__ - Global step 900 Train loss 0.64 Classification-F1 0.5585576554022097 on epoch=56
05/23/2022 12:34:29 - INFO - __main__ - Saving model with best Classification-F1: 0.45233482036608563 -> 0.5585576554022097 on epoch=56, global_step=900
05/23/2022 12:34:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=56
05/23/2022 12:34:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.67 on epoch=57
05/23/2022 12:34:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.53 on epoch=58
05/23/2022 12:34:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=58
05/23/2022 12:34:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.65 on epoch=59
05/23/2022 12:34:45 - INFO - __main__ - Global step 950 Train loss 0.59 Classification-F1 0.5275138174907233 on epoch=59
05/23/2022 12:34:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.61 on epoch=59
05/23/2022 12:34:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.59 on epoch=60
05/23/2022 12:34:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=61
05/23/2022 12:34:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.54 on epoch=61
05/23/2022 12:34:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=62
05/23/2022 12:35:01 - INFO - __main__ - Global step 1000 Train loss 0.56 Classification-F1 0.6467074592074592 on epoch=62
05/23/2022 12:35:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5585576554022097 -> 0.6467074592074592 on epoch=62, global_step=1000
05/23/2022 12:35:03 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.64 on epoch=63
05/23/2022 12:35:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=63
05/23/2022 12:35:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.63 on epoch=64
05/23/2022 12:35:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=64
05/23/2022 12:35:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=65
05/23/2022 12:35:17 - INFO - __main__ - Global step 1050 Train loss 0.58 Classification-F1 0.6192798983701526 on epoch=65
05/23/2022 12:35:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.54 on epoch=66
05/23/2022 12:35:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=66
05/23/2022 12:35:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.58 on epoch=67
05/23/2022 12:35:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=68
05/23/2022 12:35:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=68
05/23/2022 12:35:33 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.5887431834068048 on epoch=68
05/23/2022 12:35:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=69
05/23/2022 12:35:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=69
05/23/2022 12:35:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=70
05/23/2022 12:35:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=71
05/23/2022 12:35:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=71
05/23/2022 12:35:49 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.5727774287954266 on epoch=71
05/23/2022 12:35:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=72
05/23/2022 12:35:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=73
05/23/2022 12:35:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.54 on epoch=73
05/23/2022 12:35:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=74
05/23/2022 12:36:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=74
05/23/2022 12:36:04 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.6885340269486611 on epoch=74
05/23/2022 12:36:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6467074592074592 -> 0.6885340269486611 on epoch=74, global_step=1200
05/23/2022 12:36:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=75
05/23/2022 12:36:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=76
05/23/2022 12:36:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=76
05/23/2022 12:36:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=77
05/23/2022 12:36:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=78
05/23/2022 12:36:20 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.5704473878052729 on epoch=78
05/23/2022 12:36:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=78
05/23/2022 12:36:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=79
05/23/2022 12:36:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=79
05/23/2022 12:36:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=80
05/23/2022 12:36:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=81
05/23/2022 12:36:36 - INFO - __main__ - Global step 1300 Train loss 0.45 Classification-F1 0.6812071468903768 on epoch=81
05/23/2022 12:36:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=81
05/23/2022 12:36:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=82
05/23/2022 12:36:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=83
05/23/2022 12:36:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
05/23/2022 12:36:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.50 on epoch=84
05/23/2022 12:36:52 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.7153376443692427 on epoch=84
05/23/2022 12:36:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6885340269486611 -> 0.7153376443692427 on epoch=84, global_step=1350
05/23/2022 12:36:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=84
05/23/2022 12:36:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=85
05/23/2022 12:37:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=86
05/23/2022 12:37:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=86
05/23/2022 12:37:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=87
05/23/2022 12:37:08 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.6885939896513265 on epoch=87
05/23/2022 12:37:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=88
05/23/2022 12:37:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=88
05/23/2022 12:37:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=89
05/23/2022 12:37:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=89
05/23/2022 12:37:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=90
05/23/2022 12:37:25 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.6994758724353677 on epoch=90
05/23/2022 12:37:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.37 on epoch=91
05/23/2022 12:37:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=91
05/23/2022 12:37:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=92
05/23/2022 12:37:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=93
05/23/2022 12:37:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/23/2022 12:37:41 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.6684956569466252 on epoch=93
05/23/2022 12:37:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=94
05/23/2022 12:37:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=94
05/23/2022 12:37:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=95
05/23/2022 12:37:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=96
05/23/2022 12:37:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=96
05/23/2022 12:37:57 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.6366952106823462 on epoch=96
05/23/2022 12:38:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=97
05/23/2022 12:38:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=98
05/23/2022 12:38:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
05/23/2022 12:38:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=99
05/23/2022 12:38:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
05/23/2022 12:38:13 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.6937111807861287 on epoch=99
05/23/2022 12:38:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
05/23/2022 12:38:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.33 on epoch=101
05/23/2022 12:38:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=101
05/23/2022 12:38:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=102
05/23/2022 12:38:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=103
05/23/2022 12:38:29 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.6202923056449277 on epoch=103
05/23/2022 12:38:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.34 on epoch=103
05/23/2022 12:38:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=104
05/23/2022 12:38:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=104
05/23/2022 12:38:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=105
05/23/2022 12:38:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
05/23/2022 12:38:45 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.7122351342287587 on epoch=106
05/23/2022 12:38:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
05/23/2022 12:38:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=107
05/23/2022 12:38:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=108
05/23/2022 12:38:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.36 on epoch=108
05/23/2022 12:38:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=109
05/23/2022 12:39:01 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.7188002921266797 on epoch=109
05/23/2022 12:39:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7153376443692427 -> 0.7188002921266797 on epoch=109, global_step=1750
05/23/2022 12:39:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
05/23/2022 12:39:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=110
05/23/2022 12:39:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=111
05/23/2022 12:39:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
05/23/2022 12:39:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=112
05/23/2022 12:39:18 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.7210165962011738 on epoch=112
05/23/2022 12:39:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7188002921266797 -> 0.7210165962011738 on epoch=112, global_step=1800
05/23/2022 12:39:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
05/23/2022 12:39:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=113
05/23/2022 12:39:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.30 on epoch=114
05/23/2022 12:39:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=114
05/23/2022 12:39:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=115
05/23/2022 12:39:34 - INFO - __main__ - Global step 1850 Train loss 0.28 Classification-F1 0.7331633055667254 on epoch=115
05/23/2022 12:39:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7210165962011738 -> 0.7331633055667254 on epoch=115, global_step=1850
05/23/2022 12:39:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.29 on epoch=116
05/23/2022 12:39:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=116
05/23/2022 12:39:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=117
05/23/2022 12:39:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/23/2022 12:39:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=118
05/23/2022 12:39:50 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.7372536713685038 on epoch=118
05/23/2022 12:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7331633055667254 -> 0.7372536713685038 on epoch=118, global_step=1900
05/23/2022 12:39:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=119
05/23/2022 12:39:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=119
05/23/2022 12:39:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=120
05/23/2022 12:40:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.26 on epoch=121
05/23/2022 12:40:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=121
05/23/2022 12:40:06 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.6829047137404252 on epoch=121
05/23/2022 12:40:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=122
05/23/2022 12:40:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=123
05/23/2022 12:40:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=123
05/23/2022 12:40:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=124
05/23/2022 12:40:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/23/2022 12:40:22 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.7301368750962669 on epoch=124
05/23/2022 12:40:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=125
05/23/2022 12:40:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=126
05/23/2022 12:40:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=126
05/23/2022 12:40:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=127
05/23/2022 12:40:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=128
05/23/2022 12:40:38 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7357746075519374 on epoch=128
05/23/2022 12:40:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.23 on epoch=128
05/23/2022 12:40:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=129
05/23/2022 12:40:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=129
05/23/2022 12:40:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.35 on epoch=130
05/23/2022 12:40:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=131
05/23/2022 12:40:54 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.6954257941672649 on epoch=131
05/23/2022 12:40:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/23/2022 12:41:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=132
05/23/2022 12:41:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
05/23/2022 12:41:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
05/23/2022 12:41:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/23/2022 12:41:11 - INFO - __main__ - Global step 2150 Train loss 0.22 Classification-F1 0.7092590012189535 on epoch=134
05/23/2022 12:41:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=134
05/23/2022 12:41:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
05/23/2022 12:41:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/23/2022 12:41:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.24 on epoch=136
05/23/2022 12:41:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.26 on epoch=137
05/23/2022 12:41:27 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.7203395329092911 on epoch=137
05/23/2022 12:41:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
05/23/2022 12:41:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=138
05/23/2022 12:41:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=139
05/23/2022 12:41:37 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=139
05/23/2022 12:41:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
05/23/2022 12:41:43 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7148074817306808 on epoch=140
05/23/2022 12:41:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=141
05/23/2022 12:41:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=141
05/23/2022 12:41:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=142
05/23/2022 12:41:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=143
05/23/2022 12:41:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
05/23/2022 12:41:59 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.7203997935255035 on epoch=143
05/23/2022 12:42:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.29 on epoch=144
05/23/2022 12:42:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
05/23/2022 12:42:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.24 on epoch=145
05/23/2022 12:42:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/23/2022 12:42:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
05/23/2022 12:42:15 - INFO - __main__ - Global step 2350 Train loss 0.22 Classification-F1 0.7487014532455168 on epoch=146
05/23/2022 12:42:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7372536713685038 -> 0.7487014532455168 on epoch=146, global_step=2350
05/23/2022 12:42:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=147
05/23/2022 12:42:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
05/23/2022 12:42:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.23 on epoch=148
05/23/2022 12:42:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
05/23/2022 12:42:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/23/2022 12:42:32 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.7200568185003897 on epoch=149
05/23/2022 12:42:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.23 on epoch=150
05/23/2022 12:42:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=151
05/23/2022 12:42:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
05/23/2022 12:42:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
05/23/2022 12:42:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=153
05/23/2022 12:42:48 - INFO - __main__ - Global step 2450 Train loss 0.20 Classification-F1 0.718468579001885 on epoch=153
05/23/2022 12:42:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/23/2022 12:42:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=154
05/23/2022 12:42:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
05/23/2022 12:42:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/23/2022 12:43:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.27 on epoch=156
05/23/2022 12:43:04 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.7095494511303366 on epoch=156
05/23/2022 12:43:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=156
05/23/2022 12:43:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/23/2022 12:43:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
05/23/2022 12:43:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
05/23/2022 12:43:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
05/23/2022 12:43:20 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.754869110966672 on epoch=159
05/23/2022 12:43:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7487014532455168 -> 0.754869110966672 on epoch=159, global_step=2550
05/23/2022 12:43:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/23/2022 12:43:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=160
05/23/2022 12:43:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/23/2022 12:43:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
05/23/2022 12:43:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.23 on epoch=162
05/23/2022 12:43:36 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.7372465935526753 on epoch=162
05/23/2022 12:43:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=163
05/23/2022 12:43:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=163
05/23/2022 12:43:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=164
05/23/2022 12:43:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/23/2022 12:43:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=165
05/23/2022 12:43:52 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.7481358950510808 on epoch=165
05/23/2022 12:43:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
05/23/2022 12:43:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/23/2022 12:43:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.28 on epoch=167
05/23/2022 12:44:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/23/2022 12:44:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=168
05/23/2022 12:44:08 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.7314039747506338 on epoch=168
05/23/2022 12:44:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=169
05/23/2022 12:44:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/23/2022 12:44:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
05/23/2022 12:44:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=171
05/23/2022 12:44:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=171
05/23/2022 12:44:24 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.7206678584752269 on epoch=171
05/23/2022 12:44:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/23/2022 12:44:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
05/23/2022 12:44:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=173
05/23/2022 12:44:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/23/2022 12:44:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/23/2022 12:44:40 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.7264077031566818 on epoch=174
05/23/2022 12:44:42 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
05/23/2022 12:44:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=176
05/23/2022 12:44:47 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
05/23/2022 12:44:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=177
05/23/2022 12:44:52 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/23/2022 12:44:56 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.7488213789281823 on epoch=178
05/23/2022 12:44:58 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=178
05/23/2022 12:45:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/23/2022 12:45:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=179
05/23/2022 12:45:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/23/2022 12:45:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
05/23/2022 12:45:12 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.7157269882578926 on epoch=181
05/23/2022 12:45:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=181
05/23/2022 12:45:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.24 on epoch=182
05/23/2022 12:45:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=183
05/23/2022 12:45:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=183
05/23/2022 12:45:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.28 on epoch=184
05/23/2022 12:45:28 - INFO - __main__ - Global step 2950 Train loss 0.19 Classification-F1 0.7255895841163537 on epoch=184
05/23/2022 12:45:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/23/2022 12:45:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
05/23/2022 12:45:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=186
05/23/2022 12:45:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
05/23/2022 12:45:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
05/23/2022 12:45:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:45:41 - INFO - __main__ - Printing 3 examples
05/23/2022 12:45:41 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 12:45:41 - INFO - __main__ - ['sad']
05/23/2022 12:45:41 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 12:45:41 - INFO - __main__ - ['sad']
05/23/2022 12:45:41 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 12:45:41 - INFO - __main__ - ['sad']
05/23/2022 12:45:41 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:45:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:45:42 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:45:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:45:42 - INFO - __main__ - Printing 3 examples
05/23/2022 12:45:42 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 12:45:42 - INFO - __main__ - ['sad']
05/23/2022 12:45:42 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 12:45:42 - INFO - __main__ - ['sad']
05/23/2022 12:45:42 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 12:45:42 - INFO - __main__ - ['sad']
05/23/2022 12:45:42 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:45:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:45:42 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:45:44 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.743413883542775 on epoch=187
05/23/2022 12:45:44 - INFO - __main__ - save last model!
05/23/2022 12:45:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 12:45:44 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 12:45:44 - INFO - __main__ - Printing 3 examples
05/23/2022 12:45:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 12:45:44 - INFO - __main__ - ['others']
05/23/2022 12:45:44 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 12:45:44 - INFO - __main__ - ['others']
05/23/2022 12:45:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 12:45:44 - INFO - __main__ - ['others']
05/23/2022 12:45:44 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:45:46 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:45:51 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 12:45:57 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:45:57 - INFO - __main__ - task name: emo
05/23/2022 12:45:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:45:58 - INFO - __main__ - Starting training!
05/23/2022 12:47:04 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_13_0.2_8_predictions.txt
05/23/2022 12:47:04 - INFO - __main__ - Classification-F1 on test data: 0.4750
05/23/2022 12:47:05 - INFO - __main__ - prefix=emo_64_13, lr=0.2, bsz=8, dev_performance=0.754869110966672, test_performance=0.4749832900948669
05/23/2022 12:47:05 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.5, bsz=8 ...
05/23/2022 12:47:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:47:06 - INFO - __main__ - Printing 3 examples
05/23/2022 12:47:06 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:47:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:47:06 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 12:47:06 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 12:47:06 - INFO - __main__ - Printing 3 examples
05/23/2022 12:47:06 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 12:47:06 - INFO - __main__ - ['sad']
05/23/2022 12:47:06 - INFO - __main__ - Tokenizing Input ...
05/23/2022 12:47:06 - INFO - __main__ - Tokenizing Output ...
05/23/2022 12:47:06 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 12:47:23 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 12:47:23 - INFO - __main__ - task name: emo
05/23/2022 12:47:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 12:47:24 - INFO - __main__ - Starting training!
05/23/2022 12:47:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=0
05/23/2022 12:47:29 - INFO - __main__ - Step 20 Global step 20 Train loss 1.83 on epoch=1
05/23/2022 12:47:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=1
05/23/2022 12:47:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=2
05/23/2022 12:47:37 - INFO - __main__ - Step 50 Global step 50 Train loss 1.05 on epoch=3
05/23/2022 12:47:40 - INFO - __main__ - Global step 50 Train loss 2.23 Classification-F1 0.10492005461833238 on epoch=3
05/23/2022 12:47:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10492005461833238 on epoch=3, global_step=50
05/23/2022 12:47:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=3
05/23/2022 12:47:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=4
05/23/2022 12:47:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=4
05/23/2022 12:47:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=5
05/23/2022 12:47:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=6
05/23/2022 12:47:56 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.14469256233962116 on epoch=6
05/23/2022 12:47:56 - INFO - __main__ - Saving model with best Classification-F1: 0.10492005461833238 -> 0.14469256233962116 on epoch=6, global_step=100
05/23/2022 12:47:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=6
05/23/2022 12:48:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=7
05/23/2022 12:48:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=8
05/23/2022 12:48:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=8
05/23/2022 12:48:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=9
05/23/2022 12:48:12 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 12:48:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=9
05/23/2022 12:48:17 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=10
05/23/2022 12:48:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=11
05/23/2022 12:48:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.92 on epoch=11
05/23/2022 12:48:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=12
05/23/2022 12:48:27 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.216005291005291 on epoch=12
05/23/2022 12:48:27 - INFO - __main__ - Saving model with best Classification-F1: 0.14469256233962116 -> 0.216005291005291 on epoch=12, global_step=200
05/23/2022 12:48:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.81 on epoch=13
05/23/2022 12:48:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=13
05/23/2022 12:48:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=14
05/23/2022 12:48:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=14
05/23/2022 12:48:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=15
05/23/2022 12:48:43 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.3313381280261044 on epoch=15
05/23/2022 12:48:43 - INFO - __main__ - Saving model with best Classification-F1: 0.216005291005291 -> 0.3313381280261044 on epoch=15, global_step=250
05/23/2022 12:48:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.71 on epoch=16
05/23/2022 12:48:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/23/2022 12:48:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.68 on epoch=17
05/23/2022 12:48:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=18
05/23/2022 12:48:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.81 on epoch=18
05/23/2022 12:48:59 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.43765062214777944 on epoch=18
05/23/2022 12:48:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3313381280261044 -> 0.43765062214777944 on epoch=18, global_step=300
05/23/2022 12:49:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.67 on epoch=19
05/23/2022 12:49:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=19
05/23/2022 12:49:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=20
05/23/2022 12:49:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.66 on epoch=21
05/23/2022 12:49:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.73 on epoch=21
05/23/2022 12:49:15 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.3504523174059238 on epoch=21
05/23/2022 12:49:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=22
05/23/2022 12:49:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=23
05/23/2022 12:49:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=23
05/23/2022 12:49:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.67 on epoch=24
05/23/2022 12:49:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=24
05/23/2022 12:49:31 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.31371001444834407 on epoch=24
05/23/2022 12:49:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.65 on epoch=25
05/23/2022 12:49:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
05/23/2022 12:49:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.66 on epoch=26
05/23/2022 12:49:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=27
05/23/2022 12:49:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.63 on epoch=28
05/23/2022 12:49:46 - INFO - __main__ - Global step 450 Train loss 0.61 Classification-F1 0.5687355481543578 on epoch=28
05/23/2022 12:49:46 - INFO - __main__ - Saving model with best Classification-F1: 0.43765062214777944 -> 0.5687355481543578 on epoch=28, global_step=450
05/23/2022 12:49:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=28
05/23/2022 12:49:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=29
05/23/2022 12:49:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.69 on epoch=29
05/23/2022 12:49:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=30
05/23/2022 12:49:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=31
05/23/2022 12:50:02 - INFO - __main__ - Global step 500 Train loss 0.58 Classification-F1 0.43491916993517304 on epoch=31
05/23/2022 12:50:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=31
05/23/2022 12:50:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=32
05/23/2022 12:50:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=33
05/23/2022 12:50:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/23/2022 12:50:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=34
05/23/2022 12:50:18 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.534679678784116 on epoch=34
05/23/2022 12:50:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/23/2022 12:50:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=35
05/23/2022 12:50:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=36
05/23/2022 12:50:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
05/23/2022 12:50:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=37
05/23/2022 12:50:33 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.6432874038569909 on epoch=37
05/23/2022 12:50:33 - INFO - __main__ - Saving model with best Classification-F1: 0.5687355481543578 -> 0.6432874038569909 on epoch=37, global_step=600
05/23/2022 12:50:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=38
05/23/2022 12:50:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=38
05/23/2022 12:50:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/23/2022 12:50:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/23/2022 12:50:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=40
05/23/2022 12:50:49 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.6640722774678632 on epoch=40
05/23/2022 12:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6432874038569909 -> 0.6640722774678632 on epoch=40, global_step=650
05/23/2022 12:50:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/23/2022 12:50:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=41
05/23/2022 12:50:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=42
05/23/2022 12:50:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
05/23/2022 12:51:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=43
05/23/2022 12:51:05 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.7468141868424936 on epoch=43
05/23/2022 12:51:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6640722774678632 -> 0.7468141868424936 on epoch=43, global_step=700
05/23/2022 12:51:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=44
05/23/2022 12:51:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=44
05/23/2022 12:51:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=45
05/23/2022 12:51:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=46
05/23/2022 12:51:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/23/2022 12:51:21 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.705651537286912 on epoch=46
05/23/2022 12:51:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=47
05/23/2022 12:51:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/23/2022 12:51:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/23/2022 12:51:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=49
05/23/2022 12:51:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
05/23/2022 12:51:36 - INFO - __main__ - Global step 800 Train loss 0.27 Classification-F1 0.7784261728749038 on epoch=49
05/23/2022 12:51:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7468141868424936 -> 0.7784261728749038 on epoch=49, global_step=800
05/23/2022 12:51:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/23/2022 12:51:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=51
05/23/2022 12:51:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=51
05/23/2022 12:51:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=52
05/23/2022 12:51:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=53
05/23/2022 12:51:52 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.753448145140869 on epoch=53
05/23/2022 12:51:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/23/2022 12:51:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=54
05/23/2022 12:52:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
05/23/2022 12:52:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/23/2022 12:52:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=56
05/23/2022 12:52:08 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.7723741205399467 on epoch=56
05/23/2022 12:52:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=56
05/23/2022 12:52:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=57
05/23/2022 12:52:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/23/2022 12:52:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=58
05/23/2022 12:52:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=59
05/23/2022 12:52:24 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.738912725022068 on epoch=59
05/23/2022 12:52:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=59
05/23/2022 12:52:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=60
05/23/2022 12:52:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=61
05/23/2022 12:52:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=61
05/23/2022 12:52:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.12 on epoch=62
05/23/2022 12:52:40 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.7793822561641474 on epoch=62
05/23/2022 12:52:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7784261728749038 -> 0.7793822561641474 on epoch=62, global_step=1000
05/23/2022 12:52:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=63
05/23/2022 12:52:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/23/2022 12:52:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=64
05/23/2022 12:52:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/23/2022 12:52:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=65
05/23/2022 12:52:56 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.7376185332926464 on epoch=65
05/23/2022 12:52:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=66
05/23/2022 12:53:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=66
05/23/2022 12:53:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=67
05/23/2022 12:53:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/23/2022 12:53:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/23/2022 12:53:12 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.7255463705012516 on epoch=68
05/23/2022 12:53:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=69
05/23/2022 12:53:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=69
05/23/2022 12:53:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=70
05/23/2022 12:53:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=71
05/23/2022 12:53:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.14 on epoch=71
05/23/2022 12:53:28 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.7630687752932886 on epoch=71
05/23/2022 12:53:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=72
05/23/2022 12:53:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=73
05/23/2022 12:53:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=73
05/23/2022 12:53:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=74
05/23/2022 12:53:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=74
05/23/2022 12:53:44 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.6939807187003081 on epoch=74
05/23/2022 12:53:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=75
05/23/2022 12:53:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=76
05/23/2022 12:53:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=76
05/23/2022 12:53:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=77
05/23/2022 12:53:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=78
05/23/2022 12:53:59 - INFO - __main__ - Global step 1250 Train loss 0.13 Classification-F1 0.7539304578396374 on epoch=78
05/23/2022 12:54:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.11 on epoch=78
05/23/2022 12:54:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=79
05/23/2022 12:54:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=79
05/23/2022 12:54:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=80
05/23/2022 12:54:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=81
05/23/2022 12:54:15 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.7282667780865899 on epoch=81
05/23/2022 12:54:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=81
05/23/2022 12:54:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=82
05/23/2022 12:54:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=83
05/23/2022 12:54:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=83
05/23/2022 12:54:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=84
05/23/2022 12:54:31 - INFO - __main__ - Global step 1350 Train loss 0.11 Classification-F1 0.7249088799890939 on epoch=84
05/23/2022 12:54:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=84
05/23/2022 12:54:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.08 on epoch=85
05/23/2022 12:54:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=86
05/23/2022 12:54:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=86
05/23/2022 12:54:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=87
05/23/2022 12:54:47 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.7629969824323496 on epoch=87
05/23/2022 12:54:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=88
05/23/2022 12:54:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=88
05/23/2022 12:54:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/23/2022 12:54:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=89
05/23/2022 12:54:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=90
05/23/2022 12:55:03 - INFO - __main__ - Global step 1450 Train loss 0.11 Classification-F1 0.7535421374936212 on epoch=90
05/23/2022 12:55:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=91
05/23/2022 12:55:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=91
05/23/2022 12:55:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=92
05/23/2022 12:55:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=93
05/23/2022 12:55:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=93
05/23/2022 12:55:19 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7450772412880657 on epoch=93
05/23/2022 12:55:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=94
05/23/2022 12:55:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=94
05/23/2022 12:55:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=95
05/23/2022 12:55:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=96
05/23/2022 12:55:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=96
05/23/2022 12:55:35 - INFO - __main__ - Global step 1550 Train loss 0.06 Classification-F1 0.7264445365257275 on epoch=96
05/23/2022 12:55:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=97
05/23/2022 12:55:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=98
05/23/2022 12:55:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=98
05/23/2022 12:55:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=99
05/23/2022 12:55:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=99
05/23/2022 12:55:51 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.722665297746979 on epoch=99
05/23/2022 12:55:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=100
05/23/2022 12:55:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=101
05/23/2022 12:55:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
05/23/2022 12:56:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=102
05/23/2022 12:56:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=103
05/23/2022 12:56:06 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.7437492758660642 on epoch=103
05/23/2022 12:56:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=103
05/23/2022 12:56:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=104
05/23/2022 12:56:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=104
05/23/2022 12:56:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=105
05/23/2022 12:56:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=106
05/23/2022 12:56:22 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.7423873261848806 on epoch=106
05/23/2022 12:56:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=106
05/23/2022 12:56:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=107
05/23/2022 12:56:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=108
05/23/2022 12:56:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=108
05/23/2022 12:56:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=109
05/23/2022 12:56:38 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.7641365928316689 on epoch=109
05/23/2022 12:56:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=109
05/23/2022 12:56:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=110
05/23/2022 12:56:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=111
05/23/2022 12:56:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=111
05/23/2022 12:56:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=112
05/23/2022 12:56:54 - INFO - __main__ - Global step 1800 Train loss 0.06 Classification-F1 0.7435996428748426 on epoch=112
05/23/2022 12:56:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=113
05/23/2022 12:56:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=113
05/23/2022 12:57:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=114
05/23/2022 12:57:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/23/2022 12:57:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=115
05/23/2022 12:57:09 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7610908735851184 on epoch=115
05/23/2022 12:57:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=116
05/23/2022 12:57:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=116
05/23/2022 12:57:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=117
05/23/2022 12:57:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=118
05/23/2022 12:57:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=118
05/23/2022 12:57:25 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7800349134760278 on epoch=118
05/23/2022 12:57:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7793822561641474 -> 0.7800349134760278 on epoch=118, global_step=1900
05/23/2022 12:57:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=119
05/23/2022 12:57:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=119
05/23/2022 12:57:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=120
05/23/2022 12:57:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=121
05/23/2022 12:57:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=121
05/23/2022 12:57:42 - INFO - __main__ - Global step 1950 Train loss 0.05 Classification-F1 0.7729493821269928 on epoch=121
05/23/2022 12:57:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=122
05/23/2022 12:57:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
05/23/2022 12:57:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=123
05/23/2022 12:57:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=124
05/23/2022 12:57:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=124
05/23/2022 12:57:58 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7544695660020329 on epoch=124
05/23/2022 12:58:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.02 on epoch=125
05/23/2022 12:58:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=126
05/23/2022 12:58:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=126
05/23/2022 12:58:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=127
05/23/2022 12:58:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=128
05/23/2022 12:58:14 - INFO - __main__ - Global step 2050 Train loss 0.02 Classification-F1 0.772693767333236 on epoch=128
05/23/2022 12:58:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.04 on epoch=128
05/23/2022 12:58:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=129
05/23/2022 12:58:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=129
05/23/2022 12:58:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=130
05/23/2022 12:58:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/23/2022 12:58:30 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.7689497931210665 on epoch=131
05/23/2022 12:58:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=131
05/23/2022 12:58:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=132
05/23/2022 12:58:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=133
05/23/2022 12:58:40 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=133
05/23/2022 12:58:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=134
05/23/2022 12:58:47 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.7715400878191576 on epoch=134
05/23/2022 12:58:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=134
05/23/2022 12:58:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=135
05/23/2022 12:58:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=136
05/23/2022 12:58:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/23/2022 12:58:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=137
05/23/2022 12:59:03 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.7301207349081364 on epoch=137
05/23/2022 12:59:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
05/23/2022 12:59:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=138
05/23/2022 12:59:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=139
05/23/2022 12:59:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=139
05/23/2022 12:59:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=140
05/23/2022 12:59:19 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7465366254028493 on epoch=140
05/23/2022 12:59:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=141
05/23/2022 12:59:24 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=141
05/23/2022 12:59:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/23/2022 12:59:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=143
05/23/2022 12:59:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=143
05/23/2022 12:59:35 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.7616709749016239 on epoch=143
05/23/2022 12:59:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=144
05/23/2022 12:59:40 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=144
05/23/2022 12:59:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=145
05/23/2022 12:59:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=146
05/23/2022 12:59:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/23/2022 12:59:51 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7737128765037738 on epoch=146
05/23/2022 12:59:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=147
05/23/2022 12:59:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=148
05/23/2022 12:59:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=148
05/23/2022 13:00:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/23/2022 13:00:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=149
05/23/2022 13:00:07 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7373491800604194 on epoch=149
05/23/2022 13:00:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=150
05/23/2022 13:00:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=151
05/23/2022 13:00:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=151
05/23/2022 13:00:17 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=152
05/23/2022 13:00:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=153
05/23/2022 13:00:23 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.7299533962780689 on epoch=153
05/23/2022 13:00:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=153
05/23/2022 13:00:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=154
05/23/2022 13:00:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=154
05/23/2022 13:00:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=155
05/23/2022 13:00:35 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=156
05/23/2022 13:00:39 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7627149152163986 on epoch=156
05/23/2022 13:00:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=156
05/23/2022 13:00:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=157
05/23/2022 13:00:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/23/2022 13:00:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=158
05/23/2022 13:00:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=159
05/23/2022 13:00:55 - INFO - __main__ - Global step 2550 Train loss 0.02 Classification-F1 0.7645173522218934 on epoch=159
05/23/2022 13:00:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
05/23/2022 13:01:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=160
05/23/2022 13:01:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.00 on epoch=161
05/23/2022 13:01:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=161
05/23/2022 13:01:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
05/23/2022 13:01:11 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7402118751039615 on epoch=162
05/23/2022 13:01:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.00 on epoch=163
05/23/2022 13:01:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/23/2022 13:01:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/23/2022 13:01:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=164
05/23/2022 13:01:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/23/2022 13:01:27 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7627452816609525 on epoch=165
05/23/2022 13:01:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/23/2022 13:01:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=166
05/23/2022 13:01:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=167
05/23/2022 13:01:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=168
05/23/2022 13:01:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/23/2022 13:01:43 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7566185807656396 on epoch=168
05/23/2022 13:01:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
05/23/2022 13:01:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=169
05/23/2022 13:01:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=170
05/23/2022 13:01:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/23/2022 13:01:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=171
05/23/2022 13:01:59 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7804946116685246 on epoch=171
05/23/2022 13:01:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7800349134760278 -> 0.7804946116685246 on epoch=171, global_step=2750
05/23/2022 13:02:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
05/23/2022 13:02:04 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/23/2022 13:02:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=173
05/23/2022 13:02:09 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=174
05/23/2022 13:02:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/23/2022 13:02:15 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.7545870821588085 on epoch=174
05/23/2022 13:02:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/23/2022 13:02:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.00 on epoch=176
05/23/2022 13:02:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=176
05/23/2022 13:02:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=177
05/23/2022 13:02:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=178
05/23/2022 13:02:32 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7825056412012474 on epoch=178
05/23/2022 13:02:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7804946116685246 -> 0.7825056412012474 on epoch=178, global_step=2850
05/23/2022 13:02:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/23/2022 13:02:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/23/2022 13:02:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/23/2022 13:02:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=180
05/23/2022 13:02:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.00 on epoch=181
05/23/2022 13:02:48 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7653689585007223 on epoch=181
05/23/2022 13:02:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/23/2022 13:02:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=182
05/23/2022 13:02:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=183
05/23/2022 13:02:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
05/23/2022 13:03:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 13:03:05 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7604500818547734 on epoch=184
05/23/2022 13:03:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=184
05/23/2022 13:03:10 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=185
05/23/2022 13:03:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=186
05/23/2022 13:03:15 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=186
05/23/2022 13:03:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=187
05/23/2022 13:03:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:03:18 - INFO - __main__ - Printing 3 examples
05/23/2022 13:03:18 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:03:18 - INFO - __main__ - ['sad']
05/23/2022 13:03:18 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:03:18 - INFO - __main__ - ['sad']
05/23/2022 13:03:18 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:03:18 - INFO - __main__ - ['sad']
05/23/2022 13:03:18 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:03:18 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:03:19 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:03:19 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:03:19 - INFO - __main__ - Printing 3 examples
05/23/2022 13:03:19 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:03:19 - INFO - __main__ - ['sad']
05/23/2022 13:03:19 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:03:19 - INFO - __main__ - ['sad']
05/23/2022 13:03:19 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:03:19 - INFO - __main__ - ['sad']
05/23/2022 13:03:19 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:03:19 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:03:19 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:03:21 - INFO - __main__ - Global step 3000 Train loss 0.01 Classification-F1 0.7789260795742967 on epoch=187
05/23/2022 13:03:21 - INFO - __main__ - save last model!
05/23/2022 13:03:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 13:03:21 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 13:03:21 - INFO - __main__ - Printing 3 examples
05/23/2022 13:03:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 13:03:21 - INFO - __main__ - ['others']
05/23/2022 13:03:21 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 13:03:21 - INFO - __main__ - ['others']
05/23/2022 13:03:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 13:03:21 - INFO - __main__ - ['others']
05/23/2022 13:03:21 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:03:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:03:28 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 13:03:38 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:03:38 - INFO - __main__ - task name: emo
05/23/2022 13:03:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:03:39 - INFO - __main__ - Starting training!
05/23/2022 13:05:02 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_21_0.5_8_predictions.txt
05/23/2022 13:05:02 - INFO - __main__ - Classification-F1 on test data: 0.4894
05/23/2022 13:05:03 - INFO - __main__ - prefix=emo_64_21, lr=0.5, bsz=8, dev_performance=0.7825056412012474, test_performance=0.4893816270550739
05/23/2022 13:05:03 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.4, bsz=8 ...
05/23/2022 13:05:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:05:04 - INFO - __main__ - Printing 3 examples
05/23/2022 13:05:04 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:05:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:05:04 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:05:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:05:04 - INFO - __main__ - Printing 3 examples
05/23/2022 13:05:04 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:05:04 - INFO - __main__ - ['sad']
05/23/2022 13:05:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:05:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:05:04 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:05:23 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:05:23 - INFO - __main__ - task name: emo
05/23/2022 13:05:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:05:24 - INFO - __main__ - Starting training!
05/23/2022 13:05:27 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=0
05/23/2022 13:05:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.10 on epoch=1
05/23/2022 13:05:32 - INFO - __main__ - Step 30 Global step 30 Train loss 1.90 on epoch=1
05/23/2022 13:05:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.29 on epoch=2
05/23/2022 13:05:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.23 on epoch=3
05/23/2022 13:05:40 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.16737012987012986 on epoch=3
05/23/2022 13:05:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16737012987012986 on epoch=3, global_step=50
05/23/2022 13:05:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=3
05/23/2022 13:05:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=4
05/23/2022 13:05:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=4
05/23/2022 13:05:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=5
05/23/2022 13:05:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=6
05/23/2022 13:05:55 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.1401697591788393 on epoch=6
05/23/2022 13:05:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.99 on epoch=6
05/23/2022 13:06:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.82 on epoch=7
05/23/2022 13:06:03 - INFO - __main__ - Step 130 Global step 130 Train loss 1.01 on epoch=8
05/23/2022 13:06:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=8
05/23/2022 13:06:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=9
05/23/2022 13:06:11 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.13365695792880256 on epoch=9
05/23/2022 13:06:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=9
05/23/2022 13:06:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=10
05/23/2022 13:06:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=11
05/23/2022 13:06:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=11
05/23/2022 13:06:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=12
05/23/2022 13:06:26 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.15787469053513614 on epoch=12
05/23/2022 13:06:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=13
05/23/2022 13:06:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=13
05/23/2022 13:06:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=14
05/23/2022 13:06:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=14
05/23/2022 13:06:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
05/23/2022 13:06:42 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.22389351806374738 on epoch=15
05/23/2022 13:06:42 - INFO - __main__ - Saving model with best Classification-F1: 0.16737012987012986 -> 0.22389351806374738 on epoch=15, global_step=250
05/23/2022 13:06:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.83 on epoch=16
05/23/2022 13:06:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.84 on epoch=16
05/23/2022 13:06:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.83 on epoch=17
05/23/2022 13:06:52 - INFO - __main__ - Step 290 Global step 290 Train loss 0.73 on epoch=18
05/23/2022 13:06:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=18
05/23/2022 13:06:58 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.10832123850991776 on epoch=18
05/23/2022 13:07:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.80 on epoch=19
05/23/2022 13:07:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=19
05/23/2022 13:07:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=20
05/23/2022 13:07:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.77 on epoch=21
05/23/2022 13:07:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.78 on epoch=21
05/23/2022 13:07:13 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.16733839242159596 on epoch=21
05/23/2022 13:07:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=22
05/23/2022 13:07:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.73 on epoch=23
05/23/2022 13:07:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=23
05/23/2022 13:07:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.73 on epoch=24
05/23/2022 13:07:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.73 on epoch=24
05/23/2022 13:07:29 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.23562304544162277 on epoch=24
05/23/2022 13:07:29 - INFO - __main__ - Saving model with best Classification-F1: 0.22389351806374738 -> 0.23562304544162277 on epoch=24, global_step=400
05/23/2022 13:07:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=25
05/23/2022 13:07:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=26
05/23/2022 13:07:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=26
05/23/2022 13:07:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.63 on epoch=27
05/23/2022 13:07:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.63 on epoch=28
05/23/2022 13:07:44 - INFO - __main__ - Global step 450 Train loss 0.70 Classification-F1 0.47740529054618275 on epoch=28
05/23/2022 13:07:44 - INFO - __main__ - Saving model with best Classification-F1: 0.23562304544162277 -> 0.47740529054618275 on epoch=28, global_step=450
05/23/2022 13:07:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.66 on epoch=28
05/23/2022 13:07:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=29
05/23/2022 13:07:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=29
05/23/2022 13:07:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=30
05/23/2022 13:07:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=31
05/23/2022 13:08:00 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.4549034726505844 on epoch=31
05/23/2022 13:08:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=31
05/23/2022 13:08:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.57 on epoch=32
05/23/2022 13:08:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=33
05/23/2022 13:08:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=33
05/23/2022 13:08:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=34
05/23/2022 13:08:16 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.5595483870690773 on epoch=34
05/23/2022 13:08:16 - INFO - __main__ - Saving model with best Classification-F1: 0.47740529054618275 -> 0.5595483870690773 on epoch=34, global_step=550
05/23/2022 13:08:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.66 on epoch=34
05/23/2022 13:08:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=35
05/23/2022 13:08:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=36
05/23/2022 13:08:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=36
05/23/2022 13:08:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=37
05/23/2022 13:08:31 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.42107070078400266 on epoch=37
05/23/2022 13:08:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=38
05/23/2022 13:08:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=38
05/23/2022 13:08:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=39
05/23/2022 13:08:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=39
05/23/2022 13:08:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=40
05/23/2022 13:08:47 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.6862292537941556 on epoch=40
05/23/2022 13:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5595483870690773 -> 0.6862292537941556 on epoch=40, global_step=650
05/23/2022 13:08:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=41
05/23/2022 13:08:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
05/23/2022 13:08:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=42
05/23/2022 13:08:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
05/23/2022 13:08:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=43
05/23/2022 13:09:02 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.6900686229434863 on epoch=43
05/23/2022 13:09:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6862292537941556 -> 0.6900686229434863 on epoch=43, global_step=700
05/23/2022 13:09:05 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=44
05/23/2022 13:09:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/23/2022 13:09:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=45
05/23/2022 13:09:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/23/2022 13:09:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=46
05/23/2022 13:09:18 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.6922286072292938 on epoch=46
05/23/2022 13:09:18 - INFO - __main__ - Saving model with best Classification-F1: 0.6900686229434863 -> 0.6922286072292938 on epoch=46, global_step=750
05/23/2022 13:09:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/23/2022 13:09:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=48
05/23/2022 13:09:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=48
05/23/2022 13:09:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/23/2022 13:09:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
05/23/2022 13:09:34 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.7079945416901938 on epoch=49
05/23/2022 13:09:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6922286072292938 -> 0.7079945416901938 on epoch=49, global_step=800
05/23/2022 13:09:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=50
05/23/2022 13:09:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=51
05/23/2022 13:09:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
05/23/2022 13:09:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=52
05/23/2022 13:09:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=53
05/23/2022 13:09:49 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.6923299764181037 on epoch=53
05/23/2022 13:09:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/23/2022 13:09:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=54
05/23/2022 13:09:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
05/23/2022 13:09:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=55
05/23/2022 13:10:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=56
05/23/2022 13:10:05 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.6774576004060864 on epoch=56
05/23/2022 13:10:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/23/2022 13:10:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
05/23/2022 13:10:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=58
05/23/2022 13:10:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
05/23/2022 13:10:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=59
05/23/2022 13:10:21 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.7108494633880381 on epoch=59
05/23/2022 13:10:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7079945416901938 -> 0.7108494633880381 on epoch=59, global_step=950
05/23/2022 13:10:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=59
05/23/2022 13:10:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
05/23/2022 13:10:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=61
05/23/2022 13:10:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=61
05/23/2022 13:10:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
05/23/2022 13:10:36 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.6981474433157601 on epoch=62
05/23/2022 13:10:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
05/23/2022 13:10:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=63
05/23/2022 13:10:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/23/2022 13:10:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=64
05/23/2022 13:10:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
05/23/2022 13:10:52 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.7146959925483416 on epoch=65
05/23/2022 13:10:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7108494633880381 -> 0.7146959925483416 on epoch=65, global_step=1050
05/23/2022 13:10:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=66
05/23/2022 13:10:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=66
05/23/2022 13:10:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/23/2022 13:11:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=68
05/23/2022 13:11:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/23/2022 13:11:08 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.7233374566627373 on epoch=68
05/23/2022 13:11:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7146959925483416 -> 0.7233374566627373 on epoch=68, global_step=1100
05/23/2022 13:11:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/23/2022 13:11:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
05/23/2022 13:11:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=70
05/23/2022 13:11:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.14 on epoch=71
05/23/2022 13:11:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=71
05/23/2022 13:11:23 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.7213289681515352 on epoch=71
05/23/2022 13:11:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=72
05/23/2022 13:11:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=73
05/23/2022 13:11:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/23/2022 13:11:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/23/2022 13:11:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=74
05/23/2022 13:11:39 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.7124357476107647 on epoch=74
05/23/2022 13:11:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=75
05/23/2022 13:11:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/23/2022 13:11:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=76
05/23/2022 13:11:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=77
05/23/2022 13:11:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/23/2022 13:11:56 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.7526034634009193 on epoch=78
05/23/2022 13:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7233374566627373 -> 0.7526034634009193 on epoch=78, global_step=1250
05/23/2022 13:11:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
05/23/2022 13:12:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=79
05/23/2022 13:12:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/23/2022 13:12:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=80
05/23/2022 13:12:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=81
05/23/2022 13:12:12 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.6861685336476687 on epoch=81
05/23/2022 13:12:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
05/23/2022 13:12:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=82
05/23/2022 13:12:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=83
05/23/2022 13:12:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/23/2022 13:12:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=84
05/23/2022 13:12:29 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.6818700288687825 on epoch=84
05/23/2022 13:12:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=84
05/23/2022 13:12:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=85
05/23/2022 13:12:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=86
05/23/2022 13:12:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=86
05/23/2022 13:12:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/23/2022 13:12:45 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7233795644820057 on epoch=87
05/23/2022 13:12:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/23/2022 13:12:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=88
05/23/2022 13:12:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=89
05/23/2022 13:12:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=89
05/23/2022 13:12:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=90
05/23/2022 13:13:01 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.7078559052293858 on epoch=90
05/23/2022 13:13:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/23/2022 13:13:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=91
05/23/2022 13:13:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=92
05/23/2022 13:13:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=93
05/23/2022 13:13:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=93
05/23/2022 13:13:18 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.7113853371658903 on epoch=93
05/23/2022 13:13:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=94
05/23/2022 13:13:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=94
05/23/2022 13:13:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=95
05/23/2022 13:13:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
05/23/2022 13:13:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/23/2022 13:13:34 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.7212799325517573 on epoch=96
05/23/2022 13:13:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=97
05/23/2022 13:13:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=98
05/23/2022 13:13:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=98
05/23/2022 13:13:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=99
05/23/2022 13:13:46 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=99
05/23/2022 13:13:50 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.6698951392627283 on epoch=99
05/23/2022 13:13:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=100
05/23/2022 13:13:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/23/2022 13:13:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=101
05/23/2022 13:14:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
05/23/2022 13:14:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=103
05/23/2022 13:14:06 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.72472341782081 on epoch=103
05/23/2022 13:14:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/23/2022 13:14:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=104
05/23/2022 13:14:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=104
05/23/2022 13:14:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=105
05/23/2022 13:14:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/23/2022 13:14:22 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.6811794308288347 on epoch=106
05/23/2022 13:14:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=106
05/23/2022 13:14:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
05/23/2022 13:14:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=108
05/23/2022 13:14:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=108
05/23/2022 13:14:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=109
05/23/2022 13:14:39 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7137460363583061 on epoch=109
05/23/2022 13:14:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=109
05/23/2022 13:14:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/23/2022 13:14:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=111
05/23/2022 13:14:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=111
05/23/2022 13:14:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=112
05/23/2022 13:14:55 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7235221907238867 on epoch=112
05/23/2022 13:14:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=113
05/23/2022 13:15:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=113
05/23/2022 13:15:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=114
05/23/2022 13:15:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/23/2022 13:15:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=115
05/23/2022 13:15:11 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7072632467793758 on epoch=115
05/23/2022 13:15:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/23/2022 13:15:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=116
05/23/2022 13:15:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=117
05/23/2022 13:15:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=118
05/23/2022 13:15:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=118
05/23/2022 13:15:27 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.718626586115061 on epoch=118
05/23/2022 13:15:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/23/2022 13:15:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=119
05/23/2022 13:15:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=120
05/23/2022 13:15:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=121
05/23/2022 13:15:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=121
05/23/2022 13:15:43 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.6930000096146836 on epoch=121
05/23/2022 13:15:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=122
05/23/2022 13:15:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
05/23/2022 13:15:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/23/2022 13:15:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=124
05/23/2022 13:15:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=124
05/23/2022 13:15:59 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.7184278525771793 on epoch=124
05/23/2022 13:16:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/23/2022 13:16:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=126
05/23/2022 13:16:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=126
05/23/2022 13:16:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.04 on epoch=127
05/23/2022 13:16:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=128
05/23/2022 13:16:16 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7187311389973625 on epoch=128
05/23/2022 13:16:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=128
05/23/2022 13:16:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.03 on epoch=129
05/23/2022 13:16:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/23/2022 13:16:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/23/2022 13:16:28 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/23/2022 13:16:32 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.6733774157739695 on epoch=131
05/23/2022 13:16:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=131
05/23/2022 13:16:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=132
05/23/2022 13:16:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=133
05/23/2022 13:16:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=133
05/23/2022 13:16:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=134
05/23/2022 13:16:48 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.7206588389520325 on epoch=134
05/23/2022 13:16:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/23/2022 13:16:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=135
05/23/2022 13:16:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/23/2022 13:16:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=136
05/23/2022 13:17:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=137
05/23/2022 13:17:04 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7401330567910165 on epoch=137
05/23/2022 13:17:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
05/23/2022 13:17:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/23/2022 13:17:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=139
05/23/2022 13:17:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=139
05/23/2022 13:17:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=140
05/23/2022 13:17:21 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7406700025121078 on epoch=140
05/23/2022 13:17:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
05/23/2022 13:17:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=141
05/23/2022 13:17:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=142
05/23/2022 13:17:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=143
05/23/2022 13:17:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=143
05/23/2022 13:17:37 - INFO - __main__ - Global step 2300 Train loss 0.08 Classification-F1 0.7521280679175416 on epoch=143
05/23/2022 13:17:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=144
05/23/2022 13:17:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.03 on epoch=144
05/23/2022 13:17:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=145
05/23/2022 13:17:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=146
05/23/2022 13:17:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
05/23/2022 13:17:53 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7343864095909285 on epoch=146
05/23/2022 13:17:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=147
05/23/2022 13:17:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=148
05/23/2022 13:18:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
05/23/2022 13:18:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=149
05/23/2022 13:18:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/23/2022 13:18:09 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7182246876440572 on epoch=149
05/23/2022 13:18:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=150
05/23/2022 13:18:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=151
05/23/2022 13:18:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
05/23/2022 13:18:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/23/2022 13:18:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=153
05/23/2022 13:18:26 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7536766144557927 on epoch=153
05/23/2022 13:18:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7526034634009193 -> 0.7536766144557927 on epoch=153, global_step=2450
05/23/2022 13:18:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=153
05/23/2022 13:18:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/23/2022 13:18:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
05/23/2022 13:18:36 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=155
05/23/2022 13:18:39 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
05/23/2022 13:18:43 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.7154761063799429 on epoch=156
05/23/2022 13:18:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
05/23/2022 13:18:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=157
05/23/2022 13:18:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/23/2022 13:18:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/23/2022 13:18:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=159
05/23/2022 13:18:59 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7331808090805605 on epoch=159
05/23/2022 13:19:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
05/23/2022 13:19:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=160
05/23/2022 13:19:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/23/2022 13:19:09 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=161
05/23/2022 13:19:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
05/23/2022 13:19:15 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.702520447072202 on epoch=162
05/23/2022 13:19:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=163
05/23/2022 13:19:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
05/23/2022 13:19:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=164
05/23/2022 13:19:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=164
05/23/2022 13:19:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=165
05/23/2022 13:19:32 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7190914493909568 on epoch=165
05/23/2022 13:19:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/23/2022 13:19:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=166
05/23/2022 13:19:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=167
05/23/2022 13:19:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
05/23/2022 13:19:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/23/2022 13:19:48 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7283222551904617 on epoch=168
05/23/2022 13:19:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
05/23/2022 13:19:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=169
05/23/2022 13:19:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=170
05/23/2022 13:19:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/23/2022 13:20:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=171
05/23/2022 13:20:04 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7290854309687262 on epoch=171
05/23/2022 13:20:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
05/23/2022 13:20:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
05/23/2022 13:20:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/23/2022 13:20:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=174
05/23/2022 13:20:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/23/2022 13:20:20 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.7132575291188736 on epoch=174
05/23/2022 13:20:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/23/2022 13:20:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/23/2022 13:20:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/23/2022 13:20:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/23/2022 13:20:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/23/2022 13:20:37 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7311023622047244 on epoch=178
05/23/2022 13:20:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
05/23/2022 13:20:42 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
05/23/2022 13:20:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/23/2022 13:20:47 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
05/23/2022 13:20:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/23/2022 13:20:53 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7237506945047705 on epoch=181
05/23/2022 13:20:56 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/23/2022 13:20:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/23/2022 13:21:01 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
05/23/2022 13:21:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=183
05/23/2022 13:21:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/23/2022 13:21:09 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7155619989408291 on epoch=184
05/23/2022 13:21:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
05/23/2022 13:21:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/23/2022 13:21:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
05/23/2022 13:21:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
05/23/2022 13:21:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/23/2022 13:21:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:21:23 - INFO - __main__ - Printing 3 examples
05/23/2022 13:21:23 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:21:23 - INFO - __main__ - ['sad']
05/23/2022 13:21:23 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:21:23 - INFO - __main__ - ['sad']
05/23/2022 13:21:23 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:21:23 - INFO - __main__ - ['sad']
05/23/2022 13:21:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:21:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:21:24 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:21:24 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:21:24 - INFO - __main__ - Printing 3 examples
05/23/2022 13:21:24 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:21:24 - INFO - __main__ - ['sad']
05/23/2022 13:21:24 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:21:24 - INFO - __main__ - ['sad']
05/23/2022 13:21:24 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:21:24 - INFO - __main__ - ['sad']
05/23/2022 13:21:24 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:21:24 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:21:24 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:21:26 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7068010606573386 on epoch=187
05/23/2022 13:21:26 - INFO - __main__ - save last model!
05/23/2022 13:21:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 13:21:26 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 13:21:26 - INFO - __main__ - Printing 3 examples
05/23/2022 13:21:26 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 13:21:26 - INFO - __main__ - ['others']
05/23/2022 13:21:26 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 13:21:26 - INFO - __main__ - ['others']
05/23/2022 13:21:26 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 13:21:26 - INFO - __main__ - ['others']
05/23/2022 13:21:26 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:21:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:21:33 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 13:21:42 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:21:42 - INFO - __main__ - task name: emo
05/23/2022 13:21:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:21:43 - INFO - __main__ - Starting training!
05/23/2022 13:22:48 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_21_0.4_8_predictions.txt
05/23/2022 13:22:48 - INFO - __main__ - Classification-F1 on test data: 0.3531
05/23/2022 13:22:49 - INFO - __main__ - prefix=emo_64_21, lr=0.4, bsz=8, dev_performance=0.7536766144557927, test_performance=0.3531117048927728
05/23/2022 13:22:49 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.3, bsz=8 ...
05/23/2022 13:22:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:22:50 - INFO - __main__ - Printing 3 examples
05/23/2022 13:22:50 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:22:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:22:50 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:22:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:22:50 - INFO - __main__ - Printing 3 examples
05/23/2022 13:22:50 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:22:50 - INFO - __main__ - ['sad']
05/23/2022 13:22:50 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:22:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:22:50 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:23:06 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:23:06 - INFO - __main__ - task name: emo
05/23/2022 13:23:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:23:07 - INFO - __main__ - Starting training!
05/23/2022 13:23:10 - INFO - __main__ - Step 10 Global step 10 Train loss 7.44 on epoch=0
05/23/2022 13:23:12 - INFO - __main__ - Step 20 Global step 20 Train loss 4.11 on epoch=1
05/23/2022 13:23:15 - INFO - __main__ - Step 30 Global step 30 Train loss 2.13 on epoch=1
05/23/2022 13:23:17 - INFO - __main__ - Step 40 Global step 40 Train loss 1.35 on epoch=2
05/23/2022 13:23:20 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=3
05/23/2022 13:23:23 - INFO - __main__ - Global step 50 Train loss 3.24 Classification-F1 0.11289203686248447 on epoch=3
05/23/2022 13:23:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11289203686248447 on epoch=3, global_step=50
05/23/2022 13:23:26 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=3
05/23/2022 13:23:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.98 on epoch=4
05/23/2022 13:23:31 - INFO - __main__ - Step 80 Global step 80 Train loss 1.03 on epoch=4
05/23/2022 13:23:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
05/23/2022 13:23:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=6
05/23/2022 13:23:39 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.12748396952702568 on epoch=6
05/23/2022 13:23:39 - INFO - __main__ - Saving model with best Classification-F1: 0.11289203686248447 -> 0.12748396952702568 on epoch=6, global_step=100
05/23/2022 13:23:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=6
05/23/2022 13:23:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=7
05/23/2022 13:23:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=8
05/23/2022 13:23:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=8
05/23/2022 13:23:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=9
05/23/2022 13:23:55 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.12546799982381185 on epoch=9
05/23/2022 13:23:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.94 on epoch=9
05/23/2022 13:24:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=10
05/23/2022 13:24:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=11
05/23/2022 13:24:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=11
05/23/2022 13:24:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=12
05/23/2022 13:24:11 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.17403023128756145 on epoch=12
05/23/2022 13:24:11 - INFO - __main__ - Saving model with best Classification-F1: 0.12748396952702568 -> 0.17403023128756145 on epoch=12, global_step=200
05/23/2022 13:24:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=13
05/23/2022 13:24:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=13
05/23/2022 13:24:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=14
05/23/2022 13:24:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=14
05/23/2022 13:24:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.79 on epoch=15
05/23/2022 13:24:26 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.24996396647720007 on epoch=15
05/23/2022 13:24:26 - INFO - __main__ - Saving model with best Classification-F1: 0.17403023128756145 -> 0.24996396647720007 on epoch=15, global_step=250
05/23/2022 13:24:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.90 on epoch=16
05/23/2022 13:24:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/23/2022 13:24:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=17
05/23/2022 13:24:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=18
05/23/2022 13:24:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.82 on epoch=18
05/23/2022 13:24:42 - INFO - __main__ - Global step 300 Train loss 0.85 Classification-F1 0.3121795904726097 on epoch=18
05/23/2022 13:24:42 - INFO - __main__ - Saving model with best Classification-F1: 0.24996396647720007 -> 0.3121795904726097 on epoch=18, global_step=300
05/23/2022 13:24:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.82 on epoch=19
05/23/2022 13:24:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=19
05/23/2022 13:24:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.68 on epoch=20
05/23/2022 13:24:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.87 on epoch=21
05/23/2022 13:24:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.80 on epoch=21
05/23/2022 13:24:58 - INFO - __main__ - Global step 350 Train loss 0.80 Classification-F1 0.40483590391348667 on epoch=21
05/23/2022 13:24:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3121795904726097 -> 0.40483590391348667 on epoch=21, global_step=350
05/23/2022 13:25:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=22
05/23/2022 13:25:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.79 on epoch=23
05/23/2022 13:25:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=23
05/23/2022 13:25:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=24
05/23/2022 13:25:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=24
05/23/2022 13:25:14 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.1305728088336784 on epoch=24
05/23/2022 13:25:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.77 on epoch=25
05/23/2022 13:25:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.81 on epoch=26
05/23/2022 13:25:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=26
05/23/2022 13:25:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=27
05/23/2022 13:25:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.76 on epoch=28
05/23/2022 13:25:30 - INFO - __main__ - Global step 450 Train loss 0.78 Classification-F1 0.385671438044974 on epoch=28
05/23/2022 13:25:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=28
05/23/2022 13:25:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.70 on epoch=29
05/23/2022 13:25:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.72 on epoch=29
05/23/2022 13:25:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.74 on epoch=30
05/23/2022 13:25:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=31
05/23/2022 13:25:45 - INFO - __main__ - Global step 500 Train loss 0.73 Classification-F1 0.2706810035842294 on epoch=31
05/23/2022 13:25:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.75 on epoch=31
05/23/2022 13:25:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=32
05/23/2022 13:25:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=33
05/23/2022 13:25:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=33
05/23/2022 13:25:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.62 on epoch=34
05/23/2022 13:26:01 - INFO - __main__ - Global step 550 Train loss 0.68 Classification-F1 0.38829740473187324 on epoch=34
05/23/2022 13:26:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=34
05/23/2022 13:26:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.64 on epoch=35
05/23/2022 13:26:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=36
05/23/2022 13:26:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=36
05/23/2022 13:26:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.60 on epoch=37
05/23/2022 13:26:17 - INFO - __main__ - Global step 600 Train loss 0.62 Classification-F1 0.4216727716727716 on epoch=37
05/23/2022 13:26:17 - INFO - __main__ - Saving model with best Classification-F1: 0.40483590391348667 -> 0.4216727716727716 on epoch=37, global_step=600
05/23/2022 13:26:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=38
05/23/2022 13:26:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=38
05/23/2022 13:26:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=39
05/23/2022 13:26:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=39
05/23/2022 13:26:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.63 on epoch=40
05/23/2022 13:26:33 - INFO - __main__ - Global step 650 Train loss 0.60 Classification-F1 0.6272878641146695 on epoch=40
05/23/2022 13:26:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4216727716727716 -> 0.6272878641146695 on epoch=40, global_step=650
05/23/2022 13:26:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=41
05/23/2022 13:26:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.57 on epoch=41
05/23/2022 13:26:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=42
05/23/2022 13:26:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=43
05/23/2022 13:26:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=43
05/23/2022 13:26:49 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.6784164609651034 on epoch=43
05/23/2022 13:26:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6272878641146695 -> 0.6784164609651034 on epoch=43, global_step=700
05/23/2022 13:26:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=44
05/23/2022 13:26:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=44
05/23/2022 13:26:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=45
05/23/2022 13:26:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
05/23/2022 13:27:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=46
05/23/2022 13:27:05 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.561958000591541 on epoch=46
05/23/2022 13:27:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=47
05/23/2022 13:27:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=48
05/23/2022 13:27:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=48
05/23/2022 13:27:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=49
05/23/2022 13:27:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.57 on epoch=49
05/23/2022 13:27:20 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.6032259248770152 on epoch=49
05/23/2022 13:27:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/23/2022 13:27:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
05/23/2022 13:27:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.51 on epoch=51
05/23/2022 13:27:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/23/2022 13:27:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=53
05/23/2022 13:27:36 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.6887511288380495 on epoch=53
05/23/2022 13:27:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6784164609651034 -> 0.6887511288380495 on epoch=53, global_step=850
05/23/2022 13:27:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=53
05/23/2022 13:27:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=54
05/23/2022 13:27:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=54
05/23/2022 13:27:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=55
05/23/2022 13:27:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
05/23/2022 13:27:52 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.6825566173109525 on epoch=56
05/23/2022 13:27:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=56
05/23/2022 13:27:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=57
05/23/2022 13:27:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=58
05/23/2022 13:28:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=58
05/23/2022 13:28:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=59
05/23/2022 13:28:08 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.6139715515274786 on epoch=59
05/23/2022 13:28:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=59
05/23/2022 13:28:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/23/2022 13:28:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=61
05/23/2022 13:28:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/23/2022 13:28:20 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=62
05/23/2022 13:28:24 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.667611013352331 on epoch=62
05/23/2022 13:28:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
05/23/2022 13:28:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
05/23/2022 13:28:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/23/2022 13:28:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=64
05/23/2022 13:28:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=65
05/23/2022 13:28:40 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.6957712098590081 on epoch=65
05/23/2022 13:28:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6887511288380495 -> 0.6957712098590081 on epoch=65, global_step=1050
05/23/2022 13:28:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=66
05/23/2022 13:28:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
05/23/2022 13:28:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=67
05/23/2022 13:28:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/23/2022 13:28:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=68
05/23/2022 13:28:56 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.7141161119192265 on epoch=68
05/23/2022 13:28:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6957712098590081 -> 0.7141161119192265 on epoch=68, global_step=1100
05/23/2022 13:28:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=69
05/23/2022 13:29:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=69
05/23/2022 13:29:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=70
05/23/2022 13:29:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
05/23/2022 13:29:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=71
05/23/2022 13:29:12 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.6985716214365629 on epoch=71
05/23/2022 13:29:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=72
05/23/2022 13:29:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
05/23/2022 13:29:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
05/23/2022 13:29:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
05/23/2022 13:29:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=74
05/23/2022 13:29:28 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7230701886259208 on epoch=74
05/23/2022 13:29:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7141161119192265 -> 0.7230701886259208 on epoch=74, global_step=1200
05/23/2022 13:29:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=75
05/23/2022 13:29:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=76
05/23/2022 13:29:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=76
05/23/2022 13:29:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=77
05/23/2022 13:29:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=78
05/23/2022 13:29:44 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.7159351181084592 on epoch=78
05/23/2022 13:29:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=78
05/23/2022 13:29:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
05/23/2022 13:29:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=79
05/23/2022 13:29:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=80
05/23/2022 13:29:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=81
05/23/2022 13:29:59 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.7137314465643798 on epoch=81
05/23/2022 13:30:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
05/23/2022 13:30:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=82
05/23/2022 13:30:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/23/2022 13:30:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=83
05/23/2022 13:30:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=84
05/23/2022 13:30:15 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.6836462306376883 on epoch=84
05/23/2022 13:30:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=84
05/23/2022 13:30:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=85
05/23/2022 13:30:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/23/2022 13:30:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=86
05/23/2022 13:30:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=87
05/23/2022 13:30:31 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.706889031462306 on epoch=87
05/23/2022 13:30:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=88
05/23/2022 13:30:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
05/23/2022 13:30:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=89
05/23/2022 13:30:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
05/23/2022 13:30:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
05/23/2022 13:30:47 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7039528571966747 on epoch=90
05/23/2022 13:30:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=91
05/23/2022 13:30:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=91
05/23/2022 13:30:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=92
05/23/2022 13:30:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=93
05/23/2022 13:31:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/23/2022 13:31:03 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.706519940908332 on epoch=93
05/23/2022 13:31:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=94
05/23/2022 13:31:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
05/23/2022 13:31:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=95
05/23/2022 13:31:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=96
05/23/2022 13:31:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=96
05/23/2022 13:31:19 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.6891377217982899 on epoch=96
05/23/2022 13:31:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=97
05/23/2022 13:31:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=98
05/23/2022 13:31:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/23/2022 13:31:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=99
05/23/2022 13:31:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/23/2022 13:31:35 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.7013367266889241 on epoch=99
05/23/2022 13:31:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=100
05/23/2022 13:31:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=101
05/23/2022 13:31:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/23/2022 13:31:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=102
05/23/2022 13:31:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
05/23/2022 13:31:51 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.733070696138878 on epoch=103
05/23/2022 13:31:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7230701886259208 -> 0.733070696138878 on epoch=103, global_step=1650
05/23/2022 13:31:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
05/23/2022 13:31:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
05/23/2022 13:31:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/23/2022 13:32:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/23/2022 13:32:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=106
05/23/2022 13:32:07 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.7335088942065686 on epoch=106
05/23/2022 13:32:07 - INFO - __main__ - Saving model with best Classification-F1: 0.733070696138878 -> 0.7335088942065686 on epoch=106, global_step=1700
05/23/2022 13:32:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=106
05/23/2022 13:32:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=107
05/23/2022 13:32:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/23/2022 13:32:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=108
05/23/2022 13:32:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/23/2022 13:32:23 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.7081072881252912 on epoch=109
05/23/2022 13:32:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=109
05/23/2022 13:32:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
05/23/2022 13:32:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=111
05/23/2022 13:32:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=111
05/23/2022 13:32:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=112
05/23/2022 13:32:39 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.72336204455277 on epoch=112
05/23/2022 13:32:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=113
05/23/2022 13:32:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=113
05/23/2022 13:32:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=114
05/23/2022 13:32:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/23/2022 13:32:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/23/2022 13:32:55 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.6959882669713585 on epoch=115
05/23/2022 13:32:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=116
05/23/2022 13:33:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
05/23/2022 13:33:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=117
05/23/2022 13:33:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=118
05/23/2022 13:33:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=118
05/23/2022 13:33:11 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7301777164039147 on epoch=118
05/23/2022 13:33:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=119
05/23/2022 13:33:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
05/23/2022 13:33:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=120
05/23/2022 13:33:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=121
05/23/2022 13:33:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=121
05/23/2022 13:33:27 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.682818341525893 on epoch=121
05/23/2022 13:33:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/23/2022 13:33:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/23/2022 13:33:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
05/23/2022 13:33:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=124
05/23/2022 13:33:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=124
05/23/2022 13:33:43 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.7116191123188406 on epoch=124
05/23/2022 13:33:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
05/23/2022 13:33:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=126
05/23/2022 13:33:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=126
05/23/2022 13:33:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/23/2022 13:33:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=128
05/23/2022 13:33:59 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.7002565547111133 on epoch=128
05/23/2022 13:34:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/23/2022 13:34:04 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
05/23/2022 13:34:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/23/2022 13:34:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=130
05/23/2022 13:34:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=131
05/23/2022 13:34:15 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.7250329409275396 on epoch=131
05/23/2022 13:34:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=131
05/23/2022 13:34:20 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=132
05/23/2022 13:34:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=133
05/23/2022 13:34:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
05/23/2022 13:34:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/23/2022 13:34:31 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7051104797979799 on epoch=134
05/23/2022 13:34:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/23/2022 13:34:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=135
05/23/2022 13:34:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=136
05/23/2022 13:34:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/23/2022 13:34:43 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=137
05/23/2022 13:34:47 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7214967156709415 on epoch=137
05/23/2022 13:34:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
05/23/2022 13:34:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=138
05/23/2022 13:34:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=139
05/23/2022 13:34:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.03 on epoch=139
05/23/2022 13:34:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/23/2022 13:35:03 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7052161633794916 on epoch=140
05/23/2022 13:35:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=141
05/23/2022 13:35:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=141
05/23/2022 13:35:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=142
05/23/2022 13:35:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=143
05/23/2022 13:35:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=143
05/23/2022 13:35:19 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.6870102780149578 on epoch=143
05/23/2022 13:35:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=144
05/23/2022 13:35:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=144
05/23/2022 13:35:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/23/2022 13:35:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
05/23/2022 13:35:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=146
05/23/2022 13:35:35 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7569838718223943 on epoch=146
05/23/2022 13:35:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7335088942065686 -> 0.7569838718223943 on epoch=146, global_step=2350
05/23/2022 13:35:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=147
05/23/2022 13:35:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=148
05/23/2022 13:35:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/23/2022 13:35:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=149
05/23/2022 13:35:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=149
05/23/2022 13:35:51 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.7342949043292899 on epoch=149
05/23/2022 13:35:54 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/23/2022 13:35:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=151
05/23/2022 13:35:59 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/23/2022 13:36:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/23/2022 13:36:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/23/2022 13:36:07 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.7561818942363729 on epoch=153
05/23/2022 13:36:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=153
05/23/2022 13:36:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=154
05/23/2022 13:36:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
05/23/2022 13:36:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
05/23/2022 13:36:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
05/23/2022 13:36:24 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7235182249946029 on epoch=156
05/23/2022 13:36:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=156
05/23/2022 13:36:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/23/2022 13:36:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/23/2022 13:36:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=158
05/23/2022 13:36:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/23/2022 13:36:40 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7167422693378686 on epoch=159
05/23/2022 13:36:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/23/2022 13:36:45 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
05/23/2022 13:36:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/23/2022 13:36:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/23/2022 13:36:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/23/2022 13:36:56 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7419774800553364 on epoch=162
05/23/2022 13:36:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
05/23/2022 13:37:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
05/23/2022 13:37:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=164
05/23/2022 13:37:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=164
05/23/2022 13:37:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
05/23/2022 13:37:12 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7535150365458587 on epoch=165
05/23/2022 13:37:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=166
05/23/2022 13:37:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
05/23/2022 13:37:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/23/2022 13:37:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/23/2022 13:37:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/23/2022 13:37:28 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.7651785714285714 on epoch=168
05/23/2022 13:37:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7569838718223943 -> 0.7651785714285714 on epoch=168, global_step=2700
05/23/2022 13:37:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/23/2022 13:37:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/23/2022 13:37:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
05/23/2022 13:37:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/23/2022 13:37:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
05/23/2022 13:37:44 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7344008946663771 on epoch=171
05/23/2022 13:37:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
05/23/2022 13:37:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/23/2022 13:37:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=173
05/23/2022 13:37:54 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/23/2022 13:37:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/23/2022 13:38:00 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7337955563653753 on epoch=174
05/23/2022 13:38:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/23/2022 13:38:05 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/23/2022 13:38:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=176
05/23/2022 13:38:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
05/23/2022 13:38:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/23/2022 13:38:16 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6961035800831802 on epoch=178
05/23/2022 13:38:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/23/2022 13:38:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=179
05/23/2022 13:38:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/23/2022 13:38:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/23/2022 13:38:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/23/2022 13:38:32 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.723351364006013 on epoch=181
05/23/2022 13:38:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/23/2022 13:38:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/23/2022 13:38:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/23/2022 13:38:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=183
05/23/2022 13:38:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/23/2022 13:38:48 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.6958687906461878 on epoch=184
05/23/2022 13:38:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/23/2022 13:38:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
05/23/2022 13:38:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/23/2022 13:38:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
05/23/2022 13:39:01 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/23/2022 13:39:02 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:39:02 - INFO - __main__ - Printing 3 examples
05/23/2022 13:39:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:39:02 - INFO - __main__ - ['sad']
05/23/2022 13:39:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:39:02 - INFO - __main__ - ['sad']
05/23/2022 13:39:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:39:02 - INFO - __main__ - ['sad']
05/23/2022 13:39:02 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:39:02 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:39:03 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:39:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:39:03 - INFO - __main__ - Printing 3 examples
05/23/2022 13:39:03 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:39:03 - INFO - __main__ - ['sad']
05/23/2022 13:39:03 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:39:03 - INFO - __main__ - ['sad']
05/23/2022 13:39:03 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:39:03 - INFO - __main__ - ['sad']
05/23/2022 13:39:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:39:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:39:03 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:39:05 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.728041263293062 on epoch=187
05/23/2022 13:39:05 - INFO - __main__ - save last model!
05/23/2022 13:39:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 13:39:05 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 13:39:05 - INFO - __main__ - Printing 3 examples
05/23/2022 13:39:05 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 13:39:05 - INFO - __main__ - ['others']
05/23/2022 13:39:05 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 13:39:05 - INFO - __main__ - ['others']
05/23/2022 13:39:05 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 13:39:05 - INFO - __main__ - ['others']
05/23/2022 13:39:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:39:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:39:12 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 13:39:21 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:39:21 - INFO - __main__ - task name: emo
05/23/2022 13:39:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:39:22 - INFO - __main__ - Starting training!
05/23/2022 13:40:26 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_21_0.3_8_predictions.txt
05/23/2022 13:40:26 - INFO - __main__ - Classification-F1 on test data: 0.4083
05/23/2022 13:40:26 - INFO - __main__ - prefix=emo_64_21, lr=0.3, bsz=8, dev_performance=0.7651785714285714, test_performance=0.40828644247361456
05/23/2022 13:40:26 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.2, bsz=8 ...
05/23/2022 13:40:27 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:40:27 - INFO - __main__ - Printing 3 examples
05/23/2022 13:40:27 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/23/2022 13:40:27 - INFO - __main__ - ['sad']
05/23/2022 13:40:27 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/23/2022 13:40:27 - INFO - __main__ - ['sad']
05/23/2022 13:40:27 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/23/2022 13:40:27 - INFO - __main__ - ['sad']
05/23/2022 13:40:27 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:40:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:40:28 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:40:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:40:28 - INFO - __main__ - Printing 3 examples
05/23/2022 13:40:28 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/23/2022 13:40:28 - INFO - __main__ - ['sad']
05/23/2022 13:40:28 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/23/2022 13:40:28 - INFO - __main__ - ['sad']
05/23/2022 13:40:28 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/23/2022 13:40:28 - INFO - __main__ - ['sad']
05/23/2022 13:40:28 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:40:28 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:40:28 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:40:44 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:40:44 - INFO - __main__ - task name: emo
05/23/2022 13:40:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:40:44 - INFO - __main__ - Starting training!
05/23/2022 13:40:47 - INFO - __main__ - Step 10 Global step 10 Train loss 7.68 on epoch=0
05/23/2022 13:40:50 - INFO - __main__ - Step 20 Global step 20 Train loss 5.57 on epoch=1
05/23/2022 13:40:52 - INFO - __main__ - Step 30 Global step 30 Train loss 3.62 on epoch=1
05/23/2022 13:40:55 - INFO - __main__ - Step 40 Global step 40 Train loss 2.38 on epoch=2
05/23/2022 13:40:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.60 on epoch=3
05/23/2022 13:41:01 - INFO - __main__ - Global step 50 Train loss 4.17 Classification-F1 0.20128968253968255 on epoch=3
05/23/2022 13:41:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20128968253968255 on epoch=3, global_step=50
05/23/2022 13:41:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=3
05/23/2022 13:41:06 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=4
05/23/2022 13:41:08 - INFO - __main__ - Step 80 Global step 80 Train loss 1.16 on epoch=4
05/23/2022 13:41:11 - INFO - __main__ - Step 90 Global step 90 Train loss 1.12 on epoch=5
05/23/2022 13:41:13 - INFO - __main__ - Step 100 Global step 100 Train loss 1.15 on epoch=6
05/23/2022 13:41:17 - INFO - __main__ - Global step 100 Train loss 1.19 Classification-F1 0.139928080247303 on epoch=6
05/23/2022 13:41:19 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=6
05/23/2022 13:41:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.93 on epoch=7
05/23/2022 13:41:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.98 on epoch=8
05/23/2022 13:41:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.98 on epoch=8
05/23/2022 13:41:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.00 on epoch=9
05/23/2022 13:41:33 - INFO - __main__ - Global step 150 Train loss 0.99 Classification-F1 0.10800578731613214 on epoch=9
05/23/2022 13:41:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=9
05/23/2022 13:41:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=10
05/23/2022 13:41:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/23/2022 13:41:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=11
05/23/2022 13:41:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=12
05/23/2022 13:41:49 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.1617738116918594 on epoch=12
05/23/2022 13:41:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=13
05/23/2022 13:41:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=13
05/23/2022 13:41:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.86 on epoch=14
05/23/2022 13:41:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=14
05/23/2022 13:42:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.90 on epoch=15
05/23/2022 13:42:05 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.17542016806722688 on epoch=15
05/23/2022 13:42:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=16
05/23/2022 13:42:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=16
05/23/2022 13:42:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.91 on epoch=17
05/23/2022 13:42:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=18
05/23/2022 13:42:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=18
05/23/2022 13:42:21 - INFO - __main__ - Global step 300 Train loss 0.87 Classification-F1 0.3066863575723875 on epoch=18
05/23/2022 13:42:21 - INFO - __main__ - Saving model with best Classification-F1: 0.20128968253968255 -> 0.3066863575723875 on epoch=18, global_step=300
05/23/2022 13:42:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.85 on epoch=19
05/23/2022 13:42:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.86 on epoch=19
05/23/2022 13:42:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=20
05/23/2022 13:42:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.82 on epoch=21
05/23/2022 13:42:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.78 on epoch=21
05/23/2022 13:42:37 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.2663455315523442 on epoch=21
05/23/2022 13:42:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=22
05/23/2022 13:42:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=23
05/23/2022 13:42:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=23
05/23/2022 13:42:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.81 on epoch=24
05/23/2022 13:42:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.89 on epoch=24
05/23/2022 13:42:53 - INFO - __main__ - Global step 400 Train loss 0.86 Classification-F1 0.2516965449180252 on epoch=24
05/23/2022 13:42:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.82 on epoch=25
05/23/2022 13:42:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.82 on epoch=26
05/23/2022 13:43:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=26
05/23/2022 13:43:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=27
05/23/2022 13:43:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.78 on epoch=28
05/23/2022 13:43:09 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.39997686153270623 on epoch=28
05/23/2022 13:43:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3066863575723875 -> 0.39997686153270623 on epoch=28, global_step=450
05/23/2022 13:43:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=28
05/23/2022 13:43:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.73 on epoch=29
05/23/2022 13:43:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=29
05/23/2022 13:43:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=30
05/23/2022 13:43:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.75 on epoch=31
05/23/2022 13:43:25 - INFO - __main__ - Global step 500 Train loss 0.76 Classification-F1 0.39885551612966647 on epoch=31
05/23/2022 13:43:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.74 on epoch=31
05/23/2022 13:43:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.78 on epoch=32
05/23/2022 13:43:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.73 on epoch=33
05/23/2022 13:43:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=33
05/23/2022 13:43:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.71 on epoch=34
05/23/2022 13:43:41 - INFO - __main__ - Global step 550 Train loss 0.73 Classification-F1 0.2184538848977458 on epoch=34
05/23/2022 13:43:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.77 on epoch=34
05/23/2022 13:43:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.76 on epoch=35
05/23/2022 13:43:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=36
05/23/2022 13:43:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.67 on epoch=36
05/23/2022 13:43:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.70 on epoch=37
05/23/2022 13:43:57 - INFO - __main__ - Global step 600 Train loss 0.71 Classification-F1 0.3692558054357299 on epoch=37
05/23/2022 13:43:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=38
05/23/2022 13:44:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=38
05/23/2022 13:44:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.72 on epoch=39
05/23/2022 13:44:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=39
05/23/2022 13:44:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=40
05/23/2022 13:44:13 - INFO - __main__ - Global step 650 Train loss 0.64 Classification-F1 0.463072580400697 on epoch=40
05/23/2022 13:44:13 - INFO - __main__ - Saving model with best Classification-F1: 0.39997686153270623 -> 0.463072580400697 on epoch=40, global_step=650
05/23/2022 13:44:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.61 on epoch=41
05/23/2022 13:44:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=41
05/23/2022 13:44:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=42
05/23/2022 13:44:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.69 on epoch=43
05/23/2022 13:44:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=43
05/23/2022 13:44:29 - INFO - __main__ - Global step 700 Train loss 0.62 Classification-F1 0.5412090568607423 on epoch=43
05/23/2022 13:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.463072580400697 -> 0.5412090568607423 on epoch=43, global_step=700
05/23/2022 13:44:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=44
05/23/2022 13:44:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=44
05/23/2022 13:44:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=45
05/23/2022 13:44:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=46
05/23/2022 13:44:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=46
05/23/2022 13:44:45 - INFO - __main__ - Global step 750 Train loss 0.56 Classification-F1 0.6059194462611729 on epoch=46
05/23/2022 13:44:45 - INFO - __main__ - Saving model with best Classification-F1: 0.5412090568607423 -> 0.6059194462611729 on epoch=46, global_step=750
05/23/2022 13:44:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=47
05/23/2022 13:44:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.55 on epoch=48
05/23/2022 13:44:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.57 on epoch=48
05/23/2022 13:44:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=49
05/23/2022 13:44:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=49
05/23/2022 13:45:01 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.6312722485660169 on epoch=49
05/23/2022 13:45:01 - INFO - __main__ - Saving model with best Classification-F1: 0.6059194462611729 -> 0.6312722485660169 on epoch=49, global_step=800
05/23/2022 13:45:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.59 on epoch=50
05/23/2022 13:45:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=51
05/23/2022 13:45:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=51
05/23/2022 13:45:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=52
05/23/2022 13:45:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=53
05/23/2022 13:45:17 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.5751134282183641 on epoch=53
05/23/2022 13:45:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=53
05/23/2022 13:45:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=54
05/23/2022 13:45:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.60 on epoch=54
05/23/2022 13:45:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=55
05/23/2022 13:45:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=56
05/23/2022 13:45:32 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.6663914588442891 on epoch=56
05/23/2022 13:45:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6312722485660169 -> 0.6663914588442891 on epoch=56, global_step=900
05/23/2022 13:45:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.61 on epoch=56
05/23/2022 13:45:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=57
05/23/2022 13:45:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=58
05/23/2022 13:45:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=58
05/23/2022 13:45:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
05/23/2022 13:45:48 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.640220658019614 on epoch=59
05/23/2022 13:45:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=59
05/23/2022 13:45:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=60
05/23/2022 13:45:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/23/2022 13:45:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=61
05/23/2022 13:46:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/23/2022 13:46:04 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.6220499630142327 on epoch=62
05/23/2022 13:46:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=63
05/23/2022 13:46:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=63
05/23/2022 13:46:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/23/2022 13:46:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=64
05/23/2022 13:46:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=65
05/23/2022 13:46:21 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.639307988531457 on epoch=65
05/23/2022 13:46:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=66
05/23/2022 13:46:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=66
05/23/2022 13:46:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/23/2022 13:46:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=68
05/23/2022 13:46:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=68
05/23/2022 13:46:37 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.6741904075009015 on epoch=68
05/23/2022 13:46:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6663914588442891 -> 0.6741904075009015 on epoch=68, global_step=1100
05/23/2022 13:46:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=69
05/23/2022 13:46:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=69
05/23/2022 13:46:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=70
05/23/2022 13:46:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/23/2022 13:46:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=71
05/23/2022 13:46:53 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.5950369413784049 on epoch=71
05/23/2022 13:46:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=72
05/23/2022 13:46:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=73
05/23/2022 13:47:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=73
05/23/2022 13:47:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
05/23/2022 13:47:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
05/23/2022 13:47:09 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.6406852806288869 on epoch=74
05/23/2022 13:47:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/23/2022 13:47:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=76
05/23/2022 13:47:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=76
05/23/2022 13:47:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=77
05/23/2022 13:47:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=78
05/23/2022 13:47:25 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.6937143188985875 on epoch=78
05/23/2022 13:47:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6741904075009015 -> 0.6937143188985875 on epoch=78, global_step=1250
05/23/2022 13:47:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/23/2022 13:47:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=79
05/23/2022 13:47:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
05/23/2022 13:47:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
05/23/2022 13:47:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=81
05/23/2022 13:47:41 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.7177397274699353 on epoch=81
05/23/2022 13:47:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6937143188985875 -> 0.7177397274699353 on epoch=81, global_step=1300
05/23/2022 13:47:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=81
05/23/2022 13:47:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=82
05/23/2022 13:47:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=83
05/23/2022 13:47:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
05/23/2022 13:47:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/23/2022 13:47:57 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.6717645951661297 on epoch=84
05/23/2022 13:47:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=84
05/23/2022 13:48:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.36 on epoch=85
05/23/2022 13:48:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
05/23/2022 13:48:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=86
05/23/2022 13:48:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/23/2022 13:48:13 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.6999180783312653 on epoch=87
05/23/2022 13:48:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.28 on epoch=88
05/23/2022 13:48:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=88
05/23/2022 13:48:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=89
05/23/2022 13:48:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=89
05/23/2022 13:48:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=90
05/23/2022 13:48:29 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.6956702571621927 on epoch=90
05/23/2022 13:48:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=91
05/23/2022 13:48:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=91
05/23/2022 13:48:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=92
05/23/2022 13:48:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=93
05/23/2022 13:48:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=93
05/23/2022 13:48:45 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.7090024781973663 on epoch=93
05/23/2022 13:48:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=94
05/23/2022 13:48:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=94
05/23/2022 13:48:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
05/23/2022 13:48:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
05/23/2022 13:48:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=96
05/23/2022 13:49:01 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.6975537375121852 on epoch=96
05/23/2022 13:49:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=97
05/23/2022 13:49:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=98
05/23/2022 13:49:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=98
05/23/2022 13:49:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=99
05/23/2022 13:49:13 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=99
05/23/2022 13:49:17 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.6502946633198734 on epoch=99
05/23/2022 13:49:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.31 on epoch=100
05/23/2022 13:49:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=101
05/23/2022 13:49:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=101
05/23/2022 13:49:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=102
05/23/2022 13:49:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/23/2022 13:49:33 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.7197708552908277 on epoch=103
05/23/2022 13:49:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7177397274699353 -> 0.7197708552908277 on epoch=103, global_step=1650
05/23/2022 13:49:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=103
05/23/2022 13:49:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=104
05/23/2022 13:49:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/23/2022 13:49:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=105
05/23/2022 13:49:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
05/23/2022 13:49:49 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.6915341387391077 on epoch=106
05/23/2022 13:49:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=106
05/23/2022 13:49:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/23/2022 13:49:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/23/2022 13:49:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=108
05/23/2022 13:50:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=109
05/23/2022 13:50:05 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.6487750135526905 on epoch=109
05/23/2022 13:50:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=109
05/23/2022 13:50:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=110
05/23/2022 13:50:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/23/2022 13:50:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=111
05/23/2022 13:50:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=112
05/23/2022 13:50:21 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.7004836166203572 on epoch=112
05/23/2022 13:50:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=113
05/23/2022 13:50:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=113
05/23/2022 13:50:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
05/23/2022 13:50:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/23/2022 13:50:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=115
05/23/2022 13:50:37 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.7005364679660455 on epoch=115
05/23/2022 13:50:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=116
05/23/2022 13:50:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
05/23/2022 13:50:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/23/2022 13:50:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=118
05/23/2022 13:50:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/23/2022 13:50:53 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.7059554300124573 on epoch=118
05/23/2022 13:50:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=119
05/23/2022 13:50:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/23/2022 13:51:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=120
05/23/2022 13:51:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=121
05/23/2022 13:51:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/23/2022 13:51:09 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.717413655881226 on epoch=121
05/23/2022 13:51:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/23/2022 13:51:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=123
05/23/2022 13:51:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/23/2022 13:51:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=124
05/23/2022 13:51:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=124
05/23/2022 13:51:25 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.696067125957157 on epoch=124
05/23/2022 13:51:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
05/23/2022 13:51:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=126
05/23/2022 13:51:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/23/2022 13:51:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=127
05/23/2022 13:51:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=128
05/23/2022 13:51:41 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.699277675981447 on epoch=128
05/23/2022 13:51:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
05/23/2022 13:51:46 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
05/23/2022 13:51:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=129
05/23/2022 13:51:51 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
05/23/2022 13:51:53 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=131
05/23/2022 13:51:57 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.7217474353158365 on epoch=131
05/23/2022 13:51:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7197708552908277 -> 0.7217474353158365 on epoch=131, global_step=2100
05/23/2022 13:51:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=131
05/23/2022 13:52:02 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
05/23/2022 13:52:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
05/23/2022 13:52:07 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
05/23/2022 13:52:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
05/23/2022 13:52:13 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.6900663346211398 on epoch=134
05/23/2022 13:52:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/23/2022 13:52:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/23/2022 13:52:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=136
05/23/2022 13:52:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=136
05/23/2022 13:52:25 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
05/23/2022 13:52:29 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.7355447020682574 on epoch=137
05/23/2022 13:52:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7217474353158365 -> 0.7355447020682574 on epoch=137, global_step=2200
05/23/2022 13:52:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=138
05/23/2022 13:52:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
05/23/2022 13:52:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/23/2022 13:52:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
05/23/2022 13:52:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=140
05/23/2022 13:52:45 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.7252782779588126 on epoch=140
05/23/2022 13:52:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=141
05/23/2022 13:52:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/23/2022 13:52:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/23/2022 13:52:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=143
05/23/2022 13:52:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
05/23/2022 13:53:01 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.6889119063299033 on epoch=143
05/23/2022 13:53:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
05/23/2022 13:53:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=144
05/23/2022 13:53:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/23/2022 13:53:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
05/23/2022 13:53:13 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
05/23/2022 13:53:17 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.6887604319622311 on epoch=146
05/23/2022 13:53:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=147
05/23/2022 13:53:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=148
05/23/2022 13:53:24 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=148
05/23/2022 13:53:27 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/23/2022 13:53:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
05/23/2022 13:53:33 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.7017550523977167 on epoch=149
05/23/2022 13:53:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=150
05/23/2022 13:53:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/23/2022 13:53:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=151
05/23/2022 13:53:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=152
05/23/2022 13:53:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
05/23/2022 13:53:49 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.71355764304704 on epoch=153
05/23/2022 13:53:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/23/2022 13:53:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/23/2022 13:53:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
05/23/2022 13:53:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
05/23/2022 13:54:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/23/2022 13:54:05 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.6635525001468946 on epoch=156
05/23/2022 13:54:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=156
05/23/2022 13:54:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
05/23/2022 13:54:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/23/2022 13:54:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/23/2022 13:54:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
05/23/2022 13:54:21 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.6649264897215718 on epoch=159
05/23/2022 13:54:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
05/23/2022 13:54:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/23/2022 13:54:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/23/2022 13:54:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/23/2022 13:54:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=162
05/23/2022 13:54:37 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.7079865305913787 on epoch=162
05/23/2022 13:54:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
05/23/2022 13:54:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
05/23/2022 13:54:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/23/2022 13:54:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
05/23/2022 13:54:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
05/23/2022 13:54:53 - INFO - __main__ - Global step 2650 Train loss 0.09 Classification-F1 0.7281451334758916 on epoch=165
05/23/2022 13:54:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=166
05/23/2022 13:54:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=166
05/23/2022 13:55:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/23/2022 13:55:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/23/2022 13:55:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
05/23/2022 13:55:09 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.6826659680547864 on epoch=168
05/23/2022 13:55:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/23/2022 13:55:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=169
05/23/2022 13:55:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=170
05/23/2022 13:55:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/23/2022 13:55:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
05/23/2022 13:55:25 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.6954138385053689 on epoch=171
05/23/2022 13:55:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
05/23/2022 13:55:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/23/2022 13:55:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/23/2022 13:55:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=174
05/23/2022 13:55:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/23/2022 13:55:41 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.6506503856148531 on epoch=174
05/23/2022 13:55:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/23/2022 13:55:46 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/23/2022 13:55:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/23/2022 13:55:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/23/2022 13:55:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/23/2022 13:55:57 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.640676925528456 on epoch=178
05/23/2022 13:55:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/23/2022 13:56:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
05/23/2022 13:56:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/23/2022 13:56:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
05/23/2022 13:56:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/23/2022 13:56:13 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7094997767572394 on epoch=181
05/23/2022 13:56:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/23/2022 13:56:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/23/2022 13:56:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=183
05/23/2022 13:56:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/23/2022 13:56:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
05/23/2022 13:56:29 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.6973409268743775 on epoch=184
05/23/2022 13:56:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
05/23/2022 13:56:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
05/23/2022 13:56:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
05/23/2022 13:56:39 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=186
05/23/2022 13:56:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/23/2022 13:56:43 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:56:43 - INFO - __main__ - Printing 3 examples
05/23/2022 13:56:43 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:56:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:56:43 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:56:43 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:56:43 - INFO - __main__ - Printing 3 examples
05/23/2022 13:56:43 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 13:56:43 - INFO - __main__ - ['happy']
05/23/2022 13:56:43 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:56:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:56:43 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:56:45 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.6956752806642937 on epoch=187
05/23/2022 13:56:45 - INFO - __main__ - save last model!
05/23/2022 13:56:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 13:56:45 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 13:56:45 - INFO - __main__ - Printing 3 examples
05/23/2022 13:56:45 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 13:56:45 - INFO - __main__ - ['others']
05/23/2022 13:56:45 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 13:56:45 - INFO - __main__ - ['others']
05/23/2022 13:56:45 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 13:56:45 - INFO - __main__ - ['others']
05/23/2022 13:56:45 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:56:47 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:56:52 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 13:57:02 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:57:02 - INFO - __main__ - task name: emo
05/23/2022 13:57:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:57:03 - INFO - __main__ - Starting training!
05/23/2022 13:58:05 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_21_0.2_8_predictions.txt
05/23/2022 13:58:05 - INFO - __main__ - Classification-F1 on test data: 0.4247
05/23/2022 13:58:06 - INFO - __main__ - prefix=emo_64_21, lr=0.2, bsz=8, dev_performance=0.7355447020682574, test_performance=0.4246703787148911
05/23/2022 13:58:06 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.5, bsz=8 ...
05/23/2022 13:58:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:58:07 - INFO - __main__ - Printing 3 examples
05/23/2022 13:58:07 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:58:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:58:07 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 13:58:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 13:58:07 - INFO - __main__ - Printing 3 examples
05/23/2022 13:58:07 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 13:58:07 - INFO - __main__ - ['happy']
05/23/2022 13:58:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 13:58:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 13:58:07 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 13:58:23 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 13:58:23 - INFO - __main__ - task name: emo
05/23/2022 13:58:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 13:58:24 - INFO - __main__ - Starting training!
05/23/2022 13:58:26 - INFO - __main__ - Step 10 Global step 10 Train loss 6.11 on epoch=0
05/23/2022 13:58:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.20 on epoch=1
05/23/2022 13:58:31 - INFO - __main__ - Step 30 Global step 30 Train loss 1.22 on epoch=1
05/23/2022 13:58:34 - INFO - __main__ - Step 40 Global step 40 Train loss 1.15 on epoch=2
05/23/2022 13:58:36 - INFO - __main__ - Step 50 Global step 50 Train loss 1.06 on epoch=3
05/23/2022 13:58:40 - INFO - __main__ - Global step 50 Train loss 2.35 Classification-F1 0.12174704276615103 on epoch=3
05/23/2022 13:58:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12174704276615103 on epoch=3, global_step=50
05/23/2022 13:58:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=3
05/23/2022 13:58:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.91 on epoch=4
05/23/2022 13:58:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.97 on epoch=4
05/23/2022 13:58:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=5
05/23/2022 13:58:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=6
05/23/2022 13:58:56 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.2953669467787115 on epoch=6
05/23/2022 13:58:56 - INFO - __main__ - Saving model with best Classification-F1: 0.12174704276615103 -> 0.2953669467787115 on epoch=6, global_step=100
05/23/2022 13:58:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=6
05/23/2022 13:59:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=7
05/23/2022 13:59:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=8
05/23/2022 13:59:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.82 on epoch=8
05/23/2022 13:59:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=9
05/23/2022 13:59:11 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.1 on epoch=9
05/23/2022 13:59:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.90 on epoch=9
05/23/2022 13:59:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=10
05/23/2022 13:59:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=11
05/23/2022 13:59:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=11
05/23/2022 13:59:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=12
05/23/2022 13:59:27 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.11993498200394753 on epoch=12
05/23/2022 13:59:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=13
05/23/2022 13:59:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=13
05/23/2022 13:59:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=14
05/23/2022 13:59:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.84 on epoch=14
05/23/2022 13:59:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.89 on epoch=15
05/23/2022 13:59:43 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.12421444370326158 on epoch=15
05/23/2022 13:59:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.81 on epoch=16
05/23/2022 13:59:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.83 on epoch=16
05/23/2022 13:59:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=17
05/23/2022 13:59:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.82 on epoch=18
05/23/2022 13:59:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=18
05/23/2022 13:59:59 - INFO - __main__ - Global step 300 Train loss 0.81 Classification-F1 0.17002962907399766 on epoch=18
05/23/2022 14:00:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=19
05/23/2022 14:00:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=19
05/23/2022 14:00:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.75 on epoch=20
05/23/2022 14:00:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.80 on epoch=21
05/23/2022 14:00:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.77 on epoch=21
05/23/2022 14:00:15 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.2939576539331741 on epoch=21
05/23/2022 14:00:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=22
05/23/2022 14:00:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.72 on epoch=23
05/23/2022 14:00:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=23
05/23/2022 14:00:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.75 on epoch=24
05/23/2022 14:00:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.77 on epoch=24
05/23/2022 14:00:31 - INFO - __main__ - Global step 400 Train loss 0.75 Classification-F1 0.27185163378006155 on epoch=24
05/23/2022 14:00:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.67 on epoch=25
05/23/2022 14:00:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.76 on epoch=26
05/23/2022 14:00:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.74 on epoch=26
05/23/2022 14:00:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.70 on epoch=27
05/23/2022 14:00:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.75 on epoch=28
05/23/2022 14:00:46 - INFO - __main__ - Global step 450 Train loss 0.72 Classification-F1 0.26148549174864966 on epoch=28
05/23/2022 14:00:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=28
05/23/2022 14:00:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=29
05/23/2022 14:00:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.73 on epoch=29
05/23/2022 14:00:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=30
05/23/2022 14:00:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=31
05/23/2022 14:01:02 - INFO - __main__ - Global step 500 Train loss 0.70 Classification-F1 0.49225034879475393 on epoch=31
05/23/2022 14:01:02 - INFO - __main__ - Saving model with best Classification-F1: 0.2953669467787115 -> 0.49225034879475393 on epoch=31, global_step=500
05/23/2022 14:01:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.63 on epoch=31
05/23/2022 14:01:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=32
05/23/2022 14:01:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=33
05/23/2022 14:01:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=33
05/23/2022 14:01:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=34
05/23/2022 14:01:18 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.6178055045962446 on epoch=34
05/23/2022 14:01:18 - INFO - __main__ - Saving model with best Classification-F1: 0.49225034879475393 -> 0.6178055045962446 on epoch=34, global_step=550
05/23/2022 14:01:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=34
05/23/2022 14:01:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=35
05/23/2022 14:01:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=36
05/23/2022 14:01:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=36
05/23/2022 14:01:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=37
05/23/2022 14:01:34 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.655562930730716 on epoch=37
05/23/2022 14:01:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6178055045962446 -> 0.655562930730716 on epoch=37, global_step=600
05/23/2022 14:01:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=38
05/23/2022 14:01:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=38
05/23/2022 14:01:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=39
05/23/2022 14:01:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=39
05/23/2022 14:01:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
05/23/2022 14:01:50 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.6514698264698265 on epoch=40
05/23/2022 14:01:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=41
05/23/2022 14:01:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=41
05/23/2022 14:01:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=42
05/23/2022 14:02:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/23/2022 14:02:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
05/23/2022 14:02:06 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.7076210147278919 on epoch=43
05/23/2022 14:02:06 - INFO - __main__ - Saving model with best Classification-F1: 0.655562930730716 -> 0.7076210147278919 on epoch=43, global_step=700
05/23/2022 14:02:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
05/23/2022 14:02:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=44
05/23/2022 14:02:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=45
05/23/2022 14:02:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
05/23/2022 14:02:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/23/2022 14:02:22 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.7249502423795293 on epoch=46
05/23/2022 14:02:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7076210147278919 -> 0.7249502423795293 on epoch=46, global_step=750
05/23/2022 14:02:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=47
05/23/2022 14:02:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=48
05/23/2022 14:02:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=48
05/23/2022 14:02:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=49
05/23/2022 14:02:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=49
05/23/2022 14:02:38 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.77903044561597 on epoch=49
05/23/2022 14:02:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7249502423795293 -> 0.77903044561597 on epoch=49, global_step=800
05/23/2022 14:02:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=50
05/23/2022 14:02:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=51
05/23/2022 14:02:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=51
05/23/2022 14:02:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=52
05/23/2022 14:02:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/23/2022 14:02:55 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.7353573373482398 on epoch=53
05/23/2022 14:02:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=53
05/23/2022 14:03:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/23/2022 14:03:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
05/23/2022 14:03:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=55
05/23/2022 14:03:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=56
05/23/2022 14:03:11 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.7969184324447482 on epoch=56
05/23/2022 14:03:11 - INFO - __main__ - Saving model with best Classification-F1: 0.77903044561597 -> 0.7969184324447482 on epoch=56, global_step=900
05/23/2022 14:03:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=56
05/23/2022 14:03:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=57
05/23/2022 14:03:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=58
05/23/2022 14:03:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=58
05/23/2022 14:03:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=59
05/23/2022 14:03:27 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.7989298168379347 on epoch=59
05/23/2022 14:03:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7969184324447482 -> 0.7989298168379347 on epoch=59, global_step=950
05/23/2022 14:03:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=59
05/23/2022 14:03:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=60
05/23/2022 14:03:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=61
05/23/2022 14:03:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/23/2022 14:03:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=62
05/23/2022 14:03:43 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.762444318966058 on epoch=62
05/23/2022 14:03:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/23/2022 14:03:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.14 on epoch=63
05/23/2022 14:03:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=64
05/23/2022 14:03:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=64
05/23/2022 14:03:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.13 on epoch=65
05/23/2022 14:04:00 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.7613043038015284 on epoch=65
05/23/2022 14:04:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.10 on epoch=66
05/23/2022 14:04:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.13 on epoch=66
05/23/2022 14:04:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
05/23/2022 14:04:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=68
05/23/2022 14:04:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=68
05/23/2022 14:04:17 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.7376012122716648 on epoch=68
05/23/2022 14:04:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
05/23/2022 14:04:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=69
05/23/2022 14:04:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=70
05/23/2022 14:04:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=71
05/23/2022 14:04:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=71
05/23/2022 14:04:33 - INFO - __main__ - Global step 1150 Train loss 0.14 Classification-F1 0.7586400447644929 on epoch=71
05/23/2022 14:04:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=72
05/23/2022 14:04:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=73
05/23/2022 14:04:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.12 on epoch=73
05/23/2022 14:04:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=74
05/23/2022 14:04:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=74
05/23/2022 14:04:50 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.7671926133707341 on epoch=74
05/23/2022 14:04:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=75
05/23/2022 14:04:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=76
05/23/2022 14:04:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=76
05/23/2022 14:05:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.08 on epoch=77
05/23/2022 14:05:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=78
05/23/2022 14:05:06 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.7228635433264892 on epoch=78
05/23/2022 14:05:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.09 on epoch=78
05/23/2022 14:05:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=79
05/23/2022 14:05:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=79
05/23/2022 14:05:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=80
05/23/2022 14:05:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=81
05/23/2022 14:05:23 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.7581089864581766 on epoch=81
05/23/2022 14:05:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=81
05/23/2022 14:05:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=82
05/23/2022 14:05:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=83
05/23/2022 14:05:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=83
05/23/2022 14:05:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=84
05/23/2022 14:05:39 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.7640981425955788 on epoch=84
05/23/2022 14:05:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=84
05/23/2022 14:05:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=85
05/23/2022 14:05:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=86
05/23/2022 14:05:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=86
05/23/2022 14:05:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=87
05/23/2022 14:05:56 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.7544207162231811 on epoch=87
05/23/2022 14:05:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=88
05/23/2022 14:06:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=88
05/23/2022 14:06:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=89
05/23/2022 14:06:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=89
05/23/2022 14:06:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=90
05/23/2022 14:06:12 - INFO - __main__ - Global step 1450 Train loss 0.09 Classification-F1 0.7645680770468812 on epoch=90
05/23/2022 14:06:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=91
05/23/2022 14:06:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=91
05/23/2022 14:06:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=92
05/23/2022 14:06:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=93
05/23/2022 14:06:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=93
05/23/2022 14:06:29 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7597491455282583 on epoch=93
05/23/2022 14:06:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=94
05/23/2022 14:06:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=94
05/23/2022 14:06:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=95
05/23/2022 14:06:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=96
05/23/2022 14:06:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=96
05/23/2022 14:06:46 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.7435954643671218 on epoch=96
05/23/2022 14:06:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=97
05/23/2022 14:06:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=98
05/23/2022 14:06:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=98
05/23/2022 14:06:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=99
05/23/2022 14:06:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=99
05/23/2022 14:07:02 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7650237690084543 on epoch=99
05/23/2022 14:07:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=100
05/23/2022 14:07:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=101
05/23/2022 14:07:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=101
05/23/2022 14:07:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=102
05/23/2022 14:07:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=103
05/23/2022 14:07:19 - INFO - __main__ - Global step 1650 Train loss 0.06 Classification-F1 0.7640511836523943 on epoch=103
05/23/2022 14:07:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=103
05/23/2022 14:07:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=104
05/23/2022 14:07:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=104
05/23/2022 14:07:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=105
05/23/2022 14:07:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=106
05/23/2022 14:07:36 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.7596752223238437 on epoch=106
05/23/2022 14:07:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=106
05/23/2022 14:07:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=107
05/23/2022 14:07:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=108
05/23/2022 14:07:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=108
05/23/2022 14:07:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=109
05/23/2022 14:07:52 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7730743319110711 on epoch=109
05/23/2022 14:07:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=109
05/23/2022 14:07:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=110
05/23/2022 14:08:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=111
05/23/2022 14:08:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=111
05/23/2022 14:08:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=112
05/23/2022 14:08:09 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7613323490088573 on epoch=112
05/23/2022 14:08:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=113
05/23/2022 14:08:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=113
05/23/2022 14:08:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=114
05/23/2022 14:08:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/23/2022 14:08:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=115
05/23/2022 14:08:25 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.751686699420951 on epoch=115
05/23/2022 14:08:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=116
05/23/2022 14:08:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=116
05/23/2022 14:08:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=117
05/23/2022 14:08:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=118
05/23/2022 14:08:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=118
05/23/2022 14:08:42 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7577328232588976 on epoch=118
05/23/2022 14:08:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=119
05/23/2022 14:08:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=119
05/23/2022 14:08:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=120
05/23/2022 14:08:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=121
05/23/2022 14:08:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/23/2022 14:08:59 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7738735781093026 on epoch=121
05/23/2022 14:09:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=122
05/23/2022 14:09:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=123
05/23/2022 14:09:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=123
05/23/2022 14:09:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=124
05/23/2022 14:09:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=124
05/23/2022 14:09:15 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7664365409475065 on epoch=124
05/23/2022 14:09:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
05/23/2022 14:09:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=126
05/23/2022 14:09:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.02 on epoch=126
05/23/2022 14:09:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=127
05/23/2022 14:09:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
05/23/2022 14:09:32 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7556373143484898 on epoch=128
05/23/2022 14:09:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=128
05/23/2022 14:09:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=129
05/23/2022 14:09:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=129
05/23/2022 14:09:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=130
05/23/2022 14:09:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/23/2022 14:09:48 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7736668164387588 on epoch=131
05/23/2022 14:09:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=131
05/23/2022 14:09:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=132
05/23/2022 14:09:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=133
05/23/2022 14:09:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=133
05/23/2022 14:10:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=134
05/23/2022 14:10:05 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.763524011299435 on epoch=134
05/23/2022 14:10:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=134
05/23/2022 14:10:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=135
05/23/2022 14:10:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.07 on epoch=136
05/23/2022 14:10:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.06 on epoch=136
05/23/2022 14:10:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=137
05/23/2022 14:10:21 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7870833064091025 on epoch=137
05/23/2022 14:10:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.02 on epoch=138
05/23/2022 14:10:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=138
05/23/2022 14:10:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=139
05/23/2022 14:10:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=139
05/23/2022 14:10:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=140
05/23/2022 14:10:38 - INFO - __main__ - Global step 2250 Train loss 0.02 Classification-F1 0.7859418286675266 on epoch=140
05/23/2022 14:10:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=141
05/23/2022 14:10:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=141
05/23/2022 14:10:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/23/2022 14:10:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/23/2022 14:10:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=143
05/23/2022 14:10:54 - INFO - __main__ - Global step 2300 Train loss 0.03 Classification-F1 0.7664592525313595 on epoch=143
05/23/2022 14:10:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/23/2022 14:10:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/23/2022 14:11:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=145
05/23/2022 14:11:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=146
05/23/2022 14:11:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=146
05/23/2022 14:11:11 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7574072797858505 on epoch=146
05/23/2022 14:11:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=147
05/23/2022 14:11:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
05/23/2022 14:11:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=148
05/23/2022 14:11:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=149
05/23/2022 14:11:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=149
05/23/2022 14:11:28 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.7546315074996484 on epoch=149
05/23/2022 14:11:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=150
05/23/2022 14:11:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/23/2022 14:11:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=151
05/23/2022 14:11:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/23/2022 14:11:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=153
05/23/2022 14:11:45 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7762807671168144 on epoch=153
05/23/2022 14:11:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=153
05/23/2022 14:11:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=154
05/23/2022 14:11:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=154
05/23/2022 14:11:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=155
05/23/2022 14:11:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=156
05/23/2022 14:12:02 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.7762772924133811 on epoch=156
05/23/2022 14:12:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=156
05/23/2022 14:12:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
05/23/2022 14:12:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.00 on epoch=158
05/23/2022 14:12:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/23/2022 14:12:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.00 on epoch=159
05/23/2022 14:12:19 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.7597449167320109 on epoch=159
05/23/2022 14:12:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/23/2022 14:12:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/23/2022 14:12:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=161
05/23/2022 14:12:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/23/2022 14:12:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
05/23/2022 14:12:36 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.762277152216576 on epoch=162
05/23/2022 14:12:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=163
05/23/2022 14:12:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=163
05/23/2022 14:12:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=164
05/23/2022 14:12:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.01 on epoch=164
05/23/2022 14:12:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=165
05/23/2022 14:12:53 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7658806531253507 on epoch=165
05/23/2022 14:12:56 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/23/2022 14:12:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=166
05/23/2022 14:13:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=167
05/23/2022 14:13:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=168
05/23/2022 14:13:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.01 on epoch=168
05/23/2022 14:13:10 - INFO - __main__ - Global step 2700 Train loss 0.01 Classification-F1 0.7739686724565757 on epoch=168
05/23/2022 14:13:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/23/2022 14:13:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=169
05/23/2022 14:13:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=170
05/23/2022 14:13:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.00 on epoch=171
05/23/2022 14:13:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.00 on epoch=171
05/23/2022 14:13:27 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.7683423337861168 on epoch=171
05/23/2022 14:13:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/23/2022 14:13:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=173
05/23/2022 14:13:35 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.00 on epoch=173
05/23/2022 14:13:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=174
05/23/2022 14:13:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=174
05/23/2022 14:13:44 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7771627686699922 on epoch=174
05/23/2022 14:13:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.00 on epoch=175
05/23/2022 14:13:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
05/23/2022 14:13:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=176
05/23/2022 14:13:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.00 on epoch=177
05/23/2022 14:13:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
05/23/2022 14:14:01 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.7359825239744403 on epoch=178
05/23/2022 14:14:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=178
05/23/2022 14:14:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=179
05/23/2022 14:14:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.00 on epoch=179
05/23/2022 14:14:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=180
05/23/2022 14:14:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
05/23/2022 14:14:18 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.7512731705530981 on epoch=181
05/23/2022 14:14:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/23/2022 14:14:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/23/2022 14:14:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/23/2022 14:14:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.00 on epoch=183
05/23/2022 14:14:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 14:14:36 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.7434080956915149 on epoch=184
05/23/2022 14:14:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/23/2022 14:14:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=185
05/23/2022 14:14:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
05/23/2022 14:14:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.00 on epoch=186
05/23/2022 14:14:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/23/2022 14:14:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:14:49 - INFO - __main__ - Printing 3 examples
05/23/2022 14:14:49 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:14:49 - INFO - __main__ - ['happy']
05/23/2022 14:14:49 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:14:49 - INFO - __main__ - ['happy']
05/23/2022 14:14:49 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:14:49 - INFO - __main__ - ['happy']
05/23/2022 14:14:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:14:49 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:14:50 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:14:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:14:50 - INFO - __main__ - Printing 3 examples
05/23/2022 14:14:50 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:14:50 - INFO - __main__ - ['happy']
05/23/2022 14:14:50 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:14:50 - INFO - __main__ - ['happy']
05/23/2022 14:14:50 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:14:50 - INFO - __main__ - ['happy']
05/23/2022 14:14:50 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:14:50 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:14:50 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:14:52 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7769722098691152 on epoch=187
05/23/2022 14:14:52 - INFO - __main__ - save last model!
05/23/2022 14:14:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 14:14:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 14:14:52 - INFO - __main__ - Printing 3 examples
05/23/2022 14:14:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 14:14:52 - INFO - __main__ - ['others']
05/23/2022 14:14:52 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 14:14:52 - INFO - __main__ - ['others']
05/23/2022 14:14:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 14:14:52 - INFO - __main__ - ['others']
05/23/2022 14:14:52 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:14:55 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:15:00 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 14:15:09 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:15:09 - INFO - __main__ - task name: emo
05/23/2022 14:15:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:15:10 - INFO - __main__ - Starting training!
05/23/2022 14:16:33 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_42_0.5_8_predictions.txt
05/23/2022 14:16:33 - INFO - __main__ - Classification-F1 on test data: 0.4293
05/23/2022 14:16:33 - INFO - __main__ - prefix=emo_64_42, lr=0.5, bsz=8, dev_performance=0.7989298168379347, test_performance=0.4292896895118291
05/23/2022 14:16:33 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.4, bsz=8 ...
05/23/2022 14:16:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:16:34 - INFO - __main__ - Printing 3 examples
05/23/2022 14:16:34 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:16:34 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:16:34 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:16:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:16:34 - INFO - __main__ - Printing 3 examples
05/23/2022 14:16:34 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:16:34 - INFO - __main__ - ['happy']
05/23/2022 14:16:34 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:16:35 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:16:35 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:16:50 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:16:50 - INFO - __main__ - task name: emo
05/23/2022 14:16:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:16:51 - INFO - __main__ - Starting training!
05/23/2022 14:16:54 - INFO - __main__ - Step 10 Global step 10 Train loss 6.75 on epoch=0
05/23/2022 14:16:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=1
05/23/2022 14:16:59 - INFO - __main__ - Step 30 Global step 30 Train loss 1.62 on epoch=1
05/23/2022 14:17:01 - INFO - __main__ - Step 40 Global step 40 Train loss 1.21 on epoch=2
05/23/2022 14:17:04 - INFO - __main__ - Step 50 Global step 50 Train loss 1.11 on epoch=3
05/23/2022 14:17:07 - INFO - __main__ - Global step 50 Train loss 2.75 Classification-F1 0.1 on epoch=3
05/23/2022 14:17:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=3, global_step=50
05/23/2022 14:17:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=3
05/23/2022 14:17:12 - INFO - __main__ - Step 70 Global step 70 Train loss 1.03 on epoch=4
05/23/2022 14:17:15 - INFO - __main__ - Step 80 Global step 80 Train loss 1.07 on epoch=4
05/23/2022 14:17:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=5
05/23/2022 14:17:20 - INFO - __main__ - Step 100 Global step 100 Train loss 1.01 on epoch=6
05/23/2022 14:17:23 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.1411619632893992 on epoch=6
05/23/2022 14:17:23 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.1411619632893992 on epoch=6, global_step=100
05/23/2022 14:17:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=6
05/23/2022 14:17:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=7
05/23/2022 14:17:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.97 on epoch=8
05/23/2022 14:17:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=8
05/23/2022 14:17:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=9
05/23/2022 14:17:39 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.09493670886075949 on epoch=9
05/23/2022 14:17:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=9
05/23/2022 14:17:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=10
05/23/2022 14:17:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/23/2022 14:17:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.82 on epoch=11
05/23/2022 14:17:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=12
05/23/2022 14:17:55 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.12761056511056512 on epoch=12
05/23/2022 14:17:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.89 on epoch=13
05/23/2022 14:18:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.92 on epoch=13
05/23/2022 14:18:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.80 on epoch=14
05/23/2022 14:18:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.94 on epoch=14
05/23/2022 14:18:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
05/23/2022 14:18:11 - INFO - __main__ - Global step 250 Train loss 0.87 Classification-F1 0.10062893081761007 on epoch=15
05/23/2022 14:18:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=16
05/23/2022 14:18:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/23/2022 14:18:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.77 on epoch=17
05/23/2022 14:18:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.86 on epoch=18
05/23/2022 14:18:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.86 on epoch=18
05/23/2022 14:18:27 - INFO - __main__ - Global step 300 Train loss 0.84 Classification-F1 0.1084090588069118 on epoch=18
05/23/2022 14:18:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.88 on epoch=19
05/23/2022 14:18:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=19
05/23/2022 14:18:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=20
05/23/2022 14:18:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=21
05/23/2022 14:18:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.78 on epoch=21
05/23/2022 14:18:43 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.17372864436268218 on epoch=21
05/23/2022 14:18:43 - INFO - __main__ - Saving model with best Classification-F1: 0.1411619632893992 -> 0.17372864436268218 on epoch=21, global_step=350
05/23/2022 14:18:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.83 on epoch=22
05/23/2022 14:18:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.80 on epoch=23
05/23/2022 14:18:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.74 on epoch=23
05/23/2022 14:18:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=24
05/23/2022 14:18:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=24
05/23/2022 14:18:59 - INFO - __main__ - Global step 400 Train loss 0.81 Classification-F1 0.1886185347693126 on epoch=24
05/23/2022 14:18:59 - INFO - __main__ - Saving model with best Classification-F1: 0.17372864436268218 -> 0.1886185347693126 on epoch=24, global_step=400
05/23/2022 14:19:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=25
05/23/2022 14:19:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=26
05/23/2022 14:19:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=26
05/23/2022 14:19:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.75 on epoch=27
05/23/2022 14:19:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.78 on epoch=28
05/23/2022 14:19:14 - INFO - __main__ - Global step 450 Train loss 0.76 Classification-F1 0.1632659760723665 on epoch=28
05/23/2022 14:19:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.76 on epoch=28
05/23/2022 14:19:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.74 on epoch=29
05/23/2022 14:19:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=29
05/23/2022 14:19:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.82 on epoch=30
05/23/2022 14:19:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=31
05/23/2022 14:19:30 - INFO - __main__ - Global step 500 Train loss 0.78 Classification-F1 0.36792176710405666 on epoch=31
05/23/2022 14:19:30 - INFO - __main__ - Saving model with best Classification-F1: 0.1886185347693126 -> 0.36792176710405666 on epoch=31, global_step=500
05/23/2022 14:19:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=31
05/23/2022 14:19:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=32
05/23/2022 14:19:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.76 on epoch=33
05/23/2022 14:19:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=33
05/23/2022 14:19:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=34
05/23/2022 14:19:46 - INFO - __main__ - Global step 550 Train loss 0.72 Classification-F1 0.3819962864684805 on epoch=34
05/23/2022 14:19:46 - INFO - __main__ - Saving model with best Classification-F1: 0.36792176710405666 -> 0.3819962864684805 on epoch=34, global_step=550
05/23/2022 14:19:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.81 on epoch=34
05/23/2022 14:19:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.70 on epoch=35
05/23/2022 14:19:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.67 on epoch=36
05/23/2022 14:19:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.70 on epoch=36
05/23/2022 14:19:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.71 on epoch=37
05/23/2022 14:20:02 - INFO - __main__ - Global step 600 Train loss 0.72 Classification-F1 0.5053436659525334 on epoch=37
05/23/2022 14:20:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3819962864684805 -> 0.5053436659525334 on epoch=37, global_step=600
05/23/2022 14:20:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.68 on epoch=38
05/23/2022 14:20:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.64 on epoch=38
05/23/2022 14:20:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=39
05/23/2022 14:20:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.69 on epoch=39
05/23/2022 14:20:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.61 on epoch=40
05/23/2022 14:20:18 - INFO - __main__ - Global step 650 Train loss 0.64 Classification-F1 0.6010523229277162 on epoch=40
05/23/2022 14:20:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5053436659525334 -> 0.6010523229277162 on epoch=40, global_step=650
05/23/2022 14:20:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=41
05/23/2022 14:20:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=41
05/23/2022 14:20:25 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=42
05/23/2022 14:20:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=43
05/23/2022 14:20:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=43
05/23/2022 14:20:34 - INFO - __main__ - Global step 700 Train loss 0.58 Classification-F1 0.3848499233535019 on epoch=43
05/23/2022 14:20:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=44
05/23/2022 14:20:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=44
05/23/2022 14:20:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=45
05/23/2022 14:20:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=46
05/23/2022 14:20:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=46
05/23/2022 14:20:49 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.5796495865807727 on epoch=46
05/23/2022 14:20:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.58 on epoch=47
05/23/2022 14:20:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=48
05/23/2022 14:20:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=48
05/23/2022 14:21:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=49
05/23/2022 14:21:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=49
05/23/2022 14:21:06 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.7048058954005222 on epoch=49
05/23/2022 14:21:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6010523229277162 -> 0.7048058954005222 on epoch=49, global_step=800
05/23/2022 14:21:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=50
05/23/2022 14:21:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=51
05/23/2022 14:21:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=51
05/23/2022 14:21:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
05/23/2022 14:21:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=53
05/23/2022 14:21:22 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.6541258759284743 on epoch=53
05/23/2022 14:21:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=53
05/23/2022 14:21:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
05/23/2022 14:21:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=54
05/23/2022 14:21:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=55
05/23/2022 14:21:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
05/23/2022 14:21:38 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.6884118954543595 on epoch=56
05/23/2022 14:21:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=56
05/23/2022 14:21:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
05/23/2022 14:21:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=58
05/23/2022 14:21:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=58
05/23/2022 14:21:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
05/23/2022 14:21:54 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.5642598576270125 on epoch=59
05/23/2022 14:21:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
05/23/2022 14:21:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
05/23/2022 14:22:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/23/2022 14:22:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
05/23/2022 14:22:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=62
05/23/2022 14:22:10 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.6632868089764642 on epoch=62
05/23/2022 14:22:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
05/23/2022 14:22:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
05/23/2022 14:22:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
05/23/2022 14:22:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
05/23/2022 14:22:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=65
05/23/2022 14:22:26 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.7156184122788805 on epoch=65
05/23/2022 14:22:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7048058954005222 -> 0.7156184122788805 on epoch=65, global_step=1050
05/23/2022 14:22:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=66
05/23/2022 14:22:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=66
05/23/2022 14:22:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
05/23/2022 14:22:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=68
05/23/2022 14:22:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/23/2022 14:22:42 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.5513142441879327 on epoch=68
05/23/2022 14:22:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=69
05/23/2022 14:22:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=69
05/23/2022 14:22:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=70
05/23/2022 14:22:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
05/23/2022 14:22:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=71
05/23/2022 14:22:58 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.7247764151740778 on epoch=71
05/23/2022 14:22:58 - INFO - __main__ - Saving model with best Classification-F1: 0.7156184122788805 -> 0.7247764151740778 on epoch=71, global_step=1150
05/23/2022 14:23:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=72
05/23/2022 14:23:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
05/23/2022 14:23:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/23/2022 14:23:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=74
05/23/2022 14:23:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=74
05/23/2022 14:23:14 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.7226172250896685 on epoch=74
05/23/2022 14:23:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=75
05/23/2022 14:23:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=76
05/23/2022 14:23:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
05/23/2022 14:23:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/23/2022 14:23:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
05/23/2022 14:23:30 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.7308211612715618 on epoch=78
05/23/2022 14:23:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7247764151740778 -> 0.7308211612715618 on epoch=78, global_step=1250
05/23/2022 14:23:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/23/2022 14:23:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=79
05/23/2022 14:23:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=79
05/23/2022 14:23:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/23/2022 14:23:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=81
05/23/2022 14:23:46 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.7438681404440843 on epoch=81
05/23/2022 14:23:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7308211612715618 -> 0.7438681404440843 on epoch=81, global_step=1300
05/23/2022 14:23:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=81
05/23/2022 14:23:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/23/2022 14:23:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/23/2022 14:23:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=83
05/23/2022 14:23:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/23/2022 14:24:02 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.6407390952306932 on epoch=84
05/23/2022 14:24:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=84
05/23/2022 14:24:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
05/23/2022 14:24:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=86
05/23/2022 14:24:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=86
05/23/2022 14:24:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/23/2022 14:24:18 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.6957287933094385 on epoch=87
05/23/2022 14:24:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/23/2022 14:24:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/23/2022 14:24:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
05/23/2022 14:24:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=89
05/23/2022 14:24:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=90
05/23/2022 14:24:34 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.6689037621627598 on epoch=90
05/23/2022 14:24:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
05/23/2022 14:24:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=91
05/23/2022 14:24:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=92
05/23/2022 14:24:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=93
05/23/2022 14:24:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/23/2022 14:24:50 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.6653307723576021 on epoch=93
05/23/2022 14:24:52 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.13 on epoch=94
05/23/2022 14:24:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
05/23/2022 14:24:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=95
05/23/2022 14:25:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=96
05/23/2022 14:25:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=96
05/23/2022 14:25:06 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.7144247249732414 on epoch=96
05/23/2022 14:25:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=97
05/23/2022 14:25:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=98
05/23/2022 14:25:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
05/23/2022 14:25:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=99
05/23/2022 14:25:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=99
05/23/2022 14:25:22 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7174661500374471 on epoch=99
05/23/2022 14:25:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=100
05/23/2022 14:25:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=101
05/23/2022 14:25:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=101
05/23/2022 14:25:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=102
05/23/2022 14:25:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
05/23/2022 14:25:38 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7338650919560492 on epoch=103
05/23/2022 14:25:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/23/2022 14:25:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=104
05/23/2022 14:25:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=104
05/23/2022 14:25:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=105
05/23/2022 14:25:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/23/2022 14:25:53 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.7385691717194545 on epoch=106
05/23/2022 14:25:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=106
05/23/2022 14:25:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=107
05/23/2022 14:26:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=108
05/23/2022 14:26:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=108
05/23/2022 14:26:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=109
05/23/2022 14:26:09 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7074132989689891 on epoch=109
05/23/2022 14:26:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=109
05/23/2022 14:26:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=110
05/23/2022 14:26:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=111
05/23/2022 14:26:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/23/2022 14:26:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=112
05/23/2022 14:26:25 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.6821447656881883 on epoch=112
05/23/2022 14:26:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=113
05/23/2022 14:26:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=113
05/23/2022 14:26:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=114
05/23/2022 14:26:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/23/2022 14:26:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
05/23/2022 14:26:41 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.7531481060892826 on epoch=115
05/23/2022 14:26:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7438681404440843 -> 0.7531481060892826 on epoch=115, global_step=1850
05/23/2022 14:26:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
05/23/2022 14:26:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=116
05/23/2022 14:26:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=117
05/23/2022 14:26:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
05/23/2022 14:26:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=118
05/23/2022 14:26:57 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.6719739081959042 on epoch=118
05/23/2022 14:27:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=119
05/23/2022 14:27:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/23/2022 14:27:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=120
05/23/2022 14:27:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=121
05/23/2022 14:27:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=121
05/23/2022 14:27:13 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7078698031528219 on epoch=121
05/23/2022 14:27:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
05/23/2022 14:27:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
05/23/2022 14:27:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=123
05/23/2022 14:27:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=124
05/23/2022 14:27:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=124
05/23/2022 14:27:29 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.7387804734264067 on epoch=124
05/23/2022 14:27:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/23/2022 14:27:34 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.05 on epoch=126
05/23/2022 14:27:37 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=126
05/23/2022 14:27:39 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=127
05/23/2022 14:27:42 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=128
05/23/2022 14:27:45 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.7075153072874902 on epoch=128
05/23/2022 14:27:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/23/2022 14:27:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/23/2022 14:27:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=129
05/23/2022 14:27:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=130
05/23/2022 14:27:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/23/2022 14:28:01 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.7354015054126584 on epoch=131
05/23/2022 14:28:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=131
05/23/2022 14:28:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
05/23/2022 14:28:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=133
05/23/2022 14:28:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=133
05/23/2022 14:28:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=134
05/23/2022 14:28:17 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.7184180659012873 on epoch=134
05/23/2022 14:28:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
05/23/2022 14:28:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=135
05/23/2022 14:28:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=136
05/23/2022 14:28:27 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/23/2022 14:28:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=137
05/23/2022 14:28:33 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.7638622188042022 on epoch=137
05/23/2022 14:28:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7531481060892826 -> 0.7638622188042022 on epoch=137, global_step=2200
05/23/2022 14:28:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=138
05/23/2022 14:28:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.12 on epoch=138
05/23/2022 14:28:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=139
05/23/2022 14:28:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=139
05/23/2022 14:28:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=140
05/23/2022 14:28:49 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7457761418806474 on epoch=140
05/23/2022 14:28:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/23/2022 14:28:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
05/23/2022 14:28:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.01 on epoch=142
05/23/2022 14:28:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.02 on epoch=143
05/23/2022 14:29:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
05/23/2022 14:29:05 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.7245545503623383 on epoch=143
05/23/2022 14:29:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/23/2022 14:29:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/23/2022 14:29:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
05/23/2022 14:29:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/23/2022 14:29:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=146
05/23/2022 14:29:21 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7380297650525044 on epoch=146
05/23/2022 14:29:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=147
05/23/2022 14:29:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/23/2022 14:29:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=148
05/23/2022 14:29:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=149
05/23/2022 14:29:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=149
05/23/2022 14:29:37 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7445996017529846 on epoch=149
05/23/2022 14:29:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=150
05/23/2022 14:29:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=151
05/23/2022 14:29:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=151
05/23/2022 14:29:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=152
05/23/2022 14:29:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/23/2022 14:29:53 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7299624768814909 on epoch=153
05/23/2022 14:29:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/23/2022 14:29:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/23/2022 14:30:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=154
05/23/2022 14:30:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/23/2022 14:30:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=156
05/23/2022 14:30:09 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7489892659834755 on epoch=156
05/23/2022 14:30:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=156
05/23/2022 14:30:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/23/2022 14:30:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=158
05/23/2022 14:30:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/23/2022 14:30:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/23/2022 14:30:25 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7645952816925605 on epoch=159
05/23/2022 14:30:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7638622188042022 -> 0.7645952816925605 on epoch=159, global_step=2550
05/23/2022 14:30:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
05/23/2022 14:30:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/23/2022 14:30:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=161
05/23/2022 14:30:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=161
05/23/2022 14:30:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=162
05/23/2022 14:30:41 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.7250987443853724 on epoch=162
05/23/2022 14:30:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
05/23/2022 14:30:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/23/2022 14:30:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=164
05/23/2022 14:30:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=164
05/23/2022 14:30:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=165
05/23/2022 14:30:57 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.7306417806112462 on epoch=165
05/23/2022 14:30:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
05/23/2022 14:31:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=166
05/23/2022 14:31:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=167
05/23/2022 14:31:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
05/23/2022 14:31:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=168
05/23/2022 14:31:13 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.7131072577427175 on epoch=168
05/23/2022 14:31:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=169
05/23/2022 14:31:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
05/23/2022 14:31:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/23/2022 14:31:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=171
05/23/2022 14:31:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=171
05/23/2022 14:31:29 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7093535939116872 on epoch=171
05/23/2022 14:31:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/23/2022 14:31:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/23/2022 14:31:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/23/2022 14:31:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
05/23/2022 14:31:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/23/2022 14:31:45 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.6932605882276118 on epoch=174
05/23/2022 14:31:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/23/2022 14:31:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
05/23/2022 14:31:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/23/2022 14:31:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=177
05/23/2022 14:31:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/23/2022 14:32:01 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.6998211813954739 on epoch=178
05/23/2022 14:32:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/23/2022 14:32:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/23/2022 14:32:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=179
05/23/2022 14:32:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/23/2022 14:32:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/23/2022 14:32:17 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7365736900869588 on epoch=181
05/23/2022 14:32:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/23/2022 14:32:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/23/2022 14:32:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=183
05/23/2022 14:32:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
05/23/2022 14:32:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 14:32:34 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.727520249598751 on epoch=184
05/23/2022 14:32:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/23/2022 14:32:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/23/2022 14:32:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/23/2022 14:32:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=186
05/23/2022 14:32:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/23/2022 14:32:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:32:48 - INFO - __main__ - Printing 3 examples
05/23/2022 14:32:48 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:32:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:32:48 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:32:48 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:32:48 - INFO - __main__ - Printing 3 examples
05/23/2022 14:32:48 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:32:48 - INFO - __main__ - ['happy']
05/23/2022 14:32:48 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:32:48 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:32:48 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:32:51 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7323472257084407 on epoch=187
05/23/2022 14:32:51 - INFO - __main__ - save last model!
05/23/2022 14:32:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 14:32:51 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 14:32:51 - INFO - __main__ - Printing 3 examples
05/23/2022 14:32:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 14:32:51 - INFO - __main__ - ['others']
05/23/2022 14:32:51 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 14:32:51 - INFO - __main__ - ['others']
05/23/2022 14:32:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 14:32:51 - INFO - __main__ - ['others']
05/23/2022 14:32:51 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:32:53 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:32:58 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 14:33:07 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:33:07 - INFO - __main__ - task name: emo
05/23/2022 14:33:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:33:08 - INFO - __main__ - Starting training!
05/23/2022 14:34:30 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_42_0.4_8_predictions.txt
05/23/2022 14:34:30 - INFO - __main__ - Classification-F1 on test data: 0.4318
05/23/2022 14:34:30 - INFO - __main__ - prefix=emo_64_42, lr=0.4, bsz=8, dev_performance=0.7645952816925605, test_performance=0.43181326273970133
05/23/2022 14:34:30 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.3, bsz=8 ...
05/23/2022 14:34:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:34:31 - INFO - __main__ - Printing 3 examples
05/23/2022 14:34:31 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:34:31 - INFO - __main__ - ['happy']
05/23/2022 14:34:31 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:34:31 - INFO - __main__ - ['happy']
05/23/2022 14:34:31 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:34:31 - INFO - __main__ - ['happy']
05/23/2022 14:34:31 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:34:31 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:34:32 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:34:32 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:34:32 - INFO - __main__ - Printing 3 examples
05/23/2022 14:34:32 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:34:32 - INFO - __main__ - ['happy']
05/23/2022 14:34:32 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:34:32 - INFO - __main__ - ['happy']
05/23/2022 14:34:32 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:34:32 - INFO - __main__ - ['happy']
05/23/2022 14:34:32 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:34:32 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:34:32 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:34:48 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:34:48 - INFO - __main__ - task name: emo
05/23/2022 14:34:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:34:48 - INFO - __main__ - Starting training!
05/23/2022 14:34:51 - INFO - __main__ - Step 10 Global step 10 Train loss 7.69 on epoch=0
05/23/2022 14:34:54 - INFO - __main__ - Step 20 Global step 20 Train loss 4.89 on epoch=1
05/23/2022 14:34:56 - INFO - __main__ - Step 30 Global step 30 Train loss 2.34 on epoch=1
05/23/2022 14:34:58 - INFO - __main__ - Step 40 Global step 40 Train loss 1.54 on epoch=2
05/23/2022 14:35:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.25 on epoch=3
05/23/2022 14:35:04 - INFO - __main__ - Global step 50 Train loss 3.54 Classification-F1 0.1141107300342969 on epoch=3
05/23/2022 14:35:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1141107300342969 on epoch=3, global_step=50
05/23/2022 14:35:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=3
05/23/2022 14:35:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.99 on epoch=4
05/23/2022 14:35:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
05/23/2022 14:35:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=5
05/23/2022 14:35:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.02 on epoch=6
05/23/2022 14:35:20 - INFO - __main__ - Global step 100 Train loss 0.98 Classification-F1 0.1513007219317899 on epoch=6
05/23/2022 14:35:20 - INFO - __main__ - Saving model with best Classification-F1: 0.1141107300342969 -> 0.1513007219317899 on epoch=6, global_step=100
05/23/2022 14:35:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=6
05/23/2022 14:35:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=7
05/23/2022 14:35:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=8
05/23/2022 14:35:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.90 on epoch=8
05/23/2022 14:35:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=9
05/23/2022 14:35:36 - INFO - __main__ - Global step 150 Train loss 0.91 Classification-F1 0.1443260515127209 on epoch=9
05/23/2022 14:35:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.97 on epoch=9
05/23/2022 14:35:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=10
05/23/2022 14:35:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.93 on epoch=11
05/23/2022 14:35:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=11
05/23/2022 14:35:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=12
05/23/2022 14:35:52 - INFO - __main__ - Global step 200 Train loss 0.88 Classification-F1 0.14280246069719754 on epoch=12
05/23/2022 14:35:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=13
05/23/2022 14:35:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=13
05/23/2022 14:35:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.84 on epoch=14
05/23/2022 14:36:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.85 on epoch=14
05/23/2022 14:36:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
05/23/2022 14:36:07 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.19753858766771248 on epoch=15
05/23/2022 14:36:07 - INFO - __main__ - Saving model with best Classification-F1: 0.1513007219317899 -> 0.19753858766771248 on epoch=15, global_step=250
05/23/2022 14:36:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=16
05/23/2022 14:36:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=16
05/23/2022 14:36:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.78 on epoch=17
05/23/2022 14:36:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.81 on epoch=18
05/23/2022 14:36:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=18
05/23/2022 14:36:23 - INFO - __main__ - Global step 300 Train loss 0.78 Classification-F1 0.1708055608455532 on epoch=18
05/23/2022 14:36:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=19
05/23/2022 14:36:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=19
05/23/2022 14:36:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=20
05/23/2022 14:36:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=21
05/23/2022 14:36:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.76 on epoch=21
05/23/2022 14:36:39 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.16495463995463996 on epoch=21
05/23/2022 14:36:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=22
05/23/2022 14:36:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.79 on epoch=23
05/23/2022 14:36:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=23
05/23/2022 14:36:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.78 on epoch=24
05/23/2022 14:36:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.78 on epoch=24
05/23/2022 14:36:55 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.22425335329019055 on epoch=24
05/23/2022 14:36:55 - INFO - __main__ - Saving model with best Classification-F1: 0.19753858766771248 -> 0.22425335329019055 on epoch=24, global_step=400
05/23/2022 14:36:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=25
05/23/2022 14:37:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=26
05/23/2022 14:37:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=26
05/23/2022 14:37:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.81 on epoch=27
05/23/2022 14:37:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=28
05/23/2022 14:37:11 - INFO - __main__ - Global step 450 Train loss 0.77 Classification-F1 0.16194364013734436 on epoch=28
05/23/2022 14:37:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=28
05/23/2022 14:37:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=29
05/23/2022 14:37:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.77 on epoch=29
05/23/2022 14:37:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.67 on epoch=30
05/23/2022 14:37:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.70 on epoch=31
05/23/2022 14:37:26 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.4716715716715717 on epoch=31
05/23/2022 14:37:26 - INFO - __main__ - Saving model with best Classification-F1: 0.22425335329019055 -> 0.4716715716715717 on epoch=31, global_step=500
05/23/2022 14:37:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.79 on epoch=31
05/23/2022 14:37:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=32
05/23/2022 14:37:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=33
05/23/2022 14:37:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=33
05/23/2022 14:37:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=34
05/23/2022 14:37:42 - INFO - __main__ - Global step 550 Train loss 0.70 Classification-F1 0.3710483144306674 on epoch=34
05/23/2022 14:37:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=34
05/23/2022 14:37:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.71 on epoch=35
05/23/2022 14:37:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=36
05/23/2022 14:37:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=36
05/23/2022 14:37:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.59 on epoch=37
05/23/2022 14:37:58 - INFO - __main__ - Global step 600 Train loss 0.65 Classification-F1 0.48134160615503896 on epoch=37
05/23/2022 14:37:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4716715716715717 -> 0.48134160615503896 on epoch=37, global_step=600
05/23/2022 14:38:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.67 on epoch=38
05/23/2022 14:38:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=38
05/23/2022 14:38:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=39
05/23/2022 14:38:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.68 on epoch=39
05/23/2022 14:38:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.58 on epoch=40
05/23/2022 14:38:14 - INFO - __main__ - Global step 650 Train loss 0.63 Classification-F1 0.5938421211168069 on epoch=40
05/23/2022 14:38:14 - INFO - __main__ - Saving model with best Classification-F1: 0.48134160615503896 -> 0.5938421211168069 on epoch=40, global_step=650
05/23/2022 14:38:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=41
05/23/2022 14:38:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.68 on epoch=41
05/23/2022 14:38:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=42
05/23/2022 14:38:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.71 on epoch=43
05/23/2022 14:38:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.54 on epoch=43
05/23/2022 14:38:30 - INFO - __main__ - Global step 700 Train loss 0.63 Classification-F1 0.5045507905543909 on epoch=43
05/23/2022 14:38:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=44
05/23/2022 14:38:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.54 on epoch=44
05/23/2022 14:38:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=45
05/23/2022 14:38:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.53 on epoch=46
05/23/2022 14:38:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=46
05/23/2022 14:38:46 - INFO - __main__ - Global step 750 Train loss 0.55 Classification-F1 0.5745089659374816 on epoch=46
05/23/2022 14:38:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=47
05/23/2022 14:38:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=48
05/23/2022 14:38:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=48
05/23/2022 14:38:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=49
05/23/2022 14:38:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=49
05/23/2022 14:39:02 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.5921056005398111 on epoch=49
05/23/2022 14:39:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=50
05/23/2022 14:39:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.55 on epoch=51
05/23/2022 14:39:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=51
05/23/2022 14:39:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=52
05/23/2022 14:39:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.54 on epoch=53
05/23/2022 14:39:17 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.6304183139068333 on epoch=53
05/23/2022 14:39:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5938421211168069 -> 0.6304183139068333 on epoch=53, global_step=850
05/23/2022 14:39:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=53
05/23/2022 14:39:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=54
05/23/2022 14:39:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
05/23/2022 14:39:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=55
05/23/2022 14:39:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=56
05/23/2022 14:39:33 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.6274305893524839 on epoch=56
05/23/2022 14:39:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=56
05/23/2022 14:39:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
05/23/2022 14:39:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=58
05/23/2022 14:39:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.50 on epoch=58
05/23/2022 14:39:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=59
05/23/2022 14:39:49 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.6266125371049415 on epoch=59
05/23/2022 14:39:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
05/23/2022 14:39:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=60
05/23/2022 14:39:57 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/23/2022 14:39:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=61
05/23/2022 14:40:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=62
05/23/2022 14:40:05 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.5567541601670534 on epoch=62
05/23/2022 14:40:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=63
05/23/2022 14:40:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=63
05/23/2022 14:40:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=64
05/23/2022 14:40:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=64
05/23/2022 14:40:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
05/23/2022 14:40:21 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.6208140885492509 on epoch=65
05/23/2022 14:40:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=66
05/23/2022 14:40:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=66
05/23/2022 14:40:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=67
05/23/2022 14:40:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=68
05/23/2022 14:40:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/23/2022 14:40:36 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.605342703735355 on epoch=68
05/23/2022 14:40:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/23/2022 14:40:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/23/2022 14:40:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=70
05/23/2022 14:40:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/23/2022 14:40:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=71
05/23/2022 14:40:52 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.6810443111032948 on epoch=71
05/23/2022 14:40:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6304183139068333 -> 0.6810443111032948 on epoch=71, global_step=1150
05/23/2022 14:40:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=72
05/23/2022 14:40:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=73
05/23/2022 14:41:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=73
05/23/2022 14:41:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=74
05/23/2022 14:41:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=74
05/23/2022 14:41:08 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.7241867409059576 on epoch=74
05/23/2022 14:41:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6810443111032948 -> 0.7241867409059576 on epoch=74, global_step=1200
05/23/2022 14:41:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=75
05/23/2022 14:41:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
05/23/2022 14:41:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
05/23/2022 14:41:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=77
05/23/2022 14:41:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=78
05/23/2022 14:41:24 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.7313192853426815 on epoch=78
05/23/2022 14:41:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7241867409059576 -> 0.7313192853426815 on epoch=78, global_step=1250
05/23/2022 14:41:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=78
05/23/2022 14:41:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
05/23/2022 14:41:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
05/23/2022 14:41:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=80
05/23/2022 14:41:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
05/23/2022 14:41:40 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.6963453319166679 on epoch=81
05/23/2022 14:41:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=81
05/23/2022 14:41:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=82
05/23/2022 14:41:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=83
05/23/2022 14:41:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
05/23/2022 14:41:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=84
05/23/2022 14:41:56 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.7197934901705504 on epoch=84
05/23/2022 14:41:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=84
05/23/2022 14:42:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
05/23/2022 14:42:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=86
05/23/2022 14:42:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
05/23/2022 14:42:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=87
05/23/2022 14:42:12 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.6573140631743455 on epoch=87
05/23/2022 14:42:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=88
05/23/2022 14:42:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=88
05/23/2022 14:42:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=89
05/23/2022 14:42:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/23/2022 14:42:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
05/23/2022 14:42:28 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.6904965211891209 on epoch=90
05/23/2022 14:42:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=91
05/23/2022 14:42:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=91
05/23/2022 14:42:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=92
05/23/2022 14:42:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=93
05/23/2022 14:42:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
05/23/2022 14:42:43 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.6147830528157594 on epoch=93
05/23/2022 14:42:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
05/23/2022 14:42:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
05/23/2022 14:42:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
05/23/2022 14:42:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=96
05/23/2022 14:42:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
05/23/2022 14:42:59 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7338536901997085 on epoch=96
05/23/2022 14:42:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7313192853426815 -> 0.7338536901997085 on epoch=96, global_step=1550
05/23/2022 14:43:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/23/2022 14:43:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=98
05/23/2022 14:43:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
05/23/2022 14:43:09 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
05/23/2022 14:43:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/23/2022 14:43:15 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.734082323005538 on epoch=99
05/23/2022 14:43:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7338536901997085 -> 0.734082323005538 on epoch=99, global_step=1600
05/23/2022 14:43:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
05/23/2022 14:43:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/23/2022 14:43:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
05/23/2022 14:43:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=102
05/23/2022 14:43:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/23/2022 14:43:31 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.6769305605326736 on epoch=103
05/23/2022 14:43:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/23/2022 14:43:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
05/23/2022 14:43:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/23/2022 14:43:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=105
05/23/2022 14:43:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=106
05/23/2022 14:43:47 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.7114114153125426 on epoch=106
05/23/2022 14:43:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/23/2022 14:43:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
05/23/2022 14:43:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
05/23/2022 14:43:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=108
05/23/2022 14:44:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/23/2022 14:44:03 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.6948716408739299 on epoch=109
05/23/2022 14:44:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=109
05/23/2022 14:44:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=110
05/23/2022 14:44:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=111
05/23/2022 14:44:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
05/23/2022 14:44:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=112
05/23/2022 14:44:19 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.7211121276712867 on epoch=112
05/23/2022 14:44:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/23/2022 14:44:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
05/23/2022 14:44:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=114
05/23/2022 14:44:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=114
05/23/2022 14:44:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
05/23/2022 14:44:35 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.6975124817064116 on epoch=115
05/23/2022 14:44:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
05/23/2022 14:44:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/23/2022 14:44:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/23/2022 14:44:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=118
05/23/2022 14:44:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
05/23/2022 14:44:51 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.6678785187598246 on epoch=118
05/23/2022 14:44:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/23/2022 14:44:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=119
05/23/2022 14:44:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=120
05/23/2022 14:45:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=121
05/23/2022 14:45:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/23/2022 14:45:06 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.707612883420063 on epoch=121
05/23/2022 14:45:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
05/23/2022 14:45:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=123
05/23/2022 14:45:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=123
05/23/2022 14:45:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
05/23/2022 14:45:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=124
05/23/2022 14:45:22 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.718236687824474 on epoch=124
05/23/2022 14:45:25 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=125
05/23/2022 14:45:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/23/2022 14:45:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=126
05/23/2022 14:45:32 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=127
05/23/2022 14:45:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=128
05/23/2022 14:45:38 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.7334327122153209 on epoch=128
05/23/2022 14:45:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
05/23/2022 14:45:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
05/23/2022 14:45:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=129
05/23/2022 14:45:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/23/2022 14:45:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=131
05/23/2022 14:45:54 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7219446102053177 on epoch=131
05/23/2022 14:45:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=131
05/23/2022 14:45:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/23/2022 14:46:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.15 on epoch=133
05/23/2022 14:46:04 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=133
05/23/2022 14:46:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
05/23/2022 14:46:10 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.7314363111529829 on epoch=134
05/23/2022 14:46:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/23/2022 14:46:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/23/2022 14:46:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=136
05/23/2022 14:46:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=136
05/23/2022 14:46:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=137
05/23/2022 14:46:26 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7348720270302779 on epoch=137
05/23/2022 14:46:26 - INFO - __main__ - Saving model with best Classification-F1: 0.734082323005538 -> 0.7348720270302779 on epoch=137, global_step=2200
05/23/2022 14:46:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=138
05/23/2022 14:46:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
05/23/2022 14:46:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=139
05/23/2022 14:46:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
05/23/2022 14:46:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=140
05/23/2022 14:46:42 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.7492454523405288 on epoch=140
05/23/2022 14:46:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7348720270302779 -> 0.7492454523405288 on epoch=140, global_step=2250
05/23/2022 14:46:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/23/2022 14:46:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=141
05/23/2022 14:46:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=142
05/23/2022 14:46:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
05/23/2022 14:46:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=143
05/23/2022 14:46:58 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.7038458747326546 on epoch=143
05/23/2022 14:47:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=144
05/23/2022 14:47:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/23/2022 14:47:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=145
05/23/2022 14:47:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=146
05/23/2022 14:47:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=146
05/23/2022 14:47:14 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.6968392574942811 on epoch=146
05/23/2022 14:47:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
05/23/2022 14:47:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=148
05/23/2022 14:47:21 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
05/23/2022 14:47:24 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=149
05/23/2022 14:47:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/23/2022 14:47:30 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.7144302316342632 on epoch=149
05/23/2022 14:47:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
05/23/2022 14:47:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/23/2022 14:47:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=151
05/23/2022 14:47:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
05/23/2022 14:47:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/23/2022 14:47:46 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.7321134640510105 on epoch=153
05/23/2022 14:47:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=153
05/23/2022 14:47:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=154
05/23/2022 14:47:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=154
05/23/2022 14:47:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/23/2022 14:47:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=156
05/23/2022 14:48:02 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.7394433773894069 on epoch=156
05/23/2022 14:48:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/23/2022 14:48:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
05/23/2022 14:48:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=158
05/23/2022 14:48:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/23/2022 14:48:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=159
05/23/2022 14:48:18 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.7298227131249954 on epoch=159
05/23/2022 14:48:20 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
05/23/2022 14:48:23 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/23/2022 14:48:25 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/23/2022 14:48:28 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/23/2022 14:48:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
05/23/2022 14:48:34 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7009946388571078 on epoch=162
05/23/2022 14:48:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=163
05/23/2022 14:48:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
05/23/2022 14:48:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/23/2022 14:48:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
05/23/2022 14:48:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
05/23/2022 14:48:49 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.6831289590493658 on epoch=165
05/23/2022 14:48:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=166
05/23/2022 14:48:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
05/23/2022 14:48:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/23/2022 14:48:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
05/23/2022 14:49:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/23/2022 14:49:05 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.6913359719321324 on epoch=168
05/23/2022 14:49:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/23/2022 14:49:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=169
05/23/2022 14:49:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
05/23/2022 14:49:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
05/23/2022 14:49:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/23/2022 14:49:21 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.6981807542511633 on epoch=171
05/23/2022 14:49:24 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/23/2022 14:49:26 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/23/2022 14:49:29 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/23/2022 14:49:31 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
05/23/2022 14:49:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/23/2022 14:49:37 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7386231878099434 on epoch=174
05/23/2022 14:49:40 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/23/2022 14:49:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/23/2022 14:49:45 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
05/23/2022 14:49:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/23/2022 14:49:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/23/2022 14:49:53 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7425995897250254 on epoch=178
05/23/2022 14:49:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/23/2022 14:49:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/23/2022 14:50:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/23/2022 14:50:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/23/2022 14:50:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
05/23/2022 14:50:09 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7174735587665723 on epoch=181
05/23/2022 14:50:11 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/23/2022 14:50:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/23/2022 14:50:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/23/2022 14:50:19 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
05/23/2022 14:50:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/23/2022 14:50:25 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7431020790585678 on epoch=184
05/23/2022 14:50:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
05/23/2022 14:50:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=185
05/23/2022 14:50:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=186
05/23/2022 14:50:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/23/2022 14:50:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=187
05/23/2022 14:50:38 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:50:38 - INFO - __main__ - Printing 3 examples
05/23/2022 14:50:38 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:50:38 - INFO - __main__ - ['happy']
05/23/2022 14:50:38 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:50:38 - INFO - __main__ - ['happy']
05/23/2022 14:50:38 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:50:38 - INFO - __main__ - ['happy']
05/23/2022 14:50:38 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:50:38 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:50:39 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:50:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:50:39 - INFO - __main__ - Printing 3 examples
05/23/2022 14:50:39 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:50:39 - INFO - __main__ - ['happy']
05/23/2022 14:50:39 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:50:39 - INFO - __main__ - ['happy']
05/23/2022 14:50:39 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:50:39 - INFO - __main__ - ['happy']
05/23/2022 14:50:39 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:50:39 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:50:39 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:50:41 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7397435897435897 on epoch=187
05/23/2022 14:50:41 - INFO - __main__ - save last model!
05/23/2022 14:50:41 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 14:50:41 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 14:50:41 - INFO - __main__ - Printing 3 examples
05/23/2022 14:50:41 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 14:50:41 - INFO - __main__ - ['others']
05/23/2022 14:50:41 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 14:50:41 - INFO - __main__ - ['others']
05/23/2022 14:50:41 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 14:50:41 - INFO - __main__ - ['others']
05/23/2022 14:50:41 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:50:43 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:50:48 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 14:50:58 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:50:58 - INFO - __main__ - task name: emo
05/23/2022 14:50:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:50:59 - INFO - __main__ - Starting training!
05/23/2022 14:52:01 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_42_0.3_8_predictions.txt
05/23/2022 14:52:01 - INFO - __main__ - Classification-F1 on test data: 0.4561
05/23/2022 14:52:02 - INFO - __main__ - prefix=emo_64_42, lr=0.3, bsz=8, dev_performance=0.7492454523405288, test_performance=0.4560699512379901
05/23/2022 14:52:02 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.2, bsz=8 ...
05/23/2022 14:52:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:52:03 - INFO - __main__ - Printing 3 examples
05/23/2022 14:52:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:52:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:52:03 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 14:52:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 14:52:03 - INFO - __main__ - Printing 3 examples
05/23/2022 14:52:03 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/23/2022 14:52:03 - INFO - __main__ - ['happy']
05/23/2022 14:52:03 - INFO - __main__ - Tokenizing Input ...
05/23/2022 14:52:03 - INFO - __main__ - Tokenizing Output ...
05/23/2022 14:52:03 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 14:52:19 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 14:52:19 - INFO - __main__ - task name: emo
05/23/2022 14:52:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 14:52:20 - INFO - __main__ - Starting training!
05/23/2022 14:52:22 - INFO - __main__ - Step 10 Global step 10 Train loss 7.37 on epoch=0
05/23/2022 14:52:25 - INFO - __main__ - Step 20 Global step 20 Train loss 5.23 on epoch=1
05/23/2022 14:52:27 - INFO - __main__ - Step 30 Global step 30 Train loss 3.19 on epoch=1
05/23/2022 14:52:30 - INFO - __main__ - Step 40 Global step 40 Train loss 2.02 on epoch=2
05/23/2022 14:52:32 - INFO - __main__ - Step 50 Global step 50 Train loss 1.38 on epoch=3
05/23/2022 14:52:36 - INFO - __main__ - Global step 50 Train loss 3.84 Classification-F1 0.2114583333333333 on epoch=3
05/23/2022 14:52:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2114583333333333 on epoch=3, global_step=50
05/23/2022 14:52:38 - INFO - __main__ - Step 60 Global step 60 Train loss 1.28 on epoch=3
05/23/2022 14:52:41 - INFO - __main__ - Step 70 Global step 70 Train loss 1.12 on epoch=4
05/23/2022 14:52:43 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=4
05/23/2022 14:52:46 - INFO - __main__ - Step 90 Global step 90 Train loss 1.01 on epoch=5
05/23/2022 14:52:48 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=6
05/23/2022 14:52:52 - INFO - __main__ - Global step 100 Train loss 1.10 Classification-F1 0.21874686425433496 on epoch=6
05/23/2022 14:52:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2114583333333333 -> 0.21874686425433496 on epoch=6, global_step=100
05/23/2022 14:52:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.98 on epoch=6
05/23/2022 14:52:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.97 on epoch=7
05/23/2022 14:52:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.96 on epoch=8
05/23/2022 14:53:01 - INFO - __main__ - Step 140 Global step 140 Train loss 1.02 on epoch=8
05/23/2022 14:53:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.91 on epoch=9
05/23/2022 14:53:07 - INFO - __main__ - Global step 150 Train loss 0.97 Classification-F1 0.09874608150470221 on epoch=9
05/23/2022 14:53:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=9
05/23/2022 14:53:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.93 on epoch=10
05/23/2022 14:53:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=11
05/23/2022 14:53:17 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=11
05/23/2022 14:53:20 - INFO - __main__ - Step 200 Global step 200 Train loss 0.83 on epoch=12
05/23/2022 14:53:23 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.20463476854518273 on epoch=12
05/23/2022 14:53:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=13
05/23/2022 14:53:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=13
05/23/2022 14:53:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.93 on epoch=14
05/23/2022 14:53:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.81 on epoch=14
05/23/2022 14:53:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=15
05/23/2022 14:53:39 - INFO - __main__ - Global step 250 Train loss 0.89 Classification-F1 0.20320028966938697 on epoch=15
05/23/2022 14:53:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.84 on epoch=16
05/23/2022 14:53:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=16
05/23/2022 14:53:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.92 on epoch=17
05/23/2022 14:53:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.89 on epoch=18
05/23/2022 14:53:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.89 on epoch=18
05/23/2022 14:53:55 - INFO - __main__ - Global step 300 Train loss 0.88 Classification-F1 0.19281772506383327 on epoch=18
05/23/2022 14:53:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.83 on epoch=19
05/23/2022 14:54:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=19
05/23/2022 14:54:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=20
05/23/2022 14:54:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=21
05/23/2022 14:54:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.81 on epoch=21
05/23/2022 14:54:11 - INFO - __main__ - Global step 350 Train loss 0.82 Classification-F1 0.2574052812858783 on epoch=21
05/23/2022 14:54:11 - INFO - __main__ - Saving model with best Classification-F1: 0.21874686425433496 -> 0.2574052812858783 on epoch=21, global_step=350
05/23/2022 14:54:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.77 on epoch=22
05/23/2022 14:54:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.86 on epoch=23
05/23/2022 14:54:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.80 on epoch=23
05/23/2022 14:54:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=24
05/23/2022 14:54:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=24
05/23/2022 14:54:26 - INFO - __main__ - Global step 400 Train loss 0.82 Classification-F1 0.27577723313873076 on epoch=24
05/23/2022 14:54:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2574052812858783 -> 0.27577723313873076 on epoch=24, global_step=400
05/23/2022 14:54:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.85 on epoch=25
05/23/2022 14:54:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=26
05/23/2022 14:54:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.82 on epoch=26
05/23/2022 14:54:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=27
05/23/2022 14:54:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.79 on epoch=28
05/23/2022 14:54:42 - INFO - __main__ - Global step 450 Train loss 0.78 Classification-F1 0.264178827823194 on epoch=28
05/23/2022 14:54:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.78 on epoch=28
05/23/2022 14:54:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=29
05/23/2022 14:54:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.74 on epoch=29
05/23/2022 14:54:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.83 on epoch=30
05/23/2022 14:54:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.87 on epoch=31
05/23/2022 14:54:58 - INFO - __main__ - Global step 500 Train loss 0.79 Classification-F1 0.38531990117011183 on epoch=31
05/23/2022 14:54:58 - INFO - __main__ - Saving model with best Classification-F1: 0.27577723313873076 -> 0.38531990117011183 on epoch=31, global_step=500
05/23/2022 14:55:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.75 on epoch=31
05/23/2022 14:55:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.73 on epoch=32
05/23/2022 14:55:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.76 on epoch=33
05/23/2022 14:55:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.78 on epoch=33
05/23/2022 14:55:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.75 on epoch=34
05/23/2022 14:55:14 - INFO - __main__ - Global step 550 Train loss 0.75 Classification-F1 0.31705416211742776 on epoch=34
05/23/2022 14:55:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.73 on epoch=34
05/23/2022 14:55:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.84 on epoch=35
05/23/2022 14:55:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.89 on epoch=36
05/23/2022 14:55:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=36
05/23/2022 14:55:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.83 on epoch=37
05/23/2022 14:55:29 - INFO - __main__ - Global step 600 Train loss 0.81 Classification-F1 0.49994759384541076 on epoch=37
05/23/2022 14:55:29 - INFO - __main__ - Saving model with best Classification-F1: 0.38531990117011183 -> 0.49994759384541076 on epoch=37, global_step=600
05/23/2022 14:55:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=38
05/23/2022 14:55:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.76 on epoch=38
05/23/2022 14:55:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.66 on epoch=39
05/23/2022 14:55:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.75 on epoch=39
05/23/2022 14:55:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.69 on epoch=40
05/23/2022 14:55:45 - INFO - __main__ - Global step 650 Train loss 0.71 Classification-F1 0.5116635839408117 on epoch=40
05/23/2022 14:55:45 - INFO - __main__ - Saving model with best Classification-F1: 0.49994759384541076 -> 0.5116635839408117 on epoch=40, global_step=650
05/23/2022 14:55:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.67 on epoch=41
05/23/2022 14:55:50 - INFO - __main__ - Step 670 Global step 670 Train loss 0.70 on epoch=41
05/23/2022 14:55:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.67 on epoch=42
05/23/2022 14:55:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=43
05/23/2022 14:55:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.68 on epoch=43
05/23/2022 14:56:01 - INFO - __main__ - Global step 700 Train loss 0.67 Classification-F1 0.3152962168777543 on epoch=43
05/23/2022 14:56:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=44
05/23/2022 14:56:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.68 on epoch=44
05/23/2022 14:56:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.60 on epoch=45
05/23/2022 14:56:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.66 on epoch=46
05/23/2022 14:56:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.63 on epoch=46
05/23/2022 14:56:17 - INFO - __main__ - Global step 750 Train loss 0.64 Classification-F1 0.5084398053664756 on epoch=46
05/23/2022 14:56:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.69 on epoch=47
05/23/2022 14:56:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.64 on epoch=48
05/23/2022 14:56:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.54 on epoch=48
05/23/2022 14:56:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.66 on epoch=49
05/23/2022 14:56:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.56 on epoch=49
05/23/2022 14:56:32 - INFO - __main__ - Global step 800 Train loss 0.62 Classification-F1 0.539682095309195 on epoch=49
05/23/2022 14:56:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5116635839408117 -> 0.539682095309195 on epoch=49, global_step=800
05/23/2022 14:56:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.56 on epoch=50
05/23/2022 14:56:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.59 on epoch=51
05/23/2022 14:56:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.64 on epoch=51
05/23/2022 14:56:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.64 on epoch=52
05/23/2022 14:56:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.63 on epoch=53
05/23/2022 14:56:48 - INFO - __main__ - Global step 850 Train loss 0.61 Classification-F1 0.5512562800497002 on epoch=53
05/23/2022 14:56:48 - INFO - __main__ - Saving model with best Classification-F1: 0.539682095309195 -> 0.5512562800497002 on epoch=53, global_step=850
05/23/2022 14:56:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.58 on epoch=53
05/23/2022 14:56:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=54
05/23/2022 14:56:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.63 on epoch=54
05/23/2022 14:56:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=55
05/23/2022 14:57:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.62 on epoch=56
05/23/2022 14:57:04 - INFO - __main__ - Global step 900 Train loss 0.59 Classification-F1 0.4914270181511561 on epoch=56
05/23/2022 14:57:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=56
05/23/2022 14:57:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=57
05/23/2022 14:57:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.66 on epoch=58
05/23/2022 14:57:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=58
05/23/2022 14:57:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=59
05/23/2022 14:57:20 - INFO - __main__ - Global step 950 Train loss 0.55 Classification-F1 0.5676314092340734 on epoch=59
05/23/2022 14:57:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5512562800497002 -> 0.5676314092340734 on epoch=59, global_step=950
05/23/2022 14:57:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.60 on epoch=59
05/23/2022 14:57:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.58 on epoch=60
05/23/2022 14:57:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=61
05/23/2022 14:57:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=61
05/23/2022 14:57:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=62
05/23/2022 14:57:35 - INFO - __main__ - Global step 1000 Train loss 0.54 Classification-F1 0.4920438472418671 on epoch=62
05/23/2022 14:57:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.66 on epoch=63
05/23/2022 14:57:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=63
05/23/2022 14:57:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=64
05/23/2022 14:57:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=64
05/23/2022 14:57:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=65
05/23/2022 14:57:51 - INFO - __main__ - Global step 1050 Train loss 0.55 Classification-F1 0.5621592739347718 on epoch=65
05/23/2022 14:57:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=66
05/23/2022 14:57:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=66
05/23/2022 14:57:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=67
05/23/2022 14:58:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.51 on epoch=68
05/23/2022 14:58:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.55 on epoch=68
05/23/2022 14:58:07 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.5190453732759399 on epoch=68
05/23/2022 14:58:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=69
05/23/2022 14:58:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=69
05/23/2022 14:58:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/23/2022 14:58:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=71
05/23/2022 14:58:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.55 on epoch=71
05/23/2022 14:58:22 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.6821386748044396 on epoch=71
05/23/2022 14:58:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5676314092340734 -> 0.6821386748044396 on epoch=71, global_step=1150
05/23/2022 14:58:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.52 on epoch=72
05/23/2022 14:58:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=73
05/23/2022 14:58:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.52 on epoch=73
05/23/2022 14:58:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=74
05/23/2022 14:58:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=74
05/23/2022 14:58:38 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.6770360788428218 on epoch=74
05/23/2022 14:58:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=75
05/23/2022 14:58:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=76
05/23/2022 14:58:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=76
05/23/2022 14:58:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=77
05/23/2022 14:58:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=78
05/23/2022 14:58:54 - INFO - __main__ - Global step 1250 Train loss 0.47 Classification-F1 0.6478094441991041 on epoch=78
05/23/2022 14:58:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=78
05/23/2022 14:58:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=79
05/23/2022 14:59:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.50 on epoch=79
05/23/2022 14:59:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=80
05/23/2022 14:59:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=81
05/23/2022 14:59:09 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.645253386084853 on epoch=81
05/23/2022 14:59:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=81
05/23/2022 14:59:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=82
05/23/2022 14:59:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=83
05/23/2022 14:59:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=83
05/23/2022 14:59:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=84
05/23/2022 14:59:25 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.6402417027417028 on epoch=84
05/23/2022 14:59:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=84
05/23/2022 14:59:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=85
05/23/2022 14:59:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.44 on epoch=86
05/23/2022 14:59:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
05/23/2022 14:59:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/23/2022 14:59:41 - INFO - __main__ - Global step 1400 Train loss 0.38 Classification-F1 0.6331603608337713 on epoch=87
05/23/2022 14:59:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=88
05/23/2022 14:59:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=88
05/23/2022 14:59:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=89
05/23/2022 14:59:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=89
05/23/2022 14:59:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=90
05/23/2022 14:59:57 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.6189607266861743 on epoch=90
05/23/2022 14:59:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=91
05/23/2022 15:00:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=91
05/23/2022 15:00:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=92
05/23/2022 15:00:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=93
05/23/2022 15:00:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=93
05/23/2022 15:00:13 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.6182625368616828 on epoch=93
05/23/2022 15:00:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=94
05/23/2022 15:00:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.37 on epoch=94
05/23/2022 15:00:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=95
05/23/2022 15:00:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=96
05/23/2022 15:00:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=96
05/23/2022 15:00:28 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.481827685698307 on epoch=96
05/23/2022 15:00:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=97
05/23/2022 15:00:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=98
05/23/2022 15:00:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
05/23/2022 15:00:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
05/23/2022 15:00:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=99
05/23/2022 15:00:44 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.6962143309154027 on epoch=99
05/23/2022 15:00:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6821386748044396 -> 0.6962143309154027 on epoch=99, global_step=1600
05/23/2022 15:00:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
05/23/2022 15:00:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=101
05/23/2022 15:00:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
05/23/2022 15:00:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.31 on epoch=102
05/23/2022 15:00:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=103
05/23/2022 15:01:00 - INFO - __main__ - Global step 1650 Train loss 0.33 Classification-F1 0.7097811528983178 on epoch=103
05/23/2022 15:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6962143309154027 -> 0.7097811528983178 on epoch=103, global_step=1650
05/23/2022 15:01:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=103
05/23/2022 15:01:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
05/23/2022 15:01:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=104
05/23/2022 15:01:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=105
05/23/2022 15:01:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
05/23/2022 15:01:16 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.7300473599316796 on epoch=106
05/23/2022 15:01:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7097811528983178 -> 0.7300473599316796 on epoch=106, global_step=1700
05/23/2022 15:01:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=106
05/23/2022 15:01:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=107
05/23/2022 15:01:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=108
05/23/2022 15:01:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=108
05/23/2022 15:01:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=109
05/23/2022 15:01:32 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.662740682255729 on epoch=109
05/23/2022 15:01:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=109
05/23/2022 15:01:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=110
05/23/2022 15:01:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=111
05/23/2022 15:01:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=111
05/23/2022 15:01:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=112
05/23/2022 15:01:47 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.693186883440048 on epoch=112
05/23/2022 15:01:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=113
05/23/2022 15:01:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=113
05/23/2022 15:01:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=114
05/23/2022 15:01:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=114
05/23/2022 15:02:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=115
05/23/2022 15:02:03 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.692239338704455 on epoch=115
05/23/2022 15:02:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=116
05/23/2022 15:02:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=116
05/23/2022 15:02:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
05/23/2022 15:02:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=118
05/23/2022 15:02:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=118
05/23/2022 15:02:19 - INFO - __main__ - Global step 1900 Train loss 0.29 Classification-F1 0.6542260673406579 on epoch=118
05/23/2022 15:02:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
05/23/2022 15:02:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=119
05/23/2022 15:02:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=120
05/23/2022 15:02:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=121
05/23/2022 15:02:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=121
05/23/2022 15:02:35 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.6837927554548675 on epoch=121
05/23/2022 15:02:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/23/2022 15:02:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=123
05/23/2022 15:02:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=123
05/23/2022 15:02:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=124
05/23/2022 15:02:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=124
05/23/2022 15:02:51 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7031671042725451 on epoch=124
05/23/2022 15:02:53 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=125
05/23/2022 15:02:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=126
05/23/2022 15:02:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/23/2022 15:03:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
05/23/2022 15:03:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=128
05/23/2022 15:03:07 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.7178260399199436 on epoch=128
05/23/2022 15:03:09 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=128
05/23/2022 15:03:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=129
05/23/2022 15:03:14 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.36 on epoch=129
05/23/2022 15:03:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.26 on epoch=130
05/23/2022 15:03:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=131
05/23/2022 15:03:23 - INFO - __main__ - Global step 2100 Train loss 0.26 Classification-F1 0.7032571290178267 on epoch=131
05/23/2022 15:03:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=131
05/23/2022 15:03:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/23/2022 15:03:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
05/23/2022 15:03:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=133
05/23/2022 15:03:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/23/2022 15:03:39 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.6604841113352898 on epoch=134
05/23/2022 15:03:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=134
05/23/2022 15:03:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
05/23/2022 15:03:46 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=136
05/23/2022 15:03:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.25 on epoch=136
05/23/2022 15:03:51 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/23/2022 15:03:55 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.7395657573348526 on epoch=137
05/23/2022 15:03:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7300473599316796 -> 0.7395657573348526 on epoch=137, global_step=2200
05/23/2022 15:03:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=138
05/23/2022 15:04:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=138
05/23/2022 15:04:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/23/2022 15:04:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=139
05/23/2022 15:04:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=140
05/23/2022 15:04:10 - INFO - __main__ - Global step 2250 Train loss 0.26 Classification-F1 0.7248382762066824 on epoch=140
05/23/2022 15:04:13 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=141
05/23/2022 15:04:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=141
05/23/2022 15:04:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
05/23/2022 15:04:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
05/23/2022 15:04:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
05/23/2022 15:04:26 - INFO - __main__ - Global step 2300 Train loss 0.21 Classification-F1 0.6438388595239279 on epoch=143
05/23/2022 15:04:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=144
05/23/2022 15:04:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
05/23/2022 15:04:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=145
05/23/2022 15:04:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=146
05/23/2022 15:04:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
05/23/2022 15:04:42 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7222728724901212 on epoch=146
05/23/2022 15:04:45 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
05/23/2022 15:04:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
05/23/2022 15:04:50 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/23/2022 15:04:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=149
05/23/2022 15:04:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
05/23/2022 15:04:58 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.707811075897754 on epoch=149
05/23/2022 15:05:01 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=150
05/23/2022 15:05:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
05/23/2022 15:05:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=151
05/23/2022 15:05:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=152
05/23/2022 15:05:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
05/23/2022 15:05:14 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.7240041643047137 on epoch=153
05/23/2022 15:05:17 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=153
05/23/2022 15:05:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/23/2022 15:05:22 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.27 on epoch=154
05/23/2022 15:05:24 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.29 on epoch=155
05/23/2022 15:05:27 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/23/2022 15:05:30 - INFO - __main__ - Global step 2500 Train loss 0.22 Classification-F1 0.686025860232236 on epoch=156
05/23/2022 15:05:32 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.24 on epoch=156
05/23/2022 15:05:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=157
05/23/2022 15:05:37 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=158
05/23/2022 15:05:40 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/23/2022 15:05:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/23/2022 15:05:46 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.6760215265326948 on epoch=159
05/23/2022 15:05:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
05/23/2022 15:05:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=160
05/23/2022 15:05:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=161
05/23/2022 15:05:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
05/23/2022 15:05:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=162
05/23/2022 15:06:02 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.6968441644619481 on epoch=162
05/23/2022 15:06:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=163
05/23/2022 15:06:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/23/2022 15:06:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
05/23/2022 15:06:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
05/23/2022 15:06:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
05/23/2022 15:06:18 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.7248563986895189 on epoch=165
05/23/2022 15:06:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=166
05/23/2022 15:06:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=166
05/23/2022 15:06:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.16 on epoch=167
05/23/2022 15:06:28 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=168
05/23/2022 15:06:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
05/23/2022 15:06:34 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.7245051246304974 on epoch=168
05/23/2022 15:06:36 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
05/23/2022 15:06:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.19 on epoch=169
05/23/2022 15:06:41 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=170
05/23/2022 15:06:44 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=171
05/23/2022 15:06:46 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
05/23/2022 15:06:49 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.7124955475228456 on epoch=171
05/23/2022 15:06:52 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
05/23/2022 15:06:55 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/23/2022 15:06:57 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.26 on epoch=173
05/23/2022 15:06:59 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/23/2022 15:07:02 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
05/23/2022 15:07:05 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7438739761116846 on epoch=174
05/23/2022 15:07:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7395657573348526 -> 0.7438739761116846 on epoch=174, global_step=2800
05/23/2022 15:07:08 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=175
05/23/2022 15:07:10 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=176
05/23/2022 15:07:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=176
05/23/2022 15:07:15 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/23/2022 15:07:18 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=178
05/23/2022 15:07:21 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.7183457862080531 on epoch=178
05/23/2022 15:07:24 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=178
05/23/2022 15:07:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=179
05/23/2022 15:07:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.17 on epoch=179
05/23/2022 15:07:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.22 on epoch=180
05/23/2022 15:07:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/23/2022 15:07:37 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.7284192908152398 on epoch=181
05/23/2022 15:07:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=181
05/23/2022 15:07:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=182
05/23/2022 15:07:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
05/23/2022 15:07:47 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=183
05/23/2022 15:07:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
05/23/2022 15:07:53 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.7293619434678364 on epoch=184
05/23/2022 15:07:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=184
05/23/2022 15:07:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=185
05/23/2022 15:08:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=186
05/23/2022 15:08:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/23/2022 15:08:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/23/2022 15:08:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:08:07 - INFO - __main__ - Printing 3 examples
05/23/2022 15:08:07 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:08:07 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:08:07 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:08:07 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:08:07 - INFO - __main__ - Printing 3 examples
05/23/2022 15:08:07 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:08:07 - INFO - __main__ - ['others']
05/23/2022 15:08:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:08:08 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:08:08 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:08:09 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.7461376336102642 on epoch=187
05/23/2022 15:08:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7438739761116846 -> 0.7461376336102642 on epoch=187, global_step=3000
05/23/2022 15:08:09 - INFO - __main__ - save last model!
05/23/2022 15:08:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 15:08:09 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 15:08:09 - INFO - __main__ - Printing 3 examples
05/23/2022 15:08:09 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 15:08:09 - INFO - __main__ - ['others']
05/23/2022 15:08:09 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 15:08:09 - INFO - __main__ - ['others']
05/23/2022 15:08:09 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 15:08:09 - INFO - __main__ - ['others']
05/23/2022 15:08:09 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:08:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:08:17 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 15:08:23 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:08:23 - INFO - __main__ - task name: emo
05/23/2022 15:08:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:08:23 - INFO - __main__ - Starting training!
05/23/2022 15:09:30 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_42_0.2_8_predictions.txt
05/23/2022 15:09:30 - INFO - __main__ - Classification-F1 on test data: 0.4000
05/23/2022 15:09:30 - INFO - __main__ - prefix=emo_64_42, lr=0.2, bsz=8, dev_performance=0.7461376336102642, test_performance=0.40003345262950213
05/23/2022 15:09:30 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.5, bsz=8 ...
05/23/2022 15:09:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:09:31 - INFO - __main__ - Printing 3 examples
05/23/2022 15:09:31 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:09:31 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:09:31 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:09:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:09:31 - INFO - __main__ - Printing 3 examples
05/23/2022 15:09:31 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:09:31 - INFO - __main__ - ['others']
05/23/2022 15:09:31 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:09:31 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:09:32 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:09:47 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:09:47 - INFO - __main__ - task name: emo
05/23/2022 15:09:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:09:48 - INFO - __main__ - Starting training!
05/23/2022 15:09:51 - INFO - __main__ - Step 10 Global step 10 Train loss 6.88 on epoch=0
05/23/2022 15:09:53 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=1
05/23/2022 15:09:56 - INFO - __main__ - Step 30 Global step 30 Train loss 1.17 on epoch=1
05/23/2022 15:09:58 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=2
05/23/2022 15:10:01 - INFO - __main__ - Step 50 Global step 50 Train loss 1.14 on epoch=3
05/23/2022 15:10:04 - INFO - __main__ - Global step 50 Train loss 2.61 Classification-F1 0.17951775304716483 on epoch=3
05/23/2022 15:10:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17951775304716483 on epoch=3, global_step=50
05/23/2022 15:10:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=3
05/23/2022 15:10:09 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=4
05/23/2022 15:10:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
05/23/2022 15:10:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=5
05/23/2022 15:10:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.89 on epoch=6
05/23/2022 15:10:20 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.16030452557921596 on epoch=6
05/23/2022 15:10:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=6
05/23/2022 15:10:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.96 on epoch=7
05/23/2022 15:10:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=8
05/23/2022 15:10:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.85 on epoch=8
05/23/2022 15:10:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/23/2022 15:10:36 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.11290545203588681 on epoch=9
05/23/2022 15:10:39 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
05/23/2022 15:10:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=10
05/23/2022 15:10:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/23/2022 15:10:46 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=11
05/23/2022 15:10:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=12
05/23/2022 15:10:52 - INFO - __main__ - Global step 200 Train loss 0.86 Classification-F1 0.1337837837837838 on epoch=12
05/23/2022 15:10:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=13
05/23/2022 15:10:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=13
05/23/2022 15:10:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.89 on epoch=14
05/23/2022 15:11:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=14
05/23/2022 15:11:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=15
05/23/2022 15:11:08 - INFO - __main__ - Global step 250 Train loss 0.83 Classification-F1 0.26145964431118934 on epoch=15
05/23/2022 15:11:08 - INFO - __main__ - Saving model with best Classification-F1: 0.17951775304716483 -> 0.26145964431118934 on epoch=15, global_step=250
05/23/2022 15:11:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.88 on epoch=16
05/23/2022 15:11:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/23/2022 15:11:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=17
05/23/2022 15:11:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=18
05/23/2022 15:11:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=18
05/23/2022 15:11:24 - INFO - __main__ - Global step 300 Train loss 0.82 Classification-F1 0.2086046262582994 on epoch=18
05/23/2022 15:11:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
05/23/2022 15:11:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.84 on epoch=19
05/23/2022 15:11:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.78 on epoch=20
05/23/2022 15:11:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.85 on epoch=21
05/23/2022 15:11:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=21
05/23/2022 15:11:40 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.32754432839446196 on epoch=21
05/23/2022 15:11:40 - INFO - __main__ - Saving model with best Classification-F1: 0.26145964431118934 -> 0.32754432839446196 on epoch=21, global_step=350
05/23/2022 15:11:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.72 on epoch=22
05/23/2022 15:11:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.75 on epoch=23
05/23/2022 15:11:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=23
05/23/2022 15:11:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.74 on epoch=24
05/23/2022 15:11:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=24
05/23/2022 15:11:56 - INFO - __main__ - Global step 400 Train loss 0.73 Classification-F1 0.4507107976233005 on epoch=24
05/23/2022 15:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.32754432839446196 -> 0.4507107976233005 on epoch=24, global_step=400
05/23/2022 15:11:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.69 on epoch=25
05/23/2022 15:12:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.73 on epoch=26
05/23/2022 15:12:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=26
05/23/2022 15:12:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.76 on epoch=27
05/23/2022 15:12:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.67 on epoch=28
05/23/2022 15:12:12 - INFO - __main__ - Global step 450 Train loss 0.71 Classification-F1 0.43563952797393257 on epoch=28
05/23/2022 15:12:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=28
05/23/2022 15:12:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=29
05/23/2022 15:12:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=29
05/23/2022 15:12:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=30
05/23/2022 15:12:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.62 on epoch=31
05/23/2022 15:12:28 - INFO - __main__ - Global step 500 Train loss 0.64 Classification-F1 0.6764404080193553 on epoch=31
05/23/2022 15:12:28 - INFO - __main__ - Saving model with best Classification-F1: 0.4507107976233005 -> 0.6764404080193553 on epoch=31, global_step=500
05/23/2022 15:12:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=31
05/23/2022 15:12:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=32
05/23/2022 15:12:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=33
05/23/2022 15:12:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=33
05/23/2022 15:12:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=34
05/23/2022 15:12:44 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.6854402352033931 on epoch=34
05/23/2022 15:12:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6764404080193553 -> 0.6854402352033931 on epoch=34, global_step=550
05/23/2022 15:12:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/23/2022 15:12:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=35
05/23/2022 15:12:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/23/2022 15:12:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=36
05/23/2022 15:12:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=37
05/23/2022 15:13:00 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.7104818719940806 on epoch=37
05/23/2022 15:13:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6854402352033931 -> 0.7104818719940806 on epoch=37, global_step=600
05/23/2022 15:13:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/23/2022 15:13:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/23/2022 15:13:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=39
05/23/2022 15:13:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.29 on epoch=39
05/23/2022 15:13:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=40
05/23/2022 15:13:16 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.6894929982742182 on epoch=40
05/23/2022 15:13:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=41
05/23/2022 15:13:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.27 on epoch=41
05/23/2022 15:13:23 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=42
05/23/2022 15:13:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=43
05/23/2022 15:13:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=43
05/23/2022 15:13:32 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.7008051175809338 on epoch=43
05/23/2022 15:13:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=44
05/23/2022 15:13:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=44
05/23/2022 15:13:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=45
05/23/2022 15:13:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=46
05/23/2022 15:13:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=46
05/23/2022 15:13:48 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.7456289300380919 on epoch=46
05/23/2022 15:13:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7104818719940806 -> 0.7456289300380919 on epoch=46, global_step=750
05/23/2022 15:13:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=47
05/23/2022 15:13:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/23/2022 15:13:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=48
05/23/2022 15:13:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=49
05/23/2022 15:14:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/23/2022 15:14:04 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.7341916105285089 on epoch=49
05/23/2022 15:14:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=50
05/23/2022 15:14:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=51
05/23/2022 15:14:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=51
05/23/2022 15:14:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/23/2022 15:14:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/23/2022 15:14:20 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.7704205720749838 on epoch=53
05/23/2022 15:14:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7456289300380919 -> 0.7704205720749838 on epoch=53, global_step=850
05/23/2022 15:14:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=53
05/23/2022 15:14:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=54
05/23/2022 15:14:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.14 on epoch=54
05/23/2022 15:14:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=55
05/23/2022 15:14:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/23/2022 15:14:36 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.7615533065657412 on epoch=56
05/23/2022 15:14:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=56
05/23/2022 15:14:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=57
05/23/2022 15:14:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.14 on epoch=58
05/23/2022 15:14:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=58
05/23/2022 15:14:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=59
05/23/2022 15:14:52 - INFO - __main__ - Global step 950 Train loss 0.15 Classification-F1 0.7565791650729938 on epoch=59
05/23/2022 15:14:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=59
05/23/2022 15:14:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=60
05/23/2022 15:15:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=61
05/23/2022 15:15:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.13 on epoch=61
05/23/2022 15:15:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=62
05/23/2022 15:15:09 - INFO - __main__ - Global step 1000 Train loss 0.12 Classification-F1 0.7525454154582669 on epoch=62
05/23/2022 15:15:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=63
05/23/2022 15:15:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=63
05/23/2022 15:15:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=64
05/23/2022 15:15:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=64
05/23/2022 15:15:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=65
05/23/2022 15:15:25 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.768500851993184 on epoch=65
05/23/2022 15:15:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=66
05/23/2022 15:15:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=66
05/23/2022 15:15:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.12 on epoch=67
05/23/2022 15:15:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=68
05/23/2022 15:15:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=68
05/23/2022 15:15:41 - INFO - __main__ - Global step 1100 Train loss 0.11 Classification-F1 0.7665080355749726 on epoch=68
05/23/2022 15:15:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=69
05/23/2022 15:15:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.05 on epoch=69
05/23/2022 15:15:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=70
05/23/2022 15:15:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=71
05/23/2022 15:15:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=71
05/23/2022 15:15:57 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.718951537816328 on epoch=71
05/23/2022 15:16:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=72
05/23/2022 15:16:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=73
05/23/2022 15:16:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=73
05/23/2022 15:16:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=74
05/23/2022 15:16:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=74
05/23/2022 15:16:13 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.76391617969116 on epoch=74
05/23/2022 15:16:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=75
05/23/2022 15:16:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
05/23/2022 15:16:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=76
05/23/2022 15:16:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=77
05/23/2022 15:16:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=78
05/23/2022 15:16:29 - INFO - __main__ - Global step 1250 Train loss 0.09 Classification-F1 0.7513113358825705 on epoch=78
05/23/2022 15:16:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=78
05/23/2022 15:16:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=79
05/23/2022 15:16:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=79
05/23/2022 15:16:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=80
05/23/2022 15:16:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=81
05/23/2022 15:16:45 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.750990608333486 on epoch=81
05/23/2022 15:16:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=81
05/23/2022 15:16:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=82
05/23/2022 15:16:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=83
05/23/2022 15:16:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=83
05/23/2022 15:16:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=84
05/23/2022 15:17:01 - INFO - __main__ - Global step 1350 Train loss 0.06 Classification-F1 0.7634433925466556 on epoch=84
05/23/2022 15:17:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=84
05/23/2022 15:17:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=85
05/23/2022 15:17:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=86
05/23/2022 15:17:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=86
05/23/2022 15:17:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=87
05/23/2022 15:17:17 - INFO - __main__ - Global step 1400 Train loss 0.08 Classification-F1 0.7403041102399619 on epoch=87
05/23/2022 15:17:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=88
05/23/2022 15:17:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=88
05/23/2022 15:17:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=89
05/23/2022 15:17:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=89
05/23/2022 15:17:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=90
05/23/2022 15:17:34 - INFO - __main__ - Global step 1450 Train loss 0.07 Classification-F1 0.7778467147728656 on epoch=90
05/23/2022 15:17:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7704205720749838 -> 0.7778467147728656 on epoch=90, global_step=1450
05/23/2022 15:17:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=91
05/23/2022 15:17:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=91
05/23/2022 15:17:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=92
05/23/2022 15:17:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=93
05/23/2022 15:17:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=93
05/23/2022 15:17:50 - INFO - __main__ - Global step 1500 Train loss 0.07 Classification-F1 0.7634150543986609 on epoch=93
05/23/2022 15:17:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=94
05/23/2022 15:17:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=94
05/23/2022 15:17:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=95
05/23/2022 15:18:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=96
05/23/2022 15:18:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=96
05/23/2022 15:18:07 - INFO - __main__ - Global step 1550 Train loss 0.09 Classification-F1 0.7686128330123576 on epoch=96
05/23/2022 15:18:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=97
05/23/2022 15:18:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=98
05/23/2022 15:18:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=98
05/23/2022 15:18:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=99
05/23/2022 15:18:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=99
05/23/2022 15:18:23 - INFO - __main__ - Global step 1600 Train loss 0.05 Classification-F1 0.7551085918453626 on epoch=99
05/23/2022 15:18:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=100
05/23/2022 15:18:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=101
05/23/2022 15:18:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=101
05/23/2022 15:18:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=102
05/23/2022 15:18:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=103
05/23/2022 15:18:40 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7705617831611328 on epoch=103
05/23/2022 15:18:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=103
05/23/2022 15:18:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=104
05/23/2022 15:18:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=104
05/23/2022 15:18:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=105
05/23/2022 15:18:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=106
05/23/2022 15:18:57 - INFO - __main__ - Global step 1700 Train loss 0.06 Classification-F1 0.7435044672184996 on epoch=106
05/23/2022 15:18:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=106
05/23/2022 15:19:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=107
05/23/2022 15:19:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=108
05/23/2022 15:19:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=108
05/23/2022 15:19:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=109
05/23/2022 15:19:13 - INFO - __main__ - Global step 1750 Train loss 0.06 Classification-F1 0.7811653446148421 on epoch=109
05/23/2022 15:19:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7778467147728656 -> 0.7811653446148421 on epoch=109, global_step=1750
05/23/2022 15:19:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=109
05/23/2022 15:19:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=110
05/23/2022 15:19:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=111
05/23/2022 15:19:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=111
05/23/2022 15:19:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=112
05/23/2022 15:19:30 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.7757425030078771 on epoch=112
05/23/2022 15:19:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=113
05/23/2022 15:19:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=113
05/23/2022 15:19:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=114
05/23/2022 15:19:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=114
05/23/2022 15:19:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=115
05/23/2022 15:19:46 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.7682623120720694 on epoch=115
05/23/2022 15:19:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
05/23/2022 15:19:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=116
05/23/2022 15:19:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=117
05/23/2022 15:19:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=118
05/23/2022 15:19:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=118
05/23/2022 15:20:02 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7607253235984579 on epoch=118
05/23/2022 15:20:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=119
05/23/2022 15:20:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=119
05/23/2022 15:20:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=120
05/23/2022 15:20:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=121
05/23/2022 15:20:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=121
05/23/2022 15:20:19 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.7573837763422641 on epoch=121
05/23/2022 15:20:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=122
05/23/2022 15:20:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=123
05/23/2022 15:20:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=123
05/23/2022 15:20:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=124
05/23/2022 15:20:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=124
05/23/2022 15:20:36 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.7437513681982043 on epoch=124
05/23/2022 15:20:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=125
05/23/2022 15:20:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.01 on epoch=126
05/23/2022 15:20:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.01 on epoch=126
05/23/2022 15:20:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=127
05/23/2022 15:20:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.01 on epoch=128
05/23/2022 15:20:52 - INFO - __main__ - Global step 2050 Train loss 0.01 Classification-F1 0.7780556150148865 on epoch=128
05/23/2022 15:20:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=128
05/23/2022 15:20:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/23/2022 15:21:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=129
05/23/2022 15:21:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=130
05/23/2022 15:21:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=131
05/23/2022 15:21:09 - INFO - __main__ - Global step 2100 Train loss 0.04 Classification-F1 0.7724079025549613 on epoch=131
05/23/2022 15:21:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=131
05/23/2022 15:21:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=132
05/23/2022 15:21:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.01 on epoch=133
05/23/2022 15:21:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.02 on epoch=133
05/23/2022 15:21:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=134
05/23/2022 15:21:26 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.8000738946093277 on epoch=134
05/23/2022 15:21:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7811653446148421 -> 0.8000738946093277 on epoch=134, global_step=2150
05/23/2022 15:21:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=134
05/23/2022 15:21:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=135
05/23/2022 15:21:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.00 on epoch=136
05/23/2022 15:21:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=136
05/23/2022 15:21:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=137
05/23/2022 15:21:42 - INFO - __main__ - Global step 2200 Train loss 0.03 Classification-F1 0.7751873002371508 on epoch=137
05/23/2022 15:21:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
05/23/2022 15:21:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/23/2022 15:21:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.01 on epoch=139
05/23/2022 15:21:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
05/23/2022 15:21:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=140
05/23/2022 15:21:59 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.786234838851858 on epoch=140
05/23/2022 15:22:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/23/2022 15:22:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.02 on epoch=141
05/23/2022 15:22:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=142
05/23/2022 15:22:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=143
05/23/2022 15:22:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=143
05/23/2022 15:22:16 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.7595272645314053 on epoch=143
05/23/2022 15:22:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.01 on epoch=144
05/23/2022 15:22:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=144
05/23/2022 15:22:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=145
05/23/2022 15:22:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=146
05/23/2022 15:22:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=146
05/23/2022 15:22:32 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.7835895300116108 on epoch=146
05/23/2022 15:22:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
05/23/2022 15:22:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=148
05/23/2022 15:22:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=148
05/23/2022 15:22:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=149
05/23/2022 15:22:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=149
05/23/2022 15:22:49 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.7587782633013886 on epoch=149
05/23/2022 15:22:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=150
05/23/2022 15:22:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.01 on epoch=151
05/23/2022 15:22:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/23/2022 15:22:59 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=152
05/23/2022 15:23:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
05/23/2022 15:23:05 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7767579124038112 on epoch=153
05/23/2022 15:23:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=153
05/23/2022 15:23:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=154
05/23/2022 15:23:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=154
05/23/2022 15:23:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=155
05/23/2022 15:23:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.00 on epoch=156
05/23/2022 15:23:22 - INFO - __main__ - Global step 2500 Train loss 0.01 Classification-F1 0.7833274835752915 on epoch=156
05/23/2022 15:23:24 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.00 on epoch=156
05/23/2022 15:23:27 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=157
05/23/2022 15:23:29 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/23/2022 15:23:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=158
05/23/2022 15:23:35 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=159
05/23/2022 15:23:38 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7839177282048096 on epoch=159
05/23/2022 15:23:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.00 on epoch=159
05/23/2022 15:23:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/23/2022 15:23:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.01 on epoch=161
05/23/2022 15:23:48 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=161
05/23/2022 15:23:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=162
05/23/2022 15:23:55 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.7857221146370259 on epoch=162
05/23/2022 15:23:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
05/23/2022 15:24:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/23/2022 15:24:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/23/2022 15:24:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=164
05/23/2022 15:24:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/23/2022 15:24:11 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.7818975913729733 on epoch=165
05/23/2022 15:24:14 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/23/2022 15:24:17 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=166
05/23/2022 15:24:19 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=167
05/23/2022 15:24:22 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
05/23/2022 15:24:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/23/2022 15:24:28 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.7800914477632445 on epoch=168
05/23/2022 15:24:30 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/23/2022 15:24:33 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=169
05/23/2022 15:24:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/23/2022 15:24:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/23/2022 15:24:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
05/23/2022 15:24:44 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7645942038732338 on epoch=171
05/23/2022 15:24:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=172
05/23/2022 15:24:49 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/23/2022 15:24:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=173
05/23/2022 15:24:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=174
05/23/2022 15:24:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=174
05/23/2022 15:25:01 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.7706771854542436 on epoch=174
05/23/2022 15:25:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/23/2022 15:25:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/23/2022 15:25:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=176
05/23/2022 15:25:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=177
05/23/2022 15:25:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/23/2022 15:25:17 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7946839253595918 on epoch=178
05/23/2022 15:25:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=178
05/23/2022 15:25:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=179
05/23/2022 15:25:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=179
05/23/2022 15:25:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/23/2022 15:25:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
05/23/2022 15:25:34 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.8025449320608504 on epoch=181
05/23/2022 15:25:34 - INFO - __main__ - Saving model with best Classification-F1: 0.8000738946093277 -> 0.8025449320608504 on epoch=181, global_step=2900
05/23/2022 15:25:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/23/2022 15:25:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/23/2022 15:25:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/23/2022 15:25:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=183
05/23/2022 15:25:46 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/23/2022 15:25:50 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7920703984766598 on epoch=184
05/23/2022 15:25:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=184
05/23/2022 15:25:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/23/2022 15:25:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/23/2022 15:26:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
05/23/2022 15:26:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/23/2022 15:26:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:26:04 - INFO - __main__ - Printing 3 examples
05/23/2022 15:26:04 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:26:04 - INFO - __main__ - ['others']
05/23/2022 15:26:04 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:26:04 - INFO - __main__ - ['others']
05/23/2022 15:26:04 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:26:04 - INFO - __main__ - ['others']
05/23/2022 15:26:04 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:26:04 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:26:05 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:26:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:26:05 - INFO - __main__ - Printing 3 examples
05/23/2022 15:26:05 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:26:05 - INFO - __main__ - ['others']
05/23/2022 15:26:05 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:26:05 - INFO - __main__ - ['others']
05/23/2022 15:26:05 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:26:05 - INFO - __main__ - ['others']
05/23/2022 15:26:05 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:26:05 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:26:05 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:26:06 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.8004593342237915 on epoch=187
05/23/2022 15:26:06 - INFO - __main__ - save last model!
05/23/2022 15:26:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 15:26:07 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 15:26:07 - INFO - __main__ - Printing 3 examples
05/23/2022 15:26:07 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 15:26:07 - INFO - __main__ - ['others']
05/23/2022 15:26:07 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 15:26:07 - INFO - __main__ - ['others']
05/23/2022 15:26:07 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 15:26:07 - INFO - __main__ - ['others']
05/23/2022 15:26:07 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:26:09 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:26:14 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 15:26:23 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:26:23 - INFO - __main__ - task name: emo
05/23/2022 15:26:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:26:24 - INFO - __main__ - Starting training!
05/23/2022 15:27:31 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_87_0.5_8_predictions.txt
05/23/2022 15:27:31 - INFO - __main__ - Classification-F1 on test data: 0.5859
05/23/2022 15:27:32 - INFO - __main__ - prefix=emo_64_87, lr=0.5, bsz=8, dev_performance=0.8025449320608504, test_performance=0.5858562410752027
05/23/2022 15:27:32 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.4, bsz=8 ...
05/23/2022 15:27:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:27:33 - INFO - __main__ - Printing 3 examples
05/23/2022 15:27:33 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:27:33 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:27:33 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:27:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:27:33 - INFO - __main__ - Printing 3 examples
05/23/2022 15:27:33 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:27:33 - INFO - __main__ - ['others']
05/23/2022 15:27:33 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:27:33 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:27:34 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:27:49 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:27:49 - INFO - __main__ - task name: emo
05/23/2022 15:27:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:27:50 - INFO - __main__ - Starting training!
05/23/2022 15:27:53 - INFO - __main__ - Step 10 Global step 10 Train loss 6.78 on epoch=0
05/23/2022 15:27:55 - INFO - __main__ - Step 20 Global step 20 Train loss 3.20 on epoch=1
05/23/2022 15:27:58 - INFO - __main__ - Step 30 Global step 30 Train loss 1.59 on epoch=1
05/23/2022 15:28:00 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=2
05/23/2022 15:28:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.17 on epoch=3
05/23/2022 15:28:06 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.12685204690681834 on epoch=3
05/23/2022 15:28:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12685204690681834 on epoch=3, global_step=50
05/23/2022 15:28:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=3
05/23/2022 15:28:11 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=4
05/23/2022 15:28:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=4
05/23/2022 15:28:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
05/23/2022 15:28:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=6
05/23/2022 15:28:22 - INFO - __main__ - Global step 100 Train loss 1.01 Classification-F1 0.11140170701194048 on epoch=6
05/23/2022 15:28:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=6
05/23/2022 15:28:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=7
05/23/2022 15:28:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=8
05/23/2022 15:28:32 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=8
05/23/2022 15:28:35 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/23/2022 15:28:38 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.14986180596858273 on epoch=9
05/23/2022 15:28:38 - INFO - __main__ - Saving model with best Classification-F1: 0.12685204690681834 -> 0.14986180596858273 on epoch=9, global_step=150
05/23/2022 15:28:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.88 on epoch=9
05/23/2022 15:28:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.84 on epoch=10
05/23/2022 15:28:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=11
05/23/2022 15:28:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=11
05/23/2022 15:28:51 - INFO - __main__ - Step 200 Global step 200 Train loss 0.97 on epoch=12
05/23/2022 15:28:54 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.10663236134934248 on epoch=12
05/23/2022 15:28:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.92 on epoch=13
05/23/2022 15:28:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=13
05/23/2022 15:29:02 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=14
05/23/2022 15:29:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.87 on epoch=14
05/23/2022 15:29:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=15
05/23/2022 15:29:10 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.14839844543791914 on epoch=15
05/23/2022 15:29:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.80 on epoch=16
05/23/2022 15:29:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.87 on epoch=16
05/23/2022 15:29:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=17
05/23/2022 15:29:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.81 on epoch=18
05/23/2022 15:29:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=18
05/23/2022 15:29:26 - INFO - __main__ - Global step 300 Train loss 0.83 Classification-F1 0.15968774551593867 on epoch=18
05/23/2022 15:29:26 - INFO - __main__ - Saving model with best Classification-F1: 0.14986180596858273 -> 0.15968774551593867 on epoch=18, global_step=300
05/23/2022 15:29:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
05/23/2022 15:29:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.83 on epoch=19
05/23/2022 15:29:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.81 on epoch=20
05/23/2022 15:29:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.83 on epoch=21
05/23/2022 15:29:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=21
05/23/2022 15:29:42 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.41835900145956006 on epoch=21
05/23/2022 15:29:42 - INFO - __main__ - Saving model with best Classification-F1: 0.15968774551593867 -> 0.41835900145956006 on epoch=21, global_step=350
05/23/2022 15:29:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.81 on epoch=22
05/23/2022 15:29:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.76 on epoch=23
05/23/2022 15:29:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.81 on epoch=23
05/23/2022 15:29:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.76 on epoch=24
05/23/2022 15:29:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.88 on epoch=24
05/23/2022 15:29:58 - INFO - __main__ - Global step 400 Train loss 0.80 Classification-F1 0.1682468600216912 on epoch=24
05/23/2022 15:30:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.72 on epoch=25
05/23/2022 15:30:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.75 on epoch=26
05/23/2022 15:30:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=26
05/23/2022 15:30:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=27
05/23/2022 15:30:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.76 on epoch=28
05/23/2022 15:30:14 - INFO - __main__ - Global step 450 Train loss 0.74 Classification-F1 0.3489552537822758 on epoch=28
05/23/2022 15:30:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.73 on epoch=28
05/23/2022 15:30:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.73 on epoch=29
05/23/2022 15:30:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=29
05/23/2022 15:30:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=30
05/23/2022 15:30:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=31
05/23/2022 15:30:30 - INFO - __main__ - Global step 500 Train loss 0.74 Classification-F1 0.47441191225530444 on epoch=31
05/23/2022 15:30:30 - INFO - __main__ - Saving model with best Classification-F1: 0.41835900145956006 -> 0.47441191225530444 on epoch=31, global_step=500
05/23/2022 15:30:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=31
05/23/2022 15:30:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.68 on epoch=32
05/23/2022 15:30:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.63 on epoch=33
05/23/2022 15:30:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=33
05/23/2022 15:30:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.69 on epoch=34
05/23/2022 15:30:46 - INFO - __main__ - Global step 550 Train loss 0.65 Classification-F1 0.46008380762385337 on epoch=34
05/23/2022 15:30:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=34
05/23/2022 15:30:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=35
05/23/2022 15:30:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=36
05/23/2022 15:30:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.63 on epoch=36
05/23/2022 15:30:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=37
05/23/2022 15:31:01 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.6260903683710701 on epoch=37
05/23/2022 15:31:01 - INFO - __main__ - Saving model with best Classification-F1: 0.47441191225530444 -> 0.6260903683710701 on epoch=37, global_step=600
05/23/2022 15:31:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=38
05/23/2022 15:31:06 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=38
05/23/2022 15:31:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=39
05/23/2022 15:31:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=39
05/23/2022 15:31:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=40
05/23/2022 15:31:17 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.642627710617881 on epoch=40
05/23/2022 15:31:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6260903683710701 -> 0.642627710617881 on epoch=40, global_step=650
05/23/2022 15:31:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=41
05/23/2022 15:31:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
05/23/2022 15:31:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=42
05/23/2022 15:31:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
05/23/2022 15:31:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
05/23/2022 15:31:32 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.6524088700450674 on epoch=43
05/23/2022 15:31:32 - INFO - __main__ - Saving model with best Classification-F1: 0.642627710617881 -> 0.6524088700450674 on epoch=43, global_step=700
05/23/2022 15:31:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
05/23/2022 15:31:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=44
05/23/2022 15:31:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/23/2022 15:31:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
05/23/2022 15:31:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/23/2022 15:31:48 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.6585747462919593 on epoch=46
05/23/2022 15:31:48 - INFO - __main__ - Saving model with best Classification-F1: 0.6524088700450674 -> 0.6585747462919593 on epoch=46, global_step=750
05/23/2022 15:31:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
05/23/2022 15:31:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
05/23/2022 15:31:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=48
05/23/2022 15:31:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=49
05/23/2022 15:32:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=49
05/23/2022 15:32:04 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.7068851654944689 on epoch=49
05/23/2022 15:32:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6585747462919593 -> 0.7068851654944689 on epoch=49, global_step=800
05/23/2022 15:32:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/23/2022 15:32:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=51
05/23/2022 15:32:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=51
05/23/2022 15:32:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/23/2022 15:32:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
05/23/2022 15:32:19 - INFO - __main__ - Global step 850 Train loss 0.25 Classification-F1 0.7282960689271402 on epoch=53
05/23/2022 15:32:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7068851654944689 -> 0.7282960689271402 on epoch=53, global_step=850
05/23/2022 15:32:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=53
05/23/2022 15:32:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/23/2022 15:32:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
05/23/2022 15:32:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=55
05/23/2022 15:32:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=56
05/23/2022 15:32:35 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.6820554456756818 on epoch=56
05/23/2022 15:32:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=56
05/23/2022 15:32:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=57
05/23/2022 15:32:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/23/2022 15:32:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=58
05/23/2022 15:32:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=59
05/23/2022 15:32:50 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.7350840357846897 on epoch=59
05/23/2022 15:32:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7282960689271402 -> 0.7350840357846897 on epoch=59, global_step=950
05/23/2022 15:32:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=59
05/23/2022 15:32:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=60
05/23/2022 15:32:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/23/2022 15:33:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=61
05/23/2022 15:33:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=62
05/23/2022 15:33:06 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.6962126704314204 on epoch=62
05/23/2022 15:33:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=63
05/23/2022 15:33:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=63
05/23/2022 15:33:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=64
05/23/2022 15:33:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=64
05/23/2022 15:33:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=65
05/23/2022 15:33:22 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.7368190362118274 on epoch=65
05/23/2022 15:33:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7350840357846897 -> 0.7368190362118274 on epoch=65, global_step=1050
05/23/2022 15:33:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=66
05/23/2022 15:33:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=66
05/23/2022 15:33:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=67
05/23/2022 15:33:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/23/2022 15:33:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=68
05/23/2022 15:33:37 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.7340683265778808 on epoch=68
05/23/2022 15:33:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=69
05/23/2022 15:33:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=69
05/23/2022 15:33:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=70
05/23/2022 15:33:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=71
05/23/2022 15:33:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=71
05/23/2022 15:33:53 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.7498742994125904 on epoch=71
05/23/2022 15:33:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7368190362118274 -> 0.7498742994125904 on epoch=71, global_step=1150
05/23/2022 15:33:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=72
05/23/2022 15:33:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=73
05/23/2022 15:34:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=73
05/23/2022 15:34:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=74
05/23/2022 15:34:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/23/2022 15:34:09 - INFO - __main__ - Global step 1200 Train loss 0.14 Classification-F1 0.742626728110599 on epoch=74
05/23/2022 15:34:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/23/2022 15:34:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=76
05/23/2022 15:34:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=76
05/23/2022 15:34:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=77
05/23/2022 15:34:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=78
05/23/2022 15:34:25 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.7671100459126181 on epoch=78
05/23/2022 15:34:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7498742994125904 -> 0.7671100459126181 on epoch=78, global_step=1250
05/23/2022 15:34:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=78
05/23/2022 15:34:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=79
05/23/2022 15:34:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=79
05/23/2022 15:34:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=80
05/23/2022 15:34:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=81
05/23/2022 15:34:41 - INFO - __main__ - Global step 1300 Train loss 0.07 Classification-F1 0.774048617349383 on epoch=81
05/23/2022 15:34:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7671100459126181 -> 0.774048617349383 on epoch=81, global_step=1300
05/23/2022 15:34:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=81
05/23/2022 15:34:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=82
05/23/2022 15:34:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=83
05/23/2022 15:34:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=83
05/23/2022 15:34:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=84
05/23/2022 15:34:57 - INFO - __main__ - Global step 1350 Train loss 0.08 Classification-F1 0.7550311477140744 on epoch=84
05/23/2022 15:35:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=84
05/23/2022 15:35:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=85
05/23/2022 15:35:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=86
05/23/2022 15:35:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=86
05/23/2022 15:35:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=87
05/23/2022 15:35:13 - INFO - __main__ - Global step 1400 Train loss 0.09 Classification-F1 0.7771866159792975 on epoch=87
05/23/2022 15:35:14 - INFO - __main__ - Saving model with best Classification-F1: 0.774048617349383 -> 0.7771866159792975 on epoch=87, global_step=1400
05/23/2022 15:35:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.06 on epoch=88
05/23/2022 15:35:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=88
05/23/2022 15:35:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=89
05/23/2022 15:35:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=89
05/23/2022 15:35:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=90
05/23/2022 15:35:30 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.7936617743307971 on epoch=90
05/23/2022 15:35:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7771866159792975 -> 0.7936617743307971 on epoch=90, global_step=1450
05/23/2022 15:35:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=91
05/23/2022 15:35:35 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=91
05/23/2022 15:35:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=92
05/23/2022 15:35:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=93
05/23/2022 15:35:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=93
05/23/2022 15:35:46 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.7538950689565443 on epoch=93
05/23/2022 15:35:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=94
05/23/2022 15:35:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=94
05/23/2022 15:35:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=95
05/23/2022 15:35:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=96
05/23/2022 15:35:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=96
05/23/2022 15:36:02 - INFO - __main__ - Global step 1550 Train loss 0.05 Classification-F1 0.7646546300356523 on epoch=96
05/23/2022 15:36:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=97
05/23/2022 15:36:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=98
05/23/2022 15:36:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.06 on epoch=98
05/23/2022 15:36:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=99
05/23/2022 15:36:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=99
05/23/2022 15:36:18 - INFO - __main__ - Global step 1600 Train loss 0.07 Classification-F1 0.7472704726162038 on epoch=99
05/23/2022 15:36:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=100
05/23/2022 15:36:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=101
05/23/2022 15:36:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=101
05/23/2022 15:36:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=102
05/23/2022 15:36:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=103
05/23/2022 15:36:35 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.7803568961918773 on epoch=103
05/23/2022 15:36:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
05/23/2022 15:36:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=104
05/23/2022 15:36:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=104
05/23/2022 15:36:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=105
05/23/2022 15:36:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/23/2022 15:36:51 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.755921613267241 on epoch=106
05/23/2022 15:36:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=106
05/23/2022 15:36:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=107
05/23/2022 15:36:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=108
05/23/2022 15:37:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=108
05/23/2022 15:37:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=109
05/23/2022 15:37:07 - INFO - __main__ - Global step 1750 Train loss 0.04 Classification-F1 0.7438277342523762 on epoch=109
05/23/2022 15:37:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=109
05/23/2022 15:37:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=110
05/23/2022 15:37:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=111
05/23/2022 15:37:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=111
05/23/2022 15:37:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=112
05/23/2022 15:37:23 - INFO - __main__ - Global step 1800 Train loss 0.05 Classification-F1 0.7404233562306832 on epoch=112
05/23/2022 15:37:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=113
05/23/2022 15:37:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=113
05/23/2022 15:37:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=114
05/23/2022 15:37:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=114
05/23/2022 15:37:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/23/2022 15:37:39 - INFO - __main__ - Global step 1850 Train loss 0.05 Classification-F1 0.7485544441519698 on epoch=115
05/23/2022 15:37:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=116
05/23/2022 15:37:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=116
05/23/2022 15:37:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=117
05/23/2022 15:37:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=118
05/23/2022 15:37:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=118
05/23/2022 15:37:55 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.7629448665301904 on epoch=118
05/23/2022 15:37:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=119
05/23/2022 15:38:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=119
05/23/2022 15:38:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=120
05/23/2022 15:38:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=121
05/23/2022 15:38:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=121
05/23/2022 15:38:11 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.7714080726554453 on epoch=121
05/23/2022 15:38:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=122
05/23/2022 15:38:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/23/2022 15:38:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=123
05/23/2022 15:38:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.05 on epoch=124
05/23/2022 15:38:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=124
05/23/2022 15:38:27 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.7592273639067579 on epoch=124
05/23/2022 15:38:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.01 on epoch=125
05/23/2022 15:38:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/23/2022 15:38:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=126
05/23/2022 15:38:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.03 on epoch=127
05/23/2022 15:38:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.02 on epoch=128
05/23/2022 15:38:44 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.7666779738900025 on epoch=128
05/23/2022 15:38:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.03 on epoch=128
05/23/2022 15:38:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=129
05/23/2022 15:38:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=129
05/23/2022 15:38:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.02 on epoch=130
05/23/2022 15:38:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.01 on epoch=131
05/23/2022 15:39:00 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7737169552916248 on epoch=131
05/23/2022 15:39:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=131
05/23/2022 15:39:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=132
05/23/2022 15:39:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=133
05/23/2022 15:39:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/23/2022 15:39:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.02 on epoch=134
05/23/2022 15:39:16 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.7459983631625422 on epoch=134
05/23/2022 15:39:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/23/2022 15:39:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.01 on epoch=135
05/23/2022 15:39:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=136
05/23/2022 15:39:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.01 on epoch=136
05/23/2022 15:39:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=137
05/23/2022 15:39:32 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7772316520417046 on epoch=137
05/23/2022 15:39:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.01 on epoch=138
05/23/2022 15:39:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=138
05/23/2022 15:39:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=139
05/23/2022 15:39:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
05/23/2022 15:39:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.01 on epoch=140
05/23/2022 15:39:48 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.7535625898940973 on epoch=140
05/23/2022 15:39:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.01 on epoch=141
05/23/2022 15:39:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=141
05/23/2022 15:39:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=142
05/23/2022 15:39:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=143
05/23/2022 15:40:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.01 on epoch=143
05/23/2022 15:40:04 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7839303375979497 on epoch=143
05/23/2022 15:40:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=144
05/23/2022 15:40:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=144
05/23/2022 15:40:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=145
05/23/2022 15:40:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.01 on epoch=146
05/23/2022 15:40:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.00 on epoch=146
05/23/2022 15:40:20 - INFO - __main__ - Global step 2350 Train loss 0.01 Classification-F1 0.7847545375987838 on epoch=146
05/23/2022 15:40:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
05/23/2022 15:40:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=148
05/23/2022 15:40:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=148
05/23/2022 15:40:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=149
05/23/2022 15:40:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.01 on epoch=149
05/23/2022 15:40:37 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.7805959849511928 on epoch=149
05/23/2022 15:40:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.01 on epoch=150
05/23/2022 15:40:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.00 on epoch=151
05/23/2022 15:40:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=151
05/23/2022 15:40:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=152
05/23/2022 15:40:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=153
05/23/2022 15:40:53 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.7778993632733388 on epoch=153
05/23/2022 15:40:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
05/23/2022 15:40:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=154
05/23/2022 15:41:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=154
05/23/2022 15:41:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/23/2022 15:41:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=156
05/23/2022 15:41:09 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.7808772482014388 on epoch=156
05/23/2022 15:41:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=156
05/23/2022 15:41:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.01 on epoch=157
05/23/2022 15:41:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.01 on epoch=158
05/23/2022 15:41:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.00 on epoch=158
05/23/2022 15:41:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=159
05/23/2022 15:41:25 - INFO - __main__ - Global step 2550 Train loss 0.01 Classification-F1 0.7847247378446951 on epoch=159
05/23/2022 15:41:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/23/2022 15:41:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/23/2022 15:41:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=161
05/23/2022 15:41:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
05/23/2022 15:41:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/23/2022 15:41:41 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.772210380125339 on epoch=162
05/23/2022 15:41:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=163
05/23/2022 15:41:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/23/2022 15:41:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/23/2022 15:41:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.05 on epoch=164
05/23/2022 15:41:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=165
05/23/2022 15:41:58 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.7445915678524374 on epoch=165
05/23/2022 15:42:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=166
05/23/2022 15:42:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=166
05/23/2022 15:42:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.00 on epoch=167
05/23/2022 15:42:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
05/23/2022 15:42:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/23/2022 15:42:14 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.773549142182586 on epoch=168
05/23/2022 15:42:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
05/23/2022 15:42:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=169
05/23/2022 15:42:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.00 on epoch=170
05/23/2022 15:42:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/23/2022 15:42:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
05/23/2022 15:42:30 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.760338734271016 on epoch=171
05/23/2022 15:42:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.00 on epoch=172
05/23/2022 15:42:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=173
05/23/2022 15:42:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/23/2022 15:42:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.00 on epoch=174
05/23/2022 15:42:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=174
05/23/2022 15:42:46 - INFO - __main__ - Global step 2800 Train loss 0.01 Classification-F1 0.7881291069035915 on epoch=174
05/23/2022 15:42:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/23/2022 15:42:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=176
05/23/2022 15:42:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=176
05/23/2022 15:42:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=177
05/23/2022 15:42:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
05/23/2022 15:43:03 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7693336789277557 on epoch=178
05/23/2022 15:43:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/23/2022 15:43:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/23/2022 15:43:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/23/2022 15:43:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/23/2022 15:43:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=181
05/23/2022 15:43:19 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7668183618718378 on epoch=181
05/23/2022 15:43:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.00 on epoch=181
05/23/2022 15:43:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/23/2022 15:43:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/23/2022 15:43:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=183
05/23/2022 15:43:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/23/2022 15:43:35 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.7593078652685945 on epoch=184
05/23/2022 15:43:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
05/23/2022 15:43:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=185
05/23/2022 15:43:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.00 on epoch=186
05/23/2022 15:43:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/23/2022 15:43:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.00 on epoch=187
05/23/2022 15:43:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:43:49 - INFO - __main__ - Printing 3 examples
05/23/2022 15:43:49 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:43:49 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:43:49 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:43:49 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:43:49 - INFO - __main__ - Printing 3 examples
05/23/2022 15:43:49 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:43:49 - INFO - __main__ - ['others']
05/23/2022 15:43:49 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:43:49 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:43:50 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:43:52 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.7663744737592261 on epoch=187
05/23/2022 15:43:52 - INFO - __main__ - save last model!
05/23/2022 15:43:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 15:43:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 15:43:52 - INFO - __main__ - Printing 3 examples
05/23/2022 15:43:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 15:43:52 - INFO - __main__ - ['others']
05/23/2022 15:43:52 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 15:43:52 - INFO - __main__ - ['others']
05/23/2022 15:43:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 15:43:52 - INFO - __main__ - ['others']
05/23/2022 15:43:52 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:43:54 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:43:59 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 15:44:08 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:44:08 - INFO - __main__ - task name: emo
05/23/2022 15:44:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:44:09 - INFO - __main__ - Starting training!
05/23/2022 15:45:22 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_87_0.4_8_predictions.txt
05/23/2022 15:45:22 - INFO - __main__ - Classification-F1 on test data: 0.4035
05/23/2022 15:45:22 - INFO - __main__ - prefix=emo_64_87, lr=0.4, bsz=8, dev_performance=0.7936617743307971, test_performance=0.40345017160150737
05/23/2022 15:45:22 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.3, bsz=8 ...
05/23/2022 15:45:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:45:23 - INFO - __main__ - Printing 3 examples
05/23/2022 15:45:23 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:45:23 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:45:23 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 15:45:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 15:45:23 - INFO - __main__ - Printing 3 examples
05/23/2022 15:45:23 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 15:45:23 - INFO - __main__ - ['others']
05/23/2022 15:45:23 - INFO - __main__ - Tokenizing Input ...
05/23/2022 15:45:24 - INFO - __main__ - Tokenizing Output ...
05/23/2022 15:45:24 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 15:45:43 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 15:45:43 - INFO - __main__ - task name: emo
05/23/2022 15:45:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 15:45:44 - INFO - __main__ - Starting training!
05/23/2022 15:45:46 - INFO - __main__ - Step 10 Global step 10 Train loss 7.73 on epoch=0
05/23/2022 15:45:49 - INFO - __main__ - Step 20 Global step 20 Train loss 5.11 on epoch=1
05/23/2022 15:45:51 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=1
05/23/2022 15:45:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.53 on epoch=2
05/23/2022 15:45:56 - INFO - __main__ - Step 50 Global step 50 Train loss 1.15 on epoch=3
05/23/2022 15:46:00 - INFO - __main__ - Global step 50 Train loss 3.62 Classification-F1 0.12782823594180526 on epoch=3
05/23/2022 15:46:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.12782823594180526 on epoch=3, global_step=50
05/23/2022 15:46:02 - INFO - __main__ - Step 60 Global step 60 Train loss 1.06 on epoch=3
05/23/2022 15:46:05 - INFO - __main__ - Step 70 Global step 70 Train loss 1.17 on epoch=4
05/23/2022 15:46:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=4
05/23/2022 15:46:10 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=5
05/23/2022 15:46:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=6
05/23/2022 15:46:15 - INFO - __main__ - Global step 100 Train loss 1.03 Classification-F1 0.11515151515151514 on epoch=6
05/23/2022 15:46:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=6
05/23/2022 15:46:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.05 on epoch=7
05/23/2022 15:46:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=8
05/23/2022 15:46:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.83 on epoch=8
05/23/2022 15:46:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.98 on epoch=9
05/23/2022 15:46:31 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.09874608150470221 on epoch=9
05/23/2022 15:46:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=9
05/23/2022 15:46:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=10
05/23/2022 15:46:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=11
05/23/2022 15:46:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=11
05/23/2022 15:46:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.92 on epoch=12
05/23/2022 15:46:47 - INFO - __main__ - Global step 200 Train loss 0.89 Classification-F1 0.18206381883685024 on epoch=12
05/23/2022 15:46:47 - INFO - __main__ - Saving model with best Classification-F1: 0.12782823594180526 -> 0.18206381883685024 on epoch=12, global_step=200
05/23/2022 15:46:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=13
05/23/2022 15:46:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=13
05/23/2022 15:46:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=14
05/23/2022 15:46:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=14
05/23/2022 15:46:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.81 on epoch=15
05/23/2022 15:47:02 - INFO - __main__ - Global step 250 Train loss 0.86 Classification-F1 0.11074873679375287 on epoch=15
05/23/2022 15:47:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.95 on epoch=16
05/23/2022 15:47:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.86 on epoch=16
05/23/2022 15:47:10 - INFO - __main__ - Step 280 Global step 280 Train loss 1.01 on epoch=17
05/23/2022 15:47:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=18
05/23/2022 15:47:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.85 on epoch=18
05/23/2022 15:47:18 - INFO - __main__ - Global step 300 Train loss 0.92 Classification-F1 0.218252152119429 on epoch=18
05/23/2022 15:47:18 - INFO - __main__ - Saving model with best Classification-F1: 0.18206381883685024 -> 0.218252152119429 on epoch=18, global_step=300
05/23/2022 15:47:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.75 on epoch=19
05/23/2022 15:47:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.91 on epoch=19
05/23/2022 15:47:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.89 on epoch=20
05/23/2022 15:47:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=21
05/23/2022 15:47:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.79 on epoch=21
05/23/2022 15:47:34 - INFO - __main__ - Global step 350 Train loss 0.83 Classification-F1 0.28806366398218364 on epoch=21
05/23/2022 15:47:34 - INFO - __main__ - Saving model with best Classification-F1: 0.218252152119429 -> 0.28806366398218364 on epoch=21, global_step=350
05/23/2022 15:47:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.84 on epoch=22
05/23/2022 15:47:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.79 on epoch=23
05/23/2022 15:47:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=23
05/23/2022 15:47:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.83 on epoch=24
05/23/2022 15:47:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=24
05/23/2022 15:47:49 - INFO - __main__ - Global step 400 Train loss 0.81 Classification-F1 0.321902986941512 on epoch=24
05/23/2022 15:47:49 - INFO - __main__ - Saving model with best Classification-F1: 0.28806366398218364 -> 0.321902986941512 on epoch=24, global_step=400
05/23/2022 15:47:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.78 on epoch=25
05/23/2022 15:47:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.80 on epoch=26
05/23/2022 15:47:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.78 on epoch=26
05/23/2022 15:47:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.77 on epoch=27
05/23/2022 15:48:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=28
05/23/2022 15:48:05 - INFO - __main__ - Global step 450 Train loss 0.79 Classification-F1 0.38215373186252777 on epoch=28
05/23/2022 15:48:05 - INFO - __main__ - Saving model with best Classification-F1: 0.321902986941512 -> 0.38215373186252777 on epoch=28, global_step=450
05/23/2022 15:48:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=28
05/23/2022 15:48:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.75 on epoch=29
05/23/2022 15:48:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.73 on epoch=29
05/23/2022 15:48:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.77 on epoch=30
05/23/2022 15:48:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=31
05/23/2022 15:48:21 - INFO - __main__ - Global step 500 Train loss 0.75 Classification-F1 0.30779863411442354 on epoch=31
05/23/2022 15:48:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=31
05/23/2022 15:48:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.75 on epoch=32
05/23/2022 15:48:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.69 on epoch=33
05/23/2022 15:48:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=33
05/23/2022 15:48:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.87 on epoch=34
05/23/2022 15:48:36 - INFO - __main__ - Global step 550 Train loss 0.74 Classification-F1 0.24762543230915743 on epoch=34
05/23/2022 15:48:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.72 on epoch=34
05/23/2022 15:48:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=35
05/23/2022 15:48:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.67 on epoch=36
05/23/2022 15:48:46 - INFO - __main__ - Step 590 Global step 590 Train loss 0.62 on epoch=36
05/23/2022 15:48:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.67 on epoch=37
05/23/2022 15:48:52 - INFO - __main__ - Global step 600 Train loss 0.67 Classification-F1 0.3486426489858985 on epoch=37
05/23/2022 15:48:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=38
05/23/2022 15:48:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.71 on epoch=38
05/23/2022 15:48:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=39
05/23/2022 15:49:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=39
05/23/2022 15:49:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=40
05/23/2022 15:49:08 - INFO - __main__ - Global step 650 Train loss 0.65 Classification-F1 0.5568220313167004 on epoch=40
05/23/2022 15:49:08 - INFO - __main__ - Saving model with best Classification-F1: 0.38215373186252777 -> 0.5568220313167004 on epoch=40, global_step=650
05/23/2022 15:49:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.68 on epoch=41
05/23/2022 15:49:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=41
05/23/2022 15:49:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.68 on epoch=42
05/23/2022 15:49:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.75 on epoch=43
05/23/2022 15:49:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=43
05/23/2022 15:49:23 - INFO - __main__ - Global step 700 Train loss 0.66 Classification-F1 0.4409847340597691 on epoch=43
05/23/2022 15:49:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=44
05/23/2022 15:49:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.56 on epoch=44
05/23/2022 15:49:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=45
05/23/2022 15:49:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.58 on epoch=46
05/23/2022 15:49:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=46
05/23/2022 15:49:39 - INFO - __main__ - Global step 750 Train loss 0.58 Classification-F1 0.649408525942925 on epoch=46
05/23/2022 15:49:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5568220313167004 -> 0.649408525942925 on epoch=46, global_step=750
05/23/2022 15:49:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.60 on epoch=47
05/23/2022 15:49:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=48
05/23/2022 15:49:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=48
05/23/2022 15:49:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.56 on epoch=49
05/23/2022 15:49:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=49
05/23/2022 15:49:54 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.6611522198731502 on epoch=49
05/23/2022 15:49:54 - INFO - __main__ - Saving model with best Classification-F1: 0.649408525942925 -> 0.6611522198731502 on epoch=49, global_step=800
05/23/2022 15:49:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=50
05/23/2022 15:50:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=51
05/23/2022 15:50:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=51
05/23/2022 15:50:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=52
05/23/2022 15:50:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=53
05/23/2022 15:50:10 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.6935922252336459 on epoch=53
05/23/2022 15:50:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6611522198731502 -> 0.6935922252336459 on epoch=53, global_step=850
05/23/2022 15:50:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=53
05/23/2022 15:50:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=54
05/23/2022 15:50:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=54
05/23/2022 15:50:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=55
05/23/2022 15:50:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=56
05/23/2022 15:50:26 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.6628016423592012 on epoch=56
05/23/2022 15:50:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=56
05/23/2022 15:50:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=57
05/23/2022 15:50:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=58
05/23/2022 15:50:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=58
05/23/2022 15:50:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
05/23/2022 15:50:42 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.7367308309038993 on epoch=59
05/23/2022 15:50:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6935922252336459 -> 0.7367308309038993 on epoch=59, global_step=950
05/23/2022 15:50:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
05/23/2022 15:50:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
05/23/2022 15:50:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=61
05/23/2022 15:50:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=61
05/23/2022 15:50:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/23/2022 15:50:58 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.7315681389470667 on epoch=62
05/23/2022 15:51:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=63
05/23/2022 15:51:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/23/2022 15:51:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=64
05/23/2022 15:51:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
05/23/2022 15:51:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=65
05/23/2022 15:51:14 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7278986677190968 on epoch=65
05/23/2022 15:51:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=66
05/23/2022 15:51:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=66
05/23/2022 15:51:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
05/23/2022 15:51:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=68
05/23/2022 15:51:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=68
05/23/2022 15:51:30 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.6977001582177127 on epoch=68
05/23/2022 15:51:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=69
05/23/2022 15:51:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/23/2022 15:51:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.31 on epoch=70
05/23/2022 15:51:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.38 on epoch=71
05/23/2022 15:51:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/23/2022 15:51:46 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.7047995015197985 on epoch=71
05/23/2022 15:51:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/23/2022 15:51:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=73
05/23/2022 15:51:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/23/2022 15:51:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=74
05/23/2022 15:51:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
05/23/2022 15:52:01 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.6962704446502868 on epoch=74
05/23/2022 15:52:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
05/23/2022 15:52:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/23/2022 15:52:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
05/23/2022 15:52:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=77
05/23/2022 15:52:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=78
05/23/2022 15:52:17 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.7108568067176353 on epoch=78
05/23/2022 15:52:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=78
05/23/2022 15:52:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=79
05/23/2022 15:52:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.32 on epoch=79
05/23/2022 15:52:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/23/2022 15:52:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=81
05/23/2022 15:52:33 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.751822393964777 on epoch=81
05/23/2022 15:52:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7367308309038993 -> 0.751822393964777 on epoch=81, global_step=1300
05/23/2022 15:52:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=81
05/23/2022 15:52:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=82
05/23/2022 15:52:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
05/23/2022 15:52:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=83
05/23/2022 15:52:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=84
05/23/2022 15:52:49 - INFO - __main__ - Global step 1350 Train loss 0.24 Classification-F1 0.721109092495243 on epoch=84
05/23/2022 15:52:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=84
05/23/2022 15:52:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/23/2022 15:52:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=86
05/23/2022 15:52:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=86
05/23/2022 15:53:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=87
05/23/2022 15:53:04 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.744100782536694 on epoch=87
05/23/2022 15:53:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/23/2022 15:53:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=88
05/23/2022 15:53:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
05/23/2022 15:53:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=89
05/23/2022 15:53:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
05/23/2022 15:53:20 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.7332474805535374 on epoch=90
05/23/2022 15:53:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=91
05/23/2022 15:53:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=91
05/23/2022 15:53:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=92
05/23/2022 15:53:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=93
05/23/2022 15:53:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/23/2022 15:53:36 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.7423805954330451 on epoch=93
05/23/2022 15:53:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/23/2022 15:53:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=94
05/23/2022 15:53:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/23/2022 15:53:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/23/2022 15:53:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=96
05/23/2022 15:53:52 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.7072249323064279 on epoch=96
05/23/2022 15:53:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=97
05/23/2022 15:53:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/23/2022 15:53:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=98
05/23/2022 15:54:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=99
05/23/2022 15:54:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=99
05/23/2022 15:54:08 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7203947608866473 on epoch=99
05/23/2022 15:54:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=100
05/23/2022 15:54:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
05/23/2022 15:54:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=101
05/23/2022 15:54:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=102
05/23/2022 15:54:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
05/23/2022 15:54:24 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.7309337340439052 on epoch=103
05/23/2022 15:54:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=103
05/23/2022 15:54:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=104
05/23/2022 15:54:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
05/23/2022 15:54:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=105
05/23/2022 15:54:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=106
05/23/2022 15:54:40 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.7105892920101405 on epoch=106
05/23/2022 15:54:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=106
05/23/2022 15:54:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/23/2022 15:54:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=108
05/23/2022 15:54:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/23/2022 15:54:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=109
05/23/2022 15:54:55 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.7156641781641782 on epoch=109
05/23/2022 15:54:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/23/2022 15:55:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=110
05/23/2022 15:55:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/23/2022 15:55:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/23/2022 15:55:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
05/23/2022 15:55:11 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.7390304294994967 on epoch=112
05/23/2022 15:55:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=113
05/23/2022 15:55:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
05/23/2022 15:55:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/23/2022 15:55:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/23/2022 15:55:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=115
05/23/2022 15:55:27 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7280802664600293 on epoch=115
05/23/2022 15:55:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
05/23/2022 15:55:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=116
05/23/2022 15:55:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=117
05/23/2022 15:55:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=118
05/23/2022 15:55:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
05/23/2022 15:55:44 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.7418634744089749 on epoch=118
05/23/2022 15:55:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=119
05/23/2022 15:55:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=119
05/23/2022 15:55:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
05/23/2022 15:55:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=121
05/23/2022 15:55:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/23/2022 15:56:00 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7304419214151588 on epoch=121
05/23/2022 15:56:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=122
05/23/2022 15:56:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
05/23/2022 15:56:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
05/23/2022 15:56:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
05/23/2022 15:56:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/23/2022 15:56:16 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.7170821344004578 on epoch=124
05/23/2022 15:56:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
05/23/2022 15:56:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/23/2022 15:56:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=126
05/23/2022 15:56:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/23/2022 15:56:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=128
05/23/2022 15:56:32 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.7572606180041938 on epoch=128
05/23/2022 15:56:32 - INFO - __main__ - Saving model with best Classification-F1: 0.751822393964777 -> 0.7572606180041938 on epoch=128, global_step=2050
05/23/2022 15:56:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
05/23/2022 15:56:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=129
05/23/2022 15:56:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.04 on epoch=129
05/23/2022 15:56:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=130
05/23/2022 15:56:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
05/23/2022 15:56:48 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7284381704718992 on epoch=131
05/23/2022 15:56:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.04 on epoch=131
05/23/2022 15:56:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=132
05/23/2022 15:56:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/23/2022 15:56:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/23/2022 15:57:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/23/2022 15:57:04 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.7443808054981087 on epoch=134
05/23/2022 15:57:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=134
05/23/2022 15:57:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=135
05/23/2022 15:57:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/23/2022 15:57:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=136
05/23/2022 15:57:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=137
05/23/2022 15:57:20 - INFO - __main__ - Global step 2200 Train loss 0.06 Classification-F1 0.7464052210415846 on epoch=137
05/23/2022 15:57:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/23/2022 15:57:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/23/2022 15:57:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=139
05/23/2022 15:57:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=139
05/23/2022 15:57:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=140
05/23/2022 15:57:36 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7503084446564885 on epoch=140
05/23/2022 15:57:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=141
05/23/2022 15:57:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
05/23/2022 15:57:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=142
05/23/2022 15:57:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=143
05/23/2022 15:57:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=143
05/23/2022 15:57:52 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.7304122137404581 on epoch=143
05/23/2022 15:57:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=144
05/23/2022 15:57:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=144
05/23/2022 15:57:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=145
05/23/2022 15:58:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=146
05/23/2022 15:58:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=146
05/23/2022 15:58:08 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7321973678542134 on epoch=146
05/23/2022 15:58:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=147
05/23/2022 15:58:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=148
05/23/2022 15:58:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=148
05/23/2022 15:58:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
05/23/2022 15:58:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/23/2022 15:58:24 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.7446543355634265 on epoch=149
05/23/2022 15:58:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
05/23/2022 15:58:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=151
05/23/2022 15:58:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/23/2022 15:58:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/23/2022 15:58:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=153
05/23/2022 15:58:40 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7592392946671562 on epoch=153
05/23/2022 15:58:40 - INFO - __main__ - Saving model with best Classification-F1: 0.7572606180041938 -> 0.7592392946671562 on epoch=153, global_step=2450
05/23/2022 15:58:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.02 on epoch=153
05/23/2022 15:58:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
05/23/2022 15:58:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=154
05/23/2022 15:58:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/23/2022 15:58:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/23/2022 15:58:56 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7485308376135702 on epoch=156
05/23/2022 15:58:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/23/2022 15:59:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
05/23/2022 15:59:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=158
05/23/2022 15:59:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=158
05/23/2022 15:59:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
05/23/2022 15:59:12 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.7383315786750073 on epoch=159
05/23/2022 15:59:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/23/2022 15:59:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=160
05/23/2022 15:59:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=161
05/23/2022 15:59:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
05/23/2022 15:59:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/23/2022 15:59:28 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.7718025051725421 on epoch=162
05/23/2022 15:59:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7592392946671562 -> 0.7718025051725421 on epoch=162, global_step=2600
05/23/2022 15:59:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=163
05/23/2022 15:59:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/23/2022 15:59:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=164
05/23/2022 15:59:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
05/23/2022 15:59:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/23/2022 15:59:45 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7458316447784932 on epoch=165
05/23/2022 15:59:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=166
05/23/2022 15:59:50 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/23/2022 15:59:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/23/2022 15:59:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=168
05/23/2022 15:59:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
05/23/2022 16:00:01 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.7163658562818092 on epoch=168
05/23/2022 16:00:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=169
05/23/2022 16:00:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/23/2022 16:00:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/23/2022 16:00:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/23/2022 16:00:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/23/2022 16:00:18 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7432982174595779 on epoch=171
05/23/2022 16:00:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/23/2022 16:00:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/23/2022 16:00:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
05/23/2022 16:00:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=174
05/23/2022 16:00:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/23/2022 16:00:34 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.7491885084612357 on epoch=174
05/23/2022 16:00:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
05/23/2022 16:00:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
05/23/2022 16:00:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.16 on epoch=176
05/23/2022 16:00:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/23/2022 16:00:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=178
05/23/2022 16:00:50 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.7408609285618748 on epoch=178
05/23/2022 16:00:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/23/2022 16:00:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
05/23/2022 16:00:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=179
05/23/2022 16:01:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/23/2022 16:01:03 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/23/2022 16:01:07 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.7482420654212097 on epoch=181
05/23/2022 16:01:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
05/23/2022 16:01:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/23/2022 16:01:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/23/2022 16:01:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=183
05/23/2022 16:01:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/23/2022 16:01:23 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7567125473246404 on epoch=184
05/23/2022 16:01:25 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=184
05/23/2022 16:01:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/23/2022 16:01:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/23/2022 16:01:33 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
05/23/2022 16:01:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/23/2022 16:01:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 16:01:37 - INFO - __main__ - Printing 3 examples
05/23/2022 16:01:37 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:01:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:01:37 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 16:01:37 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 16:01:37 - INFO - __main__ - Printing 3 examples
05/23/2022 16:01:37 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 16:01:37 - INFO - __main__ - ['others']
05/23/2022 16:01:37 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:01:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:01:38 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 16:01:39 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7569311175746147 on epoch=187
05/23/2022 16:01:39 - INFO - __main__ - save last model!
05/23/2022 16:01:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 16:01:40 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 16:01:40 - INFO - __main__ - Printing 3 examples
05/23/2022 16:01:40 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 16:01:40 - INFO - __main__ - ['others']
05/23/2022 16:01:40 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 16:01:40 - INFO - __main__ - ['others']
05/23/2022 16:01:40 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 16:01:40 - INFO - __main__ - ['others']
05/23/2022 16:01:40 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:01:42 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:01:47 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 16:01:56 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 16:01:56 - INFO - __main__ - task name: emo
05/23/2022 16:01:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 16:01:57 - INFO - __main__ - Starting training!
05/23/2022 16:03:10 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_87_0.3_8_predictions.txt
05/23/2022 16:03:10 - INFO - __main__ - Classification-F1 on test data: 0.5540
05/23/2022 16:03:11 - INFO - __main__ - prefix=emo_64_87, lr=0.3, bsz=8, dev_performance=0.7718025051725421, test_performance=0.5539949017901511
05/23/2022 16:03:11 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.2, bsz=8 ...
05/23/2022 16:03:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 16:03:12 - INFO - __main__ - Printing 3 examples
05/23/2022 16:03:12 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:03:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:03:12 - INFO - __main__ - Loaded 256 examples from train data
05/23/2022 16:03:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/23/2022 16:03:12 - INFO - __main__ - Printing 3 examples
05/23/2022 16:03:12 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/23/2022 16:03:12 - INFO - __main__ - ['others']
05/23/2022 16:03:12 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:03:12 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:03:12 - INFO - __main__ - Loaded 256 examples from dev data
05/23/2022 16:03:31 - INFO - __main__ - try to initialize prompt embeddings
05/23/2022 16:03:31 - INFO - __main__ - task name: emo
05/23/2022 16:03:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/23/2022 16:03:32 - INFO - __main__ - Starting training!
05/23/2022 16:03:35 - INFO - __main__ - Step 10 Global step 10 Train loss 7.82 on epoch=0
05/23/2022 16:03:38 - INFO - __main__ - Step 20 Global step 20 Train loss 6.09 on epoch=1
05/23/2022 16:03:40 - INFO - __main__ - Step 30 Global step 30 Train loss 3.74 on epoch=1
05/23/2022 16:03:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.50 on epoch=2
05/23/2022 16:03:45 - INFO - __main__ - Step 50 Global step 50 Train loss 1.70 on epoch=3
05/23/2022 16:03:49 - INFO - __main__ - Global step 50 Train loss 4.37 Classification-F1 0.11380724538619276 on epoch=3
05/23/2022 16:03:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.11380724538619276 on epoch=3, global_step=50
05/23/2022 16:03:51 - INFO - __main__ - Step 60 Global step 60 Train loss 1.30 on epoch=3
05/23/2022 16:03:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.21 on epoch=4
05/23/2022 16:03:57 - INFO - __main__ - Step 80 Global step 80 Train loss 1.14 on epoch=4
05/23/2022 16:03:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.98 on epoch=5
05/23/2022 16:04:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.05 on epoch=6
05/23/2022 16:04:05 - INFO - __main__ - Global step 100 Train loss 1.13 Classification-F1 0.09874608150470221 on epoch=6
05/23/2022 16:04:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.07 on epoch=6
05/23/2022 16:04:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.07 on epoch=7
05/23/2022 16:04:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=8
05/23/2022 16:04:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.00 on epoch=8
05/23/2022 16:04:18 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=9
05/23/2022 16:04:21 - INFO - __main__ - Global step 150 Train loss 1.01 Classification-F1 0.09874608150470221 on epoch=9
05/23/2022 16:04:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=9
05/23/2022 16:04:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=10
05/23/2022 16:04:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.92 on epoch=11
05/23/2022 16:04:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.97 on epoch=11
05/23/2022 16:04:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.92 on epoch=12
05/23/2022 16:04:37 - INFO - __main__ - Global step 200 Train loss 0.93 Classification-F1 0.18621155330173078 on epoch=12
05/23/2022 16:04:37 - INFO - __main__ - Saving model with best Classification-F1: 0.11380724538619276 -> 0.18621155330173078 on epoch=12, global_step=200
05/23/2022 16:04:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=13
05/23/2022 16:04:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=13
05/23/2022 16:04:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.94 on epoch=14
05/23/2022 16:04:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.86 on epoch=14
05/23/2022 16:04:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.91 on epoch=15
05/23/2022 16:04:53 - INFO - __main__ - Global step 250 Train loss 0.91 Classification-F1 0.28586956521739126 on epoch=15
05/23/2022 16:04:53 - INFO - __main__ - Saving model with best Classification-F1: 0.18621155330173078 -> 0.28586956521739126 on epoch=15, global_step=250
05/23/2022 16:04:56 - INFO - __main__ - Step 260 Global step 260 Train loss 0.97 on epoch=16
05/23/2022 16:04:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.88 on epoch=16
05/23/2022 16:05:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.86 on epoch=17
05/23/2022 16:05:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.94 on epoch=18
05/23/2022 16:05:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.88 on epoch=18
05/23/2022 16:05:09 - INFO - __main__ - Global step 300 Train loss 0.91 Classification-F1 0.10062893081761007 on epoch=18
05/23/2022 16:05:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.93 on epoch=19
05/23/2022 16:05:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=19
05/23/2022 16:05:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=20
05/23/2022 16:05:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=21
05/23/2022 16:05:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.90 on epoch=21
05/23/2022 16:05:25 - INFO - __main__ - Global step 350 Train loss 0.87 Classification-F1 0.3542968951667028 on epoch=21
05/23/2022 16:05:25 - INFO - __main__ - Saving model with best Classification-F1: 0.28586956521739126 -> 0.3542968951667028 on epoch=21, global_step=350
05/23/2022 16:05:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.92 on epoch=22
05/23/2022 16:05:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.87 on epoch=23
05/23/2022 16:05:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.77 on epoch=23
05/23/2022 16:05:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=24
05/23/2022 16:05:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.81 on epoch=24
05/23/2022 16:05:41 - INFO - __main__ - Global step 400 Train loss 0.84 Classification-F1 0.3012991253116955 on epoch=24
05/23/2022 16:05:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.80 on epoch=25
05/23/2022 16:05:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.86 on epoch=26
05/23/2022 16:05:49 - INFO - __main__ - Step 430 Global step 430 Train loss 0.76 on epoch=26
05/23/2022 16:05:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.78 on epoch=27
05/23/2022 16:05:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=28
05/23/2022 16:05:58 - INFO - __main__ - Global step 450 Train loss 0.81 Classification-F1 0.3244852618297871 on epoch=28
05/23/2022 16:06:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=28
05/23/2022 16:06:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.85 on epoch=29
05/23/2022 16:06:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.85 on epoch=29
05/23/2022 16:06:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.80 on epoch=30
05/23/2022 16:06:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=31
05/23/2022 16:06:14 - INFO - __main__ - Global step 500 Train loss 0.83 Classification-F1 0.38244405351798766 on epoch=31
05/23/2022 16:06:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3542968951667028 -> 0.38244405351798766 on epoch=31, global_step=500
05/23/2022 16:06:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=31
05/23/2022 16:06:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.83 on epoch=32
05/23/2022 16:06:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.76 on epoch=33
05/23/2022 16:06:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.75 on epoch=33
05/23/2022 16:06:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.80 on epoch=34
05/23/2022 16:06:30 - INFO - __main__ - Global step 550 Train loss 0.78 Classification-F1 0.3954979833581629 on epoch=34
05/23/2022 16:06:30 - INFO - __main__ - Saving model with best Classification-F1: 0.38244405351798766 -> 0.3954979833581629 on epoch=34, global_step=550
05/23/2022 16:06:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.80 on epoch=34
05/23/2022 16:06:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.74 on epoch=35
05/23/2022 16:06:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=36
05/23/2022 16:06:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.79 on epoch=36
05/23/2022 16:06:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=37
05/23/2022 16:06:46 - INFO - __main__ - Global step 600 Train loss 0.77 Classification-F1 0.4343059146354844 on epoch=37
05/23/2022 16:06:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3954979833581629 -> 0.4343059146354844 on epoch=37, global_step=600
05/23/2022 16:06:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=38
05/23/2022 16:06:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=38
05/23/2022 16:06:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.76 on epoch=39
05/23/2022 16:06:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.70 on epoch=39
05/23/2022 16:06:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.65 on epoch=40
05/23/2022 16:07:02 - INFO - __main__ - Global step 650 Train loss 0.69 Classification-F1 0.21293601619688576 on epoch=40
05/23/2022 16:07:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.77 on epoch=41
05/23/2022 16:07:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.66 on epoch=41
05/23/2022 16:07:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.70 on epoch=42
05/23/2022 16:07:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.69 on epoch=43
05/23/2022 16:07:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.68 on epoch=43
05/23/2022 16:07:18 - INFO - __main__ - Global step 700 Train loss 0.70 Classification-F1 0.4600975815512909 on epoch=43
05/23/2022 16:07:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4343059146354844 -> 0.4600975815512909 on epoch=43, global_step=700
05/23/2022 16:07:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.70 on epoch=44
05/23/2022 16:07:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.62 on epoch=44
05/23/2022 16:07:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.69 on epoch=45
05/23/2022 16:07:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.68 on epoch=46
05/23/2022 16:07:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.66 on epoch=46
05/23/2022 16:07:34 - INFO - __main__ - Global step 750 Train loss 0.67 Classification-F1 0.5150323535830783 on epoch=46
05/23/2022 16:07:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4600975815512909 -> 0.5150323535830783 on epoch=46, global_step=750
05/23/2022 16:07:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.69 on epoch=47
05/23/2022 16:07:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.65 on epoch=48
05/23/2022 16:07:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=48
05/23/2022 16:07:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=49
05/23/2022 16:07:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.67 on epoch=49
05/23/2022 16:07:51 - INFO - __main__ - Global step 800 Train loss 0.63 Classification-F1 0.6260858508688469 on epoch=49
05/23/2022 16:07:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5150323535830783 -> 0.6260858508688469 on epoch=49, global_step=800
05/23/2022 16:07:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.57 on epoch=50
05/23/2022 16:07:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.69 on epoch=51
05/23/2022 16:07:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=51
05/23/2022 16:08:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.59 on epoch=52
05/23/2022 16:08:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=53
05/23/2022 16:08:07 - INFO - __main__ - Global step 850 Train loss 0.59 Classification-F1 0.3735604538150632 on epoch=53
05/23/2022 16:08:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=53
05/23/2022 16:08:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.69 on epoch=54
05/23/2022 16:08:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.64 on epoch=54
05/23/2022 16:08:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.53 on epoch=55
05/23/2022 16:08:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=56
05/23/2022 16:08:23 - INFO - __main__ - Global step 900 Train loss 0.57 Classification-F1 0.566920731707317 on epoch=56
05/23/2022 16:08:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=56
05/23/2022 16:08:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=57
05/23/2022 16:08:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.55 on epoch=58
05/23/2022 16:08:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=58
05/23/2022 16:08:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.56 on epoch=59
05/23/2022 16:08:39 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.6444336655201811 on epoch=59
05/23/2022 16:08:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6260858508688469 -> 0.6444336655201811 on epoch=59, global_step=950
05/23/2022 16:08:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=59
05/23/2022 16:08:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=60
05/23/2022 16:08:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=61
05/23/2022 16:08:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=61
05/23/2022 16:08:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=62
05/23/2022 16:08:55 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.6243946029739329 on epoch=62
05/23/2022 16:08:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=63
05/23/2022 16:09:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=63
05/23/2022 16:09:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=64
05/23/2022 16:09:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=64
05/23/2022 16:09:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.55 on epoch=65
05/23/2022 16:09:11 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.6357450917254805 on epoch=65
05/23/2022 16:09:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=66
05/23/2022 16:09:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=66
05/23/2022 16:09:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
05/23/2022 16:09:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=68
05/23/2022 16:09:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=68
05/23/2022 16:09:27 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.5916359158415008 on epoch=68
05/23/2022 16:09:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.56 on epoch=69
05/23/2022 16:09:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=69
05/23/2022 16:09:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=70
05/23/2022 16:09:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=71
05/23/2022 16:09:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=71
05/23/2022 16:09:43 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.541135083765107 on epoch=71
05/23/2022 16:09:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=72
05/23/2022 16:09:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=73
05/23/2022 16:09:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/23/2022 16:09:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=74
05/23/2022 16:09:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=74
05/23/2022 16:09:59 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.6609639958172524 on epoch=74
05/23/2022 16:09:59 - INFO - __main__ - Saving model with best Classification-F1: 0.6444336655201811 -> 0.6609639958172524 on epoch=74, global_step=1200
05/23/2022 16:10:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=75
05/23/2022 16:10:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=76
05/23/2022 16:10:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=76
05/23/2022 16:10:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=77
05/23/2022 16:10:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=78
05/23/2022 16:10:15 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.6412594491953417 on epoch=78
05/23/2022 16:10:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=78
05/23/2022 16:10:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=79
05/23/2022 16:10:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
05/23/2022 16:10:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=80
05/23/2022 16:10:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=81
05/23/2022 16:10:31 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.6176403402261633 on epoch=81
05/23/2022 16:10:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=81
05/23/2022 16:10:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=82
05/23/2022 16:10:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=83
05/23/2022 16:10:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
05/23/2022 16:10:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=84
05/23/2022 16:10:47 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.694234768823538 on epoch=84
05/23/2022 16:10:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6609639958172524 -> 0.694234768823538 on epoch=84, global_step=1350
05/23/2022 16:10:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
05/23/2022 16:10:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=85
05/23/2022 16:10:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=86
05/23/2022 16:10:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=86
05/23/2022 16:10:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/23/2022 16:11:03 - INFO - __main__ - Global step 1400 Train loss 0.39 Classification-F1 0.6938113390113453 on epoch=87
05/23/2022 16:11:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=88
05/23/2022 16:11:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=88
05/23/2022 16:11:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.39 on epoch=89
05/23/2022 16:11:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
05/23/2022 16:11:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=90
05/23/2022 16:11:19 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.6728313973031526 on epoch=90
05/23/2022 16:11:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=91
05/23/2022 16:11:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=91
05/23/2022 16:11:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=92
05/23/2022 16:11:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/23/2022 16:11:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=93
05/23/2022 16:11:35 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.6362500437103229 on epoch=93
05/23/2022 16:11:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
05/23/2022 16:11:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=94
05/23/2022 16:11:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=95
05/23/2022 16:11:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
05/23/2022 16:11:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=96
05/23/2022 16:11:51 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.6127475271295496 on epoch=96
05/23/2022 16:11:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=97
05/23/2022 16:11:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=98
05/23/2022 16:11:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=98
05/23/2022 16:12:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=99
05/23/2022 16:12:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=99
05/23/2022 16:12:07 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.6580629011421527 on epoch=99
05/23/2022 16:12:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=100
05/23/2022 16:12:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
05/23/2022 16:12:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=101
05/23/2022 16:12:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
05/23/2022 16:12:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
05/23/2022 16:12:22 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.6554215625958684 on epoch=103
05/23/2022 16:12:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=103
05/23/2022 16:12:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
05/23/2022 16:12:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=104
05/23/2022 16:12:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=105
05/23/2022 16:12:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=106
05/23/2022 16:12:38 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.6537171858917313 on epoch=106
05/23/2022 16:12:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=106
05/23/2022 16:12:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
05/23/2022 16:12:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=108
05/23/2022 16:12:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=108
05/23/2022 16:12:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
05/23/2022 16:12:54 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.6813071728904742 on epoch=109
05/23/2022 16:12:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
05/23/2022 16:13:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=110
05/23/2022 16:13:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=111
05/23/2022 16:13:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/23/2022 16:13:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
05/23/2022 16:13:10 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.709986521668039 on epoch=112
05/23/2022 16:13:10 - INFO - __main__ - Saving model with best Classification-F1: 0.694234768823538 -> 0.709986521668039 on epoch=112, global_step=1800
05/23/2022 16:13:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=113
05/23/2022 16:13:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
05/23/2022 16:13:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=114
05/23/2022 16:13:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=114
05/23/2022 16:13:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/23/2022 16:13:26 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.6955418911597405 on epoch=115
05/23/2022 16:13:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=116
05/23/2022 16:13:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=116
05/23/2022 16:13:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=117
05/23/2022 16:13:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/23/2022 16:13:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=118
05/23/2022 16:13:42 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.7170225491194137 on epoch=118
05/23/2022 16:13:42 - INFO - __main__ - Saving model with best Classification-F1: 0.709986521668039 -> 0.7170225491194137 on epoch=118, global_step=1900
05/23/2022 16:13:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=119
05/23/2022 16:13:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/23/2022 16:13:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
05/23/2022 16:13:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/23/2022 16:13:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=121
05/23/2022 16:13:58 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.6366081084120914 on epoch=121
05/23/2022 16:14:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/23/2022 16:14:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=123
05/23/2022 16:14:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
05/23/2022 16:14:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/23/2022 16:14:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=124
05/23/2022 16:14:14 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.6925286737588809 on epoch=124
05/23/2022 16:14:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=125
05/23/2022 16:14:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=126
05/23/2022 16:14:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=126
05/23/2022 16:14:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=127
05/23/2022 16:14:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
05/23/2022 16:14:30 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.7322849603619299 on epoch=128
05/23/2022 16:14:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7170225491194137 -> 0.7322849603619299 on epoch=128, global_step=2050
05/23/2022 16:14:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
05/23/2022 16:14:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=129
05/23/2022 16:14:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/23/2022 16:14:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/23/2022 16:14:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
05/23/2022 16:14:46 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.7062372062556294 on epoch=131
05/23/2022 16:14:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=131
05/23/2022 16:14:51 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
05/23/2022 16:14:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=133
05/23/2022 16:14:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=133
05/23/2022 16:14:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=134
05/23/2022 16:15:03 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7180037612035852 on epoch=134
05/23/2022 16:15:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=134
05/23/2022 16:15:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/23/2022 16:15:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=136
05/23/2022 16:15:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/23/2022 16:15:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
05/23/2022 16:15:19 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7406986186710037 on epoch=137
05/23/2022 16:15:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7322849603619299 -> 0.7406986186710037 on epoch=137, global_step=2200
05/23/2022 16:15:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
05/23/2022 16:15:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
05/23/2022 16:15:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/23/2022 16:15:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/23/2022 16:15:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=140
05/23/2022 16:15:35 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.738022888470947 on epoch=140
05/23/2022 16:15:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/23/2022 16:15:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=141
05/23/2022 16:15:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
05/23/2022 16:15:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/23/2022 16:15:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=143
05/23/2022 16:15:51 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.6848907131515827 on epoch=143
05/23/2022 16:15:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
05/23/2022 16:15:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/23/2022 16:15:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/23/2022 16:16:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=146
05/23/2022 16:16:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
05/23/2022 16:16:07 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.7313764904685958 on epoch=146
05/23/2022 16:16:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=147
05/23/2022 16:16:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=148
05/23/2022 16:16:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=148
05/23/2022 16:16:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/23/2022 16:16:20 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
05/23/2022 16:16:23 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7296731852542777 on epoch=149
05/23/2022 16:16:26 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.21 on epoch=150
05/23/2022 16:16:28 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
05/23/2022 16:16:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/23/2022 16:16:33 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
05/23/2022 16:16:36 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=153
05/23/2022 16:16:39 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7542324524525678 on epoch=153
05/23/2022 16:16:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7406986186710037 -> 0.7542324524525678 on epoch=153, global_step=2450
05/23/2022 16:16:42 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=153
05/23/2022 16:16:44 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=154
05/23/2022 16:16:47 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
05/23/2022 16:16:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
05/23/2022 16:16:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
05/23/2022 16:16:55 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7533082625975599 on epoch=156
05/23/2022 16:16:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
05/23/2022 16:17:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/23/2022 16:17:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.24 on epoch=158
05/23/2022 16:17:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
05/23/2022 16:17:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/23/2022 16:17:11 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6847117324732082 on epoch=159
05/23/2022 16:17:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/23/2022 16:17:16 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/23/2022 16:17:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/23/2022 16:17:21 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=161
05/23/2022 16:17:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
05/23/2022 16:17:27 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.7397404009512474 on epoch=162
05/23/2022 16:17:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=163
05/23/2022 16:17:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/23/2022 16:17:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=164
05/23/2022 16:17:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/23/2022 16:17:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/23/2022 16:17:43 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.757298253652215 on epoch=165
05/23/2022 16:17:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7542324524525678 -> 0.757298253652215 on epoch=165, global_step=2650
05/23/2022 16:17:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=166
05/23/2022 16:17:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
05/23/2022 16:17:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/23/2022 16:17:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/23/2022 16:17:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/23/2022 16:17:59 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.7117986628067273 on epoch=168
05/23/2022 16:18:02 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=169
05/23/2022 16:18:04 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/23/2022 16:18:07 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=170
05/23/2022 16:18:09 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/23/2022 16:18:12 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/23/2022 16:18:15 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.7162012883580261 on epoch=171
05/23/2022 16:18:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
05/23/2022 16:18:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=173
05/23/2022 16:18:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
05/23/2022 16:18:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=174
05/23/2022 16:18:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/23/2022 16:18:31 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.7035924987010962 on epoch=174
05/23/2022 16:18:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/23/2022 16:18:36 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
05/23/2022 16:18:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/23/2022 16:18:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
05/23/2022 16:18:44 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=178
05/23/2022 16:18:47 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.7382247388686963 on epoch=178
05/23/2022 16:18:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
05/23/2022 16:18:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
05/23/2022 16:18:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/23/2022 16:18:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/23/2022 16:18:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=181
05/23/2022 16:19:03 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.7428959643000004 on epoch=181
05/23/2022 16:19:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/23/2022 16:19:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/23/2022 16:19:10 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.15 on epoch=183
05/23/2022 16:19:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=183
05/23/2022 16:19:15 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=184
05/23/2022 16:19:19 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.7436965461231906 on epoch=184
05/23/2022 16:19:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
05/23/2022 16:19:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.08 on epoch=185
05/23/2022 16:19:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/23/2022 16:19:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=186
05/23/2022 16:19:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/23/2022 16:19:35 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.7217076442663931 on epoch=187
05/23/2022 16:19:35 - INFO - __main__ - save last model!
05/23/2022 16:19:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/23/2022 16:19:35 - INFO - __main__ - Start tokenizing ... 5509 instances
05/23/2022 16:19:35 - INFO - __main__ - Printing 3 examples
05/23/2022 16:19:35 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/23/2022 16:19:35 - INFO - __main__ - ['others']
05/23/2022 16:19:35 - INFO - __main__ -  [emo] what you like very little things ok
05/23/2022 16:19:35 - INFO - __main__ - ['others']
05/23/2022 16:19:35 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/23/2022 16:19:35 - INFO - __main__ - ['others']
05/23/2022 16:19:35 - INFO - __main__ - Tokenizing Input ...
05/23/2022 16:19:37 - INFO - __main__ - Tokenizing Output ...
05/23/2022 16:19:42 - INFO - __main__ - Loaded 5509 examples from test data
05/23/2022 16:20:58 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-emo/emo_64_87_0.2_8_predictions.txt
05/23/2022 16:20:58 - INFO - __main__ - Classification-F1 on test data: 0.4456
05/23/2022 16:20:58 - INFO - __main__ - prefix=emo_64_87, lr=0.2, bsz=8, dev_performance=0.757298253652215, test_performance=0.44556740574655274
