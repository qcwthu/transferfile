05/25/2022 11:55:57 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/25/2022 11:55:57 - INFO - __main__ - models/T5-large-cls2cls-down64shot/singletask-anli
05/25/2022 11:55:57 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-cls2cls-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-cls2cls-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='6,7')
05/25/2022 11:55:57 - INFO - __main__ - models/T5-large-cls2cls-down64shot/singletask-anli
05/25/2022 11:55:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/25/2022 11:55:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/25/2022 11:55:59 - INFO - __main__ - args.device: cuda:0
05/25/2022 11:55:59 - INFO - __main__ - Using 2 gpus
05/25/2022 11:55:59 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/25/2022 11:55:59 - INFO - __main__ - args.device: cuda:1
05/25/2022 11:55:59 - INFO - __main__ - Using 2 gpus
05/25/2022 11:55:59 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/25/2022 11:56:03 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.5, bsz=8 ...
05/25/2022 11:56:04 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 11:56:04 - INFO - __main__ - Printing 3 examples
05/25/2022 11:56:04 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ - Printing 3 examples
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 11:56:04 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 11:56:04 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 11:56:04 - INFO - __main__ - Printing 3 examples
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 11:56:04 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 11:56:04 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 11:56:04 - INFO - __main__ - Printing 3 examples
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 11:56:04 - INFO - __main__ - ['neutral']
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 11:56:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 11:56:05 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 11:56:05 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 11:56:22 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 11:56:22 - INFO - __main__ - task name: anli
05/25/2022 11:56:22 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 11:56:22 - INFO - __main__ - task name: anli
05/25/2022 11:56:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 11:56:23 - INFO - __main__ - Starting training!
05/25/2022 11:56:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 11:56:23 - INFO - __main__ - Starting training!
05/25/2022 11:56:27 - INFO - __main__ - Step 10 Global step 10 Train loss 5.95 on epoch=0
05/25/2022 11:56:29 - INFO - __main__ - Step 20 Global step 20 Train loss 2.20 on epoch=1
05/25/2022 11:56:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.95 on epoch=2
05/25/2022 11:56:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.81 on epoch=3
05/25/2022 11:56:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.71 on epoch=4
05/25/2022 11:56:41 - INFO - __main__ - Global step 50 Train loss 2.12 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 11:56:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 11:56:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=4
05/25/2022 11:56:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=5
05/25/2022 11:56:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=6
05/25/2022 11:56:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=7
05/25/2022 11:56:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=8
05/25/2022 11:56:58 - INFO - __main__ - Global step 100 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 11:57:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
05/25/2022 11:57:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=9
05/25/2022 11:57:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=10
05/25/2022 11:57:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=11
05/25/2022 11:57:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=12
05/25/2022 11:57:15 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 11:57:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=13
05/25/2022 11:57:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
05/25/2022 11:57:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=14
05/25/2022 11:57:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=15
05/25/2022 11:57:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.53 on epoch=16
05/25/2022 11:57:33 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 11:57:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=17
05/25/2022 11:57:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=18
05/25/2022 11:57:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/25/2022 11:57:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
05/25/2022 11:57:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=20
05/25/2022 11:57:52 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.21674673278578438 on epoch=20
05/25/2022 11:57:52 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21674673278578438 on epoch=20, global_step=250
05/25/2022 11:57:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=21
05/25/2022 11:57:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/25/2022 11:58:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
05/25/2022 11:58:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=24
05/25/2022 11:58:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
05/25/2022 11:58:10 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 11:58:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=25
05/25/2022 11:58:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=26
05/25/2022 11:58:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/25/2022 11:58:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/25/2022 11:58:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
05/25/2022 11:58:27 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.178336534543973 on epoch=29
05/25/2022 11:58:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=29
05/25/2022 11:58:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=30
05/25/2022 11:58:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
05/25/2022 11:58:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=32
05/25/2022 11:58:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/25/2022 11:58:46 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.2686566132188319 on epoch=33
05/25/2022 11:58:46 - INFO - __main__ - Saving model with best Classification-F1: 0.21674673278578438 -> 0.2686566132188319 on epoch=33, global_step=400
05/25/2022 11:58:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/25/2022 11:58:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=34
05/25/2022 11:58:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
05/25/2022 11:58:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
05/25/2022 11:58:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=37
05/25/2022 11:59:03 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 11:59:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=38
05/25/2022 11:59:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=39
05/25/2022 11:59:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=39
05/25/2022 11:59:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=40
05/25/2022 11:59:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
05/25/2022 11:59:21 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 11:59:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
05/25/2022 11:59:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/25/2022 11:59:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/25/2022 11:59:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
05/25/2022 11:59:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
05/25/2022 11:59:38 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.21837593896417426 on epoch=45
05/25/2022 11:59:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
05/25/2022 11:59:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
05/25/2022 11:59:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=48
05/25/2022 11:59:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
05/25/2022 11:59:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/25/2022 11:59:55 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 11:59:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
05/25/2022 12:00:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/25/2022 12:00:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
05/25/2022 12:00:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
05/25/2022 12:00:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
05/25/2022 12:00:13 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.22830860739357473 on epoch=54
05/25/2022 12:00:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=54
05/25/2022 12:00:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=55
05/25/2022 12:00:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=56
05/25/2022 12:00:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=57
05/25/2022 12:00:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/25/2022 12:00:32 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 12:00:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
05/25/2022 12:00:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/25/2022 12:00:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=60
05/25/2022 12:00:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 12:00:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
05/25/2022 12:00:50 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 12:00:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
05/25/2022 12:00:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
05/25/2022 12:00:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=64
05/25/2022 12:01:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=65
05/25/2022 12:01:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
05/25/2022 12:01:07 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 12:01:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=67
05/25/2022 12:01:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=68
05/25/2022 12:01:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=69
05/25/2022 12:01:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=69
05/25/2022 12:01:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
05/25/2022 12:01:25 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.20175438596491227 on epoch=70
05/25/2022 12:01:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=71
05/25/2022 12:01:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=72
05/25/2022 12:01:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=73
05/25/2022 12:01:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=74
05/25/2022 12:01:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
05/25/2022 12:01:43 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=74
05/25/2022 12:01:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=75
05/25/2022 12:01:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=76
05/25/2022 12:01:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=77
05/25/2022 12:01:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/25/2022 12:01:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=79
05/25/2022 12:02:01 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.2706731738216579 on epoch=79
05/25/2022 12:02:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2686566132188319 -> 0.2706731738216579 on epoch=79, global_step=950
05/25/2022 12:02:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=79
05/25/2022 12:02:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=80
05/25/2022 12:02:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
05/25/2022 12:02:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=82
05/25/2022 12:02:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
05/25/2022 12:02:20 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=83
05/25/2022 12:02:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=84
05/25/2022 12:02:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=84
05/25/2022 12:02:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
05/25/2022 12:02:30 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
05/25/2022 12:02:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
05/25/2022 12:02:38 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=87
05/25/2022 12:02:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=88
05/25/2022 12:02:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=89
05/25/2022 12:02:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=89
05/25/2022 12:02:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
05/25/2022 12:02:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/25/2022 12:02:56 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 12:02:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=92
05/25/2022 12:03:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
05/25/2022 12:03:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=94
05/25/2022 12:03:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=94
05/25/2022 12:03:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=95
05/25/2022 12:03:14 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.21043083900226758 on epoch=95
05/25/2022 12:03:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=96
05/25/2022 12:03:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=97
05/25/2022 12:03:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
05/25/2022 12:03:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
05/25/2022 12:03:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
05/25/2022 12:03:31 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.21150157032509972 on epoch=99
05/25/2022 12:03:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=100
05/25/2022 12:03:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
05/25/2022 12:03:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
05/25/2022 12:03:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/25/2022 12:03:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
05/25/2022 12:03:48 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.25499577821559244 on epoch=104
05/25/2022 12:03:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/25/2022 12:03:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=105
05/25/2022 12:03:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=106
05/25/2022 12:03:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
05/25/2022 12:04:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=108
05/25/2022 12:04:05 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=108
05/25/2022 12:04:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
05/25/2022 12:04:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
05/25/2022 12:04:13 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
05/25/2022 12:04:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
05/25/2022 12:04:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=112
05/25/2022 12:04:22 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 12:04:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
05/25/2022 12:04:27 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=114
05/25/2022 12:04:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=114
05/25/2022 12:04:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=115
05/25/2022 12:04:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=116
05/25/2022 12:04:39 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.16732026143790854 on epoch=116
05/25/2022 12:04:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=117
05/25/2022 12:04:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=118
05/25/2022 12:04:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
05/25/2022 12:04:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
05/25/2022 12:04:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 12:04:56 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.19205537476109302 on epoch=120
05/25/2022 12:04:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
05/25/2022 12:05:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=122
05/25/2022 12:05:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=123
05/25/2022 12:05:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=124
05/25/2022 12:05:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
05/25/2022 12:05:14 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.1785932000078658 on epoch=124
05/25/2022 12:05:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=125
05/25/2022 12:05:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=126
05/25/2022 12:05:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
05/25/2022 12:05:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=128
05/25/2022 12:05:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=129
05/25/2022 12:05:31 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.23445134660461556 on epoch=129
05/25/2022 12:05:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
05/25/2022 12:05:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=130
05/25/2022 12:05:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
05/25/2022 12:05:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
05/25/2022 12:05:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
05/25/2022 12:05:49 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.19307467667918232 on epoch=133
05/25/2022 12:05:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=134
05/25/2022 12:05:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=134
05/25/2022 12:05:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=135
05/25/2022 12:05:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=136
05/25/2022 12:06:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=137
05/25/2022 12:06:07 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.21808597268141652 on epoch=137
05/25/2022 12:06:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=138
05/25/2022 12:06:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=139
05/25/2022 12:06:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
05/25/2022 12:06:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=140
05/25/2022 12:06:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=141
05/25/2022 12:06:25 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.1679790026246719 on epoch=141
05/25/2022 12:06:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
05/25/2022 12:06:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
05/25/2022 12:06:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=144
05/25/2022 12:06:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=144
05/25/2022 12:06:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
05/25/2022 12:06:43 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.22760953294205924 on epoch=145
05/25/2022 12:06:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=146
05/25/2022 12:06:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
05/25/2022 12:06:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=148
05/25/2022 12:06:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=149
05/25/2022 12:06:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=149
05/25/2022 12:07:02 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.1971326164874552 on epoch=149
05/25/2022 12:07:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=150
05/25/2022 12:07:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=151
05/25/2022 12:07:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=152
05/25/2022 12:07:12 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=153
05/25/2022 12:07:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=154
05/25/2022 12:07:20 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.26436542925904627 on epoch=154
05/25/2022 12:07:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
05/25/2022 12:07:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=155
05/25/2022 12:07:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
05/25/2022 12:07:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=157
05/25/2022 12:07:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
05/25/2022 12:07:38 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.24769078258613822 on epoch=158
05/25/2022 12:07:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=159
05/25/2022 12:07:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=159
05/25/2022 12:07:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
05/25/2022 12:07:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=161
05/25/2022 12:07:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=162
05/25/2022 12:07:56 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.24148980834225497 on epoch=162
05/25/2022 12:07:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=163
05/25/2022 12:08:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=164
05/25/2022 12:08:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=164
05/25/2022 12:08:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
05/25/2022 12:08:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=166
05/25/2022 12:08:13 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.18931039128510116 on epoch=166
05/25/2022 12:08:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=167
05/25/2022 12:08:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=168
05/25/2022 12:08:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=169
05/25/2022 12:08:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=169
05/25/2022 12:08:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=170
05/25/2022 12:08:30 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.18765133171912837 on epoch=170
05/25/2022 12:08:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.45 on epoch=171
05/25/2022 12:08:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.39 on epoch=172
05/25/2022 12:08:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.38 on epoch=173
05/25/2022 12:08:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=174
05/25/2022 12:08:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.38 on epoch=174
05/25/2022 12:08:48 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.23431839372522223 on epoch=174
05/25/2022 12:08:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
05/25/2022 12:08:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
05/25/2022 12:08:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=177
05/25/2022 12:08:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=178
05/25/2022 12:09:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=179
05/25/2022 12:09:06 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.26119708507328254 on epoch=179
05/25/2022 12:09:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=179
05/25/2022 12:09:11 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=180
05/25/2022 12:09:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
05/25/2022 12:09:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=182
05/25/2022 12:09:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=183
05/25/2022 12:09:25 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.2776972624798712 on epoch=183
05/25/2022 12:09:25 - INFO - __main__ - Saving model with best Classification-F1: 0.2706731738216579 -> 0.2776972624798712 on epoch=183, global_step=2200
05/25/2022 12:09:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=184
05/25/2022 12:09:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
05/25/2022 12:09:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=185
05/25/2022 12:09:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=186
05/25/2022 12:09:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
05/25/2022 12:09:43 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.2398108747044917 on epoch=187
05/25/2022 12:09:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=188
05/25/2022 12:09:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=189
05/25/2022 12:09:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.36 on epoch=189
05/25/2022 12:09:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=190
05/25/2022 12:09:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=191
05/25/2022 12:10:01 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.18892001244942422 on epoch=191
05/25/2022 12:10:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
05/25/2022 12:10:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
05/25/2022 12:10:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=194
05/25/2022 12:10:12 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=194
05/25/2022 12:10:15 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=195
05/25/2022 12:10:21 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.2827155802314151 on epoch=195
05/25/2022 12:10:21 - INFO - __main__ - Saving model with best Classification-F1: 0.2776972624798712 -> 0.2827155802314151 on epoch=195, global_step=2350
05/25/2022 12:10:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.41 on epoch=196
05/25/2022 12:10:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.39 on epoch=197
05/25/2022 12:10:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.39 on epoch=198
05/25/2022 12:10:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=199
05/25/2022 12:10:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.45 on epoch=199
05/25/2022 12:10:40 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.2625191380556187 on epoch=199
05/25/2022 12:10:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=200
05/25/2022 12:10:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=201
05/25/2022 12:10:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.44 on epoch=202
05/25/2022 12:10:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=203
05/25/2022 12:10:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=204
05/25/2022 12:10:59 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.25427554901239113 on epoch=204
05/25/2022 12:11:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=204
05/25/2022 12:11:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=205
05/25/2022 12:11:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=206
05/25/2022 12:11:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=207
05/25/2022 12:11:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=208
05/25/2022 12:11:17 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.25442254510337564 on epoch=208
05/25/2022 12:11:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=209
05/25/2022 12:11:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
05/25/2022 12:11:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=210
05/25/2022 12:11:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=211
05/25/2022 12:11:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
05/25/2022 12:11:36 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.23348206212656342 on epoch=212
05/25/2022 12:11:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=213
05/25/2022 12:11:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
05/25/2022 12:11:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.32 on epoch=214
05/25/2022 12:11:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=215
05/25/2022 12:11:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=216
05/25/2022 12:11:55 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.20686369871555058 on epoch=216
05/25/2022 12:11:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=217
05/25/2022 12:12:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.39 on epoch=218
05/25/2022 12:12:03 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.33 on epoch=219
05/25/2022 12:12:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=219
05/25/2022 12:12:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.39 on epoch=220
05/25/2022 12:12:13 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.30903233499548405 on epoch=220
05/25/2022 12:12:13 - INFO - __main__ - Saving model with best Classification-F1: 0.2827155802314151 -> 0.30903233499548405 on epoch=220, global_step=2650
05/25/2022 12:12:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
05/25/2022 12:12:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=222
05/25/2022 12:12:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=223
05/25/2022 12:12:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=224
05/25/2022 12:12:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.39 on epoch=224
05/25/2022 12:12:32 - INFO - __main__ - Global step 2700 Train loss 0.39 Classification-F1 0.25282836916737894 on epoch=224
05/25/2022 12:12:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
05/25/2022 12:12:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.36 on epoch=226
05/25/2022 12:12:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
05/25/2022 12:12:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.38 on epoch=228
05/25/2022 12:12:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.38 on epoch=229
05/25/2022 12:12:51 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.2992587544774767 on epoch=229
05/25/2022 12:12:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.32 on epoch=229
05/25/2022 12:12:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=230
05/25/2022 12:12:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.33 on epoch=231
05/25/2022 12:13:02 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=232
05/25/2022 12:13:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=233
05/25/2022 12:13:10 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.31943187531422823 on epoch=233
05/25/2022 12:13:10 - INFO - __main__ - Saving model with best Classification-F1: 0.30903233499548405 -> 0.31943187531422823 on epoch=233, global_step=2800
05/25/2022 12:13:13 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.34 on epoch=234
05/25/2022 12:13:16 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.37 on epoch=234
05/25/2022 12:13:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=235
05/25/2022 12:13:21 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
05/25/2022 12:13:24 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.32 on epoch=237
05/25/2022 12:13:30 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.33743099159175066 on epoch=237
05/25/2022 12:13:30 - INFO - __main__ - Saving model with best Classification-F1: 0.31943187531422823 -> 0.33743099159175066 on epoch=237, global_step=2850
05/25/2022 12:13:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.40 on epoch=238
05/25/2022 12:13:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=239
05/25/2022 12:13:38 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.37 on epoch=239
05/25/2022 12:13:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=240
05/25/2022 12:13:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=241
05/25/2022 12:13:48 - INFO - __main__ - Global step 2900 Train loss 0.36 Classification-F1 0.2616176591714027 on epoch=241
05/25/2022 12:13:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=242
05/25/2022 12:13:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=243
05/25/2022 12:13:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=244
05/25/2022 12:13:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.33 on epoch=244
05/25/2022 12:14:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=245
05/25/2022 12:14:07 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.3313605143207887 on epoch=245
05/25/2022 12:14:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=246
05/25/2022 12:14:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.37 on epoch=247
05/25/2022 12:14:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.36 on epoch=248
05/25/2022 12:14:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.35 on epoch=249
05/25/2022 12:14:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.35 on epoch=249
05/25/2022 12:14:22 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:14:22 - INFO - __main__ - Printing 3 examples
05/25/2022 12:14:22 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:14:22 - INFO - __main__ - ['neutral']
05/25/2022 12:14:22 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:14:22 - INFO - __main__ - ['neutral']
05/25/2022 12:14:22 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:14:22 - INFO - __main__ - ['neutral']
05/25/2022 12:14:22 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:14:22 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:14:23 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:14:23 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:14:23 - INFO - __main__ - Printing 3 examples
05/25/2022 12:14:23 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:14:23 - INFO - __main__ - ['neutral']
05/25/2022 12:14:23 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:14:23 - INFO - __main__ - ['neutral']
05/25/2022 12:14:23 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:14:23 - INFO - __main__ - ['neutral']
05/25/2022 12:14:23 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:14:23 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:14:23 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:14:27 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.3063866826243064 on epoch=249
05/25/2022 12:14:27 - INFO - __main__ - save last model!
05/25/2022 12:14:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 12:14:27 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 12:14:27 - INFO - __main__ - Printing 3 examples
05/25/2022 12:14:27 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 12:14:27 - INFO - __main__ - ['contradiction']
05/25/2022 12:14:27 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 12:14:27 - INFO - __main__ - ['entailment']
05/25/2022 12:14:27 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 12:14:27 - INFO - __main__ - ['contradiction']
05/25/2022 12:14:27 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:14:27 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:14:28 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 12:14:41 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:14:41 - INFO - __main__ - task name: anli
05/25/2022 12:14:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:14:42 - INFO - __main__ - Starting training!
05/25/2022 12:15:04 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_100_0.5_8_predictions.txt
05/25/2022 12:15:04 - INFO - __main__ - Classification-F1 on test data: 0.3009
05/25/2022 12:15:04 - INFO - __main__ - prefix=anli_64_100, lr=0.5, bsz=8, dev_performance=0.33743099159175066, test_performance=0.3009075410846302
05/25/2022 12:15:04 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.4, bsz=8 ...
05/25/2022 12:15:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:15:05 - INFO - __main__ - Printing 3 examples
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:15:05 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:15:05 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:15:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:15:05 - INFO - __main__ - Printing 3 examples
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:15:05 - INFO - __main__ - ['neutral']
05/25/2022 12:15:05 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:15:05 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:15:06 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:15:24 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:15:24 - INFO - __main__ - task name: anli
05/25/2022 12:15:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:15:25 - INFO - __main__ - Starting training!
05/25/2022 12:15:28 - INFO - __main__ - Step 10 Global step 10 Train loss 6.12 on epoch=0
05/25/2022 12:15:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=1
05/25/2022 12:15:33 - INFO - __main__ - Step 30 Global step 30 Train loss 1.36 on epoch=2
05/25/2022 12:15:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=3
05/25/2022 12:15:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=4
05/25/2022 12:15:44 - INFO - __main__ - Global step 50 Train loss 2.38 Classification-F1 0.22984847985798104 on epoch=4
05/25/2022 12:15:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.22984847985798104 on epoch=4, global_step=50
05/25/2022 12:15:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=4
05/25/2022 12:15:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=5
05/25/2022 12:15:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.66 on epoch=6
05/25/2022 12:15:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=7
05/25/2022 12:15:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=8
05/25/2022 12:16:02 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.1785932000078658 on epoch=8
05/25/2022 12:16:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=9
05/25/2022 12:16:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=9
05/25/2022 12:16:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=10
05/25/2022 12:16:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=11
05/25/2022 12:16:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=12
05/25/2022 12:16:21 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 12:16:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=13
05/25/2022 12:16:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
05/25/2022 12:16:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
05/25/2022 12:16:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=15
05/25/2022 12:16:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=16
05/25/2022 12:16:39 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 12:16:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=17
05/25/2022 12:16:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=18
05/25/2022 12:16:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
05/25/2022 12:16:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
05/25/2022 12:16:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=20
05/25/2022 12:16:58 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.2059774964838256 on epoch=20
05/25/2022 12:17:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=21
05/25/2022 12:17:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=22
05/25/2022 12:17:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=23
05/25/2022 12:17:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=24
05/25/2022 12:17:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/25/2022 12:17:16 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16732026143790854 on epoch=24
05/25/2022 12:17:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=25
05/25/2022 12:17:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=26
05/25/2022 12:17:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=27
05/25/2022 12:17:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=28
05/25/2022 12:17:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
05/25/2022 12:17:36 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.25209785290885794 on epoch=29
05/25/2022 12:17:36 - INFO - __main__ - Saving model with best Classification-F1: 0.22984847985798104 -> 0.25209785290885794 on epoch=29, global_step=350
05/25/2022 12:17:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=29
05/25/2022 12:17:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=30
05/25/2022 12:17:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=31
05/25/2022 12:17:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=32
05/25/2022 12:17:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
05/25/2022 12:17:54 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.1754553408096715 on epoch=33
05/25/2022 12:17:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/25/2022 12:17:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=34
05/25/2022 12:18:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
05/25/2022 12:18:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
05/25/2022 12:18:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=37
05/25/2022 12:18:12 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 12:18:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=38
05/25/2022 12:18:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/25/2022 12:18:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/25/2022 12:18:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
05/25/2022 12:18:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=41
05/25/2022 12:18:31 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.1679790026246719 on epoch=41
05/25/2022 12:18:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
05/25/2022 12:18:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=43
05/25/2022 12:18:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
05/25/2022 12:18:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=44
05/25/2022 12:18:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
05/25/2022 12:18:50 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 12:18:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/25/2022 12:18:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=47
05/25/2022 12:18:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
05/25/2022 12:19:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
05/25/2022 12:19:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/25/2022 12:19:08 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.20355600806983806 on epoch=49
05/25/2022 12:19:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
05/25/2022 12:19:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=51
05/25/2022 12:19:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=52
05/25/2022 12:19:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=53
05/25/2022 12:19:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=54
05/25/2022 12:19:27 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.3072831481256484 on epoch=54
05/25/2022 12:19:27 - INFO - __main__ - Saving model with best Classification-F1: 0.25209785290885794 -> 0.3072831481256484 on epoch=54, global_step=650
05/25/2022 12:19:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=54
05/25/2022 12:19:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/25/2022 12:19:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
05/25/2022 12:19:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=57
05/25/2022 12:19:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/25/2022 12:19:46 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 12:19:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=59
05/25/2022 12:19:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=59
05/25/2022 12:19:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=60
05/25/2022 12:19:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
05/25/2022 12:19:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=62
05/25/2022 12:20:05 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 12:20:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
05/25/2022 12:20:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
05/25/2022 12:20:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=64
05/25/2022 12:20:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
05/25/2022 12:20:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
05/25/2022 12:20:23 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 12:20:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
05/25/2022 12:20:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/25/2022 12:20:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=69
05/25/2022 12:20:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
05/25/2022 12:20:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=70
05/25/2022 12:20:42 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.2558922558922559 on epoch=70
05/25/2022 12:20:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
05/25/2022 12:20:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=72
05/25/2022 12:20:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
05/25/2022 12:20:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
05/25/2022 12:20:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=74
05/25/2022 12:21:01 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.178080012725682 on epoch=74
05/25/2022 12:21:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=75
05/25/2022 12:21:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
05/25/2022 12:21:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/25/2022 12:21:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/25/2022 12:21:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/25/2022 12:21:20 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.2490842490842491 on epoch=79
05/25/2022 12:21:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=79
05/25/2022 12:21:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/25/2022 12:21:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=81
05/25/2022 12:21:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=82
05/25/2022 12:21:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
05/25/2022 12:21:38 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.18399830629174121 on epoch=83
05/25/2022 12:21:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/25/2022 12:21:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=84
05/25/2022 12:21:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=85
05/25/2022 12:21:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=86
05/25/2022 12:21:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=87
05/25/2022 12:21:57 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 12:22:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/25/2022 12:22:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
05/25/2022 12:22:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=89
05/25/2022 12:22:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=90
05/25/2022 12:22:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/25/2022 12:22:16 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 12:22:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
05/25/2022 12:22:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
05/25/2022 12:22:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=94
05/25/2022 12:22:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=94
05/25/2022 12:22:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=95
05/25/2022 12:22:35 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.21529003720784545 on epoch=95
05/25/2022 12:22:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=96
05/25/2022 12:22:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
05/25/2022 12:22:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=98
05/25/2022 12:22:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=99
05/25/2022 12:22:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
05/25/2022 12:22:53 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.16864295125164688 on epoch=99
05/25/2022 12:22:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=100
05/25/2022 12:22:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=101
05/25/2022 12:23:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=102
05/25/2022 12:23:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
05/25/2022 12:23:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=104
05/25/2022 12:23:12 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.22030111105974448 on epoch=104
05/25/2022 12:23:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=104
05/25/2022 12:23:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=105
05/25/2022 12:23:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
05/25/2022 12:23:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=107
05/25/2022 12:23:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=108
05/25/2022 12:23:31 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.23674550381867457 on epoch=108
05/25/2022 12:23:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=109
05/25/2022 12:23:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
05/25/2022 12:23:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
05/25/2022 12:23:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=111
05/25/2022 12:23:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=112
05/25/2022 12:23:50 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 12:23:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=113
05/25/2022 12:23:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=114
05/25/2022 12:23:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=114
05/25/2022 12:24:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.47 on epoch=115
05/25/2022 12:24:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/25/2022 12:24:09 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 12:24:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=117
05/25/2022 12:24:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
05/25/2022 12:24:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
05/25/2022 12:24:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
05/25/2022 12:24:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 12:24:27 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.17382508558979146 on epoch=120
05/25/2022 12:24:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=121
05/25/2022 12:24:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
05/25/2022 12:24:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=123
05/25/2022 12:24:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=124
05/25/2022 12:24:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
05/25/2022 12:24:46 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.23825258841272265 on epoch=124
05/25/2022 12:24:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=125
05/25/2022 12:24:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=126
05/25/2022 12:24:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=127
05/25/2022 12:24:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
05/25/2022 12:24:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
05/25/2022 12:25:05 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.2670692431561997 on epoch=129
05/25/2022 12:25:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
05/25/2022 12:25:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=130
05/25/2022 12:25:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
05/25/2022 12:25:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=132
05/25/2022 12:25:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
05/25/2022 12:25:24 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.3047645868024946 on epoch=133
05/25/2022 12:25:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=134
05/25/2022 12:25:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=134
05/25/2022 12:25:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=135
05/25/2022 12:25:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=136
05/25/2022 12:25:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=137
05/25/2022 12:25:43 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 12:25:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
05/25/2022 12:25:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=139
05/25/2022 12:25:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
05/25/2022 12:25:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=140
05/25/2022 12:25:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=141
05/25/2022 12:26:01 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 12:26:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=142
05/25/2022 12:26:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
05/25/2022 12:26:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=144
05/25/2022 12:26:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=144
05/25/2022 12:26:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
05/25/2022 12:26:20 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.2757210939659963 on epoch=145
05/25/2022 12:26:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=146
05/25/2022 12:26:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=147
05/25/2022 12:26:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=148
05/25/2022 12:26:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=149
05/25/2022 12:26:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=149
05/25/2022 12:26:39 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.16732026143790854 on epoch=149
05/25/2022 12:26:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=150
05/25/2022 12:26:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=151
05/25/2022 12:26:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=152
05/25/2022 12:26:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=153
05/25/2022 12:26:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
05/25/2022 12:26:58 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.2327760891590679 on epoch=154
05/25/2022 12:27:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=154
05/25/2022 12:27:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 12:27:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=156
05/25/2022 12:27:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=157
05/25/2022 12:27:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=158
05/25/2022 12:27:17 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.2727466815186113 on epoch=158
05/25/2022 12:27:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=159
05/25/2022 12:27:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=159
05/25/2022 12:27:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=160
05/25/2022 12:27:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=161
05/25/2022 12:27:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.42 on epoch=162
05/25/2022 12:27:36 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.24143460288038598 on epoch=162
05/25/2022 12:27:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
05/25/2022 12:27:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=164
05/25/2022 12:27:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=164
05/25/2022 12:27:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=165
05/25/2022 12:27:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=166
05/25/2022 12:27:55 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.18603969823482017 on epoch=166
05/25/2022 12:27:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
05/25/2022 12:28:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=168
05/25/2022 12:28:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=169
05/25/2022 12:28:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
05/25/2022 12:28:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=170
05/25/2022 12:28:14 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.23505455118358345 on epoch=170
05/25/2022 12:28:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=171
05/25/2022 12:28:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=172
05/25/2022 12:28:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=173
05/25/2022 12:28:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 12:28:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=174
05/25/2022 12:28:32 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.20375469336670837 on epoch=174
05/25/2022 12:28:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
05/25/2022 12:28:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=176
05/25/2022 12:28:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
05/25/2022 12:28:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=178
05/25/2022 12:28:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=179
05/25/2022 12:28:51 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.31959876543209875 on epoch=179
05/25/2022 12:28:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3072831481256484 -> 0.31959876543209875 on epoch=179, global_step=2150
05/25/2022 12:28:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=179
05/25/2022 12:28:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=180
05/25/2022 12:28:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=181
05/25/2022 12:29:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=182
05/25/2022 12:29:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
05/25/2022 12:29:10 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.2710725095407291 on epoch=183
05/25/2022 12:29:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=184
05/25/2022 12:29:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
05/25/2022 12:29:18 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=185
05/25/2022 12:29:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=186
05/25/2022 12:29:23 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
05/25/2022 12:29:28 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.1775766716943188 on epoch=187
05/25/2022 12:29:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=188
05/25/2022 12:29:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
05/25/2022 12:29:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=189
05/25/2022 12:29:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=190
05/25/2022 12:29:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
05/25/2022 12:29:47 - INFO - __main__ - Global step 2300 Train loss 0.39 Classification-F1 0.20375469336670837 on epoch=191
05/25/2022 12:29:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.39 on epoch=192
05/25/2022 12:29:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
05/25/2022 12:29:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=194
05/25/2022 12:29:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=194
05/25/2022 12:30:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
05/25/2022 12:30:06 - INFO - __main__ - Global step 2350 Train loss 0.39 Classification-F1 0.2492702344095533 on epoch=195
05/25/2022 12:30:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=196
05/25/2022 12:30:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
05/25/2022 12:30:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=198
05/25/2022 12:30:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=199
05/25/2022 12:30:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=199
05/25/2022 12:30:25 - INFO - __main__ - Global step 2400 Train loss 0.39 Classification-F1 0.32207540126913026 on epoch=199
05/25/2022 12:30:25 - INFO - __main__ - Saving model with best Classification-F1: 0.31959876543209875 -> 0.32207540126913026 on epoch=199, global_step=2400
05/25/2022 12:30:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
05/25/2022 12:30:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=201
05/25/2022 12:30:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=202
05/25/2022 12:30:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=203
05/25/2022 12:30:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=204
05/25/2022 12:30:44 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.3531673062573286 on epoch=204
05/25/2022 12:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.32207540126913026 -> 0.3531673062573286 on epoch=204, global_step=2450
05/25/2022 12:30:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=204
05/25/2022 12:30:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=205
05/25/2022 12:30:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
05/25/2022 12:30:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
05/25/2022 12:30:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=208
05/25/2022 12:31:03 - INFO - __main__ - Global step 2500 Train loss 0.40 Classification-F1 0.23668468468468465 on epoch=208
05/25/2022 12:31:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.38 on epoch=209
05/25/2022 12:31:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
05/25/2022 12:31:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.38 on epoch=210
05/25/2022 12:31:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=211
05/25/2022 12:31:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
05/25/2022 12:31:22 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.29083685156745637 on epoch=212
05/25/2022 12:31:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=213
05/25/2022 12:31:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=214
05/25/2022 12:31:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=214
05/25/2022 12:31:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=215
05/25/2022 12:31:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=216
05/25/2022 12:31:41 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.25957918050941303 on epoch=216
05/25/2022 12:31:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=217
05/25/2022 12:31:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.37 on epoch=218
05/25/2022 12:31:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=219
05/25/2022 12:31:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.38 on epoch=219
05/25/2022 12:31:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
05/25/2022 12:31:59 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.3592779396733376 on epoch=220
05/25/2022 12:32:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3531673062573286 -> 0.3592779396733376 on epoch=220, global_step=2650
05/25/2022 12:32:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=221
05/25/2022 12:32:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.39 on epoch=222
05/25/2022 12:32:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=223
05/25/2022 12:32:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=224
05/25/2022 12:32:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.37 on epoch=224
05/25/2022 12:32:18 - INFO - __main__ - Global step 2700 Train loss 0.39 Classification-F1 0.24273748723186928 on epoch=224
05/25/2022 12:32:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
05/25/2022 12:32:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
05/25/2022 12:32:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=227
05/25/2022 12:32:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=228
05/25/2022 12:32:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=229
05/25/2022 12:32:37 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.3335097001763669 on epoch=229
05/25/2022 12:32:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=229
05/25/2022 12:32:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=230
05/25/2022 12:32:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=231
05/25/2022 12:32:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=232
05/25/2022 12:32:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.44 on epoch=233
05/25/2022 12:32:56 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.3020978192129748 on epoch=233
05/25/2022 12:32:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
05/25/2022 12:33:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=234
05/25/2022 12:33:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.44 on epoch=235
05/25/2022 12:33:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=236
05/25/2022 12:33:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.37 on epoch=237
05/25/2022 12:33:15 - INFO - __main__ - Global step 2850 Train loss 0.41 Classification-F1 0.20516704535891608 on epoch=237
05/25/2022 12:33:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
05/25/2022 12:33:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=239
05/25/2022 12:33:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=239
05/25/2022 12:33:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=240
05/25/2022 12:33:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=241
05/25/2022 12:33:33 - INFO - __main__ - Global step 2900 Train loss 0.39 Classification-F1 0.24002588877074568 on epoch=241
05/25/2022 12:33:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=242
05/25/2022 12:33:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=243
05/25/2022 12:33:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.40 on epoch=244
05/25/2022 12:33:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.37 on epoch=244
05/25/2022 12:33:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=245
05/25/2022 12:33:52 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.25192564918592314 on epoch=245
05/25/2022 12:33:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.40 on epoch=246
05/25/2022 12:33:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.37 on epoch=247
05/25/2022 12:34:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=248
05/25/2022 12:34:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.38 on epoch=249
05/25/2022 12:34:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=249
05/25/2022 12:34:07 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:34:07 - INFO - __main__ - Printing 3 examples
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:34:07 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:34:07 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:34:07 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:34:07 - INFO - __main__ - Printing 3 examples
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:34:07 - INFO - __main__ - ['neutral']
05/25/2022 12:34:07 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:34:07 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:34:07 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:34:11 - INFO - __main__ - Global step 3000 Train loss 0.39 Classification-F1 0.1807527116546553 on epoch=249
05/25/2022 12:34:11 - INFO - __main__ - save last model!
05/25/2022 12:34:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 12:34:11 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 12:34:11 - INFO - __main__ - Printing 3 examples
05/25/2022 12:34:11 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 12:34:11 - INFO - __main__ - ['contradiction']
05/25/2022 12:34:11 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 12:34:11 - INFO - __main__ - ['entailment']
05/25/2022 12:34:11 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 12:34:11 - INFO - __main__ - ['contradiction']
05/25/2022 12:34:11 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:34:11 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:34:12 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 12:34:22 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:34:22 - INFO - __main__ - task name: anli
05/25/2022 12:34:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:34:23 - INFO - __main__ - Starting training!
05/25/2022 12:34:41 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_100_0.4_8_predictions.txt
05/25/2022 12:34:41 - INFO - __main__ - Classification-F1 on test data: 0.2132
05/25/2022 12:34:42 - INFO - __main__ - prefix=anli_64_100, lr=0.4, bsz=8, dev_performance=0.3592779396733376, test_performance=0.21320899057994938
05/25/2022 12:34:42 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.3, bsz=8 ...
05/25/2022 12:34:43 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:34:43 - INFO - __main__ - Printing 3 examples
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:34:43 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:34:43 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:34:43 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:34:43 - INFO - __main__ - Printing 3 examples
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:34:43 - INFO - __main__ - ['neutral']
05/25/2022 12:34:43 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:34:43 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:34:43 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:34:58 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:34:58 - INFO - __main__ - task name: anli
05/25/2022 12:34:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:34:59 - INFO - __main__ - Starting training!
05/25/2022 12:35:02 - INFO - __main__ - Step 10 Global step 10 Train loss 6.27 on epoch=0
05/25/2022 12:35:05 - INFO - __main__ - Step 20 Global step 20 Train loss 3.29 on epoch=1
05/25/2022 12:35:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.80 on epoch=2
05/25/2022 12:35:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.12 on epoch=3
05/25/2022 12:35:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.92 on epoch=4
05/25/2022 12:35:17 - INFO - __main__ - Global step 50 Train loss 2.68 Classification-F1 0.2594966761633428 on epoch=4
05/25/2022 12:35:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2594966761633428 on epoch=4, global_step=50
05/25/2022 12:35:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.75 on epoch=4
05/25/2022 12:35:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.70 on epoch=5
05/25/2022 12:35:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=6
05/25/2022 12:35:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=7
05/25/2022 12:35:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=8
05/25/2022 12:35:36 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 12:35:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=9
05/25/2022 12:35:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=9
05/25/2022 12:35:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=10
05/25/2022 12:35:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=11
05/25/2022 12:35:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=12
05/25/2022 12:35:54 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 12:35:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 12:35:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
05/25/2022 12:36:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=14
05/25/2022 12:36:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=15
05/25/2022 12:36:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=16
05/25/2022 12:36:13 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 12:36:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=17
05/25/2022 12:36:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=18
05/25/2022 12:36:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/25/2022 12:36:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
05/25/2022 12:36:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=20
05/25/2022 12:36:33 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.26811667723525684 on epoch=20
05/25/2022 12:36:33 - INFO - __main__ - Saving model with best Classification-F1: 0.2594966761633428 -> 0.26811667723525684 on epoch=20, global_step=250
05/25/2022 12:36:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=21
05/25/2022 12:36:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=22
05/25/2022 12:36:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=23
05/25/2022 12:36:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
05/25/2022 12:36:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/25/2022 12:36:52 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 12:36:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=25
05/25/2022 12:36:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=26
05/25/2022 12:37:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=27
05/25/2022 12:37:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=28
05/25/2022 12:37:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=29
05/25/2022 12:37:12 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.2299900278171417 on epoch=29
05/25/2022 12:37:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/25/2022 12:37:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=30
05/25/2022 12:37:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
05/25/2022 12:37:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=32
05/25/2022 12:37:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 12:37:31 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 12:37:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=34
05/25/2022 12:37:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=34
05/25/2022 12:37:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=35
05/25/2022 12:37:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
05/25/2022 12:37:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/25/2022 12:37:50 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 12:37:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/25/2022 12:37:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=39
05/25/2022 12:37:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=39
05/25/2022 12:38:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=40
05/25/2022 12:38:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=41
05/25/2022 12:38:09 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 12:38:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=42
05/25/2022 12:38:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
05/25/2022 12:38:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/25/2022 12:38:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=44
05/25/2022 12:38:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=45
05/25/2022 12:38:28 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.2281309873780586 on epoch=45
05/25/2022 12:38:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/25/2022 12:38:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=47
05/25/2022 12:38:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
05/25/2022 12:38:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
05/25/2022 12:38:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=49
05/25/2022 12:38:47 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.17592592592592593 on epoch=49
05/25/2022 12:38:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
05/25/2022 12:38:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
05/25/2022 12:38:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=52
05/25/2022 12:38:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
05/25/2022 12:39:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=54
05/25/2022 12:39:06 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.2407763076777161 on epoch=54
05/25/2022 12:39:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=54
05/25/2022 12:39:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=55
05/25/2022 12:39:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
05/25/2022 12:39:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=57
05/25/2022 12:39:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/25/2022 12:39:25 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 12:39:28 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=59
05/25/2022 12:39:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=59
05/25/2022 12:39:33 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
05/25/2022 12:39:36 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=61
05/25/2022 12:39:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
05/25/2022 12:39:44 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.24380032206119162 on epoch=62
05/25/2022 12:39:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
05/25/2022 12:39:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=64
05/25/2022 12:39:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=64
05/25/2022 12:39:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=65
05/25/2022 12:39:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
05/25/2022 12:40:03 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 12:40:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=67
05/25/2022 12:40:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/25/2022 12:40:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=69
05/25/2022 12:40:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
05/25/2022 12:40:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=70
05/25/2022 12:40:22 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.22618520159503763 on epoch=70
05/25/2022 12:40:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=71
05/25/2022 12:40:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=72
05/25/2022 12:40:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=73
05/25/2022 12:40:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=74
05/25/2022 12:40:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
05/25/2022 12:40:41 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.27215587680703957 on epoch=74
05/25/2022 12:40:41 - INFO - __main__ - Saving model with best Classification-F1: 0.26811667723525684 -> 0.27215587680703957 on epoch=74, global_step=900
05/25/2022 12:40:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=75
05/25/2022 12:40:47 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=76
05/25/2022 12:40:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=77
05/25/2022 12:40:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=78
05/25/2022 12:40:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=79
05/25/2022 12:41:01 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.1647058823529412 on epoch=79
05/25/2022 12:41:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=79
05/25/2022 12:41:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/25/2022 12:41:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=81
05/25/2022 12:41:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=82
05/25/2022 12:41:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
05/25/2022 12:41:20 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.28759278897136803 on epoch=83
05/25/2022 12:41:20 - INFO - __main__ - Saving model with best Classification-F1: 0.27215587680703957 -> 0.28759278897136803 on epoch=83, global_step=1000
05/25/2022 12:41:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
05/25/2022 12:41:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=84
05/25/2022 12:41:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=85
05/25/2022 12:41:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=86
05/25/2022 12:41:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
05/25/2022 12:41:39 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.2891791312843945 on epoch=87
05/25/2022 12:41:39 - INFO - __main__ - Saving model with best Classification-F1: 0.28759278897136803 -> 0.2891791312843945 on epoch=87, global_step=1050
05/25/2022 12:41:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=88
05/25/2022 12:41:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
05/25/2022 12:41:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=89
05/25/2022 12:41:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
05/25/2022 12:41:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
05/25/2022 12:41:59 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 12:42:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=92
05/25/2022 12:42:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=93
05/25/2022 12:42:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=94
05/25/2022 12:42:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
05/25/2022 12:42:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
05/25/2022 12:42:18 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.2489025602043701 on epoch=95
05/25/2022 12:42:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
05/25/2022 12:42:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
05/25/2022 12:42:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
05/25/2022 12:42:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
05/25/2022 12:42:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=99
05/25/2022 12:42:37 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 12:42:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=100
05/25/2022 12:42:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=101
05/25/2022 12:42:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
05/25/2022 12:42:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=103
05/25/2022 12:42:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=104
05/25/2022 12:42:56 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 12:42:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=104
05/25/2022 12:43:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=105
05/25/2022 12:43:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=106
05/25/2022 12:43:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=107
05/25/2022 12:43:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
05/25/2022 12:43:15 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.26253629699076786 on epoch=108
05/25/2022 12:43:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=109
05/25/2022 12:43:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
05/25/2022 12:43:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
05/25/2022 12:43:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=111
05/25/2022 12:43:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=112
05/25/2022 12:43:34 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.19100438134324219 on epoch=112
05/25/2022 12:43:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=113
05/25/2022 12:43:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
05/25/2022 12:43:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
05/25/2022 12:43:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=115
05/25/2022 12:43:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/25/2022 12:43:53 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 12:43:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
05/25/2022 12:43:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=118
05/25/2022 12:44:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=119
05/25/2022 12:44:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=119
05/25/2022 12:44:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=120
05/25/2022 12:44:12 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.2805590321234877 on epoch=120
05/25/2022 12:44:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=121
05/25/2022 12:44:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
05/25/2022 12:44:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=123
05/25/2022 12:44:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=124
05/25/2022 12:44:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=124
05/25/2022 12:44:32 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.22969404186795495 on epoch=124
05/25/2022 12:44:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=125
05/25/2022 12:44:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=126
05/25/2022 12:44:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=127
05/25/2022 12:44:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
05/25/2022 12:44:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=129
05/25/2022 12:44:51 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.16272965879265092 on epoch=129
05/25/2022 12:44:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
05/25/2022 12:44:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=130
05/25/2022 12:44:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=131
05/25/2022 12:45:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
05/25/2022 12:45:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=133
05/25/2022 12:45:10 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=133
05/25/2022 12:45:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=134
05/25/2022 12:45:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=134
05/25/2022 12:45:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=135
05/25/2022 12:45:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=136
05/25/2022 12:45:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=137
05/25/2022 12:45:29 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.1792624180683882 on epoch=137
05/25/2022 12:45:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=138
05/25/2022 12:45:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=139
05/25/2022 12:45:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=139
05/25/2022 12:45:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=140
05/25/2022 12:45:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=141
05/25/2022 12:45:47 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.1926530612244898 on epoch=141
05/25/2022 12:45:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
05/25/2022 12:45:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=143
05/25/2022 12:45:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=144
05/25/2022 12:45:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=144
05/25/2022 12:46:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=145
05/25/2022 12:46:07 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.2590421923755257 on epoch=145
05/25/2022 12:46:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 12:46:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=147
05/25/2022 12:46:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=148
05/25/2022 12:46:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=149
05/25/2022 12:46:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=149
05/25/2022 12:46:26 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.2355030165408847 on epoch=149
05/25/2022 12:46:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=150
05/25/2022 12:46:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
05/25/2022 12:46:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=152
05/25/2022 12:46:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 12:46:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=154
05/25/2022 12:46:45 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.21794465092537832 on epoch=154
05/25/2022 12:46:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=154
05/25/2022 12:46:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 12:46:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=156
05/25/2022 12:46:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=157
05/25/2022 12:46:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=158
05/25/2022 12:47:04 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.25283605283605287 on epoch=158
05/25/2022 12:47:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=159
05/25/2022 12:47:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=159
05/25/2022 12:47:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=160
05/25/2022 12:47:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=161
05/25/2022 12:47:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=162
05/25/2022 12:47:23 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.22363042814532985 on epoch=162
05/25/2022 12:47:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
05/25/2022 12:47:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=164
05/25/2022 12:47:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
05/25/2022 12:47:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
05/25/2022 12:47:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=166
05/25/2022 12:47:42 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.23696969696969697 on epoch=166
05/25/2022 12:47:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=167
05/25/2022 12:47:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=168
05/25/2022 12:47:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
05/25/2022 12:47:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=169
05/25/2022 12:47:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=170
05/25/2022 12:48:01 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.3435459213459653 on epoch=170
05/25/2022 12:48:01 - INFO - __main__ - Saving model with best Classification-F1: 0.2891791312843945 -> 0.3435459213459653 on epoch=170, global_step=2050
05/25/2022 12:48:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=171
05/25/2022 12:48:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=172
05/25/2022 12:48:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=173
05/25/2022 12:48:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=174
05/25/2022 12:48:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=174
05/25/2022 12:48:20 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.2998133509003074 on epoch=174
05/25/2022 12:48:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=175
05/25/2022 12:48:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.38 on epoch=176
05/25/2022 12:48:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.37 on epoch=177
05/25/2022 12:48:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
05/25/2022 12:48:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=179
05/25/2022 12:48:39 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.27827100435796087 on epoch=179
05/25/2022 12:48:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=179
05/25/2022 12:48:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
05/25/2022 12:48:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=181
05/25/2022 12:48:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=182
05/25/2022 12:48:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.38 on epoch=183
05/25/2022 12:48:58 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.22942294965797697 on epoch=183
05/25/2022 12:49:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=184
05/25/2022 12:49:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 12:49:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=185
05/25/2022 12:49:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=186
05/25/2022 12:49:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=187
05/25/2022 12:49:17 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.21159783062161033 on epoch=187
05/25/2022 12:49:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=188
05/25/2022 12:49:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=189
05/25/2022 12:49:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.38 on epoch=189
05/25/2022 12:49:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.39 on epoch=190
05/25/2022 12:49:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=191
05/25/2022 12:49:36 - INFO - __main__ - Global step 2300 Train loss 0.38 Classification-F1 0.25185519680891094 on epoch=191
05/25/2022 12:49:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
05/25/2022 12:49:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=193
05/25/2022 12:49:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=194
05/25/2022 12:49:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.40 on epoch=194
05/25/2022 12:49:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=195
05/25/2022 12:49:55 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.2416493479112075 on epoch=195
05/25/2022 12:49:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=196
05/25/2022 12:50:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.42 on epoch=197
05/25/2022 12:50:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=198
05/25/2022 12:50:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=199
05/25/2022 12:50:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=199
05/25/2022 12:50:14 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.2215531660692951 on epoch=199
05/25/2022 12:50:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=200
05/25/2022 12:50:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=201
05/25/2022 12:50:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=202
05/25/2022 12:50:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.37 on epoch=203
05/25/2022 12:50:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=204
05/25/2022 12:50:33 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.3284329524776068 on epoch=204
05/25/2022 12:50:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=204
05/25/2022 12:50:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.38 on epoch=205
05/25/2022 12:50:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=206
05/25/2022 12:50:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.34 on epoch=207
05/25/2022 12:50:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
05/25/2022 12:50:52 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.2887880192433711 on epoch=208
05/25/2022 12:50:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.35 on epoch=209
05/25/2022 12:50:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.35 on epoch=209
05/25/2022 12:51:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=210
05/25/2022 12:51:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=211
05/25/2022 12:51:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.37 on epoch=212
05/25/2022 12:51:11 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.20702563339778868 on epoch=212
05/25/2022 12:51:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 12:51:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.41 on epoch=214
05/25/2022 12:51:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=214
05/25/2022 12:51:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=215
05/25/2022 12:51:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.36 on epoch=216
05/25/2022 12:51:30 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.2603110209601081 on epoch=216
05/25/2022 12:51:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=217
05/25/2022 12:51:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=218
05/25/2022 12:51:38 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=219
05/25/2022 12:51:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.36 on epoch=219
05/25/2022 12:51:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
05/25/2022 12:51:50 - INFO - __main__ - Global step 2650 Train loss 0.35 Classification-F1 0.2492788040521073 on epoch=220
05/25/2022 12:51:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
05/25/2022 12:51:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=222
05/25/2022 12:51:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=223
05/25/2022 12:52:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=224
05/25/2022 12:52:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.34 on epoch=224
05/25/2022 12:52:09 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.2693221009010483 on epoch=224
05/25/2022 12:52:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=225
05/25/2022 12:52:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=226
05/25/2022 12:52:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.38 on epoch=227
05/25/2022 12:52:19 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=228
05/25/2022 12:52:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=229
05/25/2022 12:52:28 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.2803985353871135 on epoch=229
05/25/2022 12:52:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.39 on epoch=229
05/25/2022 12:52:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=230
05/25/2022 12:52:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=231
05/25/2022 12:52:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.37 on epoch=232
05/25/2022 12:52:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=233
05/25/2022 12:52:47 - INFO - __main__ - Global step 2800 Train loss 0.38 Classification-F1 0.28994008199306215 on epoch=233
05/25/2022 12:52:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=234
05/25/2022 12:52:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.31 on epoch=234
05/25/2022 12:52:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
05/25/2022 12:52:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=236
05/25/2022 12:53:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.34 on epoch=237
05/25/2022 12:53:06 - INFO - __main__ - Global step 2850 Train loss 0.35 Classification-F1 0.29525394387145537 on epoch=237
05/25/2022 12:53:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=238
05/25/2022 12:53:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.33 on epoch=239
05/25/2022 12:53:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=239
05/25/2022 12:53:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=240
05/25/2022 12:53:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
05/25/2022 12:53:25 - INFO - __main__ - Global step 2900 Train loss 0.36 Classification-F1 0.2640164713335445 on epoch=241
05/25/2022 12:53:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=242
05/25/2022 12:53:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=243
05/25/2022 12:53:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=244
05/25/2022 12:53:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=244
05/25/2022 12:53:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=245
05/25/2022 12:53:44 - INFO - __main__ - Global step 2950 Train loss 0.35 Classification-F1 0.25291356852818647 on epoch=245
05/25/2022 12:53:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.38 on epoch=246
05/25/2022 12:53:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.35 on epoch=247
05/25/2022 12:53:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.40 on epoch=248
05/25/2022 12:53:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=249
05/25/2022 12:53:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.35 on epoch=249
05/25/2022 12:53:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:53:59 - INFO - __main__ - Printing 3 examples
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:53:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:53:59 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:53:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:53:59 - INFO - __main__ - Printing 3 examples
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:53:59 - INFO - __main__ - ['neutral']
05/25/2022 12:53:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:53:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:54:00 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:54:03 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.307460095048857 on epoch=249
05/25/2022 12:54:03 - INFO - __main__ - save last model!
05/25/2022 12:54:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 12:54:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 12:54:03 - INFO - __main__ - Printing 3 examples
05/25/2022 12:54:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 12:54:03 - INFO - __main__ - ['contradiction']
05/25/2022 12:54:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 12:54:03 - INFO - __main__ - ['entailment']
05/25/2022 12:54:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 12:54:03 - INFO - __main__ - ['contradiction']
05/25/2022 12:54:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:54:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:54:05 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 12:54:14 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:54:14 - INFO - __main__ - task name: anli
05/25/2022 12:54:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:54:15 - INFO - __main__ - Starting training!
05/25/2022 12:54:36 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_100_0.3_8_predictions.txt
05/25/2022 12:54:36 - INFO - __main__ - Classification-F1 on test data: 0.3014
05/25/2022 12:54:36 - INFO - __main__ - prefix=anli_64_100, lr=0.3, bsz=8, dev_performance=0.3435459213459653, test_performance=0.3014438660503332
05/25/2022 12:54:36 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.2, bsz=8 ...
05/25/2022 12:54:37 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:54:37 - INFO - __main__ - Printing 3 examples
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:54:37 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:54:37 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 12:54:37 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 12:54:37 - INFO - __main__ - Printing 3 examples
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/25/2022 12:54:37 - INFO - __main__ - ['neutral']
05/25/2022 12:54:37 - INFO - __main__ - Tokenizing Input ...
05/25/2022 12:54:37 - INFO - __main__ - Tokenizing Output ...
05/25/2022 12:54:37 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 12:54:52 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 12:54:52 - INFO - __main__ - task name: anli
05/25/2022 12:54:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 12:54:53 - INFO - __main__ - Starting training!
05/25/2022 12:54:56 - INFO - __main__ - Step 10 Global step 10 Train loss 6.98 on epoch=0
05/25/2022 12:54:58 - INFO - __main__ - Step 20 Global step 20 Train loss 4.74 on epoch=1
05/25/2022 12:55:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.84 on epoch=2
05/25/2022 12:55:04 - INFO - __main__ - Step 40 Global step 40 Train loss 1.81 on epoch=3
05/25/2022 12:55:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.38 on epoch=4
05/25/2022 12:55:12 - INFO - __main__ - Global step 50 Train loss 3.55 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 12:55:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 12:55:14 - INFO - __main__ - Step 60 Global step 60 Train loss 1.08 on epoch=4
05/25/2022 12:55:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=5
05/25/2022 12:55:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=6
05/25/2022 12:55:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.81 on epoch=7
05/25/2022 12:55:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.74 on epoch=8
05/25/2022 12:55:31 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 12:55:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.68 on epoch=9
05/25/2022 12:55:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.68 on epoch=9
05/25/2022 12:55:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=10
05/25/2022 12:55:41 - INFO - __main__ - Step 140 Global step 140 Train loss 1.07 on epoch=11
05/25/2022 12:55:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
05/25/2022 12:55:48 - INFO - __main__ - Global step 150 Train loss 0.73 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 12:55:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 12:55:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=14
05/25/2022 12:55:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=14
05/25/2022 12:55:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=15
05/25/2022 12:56:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=16
05/25/2022 12:56:05 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 12:56:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=17
05/25/2022 12:56:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=18
05/25/2022 12:56:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=19
05/25/2022 12:56:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=19
05/25/2022 12:56:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=20
05/25/2022 12:56:22 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 12:56:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=21
05/25/2022 12:56:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.56 on epoch=22
05/25/2022 12:56:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=23
05/25/2022 12:56:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=24
05/25/2022 12:56:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/25/2022 12:56:40 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 12:56:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=25
05/25/2022 12:56:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=26
05/25/2022 12:56:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/25/2022 12:56:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=28
05/25/2022 12:56:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=29
05/25/2022 12:56:57 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 12:57:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 12:57:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
05/25/2022 12:57:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=31
05/25/2022 12:57:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
05/25/2022 12:57:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=33
05/25/2022 12:57:14 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 12:57:17 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=34
05/25/2022 12:57:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=34
05/25/2022 12:57:22 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
05/25/2022 12:57:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=36
05/25/2022 12:57:27 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/25/2022 12:57:32 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.17066666666666666 on epoch=37
05/25/2022 12:57:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17066666666666666 on epoch=37, global_step=450
05/25/2022 12:57:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=38
05/25/2022 12:57:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=39
05/25/2022 12:57:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/25/2022 12:57:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=40
05/25/2022 12:57:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/25/2022 12:57:49 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 12:57:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=42
05/25/2022 12:57:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/25/2022 12:57:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
05/25/2022 12:57:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
05/25/2022 12:58:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=45
05/25/2022 12:58:06 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.17066666666666666 on epoch=45
05/25/2022 12:58:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=46
05/25/2022 12:58:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=47
05/25/2022 12:58:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
05/25/2022 12:58:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=49
05/25/2022 12:58:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=49
05/25/2022 12:58:24 - INFO - __main__ - Global step 600 Train loss 0.49 Classification-F1 0.16732026143790854 on epoch=49
05/25/2022 12:58:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=50
05/25/2022 12:58:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/25/2022 12:58:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=52
05/25/2022 12:58:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=53
05/25/2022 12:58:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=54
05/25/2022 12:58:42 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.16073781291172592 on epoch=54
05/25/2022 12:58:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
05/25/2022 12:58:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=55
05/25/2022 12:58:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
05/25/2022 12:58:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
05/25/2022 12:58:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=58
05/25/2022 12:59:00 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 12:59:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=59
05/25/2022 12:59:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=59
05/25/2022 12:59:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=60
05/25/2022 12:59:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 12:59:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=62
05/25/2022 12:59:17 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 12:59:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=63
05/25/2022 12:59:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
05/25/2022 12:59:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
05/25/2022 12:59:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=65
05/25/2022 12:59:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
05/25/2022 12:59:34 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 12:59:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
05/25/2022 12:59:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=68
05/25/2022 12:59:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=69
05/25/2022 12:59:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
05/25/2022 12:59:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=70
05/25/2022 12:59:52 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 12:59:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=71
05/25/2022 12:59:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
05/25/2022 13:00:00 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=73
05/25/2022 13:00:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=74
05/25/2022 13:00:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=74
05/25/2022 13:00:11 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
05/25/2022 13:00:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
05/25/2022 13:00:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=76
05/25/2022 13:00:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=77
05/25/2022 13:00:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=78
05/25/2022 13:00:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
05/25/2022 13:00:29 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.3474250764525994 on epoch=79
05/25/2022 13:00:29 - INFO - __main__ - Saving model with best Classification-F1: 0.17066666666666666 -> 0.3474250764525994 on epoch=79, global_step=950
05/25/2022 13:00:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=79
05/25/2022 13:00:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/25/2022 13:00:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=81
05/25/2022 13:00:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=82
05/25/2022 13:00:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
05/25/2022 13:00:48 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.34908428943458686 on epoch=83
05/25/2022 13:00:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3474250764525994 -> 0.34908428943458686 on epoch=83, global_step=1000
05/25/2022 13:00:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=84
05/25/2022 13:00:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
05/25/2022 13:00:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=85
05/25/2022 13:00:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
05/25/2022 13:01:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
05/25/2022 13:01:06 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.20969986086265158 on epoch=87
05/25/2022 13:01:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.50 on epoch=88
05/25/2022 13:01:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=89
05/25/2022 13:01:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=89
05/25/2022 13:01:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=90
05/25/2022 13:01:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/25/2022 13:01:24 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.2815681796555334 on epoch=91
05/25/2022 13:01:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=92
05/25/2022 13:01:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=93
05/25/2022 13:01:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=94
05/25/2022 13:01:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=94
05/25/2022 13:01:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=95
05/25/2022 13:01:43 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.32360001917894415 on epoch=95
05/25/2022 13:01:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=96
05/25/2022 13:01:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=97
05/25/2022 13:01:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=98
05/25/2022 13:01:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.51 on epoch=99
05/25/2022 13:01:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=99
05/25/2022 13:02:01 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 13:02:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=100
05/25/2022 13:02:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=101
05/25/2022 13:02:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=102
05/25/2022 13:02:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
05/25/2022 13:02:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=104
05/25/2022 13:02:20 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2552510107204098 on epoch=104
05/25/2022 13:02:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=104
05/25/2022 13:02:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
05/25/2022 13:02:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=106
05/25/2022 13:02:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=107
05/25/2022 13:02:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=108
05/25/2022 13:02:39 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.2784186138223405 on epoch=108
05/25/2022 13:02:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
05/25/2022 13:02:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
05/25/2022 13:02:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
05/25/2022 13:02:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=111
05/25/2022 13:02:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
05/25/2022 13:02:58 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.1686746987951807 on epoch=112
05/25/2022 13:03:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
05/25/2022 13:03:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=114
05/25/2022 13:03:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=114
05/25/2022 13:03:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=115
05/25/2022 13:03:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=116
05/25/2022 13:03:16 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 13:03:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=117
05/25/2022 13:03:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=118
05/25/2022 13:03:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=119
05/25/2022 13:03:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=119
05/25/2022 13:03:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
05/25/2022 13:03:34 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.23071940052873927 on epoch=120
05/25/2022 13:03:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=121
05/25/2022 13:03:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
05/25/2022 13:03:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=123
05/25/2022 13:03:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=124
05/25/2022 13:03:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=124
05/25/2022 13:03:52 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=124
05/25/2022 13:03:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=125
05/25/2022 13:03:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=126
05/25/2022 13:04:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
05/25/2022 13:04:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=128
05/25/2022 13:04:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=129
05/25/2022 13:04:10 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.3075365800823175 on epoch=129
05/25/2022 13:04:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=129
05/25/2022 13:04:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=130
05/25/2022 13:04:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=131
05/25/2022 13:04:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=132
05/25/2022 13:04:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
05/25/2022 13:04:29 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.23130677847658979 on epoch=133
05/25/2022 13:04:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=134
05/25/2022 13:04:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=134
05/25/2022 13:04:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
05/25/2022 13:04:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
05/25/2022 13:04:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.46 on epoch=137
05/25/2022 13:04:48 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.16864295125164688 on epoch=137
05/25/2022 13:04:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=138
05/25/2022 13:04:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 13:04:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
05/25/2022 13:04:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
05/25/2022 13:05:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=141
05/25/2022 13:05:06 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 13:05:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
05/25/2022 13:05:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
05/25/2022 13:05:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=144
05/25/2022 13:05:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=144
05/25/2022 13:05:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=145
05/25/2022 13:05:24 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.1881810228266921 on epoch=145
05/25/2022 13:05:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 13:05:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=147
05/25/2022 13:05:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
05/25/2022 13:05:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=149
05/25/2022 13:05:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=149
05/25/2022 13:05:43 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=149
05/25/2022 13:05:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=150
05/25/2022 13:05:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=151
05/25/2022 13:05:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=152
05/25/2022 13:05:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 13:05:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=154
05/25/2022 13:06:02 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.21021237726992734 on epoch=154
05/25/2022 13:06:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=154
05/25/2022 13:06:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=155
05/25/2022 13:06:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=156
05/25/2022 13:06:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
05/25/2022 13:06:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=158
05/25/2022 13:06:21 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.21982262431700636 on epoch=158
05/25/2022 13:06:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=159
05/25/2022 13:06:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=159
05/25/2022 13:06:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=160
05/25/2022 13:06:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=161
05/25/2022 13:06:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=162
05/25/2022 13:06:40 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=162
05/25/2022 13:06:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=163
05/25/2022 13:06:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=164
05/25/2022 13:06:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=164
05/25/2022 13:06:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=165
05/25/2022 13:06:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
05/25/2022 13:06:58 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
05/25/2022 13:07:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=167
05/25/2022 13:07:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=168
05/25/2022 13:07:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
05/25/2022 13:07:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.45 on epoch=169
05/25/2022 13:07:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.45 on epoch=170
05/25/2022 13:07:17 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.1989722270338934 on epoch=170
05/25/2022 13:07:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=171
05/25/2022 13:07:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
05/25/2022 13:07:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
05/25/2022 13:07:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 13:07:30 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.38 on epoch=174
05/25/2022 13:07:35 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=174
05/25/2022 13:07:38 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=175
05/25/2022 13:07:41 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=176
05/25/2022 13:07:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=177
05/25/2022 13:07:46 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=178
05/25/2022 13:07:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=179
05/25/2022 13:07:54 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.1939047619047619 on epoch=179
05/25/2022 13:07:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=179
05/25/2022 13:07:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=180
05/25/2022 13:08:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=181
05/25/2022 13:08:05 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.45 on epoch=182
05/25/2022 13:08:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
05/25/2022 13:08:13 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.24554591439174822 on epoch=183
05/25/2022 13:08:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.44 on epoch=184
05/25/2022 13:08:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
05/25/2022 13:08:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=185
05/25/2022 13:08:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=186
05/25/2022 13:08:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
05/25/2022 13:08:31 - INFO - __main__ - Global step 2250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=187
05/25/2022 13:08:34 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=188
05/25/2022 13:08:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=189
05/25/2022 13:08:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.45 on epoch=189
05/25/2022 13:08:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.39 on epoch=190
05/25/2022 13:08:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=191
05/25/2022 13:08:50 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.16732026143790854 on epoch=191
05/25/2022 13:08:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
05/25/2022 13:08:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=193
05/25/2022 13:08:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=194
05/25/2022 13:09:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=194
05/25/2022 13:09:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
05/25/2022 13:09:09 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=195
05/25/2022 13:09:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=196
05/25/2022 13:09:14 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=197
05/25/2022 13:09:17 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=198
05/25/2022 13:09:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=199
05/25/2022 13:09:22 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=199
05/25/2022 13:09:27 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=199
05/25/2022 13:09:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.39 on epoch=200
05/25/2022 13:09:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=201
05/25/2022 13:09:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=202
05/25/2022 13:09:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
05/25/2022 13:09:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=204
05/25/2022 13:09:46 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.2119119948448863 on epoch=204
05/25/2022 13:09:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
05/25/2022 13:09:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=205
05/25/2022 13:09:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=206
05/25/2022 13:09:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
05/25/2022 13:09:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.44 on epoch=208
05/25/2022 13:10:05 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.24424558509505767 on epoch=208
05/25/2022 13:10:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
05/25/2022 13:10:10 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=209
05/25/2022 13:10:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=210
05/25/2022 13:10:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.47 on epoch=211
05/25/2022 13:10:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=212
05/25/2022 13:10:23 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.19279363397010454 on epoch=212
05/25/2022 13:10:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 13:10:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=214
05/25/2022 13:10:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.41 on epoch=214
05/25/2022 13:10:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.36 on epoch=215
05/25/2022 13:10:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=216
05/25/2022 13:10:42 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.24054734240547346 on epoch=216
05/25/2022 13:10:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=217
05/25/2022 13:10:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=218
05/25/2022 13:10:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.43 on epoch=219
05/25/2022 13:10:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
05/25/2022 13:10:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.39 on epoch=220
05/25/2022 13:11:01 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.2845737193563281 on epoch=220
05/25/2022 13:11:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=221
05/25/2022 13:11:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=222
05/25/2022 13:11:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=223
05/25/2022 13:11:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.43 on epoch=224
05/25/2022 13:11:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.38 on epoch=224
05/25/2022 13:11:20 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.2129389690286296 on epoch=224
05/25/2022 13:11:22 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.37 on epoch=225
05/25/2022 13:11:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=226
05/25/2022 13:11:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=227
05/25/2022 13:11:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=228
05/25/2022 13:11:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=229
05/25/2022 13:11:39 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.31001211015523017 on epoch=229
05/25/2022 13:11:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=229
05/25/2022 13:11:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.42 on epoch=230
05/25/2022 13:11:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=231
05/25/2022 13:11:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.40 on epoch=232
05/25/2022 13:11:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.40 on epoch=233
05/25/2022 13:11:57 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.25746418607728305 on epoch=233
05/25/2022 13:12:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=234
05/25/2022 13:12:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
05/25/2022 13:12:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.38 on epoch=235
05/25/2022 13:12:08 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.44 on epoch=236
05/25/2022 13:12:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.37 on epoch=237
05/25/2022 13:12:16 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.22970786944523744 on epoch=237
05/25/2022 13:12:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=238
05/25/2022 13:12:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=239
05/25/2022 13:12:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=239
05/25/2022 13:12:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
05/25/2022 13:12:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=241
05/25/2022 13:12:34 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.2092476489028213 on epoch=241
05/25/2022 13:12:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=242
05/25/2022 13:12:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.41 on epoch=243
05/25/2022 13:12:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
05/25/2022 13:12:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=244
05/25/2022 13:12:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.41 on epoch=245
05/25/2022 13:12:53 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.26908682984590937 on epoch=245
05/25/2022 13:12:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
05/25/2022 13:12:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=247
05/25/2022 13:13:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=248
05/25/2022 13:13:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=249
05/25/2022 13:13:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=249
05/25/2022 13:13:07 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:13:07 - INFO - __main__ - Printing 3 examples
05/25/2022 13:13:07 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:13:07 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:07 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:13:07 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:07 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:13:07 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:07 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:13:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:13:08 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:13:08 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:13:08 - INFO - __main__ - Printing 3 examples
05/25/2022 13:13:08 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:13:08 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:08 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:13:08 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:08 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:13:08 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:08 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:13:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:13:08 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:13:12 - INFO - __main__ - Global step 3000 Train loss 0.39 Classification-F1 0.21779535578524287 on epoch=249
05/25/2022 13:13:12 - INFO - __main__ - save last model!
05/25/2022 13:13:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 13:13:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 13:13:12 - INFO - __main__ - Printing 3 examples
05/25/2022 13:13:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 13:13:12 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 13:13:12 - INFO - __main__ - ['entailment']
05/25/2022 13:13:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 13:13:12 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:12 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:13:13 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:13:14 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 13:13:24 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:13:24 - INFO - __main__ - task name: anli
05/25/2022 13:13:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:13:25 - INFO - __main__ - Starting training!
05/25/2022 13:13:43 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_100_0.2_8_predictions.txt
05/25/2022 13:13:43 - INFO - __main__ - Classification-F1 on test data: 0.2318
05/25/2022 13:13:43 - INFO - __main__ - prefix=anli_64_100, lr=0.2, bsz=8, dev_performance=0.34908428943458686, test_performance=0.2317850044655773
05/25/2022 13:13:43 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.5, bsz=8 ...
05/25/2022 13:13:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:13:44 - INFO - __main__ - Printing 3 examples
05/25/2022 13:13:44 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:13:44 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:44 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:13:44 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:44 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:13:44 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:13:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:13:45 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:13:45 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:13:45 - INFO - __main__ - Printing 3 examples
05/25/2022 13:13:45 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:13:45 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:45 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:13:45 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:45 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:13:45 - INFO - __main__ - ['contradiction']
05/25/2022 13:13:45 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:13:45 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:13:45 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:14:04 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:14:04 - INFO - __main__ - task name: anli
05/25/2022 13:14:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:14:05 - INFO - __main__ - Starting training!
05/25/2022 13:14:08 - INFO - __main__ - Step 10 Global step 10 Train loss 5.66 on epoch=0
05/25/2022 13:14:10 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=1
05/25/2022 13:14:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.20 on epoch=2
05/25/2022 13:14:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=3
05/25/2022 13:14:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=4
05/25/2022 13:14:25 - INFO - __main__ - Global step 50 Train loss 2.27 Classification-F1 0.27323105257099634 on epoch=4
05/25/2022 13:14:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.27323105257099634 on epoch=4, global_step=50
05/25/2022 13:14:27 - INFO - __main__ - Step 60 Global step 60 Train loss 0.79 on epoch=4
05/25/2022 13:14:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.64 on epoch=5
05/25/2022 13:14:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=6
05/25/2022 13:14:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=7
05/25/2022 13:14:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=8
05/25/2022 13:14:42 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.19314546320876727 on epoch=8
05/25/2022 13:14:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=9
05/25/2022 13:14:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=9
05/25/2022 13:14:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=10
05/25/2022 13:14:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=11
05/25/2022 13:14:55 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=12
05/25/2022 13:15:00 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.1775766716943188 on epoch=12
05/25/2022 13:15:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 13:15:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
05/25/2022 13:15:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.54 on epoch=14
05/25/2022 13:15:11 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=15
05/25/2022 13:15:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=16
05/25/2022 13:15:19 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.1775766716943188 on epoch=16
05/25/2022 13:15:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=17
05/25/2022 13:15:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
05/25/2022 13:15:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
05/25/2022 13:15:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=19
05/25/2022 13:15:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=20
05/25/2022 13:15:38 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.28258258258258256 on epoch=20
05/25/2022 13:15:38 - INFO - __main__ - Saving model with best Classification-F1: 0.27323105257099634 -> 0.28258258258258256 on epoch=20, global_step=250
05/25/2022 13:15:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=21
05/25/2022 13:15:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=22
05/25/2022 13:15:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
05/25/2022 13:15:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
05/25/2022 13:15:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=24
05/25/2022 13:15:56 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.23777777777777778 on epoch=24
05/25/2022 13:15:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=25
05/25/2022 13:16:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.48 on epoch=26
05/25/2022 13:16:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=27
05/25/2022 13:16:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/25/2022 13:16:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
05/25/2022 13:16:15 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.20091270066828526 on epoch=29
05/25/2022 13:16:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=29
05/25/2022 13:16:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
05/25/2022 13:16:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=31
05/25/2022 13:16:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
05/25/2022 13:16:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=33
05/25/2022 13:16:34 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 13:16:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=34
05/25/2022 13:16:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=34
05/25/2022 13:16:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
05/25/2022 13:16:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
05/25/2022 13:16:47 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=37
05/25/2022 13:16:53 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.19153973439687724 on epoch=37
05/25/2022 13:16:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=38
05/25/2022 13:16:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/25/2022 13:17:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/25/2022 13:17:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
05/25/2022 13:17:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=41
05/25/2022 13:17:12 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 13:17:14 - INFO - __main__ - Step 510 Global step 510 Train loss 2.60 on epoch=42
05/25/2022 13:17:17 - INFO - __main__ - Step 520 Global step 520 Train loss 4.84 on epoch=43
05/25/2022 13:17:19 - INFO - __main__ - Step 530 Global step 530 Train loss 4.86 on epoch=44
05/25/2022 13:17:22 - INFO - __main__ - Step 540 Global step 540 Train loss 3.39 on epoch=44
05/25/2022 13:17:25 - INFO - __main__ - Step 550 Global step 550 Train loss 5.39 on epoch=45
05/25/2022 13:17:45 - INFO - __main__ - Global step 550 Train loss 4.22 Classification-F1 0.11174159755565254 on epoch=45
05/25/2022 13:17:47 - INFO - __main__ - Step 560 Global step 560 Train loss 5.38 on epoch=46
05/25/2022 13:17:50 - INFO - __main__ - Step 570 Global step 570 Train loss 5.42 on epoch=47
05/25/2022 13:17:52 - INFO - __main__ - Step 580 Global step 580 Train loss 5.93 on epoch=48
05/25/2022 13:17:55 - INFO - __main__ - Step 590 Global step 590 Train loss 5.24 on epoch=49
05/25/2022 13:17:57 - INFO - __main__ - Step 600 Global step 600 Train loss 5.23 on epoch=49
05/25/2022 13:18:08 - INFO - __main__ - Global step 600 Train loss 5.44 Classification-F1 0.15485300040273864 on epoch=49
05/25/2022 13:18:11 - INFO - __main__ - Step 610 Global step 610 Train loss 4.85 on epoch=50
05/25/2022 13:18:13 - INFO - __main__ - Step 620 Global step 620 Train loss 4.30 on epoch=51
05/25/2022 13:18:16 - INFO - __main__ - Step 630 Global step 630 Train loss 3.67 on epoch=52
05/25/2022 13:18:18 - INFO - __main__ - Step 640 Global step 640 Train loss 3.72 on epoch=53
05/25/2022 13:18:21 - INFO - __main__ - Step 650 Global step 650 Train loss 3.36 on epoch=54
05/25/2022 13:18:27 - INFO - __main__ - Global step 650 Train loss 3.98 Classification-F1 0.3071355879789716 on epoch=54
05/25/2022 13:18:27 - INFO - __main__ - Saving model with best Classification-F1: 0.28258258258258256 -> 0.3071355879789716 on epoch=54, global_step=650
05/25/2022 13:18:30 - INFO - __main__ - Step 660 Global step 660 Train loss 2.94 on epoch=54
05/25/2022 13:18:32 - INFO - __main__ - Step 670 Global step 670 Train loss 2.58 on epoch=55
05/25/2022 13:18:35 - INFO - __main__ - Step 680 Global step 680 Train loss 2.71 on epoch=56
05/25/2022 13:18:38 - INFO - __main__ - Step 690 Global step 690 Train loss 2.46 on epoch=57
05/25/2022 13:18:40 - INFO - __main__ - Step 700 Global step 700 Train loss 2.36 on epoch=58
05/25/2022 13:18:46 - INFO - __main__ - Global step 700 Train loss 2.61 Classification-F1 0.2963749311944641 on epoch=58
05/25/2022 13:18:49 - INFO - __main__ - Step 710 Global step 710 Train loss 2.20 on epoch=59
05/25/2022 13:18:51 - INFO - __main__ - Step 720 Global step 720 Train loss 2.05 on epoch=59
05/25/2022 13:18:54 - INFO - __main__ - Step 730 Global step 730 Train loss 1.82 on epoch=60
05/25/2022 13:18:56 - INFO - __main__ - Step 740 Global step 740 Train loss 1.91 on epoch=61
05/25/2022 13:18:59 - INFO - __main__ - Step 750 Global step 750 Train loss 1.62 on epoch=62
05/25/2022 13:19:05 - INFO - __main__ - Global step 750 Train loss 1.92 Classification-F1 0.16272965879265092 on epoch=62
05/25/2022 13:19:07 - INFO - __main__ - Step 760 Global step 760 Train loss 1.51 on epoch=63
05/25/2022 13:19:10 - INFO - __main__ - Step 770 Global step 770 Train loss 1.54 on epoch=64
05/25/2022 13:19:13 - INFO - __main__ - Step 780 Global step 780 Train loss 1.46 on epoch=64
05/25/2022 13:19:15 - INFO - __main__ - Step 790 Global step 790 Train loss 1.16 on epoch=65
05/25/2022 13:19:18 - INFO - __main__ - Step 800 Global step 800 Train loss 1.34 on epoch=66
05/25/2022 13:19:23 - INFO - __main__ - Global step 800 Train loss 1.40 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 13:19:26 - INFO - __main__ - Step 810 Global step 810 Train loss 1.36 on epoch=67
05/25/2022 13:19:28 - INFO - __main__ - Step 820 Global step 820 Train loss 1.31 on epoch=68
05/25/2022 13:19:31 - INFO - __main__ - Step 830 Global step 830 Train loss 1.29 on epoch=69
05/25/2022 13:19:34 - INFO - __main__ - Step 840 Global step 840 Train loss 1.24 on epoch=69
05/25/2022 13:19:36 - INFO - __main__ - Step 850 Global step 850 Train loss 1.12 on epoch=70
05/25/2022 13:19:41 - INFO - __main__ - Global step 850 Train loss 1.26 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 13:19:43 - INFO - __main__ - Step 860 Global step 860 Train loss 1.06 on epoch=71
05/25/2022 13:19:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.98 on epoch=72
05/25/2022 13:19:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.96 on epoch=73
05/25/2022 13:19:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.90 on epoch=74
05/25/2022 13:19:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.93 on epoch=74
05/25/2022 13:19:58 - INFO - __main__ - Global step 900 Train loss 0.96 Classification-F1 0.16666666666666666 on epoch=74
05/25/2022 13:20:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=75
05/25/2022 13:20:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.94 on epoch=76
05/25/2022 13:20:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.76 on epoch=77
05/25/2022 13:20:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.86 on epoch=78
05/25/2022 13:20:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.83 on epoch=79
05/25/2022 13:20:16 - INFO - __main__ - Global step 950 Train loss 0.85 Classification-F1 0.16666666666666666 on epoch=79
05/25/2022 13:20:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.81 on epoch=79
05/25/2022 13:20:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.80 on epoch=80
05/25/2022 13:20:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.83 on epoch=81
05/25/2022 13:20:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.71 on epoch=82
05/25/2022 13:20:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=83
05/25/2022 13:20:34 - INFO - __main__ - Global step 1000 Train loss 0.79 Classification-F1 0.16666666666666666 on epoch=83
05/25/2022 13:20:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.72 on epoch=84
05/25/2022 13:20:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.72 on epoch=84
05/25/2022 13:20:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.72 on epoch=85
05/25/2022 13:20:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.75 on epoch=86
05/25/2022 13:20:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.65 on epoch=87
05/25/2022 13:20:52 - INFO - __main__ - Global step 1050 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 13:20:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.61 on epoch=88
05/25/2022 13:20:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=89
05/25/2022 13:21:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=89
05/25/2022 13:21:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.73 on epoch=90
05/25/2022 13:21:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.76 on epoch=91
05/25/2022 13:21:10 - INFO - __main__ - Global step 1100 Train loss 0.68 Classification-F1 0.1775766716943188 on epoch=91
05/25/2022 13:21:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.68 on epoch=92
05/25/2022 13:21:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.76 on epoch=93
05/25/2022 13:21:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.65 on epoch=94
05/25/2022 13:21:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.64 on epoch=94
05/25/2022 13:21:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=95
05/25/2022 13:21:28 - INFO - __main__ - Global step 1150 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=95
05/25/2022 13:21:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=96
05/25/2022 13:21:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.63 on epoch=97
05/25/2022 13:21:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.57 on epoch=98
05/25/2022 13:21:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.60 on epoch=99
05/25/2022 13:21:41 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.61 on epoch=99
05/25/2022 13:21:47 - INFO - __main__ - Global step 1200 Train loss 0.62 Classification-F1 0.24852309344790546 on epoch=99
05/25/2022 13:21:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.54 on epoch=100
05/25/2022 13:21:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.55 on epoch=101
05/25/2022 13:21:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=102
05/25/2022 13:21:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.59 on epoch=103
05/25/2022 13:22:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.53 on epoch=104
05/25/2022 13:22:05 - INFO - __main__ - Global step 1250 Train loss 0.55 Classification-F1 0.3364455857959417 on epoch=104
05/25/2022 13:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3071355879789716 -> 0.3364455857959417 on epoch=104, global_step=1250
05/25/2022 13:22:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=104
05/25/2022 13:22:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=105
05/25/2022 13:22:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.56 on epoch=106
05/25/2022 13:22:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=107
05/25/2022 13:22:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.55 on epoch=108
05/25/2022 13:22:24 - INFO - __main__ - Global step 1300 Train loss 0.53 Classification-F1 0.26626011648135545 on epoch=108
05/25/2022 13:22:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.57 on epoch=109
05/25/2022 13:22:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.58 on epoch=109
05/25/2022 13:22:32 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=110
05/25/2022 13:22:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=111
05/25/2022 13:22:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=112
05/25/2022 13:22:43 - INFO - __main__ - Global step 1350 Train loss 0.55 Classification-F1 0.20509209100758397 on epoch=112
05/25/2022 13:22:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.52 on epoch=113
05/25/2022 13:22:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.51 on epoch=114
05/25/2022 13:22:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.51 on epoch=114
05/25/2022 13:22:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=115
05/25/2022 13:22:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=116
05/25/2022 13:23:02 - INFO - __main__ - Global step 1400 Train loss 0.52 Classification-F1 0.300834050738823 on epoch=116
05/25/2022 13:23:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=117
05/25/2022 13:23:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=118
05/25/2022 13:23:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.54 on epoch=119
05/25/2022 13:23:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.50 on epoch=119
05/25/2022 13:23:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=120
05/25/2022 13:23:20 - INFO - __main__ - Global step 1450 Train loss 0.51 Classification-F1 0.29988940905971473 on epoch=120
05/25/2022 13:23:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=121
05/25/2022 13:23:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=122
05/25/2022 13:23:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=123
05/25/2022 13:23:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.53 on epoch=124
05/25/2022 13:23:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.54 on epoch=124
05/25/2022 13:23:39 - INFO - __main__ - Global step 1500 Train loss 0.51 Classification-F1 0.2613756613756614 on epoch=124
05/25/2022 13:23:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.61 on epoch=125
05/25/2022 13:23:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.56 on epoch=126
05/25/2022 13:23:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.54 on epoch=127
05/25/2022 13:23:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=128
05/25/2022 13:23:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=129
05/25/2022 13:23:58 - INFO - __main__ - Global step 1550 Train loss 0.55 Classification-F1 0.2440327289173965 on epoch=129
05/25/2022 13:24:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.53 on epoch=129
05/25/2022 13:24:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=130
05/25/2022 13:24:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=131
05/25/2022 13:24:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.59 on epoch=132
05/25/2022 13:24:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.55 on epoch=133
05/25/2022 13:24:16 - INFO - __main__ - Global step 1600 Train loss 0.53 Classification-F1 0.22207573427085622 on epoch=133
05/25/2022 13:24:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.52 on epoch=134
05/25/2022 13:24:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=134
05/25/2022 13:24:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=135
05/25/2022 13:24:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.57 on epoch=136
05/25/2022 13:24:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=137
05/25/2022 13:24:35 - INFO - __main__ - Global step 1650 Train loss 0.53 Classification-F1 0.2343907682857268 on epoch=137
05/25/2022 13:24:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=138
05/25/2022 13:24:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=139
05/25/2022 13:24:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=139
05/25/2022 13:24:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.54 on epoch=140
05/25/2022 13:24:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=141
05/25/2022 13:24:54 - INFO - __main__ - Global step 1700 Train loss 0.52 Classification-F1 0.21872450693411394 on epoch=141
05/25/2022 13:24:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=142
05/25/2022 13:24:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.52 on epoch=143
05/25/2022 13:25:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.56 on epoch=144
05/25/2022 13:25:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=144
05/25/2022 13:25:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=145
05/25/2022 13:25:12 - INFO - __main__ - Global step 1750 Train loss 0.51 Classification-F1 0.26717910112656335 on epoch=145
05/25/2022 13:25:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=146
05/25/2022 13:25:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.47 on epoch=147
05/25/2022 13:25:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.55 on epoch=148
05/25/2022 13:25:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.52 on epoch=149
05/25/2022 13:25:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=149
05/25/2022 13:25:31 - INFO - __main__ - Global step 1800 Train loss 0.50 Classification-F1 0.22830860739357473 on epoch=149
05/25/2022 13:25:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=150
05/25/2022 13:25:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=151
05/25/2022 13:25:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=152
05/25/2022 13:25:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=153
05/25/2022 13:25:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.53 on epoch=154
05/25/2022 13:25:49 - INFO - __main__ - Global step 1850 Train loss 0.50 Classification-F1 0.2164846366681229 on epoch=154
05/25/2022 13:25:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=154
05/25/2022 13:25:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=155
05/25/2022 13:25:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=156
05/25/2022 13:26:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.49 on epoch=157
05/25/2022 13:26:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.58 on epoch=158
05/25/2022 13:26:08 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.2365374089512021 on epoch=158
05/25/2022 13:26:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=159
05/25/2022 13:26:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.51 on epoch=159
05/25/2022 13:26:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.46 on epoch=160
05/25/2022 13:26:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=161
05/25/2022 13:26:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=162
05/25/2022 13:26:27 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.24925626926740438 on epoch=162
05/25/2022 13:26:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=163
05/25/2022 13:26:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=164
05/25/2022 13:26:35 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.48 on epoch=164
05/25/2022 13:26:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=165
05/25/2022 13:26:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.53 on epoch=166
05/25/2022 13:26:46 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.27245684598625775 on epoch=166
05/25/2022 13:26:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.51 on epoch=167
05/25/2022 13:26:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=168
05/25/2022 13:26:54 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=169
05/25/2022 13:26:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.50 on epoch=169
05/25/2022 13:26:59 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=170
05/25/2022 13:27:04 - INFO - __main__ - Global step 2050 Train loss 0.49 Classification-F1 0.2370578322080913 on epoch=170
05/25/2022 13:27:07 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.51 on epoch=171
05/25/2022 13:27:10 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=172
05/25/2022 13:27:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=173
05/25/2022 13:27:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
05/25/2022 13:27:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.49 on epoch=174
05/25/2022 13:27:23 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.25187762806164554 on epoch=174
05/25/2022 13:27:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
05/25/2022 13:27:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.51 on epoch=176
05/25/2022 13:27:31 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=177
05/25/2022 13:27:34 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=178
05/25/2022 13:27:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.48 on epoch=179
05/25/2022 13:27:42 - INFO - __main__ - Global step 2150 Train loss 0.47 Classification-F1 0.2598851517637408 on epoch=179
05/25/2022 13:27:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.49 on epoch=179
05/25/2022 13:27:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.42 on epoch=180
05/25/2022 13:27:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.50 on epoch=181
05/25/2022 13:27:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
05/25/2022 13:27:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=183
05/25/2022 13:28:01 - INFO - __main__ - Global step 2200 Train loss 0.45 Classification-F1 0.25973798691068023 on epoch=183
05/25/2022 13:28:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.50 on epoch=184
05/25/2022 13:28:06 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=184
05/25/2022 13:28:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=185
05/25/2022 13:28:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.48 on epoch=186
05/25/2022 13:28:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.45 on epoch=187
05/25/2022 13:28:20 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.3332704333999943 on epoch=187
05/25/2022 13:28:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=188
05/25/2022 13:28:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=189
05/25/2022 13:28:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
05/25/2022 13:28:31 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=190
05/25/2022 13:28:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=191
05/25/2022 13:28:39 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.23908880033307559 on epoch=191
05/25/2022 13:28:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.45 on epoch=192
05/25/2022 13:28:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=193
05/25/2022 13:28:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=194
05/25/2022 13:28:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=194
05/25/2022 13:28:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.48 on epoch=195
05/25/2022 13:28:58 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.16535433070866143 on epoch=195
05/25/2022 13:29:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.48 on epoch=196
05/25/2022 13:29:03 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
05/25/2022 13:29:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=198
05/25/2022 13:29:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.50 on epoch=199
05/25/2022 13:29:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.52 on epoch=199
05/25/2022 13:29:16 - INFO - __main__ - Global step 2400 Train loss 0.48 Classification-F1 0.2815475932227201 on epoch=199
05/25/2022 13:29:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.50 on epoch=200
05/25/2022 13:29:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=201
05/25/2022 13:29:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=202
05/25/2022 13:29:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=203
05/25/2022 13:29:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.46 on epoch=204
05/25/2022 13:29:35 - INFO - __main__ - Global step 2450 Train loss 0.45 Classification-F1 0.19827586206896552 on epoch=204
05/25/2022 13:29:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.47 on epoch=204
05/25/2022 13:29:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=205
05/25/2022 13:29:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.47 on epoch=206
05/25/2022 13:29:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.46 on epoch=207
05/25/2022 13:29:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.48 on epoch=208
05/25/2022 13:29:54 - INFO - __main__ - Global step 2500 Train loss 0.47 Classification-F1 0.33471805554304296 on epoch=208
05/25/2022 13:29:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=209
05/25/2022 13:29:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=209
05/25/2022 13:30:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=210
05/25/2022 13:30:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.50 on epoch=211
05/25/2022 13:30:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.47 on epoch=212
05/25/2022 13:30:13 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.25664786462530215 on epoch=212
05/25/2022 13:30:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.46 on epoch=213
05/25/2022 13:30:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.47 on epoch=214
05/25/2022 13:30:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
05/25/2022 13:30:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=215
05/25/2022 13:30:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.48 on epoch=216
05/25/2022 13:30:32 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.27713180110373004 on epoch=216
05/25/2022 13:30:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=217
05/25/2022 13:30:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=218
05/25/2022 13:30:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.47 on epoch=219
05/25/2022 13:30:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.49 on epoch=219
05/25/2022 13:30:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=220
05/25/2022 13:30:50 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.23993465027947788 on epoch=220
05/25/2022 13:30:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.51 on epoch=221
05/25/2022 13:30:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=222
05/25/2022 13:30:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=223
05/25/2022 13:31:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.47 on epoch=224
05/25/2022 13:31:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=224
05/25/2022 13:31:09 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.27894331248481125 on epoch=224
05/25/2022 13:31:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=225
05/25/2022 13:31:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.49 on epoch=226
05/25/2022 13:31:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.41 on epoch=227
05/25/2022 13:31:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.48 on epoch=228
05/25/2022 13:31:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.48 on epoch=229
05/25/2022 13:31:28 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.3065040650406504 on epoch=229
05/25/2022 13:31:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=229
05/25/2022 13:31:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=230
05/25/2022 13:31:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.47 on epoch=231
05/25/2022 13:31:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=232
05/25/2022 13:31:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.49 on epoch=233
05/25/2022 13:31:47 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.35574597224081755 on epoch=233
05/25/2022 13:31:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3364455857959417 -> 0.35574597224081755 on epoch=233, global_step=2800
05/25/2022 13:31:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.43 on epoch=234
05/25/2022 13:31:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.48 on epoch=234
05/25/2022 13:31:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.46 on epoch=235
05/25/2022 13:31:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.44 on epoch=236
05/25/2022 13:32:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.45 on epoch=237
05/25/2022 13:32:07 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.2985875435400888 on epoch=237
05/25/2022 13:32:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
05/25/2022 13:32:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.43 on epoch=239
05/25/2022 13:32:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.50 on epoch=239
05/25/2022 13:32:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.44 on epoch=240
05/25/2022 13:32:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.51 on epoch=241
05/25/2022 13:32:26 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.2733333333333333 on epoch=241
05/25/2022 13:32:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.47 on epoch=242
05/25/2022 13:32:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=243
05/25/2022 13:32:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=244
05/25/2022 13:32:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.41 on epoch=244
05/25/2022 13:32:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.46 on epoch=245
05/25/2022 13:32:45 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.27726574500768053 on epoch=245
05/25/2022 13:32:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
05/25/2022 13:32:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.46 on epoch=247
05/25/2022 13:32:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=248
05/25/2022 13:32:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.47 on epoch=249
05/25/2022 13:32:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.47 on epoch=249
05/25/2022 13:32:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:32:59 - INFO - __main__ - Printing 3 examples
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:32:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:32:59 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:32:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:32:59 - INFO - __main__ - Printing 3 examples
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:32:59 - INFO - __main__ - ['contradiction']
05/25/2022 13:32:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:32:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:33:00 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:33:04 - INFO - __main__ - Global step 3000 Train loss 0.46 Classification-F1 0.2836683941085967 on epoch=249
05/25/2022 13:33:04 - INFO - __main__ - save last model!
05/25/2022 13:33:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 13:33:04 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 13:33:04 - INFO - __main__ - Printing 3 examples
05/25/2022 13:33:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 13:33:04 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 13:33:04 - INFO - __main__ - ['entailment']
05/25/2022 13:33:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 13:33:04 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:33:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:33:05 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 13:33:14 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:33:14 - INFO - __main__ - task name: anli
05/25/2022 13:33:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:33:15 - INFO - __main__ - Starting training!
05/25/2022 13:33:36 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_13_0.5_8_predictions.txt
05/25/2022 13:33:36 - INFO - __main__ - Classification-F1 on test data: 0.2553
05/25/2022 13:33:36 - INFO - __main__ - prefix=anli_64_13, lr=0.5, bsz=8, dev_performance=0.35574597224081755, test_performance=0.2552800613262502
05/25/2022 13:33:36 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.4, bsz=8 ...
05/25/2022 13:33:37 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:33:37 - INFO - __main__ - Printing 3 examples
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:33:37 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:33:37 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:33:37 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:33:37 - INFO - __main__ - Printing 3 examples
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:33:37 - INFO - __main__ - ['contradiction']
05/25/2022 13:33:37 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:33:38 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:33:38 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:33:53 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:33:53 - INFO - __main__ - task name: anli
05/25/2022 13:33:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:33:53 - INFO - __main__ - Starting training!
05/25/2022 13:33:56 - INFO - __main__ - Step 10 Global step 10 Train loss 6.09 on epoch=0
05/25/2022 13:33:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.61 on epoch=1
05/25/2022 13:34:02 - INFO - __main__ - Step 30 Global step 30 Train loss 1.02 on epoch=2
05/25/2022 13:34:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.76 on epoch=3
05/25/2022 13:34:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.66 on epoch=4
05/25/2022 13:34:13 - INFO - __main__ - Global step 50 Train loss 2.23 Classification-F1 0.2739654610622353 on epoch=4
05/25/2022 13:34:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2739654610622353 on epoch=4, global_step=50
05/25/2022 13:34:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=4
05/25/2022 13:34:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=5
05/25/2022 13:34:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=6
05/25/2022 13:34:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=7
05/25/2022 13:34:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=8
05/25/2022 13:34:30 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.21945881631843667 on epoch=8
05/25/2022 13:34:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=9
05/25/2022 13:34:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=9
05/25/2022 13:34:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=10
05/25/2022 13:34:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=11
05/25/2022 13:34:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.58 on epoch=12
05/25/2022 13:34:49 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.26515691398104896 on epoch=12
05/25/2022 13:34:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=13
05/25/2022 13:34:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=14
05/25/2022 13:34:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/25/2022 13:34:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=15
05/25/2022 13:35:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/25/2022 13:35:06 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 13:35:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=17
05/25/2022 13:35:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.55 on epoch=18
05/25/2022 13:35:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/25/2022 13:35:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
05/25/2022 13:35:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=20
05/25/2022 13:35:25 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.2498758072528565 on epoch=20
05/25/2022 13:35:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/25/2022 13:35:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=22
05/25/2022 13:35:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=23
05/25/2022 13:35:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=24
05/25/2022 13:35:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=24
05/25/2022 13:35:44 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.233202614379085 on epoch=24
05/25/2022 13:35:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
05/25/2022 13:35:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=26
05/25/2022 13:35:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/25/2022 13:35:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=28
05/25/2022 13:35:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=29
05/25/2022 13:36:03 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.29206453416979733 on epoch=29
05/25/2022 13:36:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2739654610622353 -> 0.29206453416979733 on epoch=29, global_step=350
05/25/2022 13:36:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
05/25/2022 13:36:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=30
05/25/2022 13:36:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=31
05/25/2022 13:36:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
05/25/2022 13:36:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
05/25/2022 13:36:23 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.3510775706144961 on epoch=33
05/25/2022 13:36:23 - INFO - __main__ - Saving model with best Classification-F1: 0.29206453416979733 -> 0.3510775706144961 on epoch=33, global_step=400
05/25/2022 13:36:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=34
05/25/2022 13:36:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=34
05/25/2022 13:36:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=35
05/25/2022 13:36:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
05/25/2022 13:36:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=37
05/25/2022 13:36:42 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 13:36:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/25/2022 13:36:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/25/2022 13:36:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
05/25/2022 13:36:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=40
05/25/2022 13:36:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=41
05/25/2022 13:37:01 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 13:37:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
05/25/2022 13:37:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=43
05/25/2022 13:37:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
05/25/2022 13:37:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=44
05/25/2022 13:37:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=45
05/25/2022 13:37:19 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 13:37:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/25/2022 13:37:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=47
05/25/2022 13:37:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=48
05/25/2022 13:37:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
05/25/2022 13:37:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=49
05/25/2022 13:37:38 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.2250127269641948 on epoch=49
05/25/2022 13:37:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
05/25/2022 13:37:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
05/25/2022 13:37:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
05/25/2022 13:37:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
05/25/2022 13:37:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=54
05/25/2022 13:37:58 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.1975800817539948 on epoch=54
05/25/2022 13:38:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
05/25/2022 13:38:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/25/2022 13:38:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
05/25/2022 13:38:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=57
05/25/2022 13:38:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
05/25/2022 13:38:17 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.295522110739502 on epoch=58
05/25/2022 13:38:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
05/25/2022 13:38:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
05/25/2022 13:38:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=60
05/25/2022 13:38:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 13:38:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
05/25/2022 13:38:36 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.2682721233453001 on epoch=62
05/25/2022 13:38:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
05/25/2022 13:38:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=64
05/25/2022 13:38:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=64
05/25/2022 13:38:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/25/2022 13:38:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
05/25/2022 13:38:55 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 13:38:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=67
05/25/2022 13:39:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=68
05/25/2022 13:39:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=69
05/25/2022 13:39:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=69
05/25/2022 13:39:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
05/25/2022 13:39:14 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 13:39:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
05/25/2022 13:39:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=72
05/25/2022 13:39:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=73
05/25/2022 13:39:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
05/25/2022 13:39:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=74
05/25/2022 13:39:33 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.2809556134991655 on epoch=74
05/25/2022 13:39:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=75
05/25/2022 13:39:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=76
05/25/2022 13:39:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=77
05/25/2022 13:39:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=78
05/25/2022 13:39:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=79
05/25/2022 13:39:53 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.1794219130041292 on epoch=79
05/25/2022 13:39:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=79
05/25/2022 13:39:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
05/25/2022 13:40:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=81
05/25/2022 13:40:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=82
05/25/2022 13:40:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=83
05/25/2022 13:40:12 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.21294567943134038 on epoch=83
05/25/2022 13:40:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=84
05/25/2022 13:40:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
05/25/2022 13:40:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
05/25/2022 13:40:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
05/25/2022 13:40:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
05/25/2022 13:40:31 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.2111111111111111 on epoch=87
05/25/2022 13:40:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=88
05/25/2022 13:40:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
05/25/2022 13:40:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
05/25/2022 13:40:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=90
05/25/2022 13:40:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=91
05/25/2022 13:40:50 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 13:40:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=92
05/25/2022 13:40:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
05/25/2022 13:40:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=94
05/25/2022 13:41:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
05/25/2022 13:41:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=95
05/25/2022 13:41:09 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.1881810228266921 on epoch=95
05/25/2022 13:41:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=96
05/25/2022 13:41:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=97
05/25/2022 13:41:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=98
05/25/2022 13:41:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=99
05/25/2022 13:41:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
05/25/2022 13:41:28 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.2551095958813299 on epoch=99
05/25/2022 13:41:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=100
05/25/2022 13:41:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
05/25/2022 13:41:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=102
05/25/2022 13:41:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
05/25/2022 13:41:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
05/25/2022 13:41:47 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.30416980496132445 on epoch=104
05/25/2022 13:41:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=104
05/25/2022 13:41:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=105
05/25/2022 13:41:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
05/25/2022 13:41:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
05/25/2022 13:42:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=108
05/25/2022 13:42:06 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.26991596638655463 on epoch=108
05/25/2022 13:42:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
05/25/2022 13:42:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=109
05/25/2022 13:42:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=110
05/25/2022 13:42:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=111
05/25/2022 13:42:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=112
05/25/2022 13:42:25 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.2435197203329699 on epoch=112
05/25/2022 13:42:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/25/2022 13:42:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=114
05/25/2022 13:42:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=114
05/25/2022 13:42:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=115
05/25/2022 13:42:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
05/25/2022 13:42:44 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.17806437015668306 on epoch=116
05/25/2022 13:42:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=117
05/25/2022 13:42:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
05/25/2022 13:42:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
05/25/2022 13:42:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=119
05/25/2022 13:42:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.59 on epoch=120
05/25/2022 13:43:03 - INFO - __main__ - Global step 1450 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=120
05/25/2022 13:43:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
05/25/2022 13:43:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=122
05/25/2022 13:43:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
05/25/2022 13:43:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=124
05/25/2022 13:43:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=124
05/25/2022 13:43:22 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.2087619047619048 on epoch=124
05/25/2022 13:43:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=125
05/25/2022 13:43:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=126
05/25/2022 13:43:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=127
05/25/2022 13:43:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=128
05/25/2022 13:43:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=129
05/25/2022 13:43:41 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.32576786198225116 on epoch=129
05/25/2022 13:43:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=129
05/25/2022 13:43:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=130
05/25/2022 13:43:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
05/25/2022 13:43:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=132
05/25/2022 13:43:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=133
05/25/2022 13:44:01 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.3024943310657597 on epoch=133
05/25/2022 13:44:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=134
05/25/2022 13:44:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
05/25/2022 13:44:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=135
05/25/2022 13:44:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
05/25/2022 13:44:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=137
05/25/2022 13:44:20 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.25626898354171085 on epoch=137
05/25/2022 13:44:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=138
05/25/2022 13:44:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=139
05/25/2022 13:44:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
05/25/2022 13:44:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.40 on epoch=140
05/25/2022 13:44:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
05/25/2022 13:44:39 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.33023027335823546 on epoch=141
05/25/2022 13:44:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
05/25/2022 13:44:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
05/25/2022 13:44:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=144
05/25/2022 13:44:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
05/25/2022 13:44:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
05/25/2022 13:44:58 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.2810383403931791 on epoch=145
05/25/2022 13:45:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=146
05/25/2022 13:45:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=147
05/25/2022 13:45:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
05/25/2022 13:45:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
05/25/2022 13:45:11 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
05/25/2022 13:45:17 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.26029924164787865 on epoch=149
05/25/2022 13:45:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=150
05/25/2022 13:45:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
05/25/2022 13:45:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=152
05/25/2022 13:45:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 13:45:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=154
05/25/2022 13:45:36 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.26311131790583847 on epoch=154
05/25/2022 13:45:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
05/25/2022 13:45:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=155
05/25/2022 13:45:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
05/25/2022 13:45:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=157
05/25/2022 13:45:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=158
05/25/2022 13:45:55 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.27958640458640455 on epoch=158
05/25/2022 13:45:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=159
05/25/2022 13:46:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=159
05/25/2022 13:46:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=160
05/25/2022 13:46:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
05/25/2022 13:46:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
05/25/2022 13:46:14 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.25218547956257703 on epoch=162
05/25/2022 13:46:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
05/25/2022 13:46:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=164
05/25/2022 13:46:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=164
05/25/2022 13:46:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=165
05/25/2022 13:46:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=166
05/25/2022 13:46:33 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.1785932000078658 on epoch=166
05/25/2022 13:46:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=167
05/25/2022 13:46:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
05/25/2022 13:46:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
05/25/2022 13:46:44 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=169
05/25/2022 13:46:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=170
05/25/2022 13:46:53 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.3662231012734933 on epoch=170
05/25/2022 13:46:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3510775706144961 -> 0.3662231012734933 on epoch=170, global_step=2050
05/25/2022 13:46:55 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=171
05/25/2022 13:46:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.39 on epoch=172
05/25/2022 13:47:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=173
05/25/2022 13:47:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=174
05/25/2022 13:47:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=174
05/25/2022 13:47:12 - INFO - __main__ - Global step 2100 Train loss 0.39 Classification-F1 0.37605411130912536 on epoch=174
05/25/2022 13:47:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3662231012734933 -> 0.37605411130912536 on epoch=174, global_step=2100
05/25/2022 13:47:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
05/25/2022 13:47:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.35 on epoch=176
05/25/2022 13:47:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=177
05/25/2022 13:47:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.37 on epoch=178
05/25/2022 13:47:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.35 on epoch=179
05/25/2022 13:47:31 - INFO - __main__ - Global step 2150 Train loss 0.37 Classification-F1 0.30433469844633204 on epoch=179
05/25/2022 13:47:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.33 on epoch=179
05/25/2022 13:47:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
05/25/2022 13:47:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=181
05/25/2022 13:47:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
05/25/2022 13:47:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.45 on epoch=183
05/25/2022 13:47:50 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.3852258852258852 on epoch=183
05/25/2022 13:47:50 - INFO - __main__ - Saving model with best Classification-F1: 0.37605411130912536 -> 0.3852258852258852 on epoch=183, global_step=2200
05/25/2022 13:47:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.35 on epoch=184
05/25/2022 13:47:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
05/25/2022 13:47:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=185
05/25/2022 13:48:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.38 on epoch=186
05/25/2022 13:48:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=187
05/25/2022 13:48:09 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.4441886458802529 on epoch=187
05/25/2022 13:48:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3852258852258852 -> 0.4441886458802529 on epoch=187, global_step=2250
05/25/2022 13:48:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=188
05/25/2022 13:48:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=189
05/25/2022 13:48:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.36 on epoch=189
05/25/2022 13:48:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.34 on epoch=190
05/25/2022 13:48:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
05/25/2022 13:48:29 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3528868964944714 on epoch=191
05/25/2022 13:48:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=192
05/25/2022 13:48:34 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
05/25/2022 13:48:37 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=194
05/25/2022 13:48:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=194
05/25/2022 13:48:42 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.33 on epoch=195
05/25/2022 13:48:48 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.3284887932860025 on epoch=195
05/25/2022 13:48:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=196
05/25/2022 13:48:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=197
05/25/2022 13:48:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=198
05/25/2022 13:48:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=199
05/25/2022 13:49:01 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=199
05/25/2022 13:49:07 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.49106562703053935 on epoch=199
05/25/2022 13:49:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4441886458802529 -> 0.49106562703053935 on epoch=199, global_step=2400
05/25/2022 13:49:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
05/25/2022 13:49:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=201
05/25/2022 13:49:15 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.34 on epoch=202
05/25/2022 13:49:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.35 on epoch=203
05/25/2022 13:49:20 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=204
05/25/2022 13:49:32 - INFO - __main__ - Global step 2450 Train loss 0.35 Classification-F1 0.25501346479607345 on epoch=204
05/25/2022 13:49:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.35 on epoch=204
05/25/2022 13:49:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=205
05/25/2022 13:49:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.32 on epoch=206
05/25/2022 13:49:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.35 on epoch=207
05/25/2022 13:49:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=208
05/25/2022 13:49:52 - INFO - __main__ - Global step 2500 Train loss 0.34 Classification-F1 0.43137386189829124 on epoch=208
05/25/2022 13:49:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.33 on epoch=209
05/25/2022 13:49:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
05/25/2022 13:50:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=210
05/25/2022 13:50:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=211
05/25/2022 13:50:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=212
05/25/2022 13:50:12 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.4223824179613911 on epoch=212
05/25/2022 13:50:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=213
05/25/2022 13:50:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.33 on epoch=214
05/25/2022 13:50:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=214
05/25/2022 13:50:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=215
05/25/2022 13:50:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=216
05/25/2022 13:50:31 - INFO - __main__ - Global step 2600 Train loss 0.33 Classification-F1 0.4357818665905957 on epoch=216
05/25/2022 13:50:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.32 on epoch=217
05/25/2022 13:50:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=218
05/25/2022 13:50:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.31 on epoch=219
05/25/2022 13:50:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.33 on epoch=219
05/25/2022 13:50:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=220
05/25/2022 13:50:50 - INFO - __main__ - Global step 2650 Train loss 0.33 Classification-F1 0.2800352562422584 on epoch=220
05/25/2022 13:50:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=221
05/25/2022 13:50:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.32 on epoch=222
05/25/2022 13:50:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.30 on epoch=223
05/25/2022 13:51:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.27 on epoch=224
05/25/2022 13:51:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.33 on epoch=224
05/25/2022 13:51:09 - INFO - __main__ - Global step 2700 Train loss 0.31 Classification-F1 0.39864408788733874 on epoch=224
05/25/2022 13:51:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.34 on epoch=225
05/25/2022 13:51:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.29 on epoch=226
05/25/2022 13:51:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.32 on epoch=227
05/25/2022 13:51:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=228
05/25/2022 13:51:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=229
05/25/2022 13:51:29 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.25892386805468454 on epoch=229
05/25/2022 13:51:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=229
05/25/2022 13:51:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.33 on epoch=230
05/25/2022 13:51:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=231
05/25/2022 13:51:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=232
05/25/2022 13:51:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=233
05/25/2022 13:51:49 - INFO - __main__ - Global step 2800 Train loss 0.31 Classification-F1 0.2788319080201809 on epoch=233
05/25/2022 13:51:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.25 on epoch=234
05/25/2022 13:51:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=234
05/25/2022 13:51:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.33 on epoch=235
05/25/2022 13:52:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.31 on epoch=236
05/25/2022 13:52:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=237
05/25/2022 13:52:10 - INFO - __main__ - Global step 2850 Train loss 0.29 Classification-F1 0.4455146819967122 on epoch=237
05/25/2022 13:52:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=238
05/25/2022 13:52:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=239
05/25/2022 13:52:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=239
05/25/2022 13:52:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.30 on epoch=240
05/25/2022 13:52:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.22 on epoch=241
05/25/2022 13:52:30 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.41495677780944246 on epoch=241
05/25/2022 13:52:33 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=242
05/25/2022 13:52:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=243
05/25/2022 13:52:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=244
05/25/2022 13:52:41 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.23 on epoch=244
05/25/2022 13:52:44 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.26 on epoch=245
05/25/2022 13:52:51 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.4316497141337588 on epoch=245
05/25/2022 13:52:54 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.24 on epoch=246
05/25/2022 13:52:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.25 on epoch=247
05/25/2022 13:52:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=248
05/25/2022 13:53:02 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=249
05/25/2022 13:53:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.28 on epoch=249
05/25/2022 13:53:06 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:53:06 - INFO - __main__ - Printing 3 examples
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:53:06 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:53:06 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:53:06 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:53:06 - INFO - __main__ - Printing 3 examples
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:53:06 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:53:07 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:53:07 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:53:12 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.4125362527018523 on epoch=249
05/25/2022 13:53:12 - INFO - __main__ - save last model!
05/25/2022 13:53:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 13:53:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 13:53:12 - INFO - __main__ - Printing 3 examples
05/25/2022 13:53:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 13:53:12 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 13:53:12 - INFO - __main__ - ['entailment']
05/25/2022 13:53:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 13:53:12 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:12 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:53:13 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:53:14 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 13:53:21 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:53:21 - INFO - __main__ - task name: anli
05/25/2022 13:53:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:53:22 - INFO - __main__ - Starting training!
05/25/2022 13:53:51 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_13_0.4_8_predictions.txt
05/25/2022 13:53:51 - INFO - __main__ - Classification-F1 on test data: 0.3100
05/25/2022 13:53:52 - INFO - __main__ - prefix=anli_64_13, lr=0.4, bsz=8, dev_performance=0.49106562703053935, test_performance=0.3100168067226891
05/25/2022 13:53:52 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.3, bsz=8 ...
05/25/2022 13:53:52 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:53:52 - INFO - __main__ - Printing 3 examples
05/25/2022 13:53:52 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 13:53:52 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:52 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 13:53:52 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:52 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 13:53:52 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:53:53 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:53:53 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 13:53:53 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 13:53:53 - INFO - __main__ - Printing 3 examples
05/25/2022 13:53:53 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 13:53:53 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:53 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 13:53:53 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:53 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 13:53:53 - INFO - __main__ - ['contradiction']
05/25/2022 13:53:53 - INFO - __main__ - Tokenizing Input ...
05/25/2022 13:53:53 - INFO - __main__ - Tokenizing Output ...
05/25/2022 13:53:53 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 13:54:08 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 13:54:08 - INFO - __main__ - task name: anli
05/25/2022 13:54:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 13:54:08 - INFO - __main__ - Starting training!
05/25/2022 13:54:11 - INFO - __main__ - Step 10 Global step 10 Train loss 6.15 on epoch=0
05/25/2022 13:54:14 - INFO - __main__ - Step 20 Global step 20 Train loss 3.06 on epoch=1
05/25/2022 13:54:17 - INFO - __main__ - Step 30 Global step 30 Train loss 1.47 on epoch=2
05/25/2022 13:54:19 - INFO - __main__ - Step 40 Global step 40 Train loss 1.04 on epoch=3
05/25/2022 13:54:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.83 on epoch=4
05/25/2022 13:54:26 - INFO - __main__ - Global step 50 Train loss 2.51 Classification-F1 0.2890726096333573 on epoch=4
05/25/2022 13:54:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2890726096333573 on epoch=4, global_step=50
05/25/2022 13:54:29 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=4
05/25/2022 13:54:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=5
05/25/2022 13:54:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
05/25/2022 13:54:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.61 on epoch=7
05/25/2022 13:54:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=8
05/25/2022 13:54:44 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.24079181222038362 on epoch=8
05/25/2022 13:54:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=9
05/25/2022 13:54:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=9
05/25/2022 13:54:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=10
05/25/2022 13:54:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=11
05/25/2022 13:54:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
05/25/2022 13:55:02 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 13:55:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 13:55:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=14
05/25/2022 13:55:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=14
05/25/2022 13:55:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=15
05/25/2022 13:55:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/25/2022 13:55:21 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16535433070866143 on epoch=16
05/25/2022 13:55:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=17
05/25/2022 13:55:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
05/25/2022 13:55:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=19
05/25/2022 13:55:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
05/25/2022 13:55:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=20
05/25/2022 13:55:39 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 13:55:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/25/2022 13:55:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=22
05/25/2022 13:55:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
05/25/2022 13:55:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=24
05/25/2022 13:55:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=24
05/25/2022 13:55:58 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.30299227607890344 on epoch=24
05/25/2022 13:55:58 - INFO - __main__ - Saving model with best Classification-F1: 0.2890726096333573 -> 0.30299227607890344 on epoch=24, global_step=300
05/25/2022 13:56:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/25/2022 13:56:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/25/2022 13:56:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/25/2022 13:56:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=28
05/25/2022 13:56:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=29
05/25/2022 13:56:17 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.249190785214659 on epoch=29
05/25/2022 13:56:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=29
05/25/2022 13:56:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
05/25/2022 13:56:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/25/2022 13:56:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
05/25/2022 13:56:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 13:56:36 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.28366718854405965 on epoch=33
05/25/2022 13:56:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/25/2022 13:56:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
05/25/2022 13:56:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=35
05/25/2022 13:56:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=36
05/25/2022 13:56:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/25/2022 13:56:55 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.26509803921568625 on epoch=37
05/25/2022 13:56:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=38
05/25/2022 13:57:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=39
05/25/2022 13:57:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=39
05/25/2022 13:57:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=40
05/25/2022 13:57:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/25/2022 13:57:14 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 13:57:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=42
05/25/2022 13:57:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=43
05/25/2022 13:57:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=44
05/25/2022 13:57:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
05/25/2022 13:57:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
05/25/2022 13:57:32 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.2391989378888942 on epoch=45
05/25/2022 13:57:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/25/2022 13:57:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
05/25/2022 13:57:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/25/2022 13:57:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=49
05/25/2022 13:57:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=49
05/25/2022 13:57:51 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.25420875420875416 on epoch=49
05/25/2022 13:57:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=50
05/25/2022 13:57:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=51
05/25/2022 13:57:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
05/25/2022 13:58:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
05/25/2022 13:58:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=54
05/25/2022 13:58:10 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.31333692164495347 on epoch=54
05/25/2022 13:58:10 - INFO - __main__ - Saving model with best Classification-F1: 0.30299227607890344 -> 0.31333692164495347 on epoch=54, global_step=650
05/25/2022 13:58:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=54
05/25/2022 13:58:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/25/2022 13:58:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
05/25/2022 13:58:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
05/25/2022 13:58:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=58
05/25/2022 13:58:29 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.273457072633274 on epoch=58
05/25/2022 13:58:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
05/25/2022 13:58:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=59
05/25/2022 13:58:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=60
05/25/2022 13:58:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
05/25/2022 13:58:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
05/25/2022 13:58:48 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.27170375950863757 on epoch=62
05/25/2022 13:58:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
05/25/2022 13:58:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
05/25/2022 13:58:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
05/25/2022 13:58:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
05/25/2022 13:59:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=66
05/25/2022 13:59:07 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 13:59:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
05/25/2022 13:59:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 13:59:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=69
05/25/2022 13:59:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=69
05/25/2022 13:59:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=70
05/25/2022 13:59:25 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=70
05/25/2022 13:59:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
05/25/2022 13:59:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.43 on epoch=72
05/25/2022 13:59:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/25/2022 13:59:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=74
05/25/2022 13:59:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=74
05/25/2022 13:59:44 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.269317738791423 on epoch=74
05/25/2022 13:59:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=75
05/25/2022 13:59:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=76
05/25/2022 13:59:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/25/2022 13:59:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=78
05/25/2022 13:59:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/25/2022 14:00:02 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.350228958160685 on epoch=79
05/25/2022 14:00:02 - INFO - __main__ - Saving model with best Classification-F1: 0.31333692164495347 -> 0.350228958160685 on epoch=79, global_step=950
05/25/2022 14:00:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=79
05/25/2022 14:00:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=80
05/25/2022 14:00:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=81
05/25/2022 14:00:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
05/25/2022 14:00:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
05/25/2022 14:00:22 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.31629032080316505 on epoch=83
05/25/2022 14:00:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
05/25/2022 14:00:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=84
05/25/2022 14:00:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=85
05/25/2022 14:00:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
05/25/2022 14:00:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
05/25/2022 14:00:40 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.18585994901784375 on epoch=87
05/25/2022 14:00:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/25/2022 14:00:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=89
05/25/2022 14:00:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=89
05/25/2022 14:00:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
05/25/2022 14:00:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=91
05/25/2022 14:00:59 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 14:01:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=92
05/25/2022 14:01:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 14:01:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=94
05/25/2022 14:01:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
05/25/2022 14:01:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=95
05/25/2022 14:01:17 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.1914889605369836 on epoch=95
05/25/2022 14:01:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
05/25/2022 14:01:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=97
05/25/2022 14:01:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
05/25/2022 14:01:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=99
05/25/2022 14:01:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=99
05/25/2022 14:01:36 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.22213595104727044 on epoch=99
05/25/2022 14:01:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=100
05/25/2022 14:01:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=101
05/25/2022 14:01:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
05/25/2022 14:01:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=103
05/25/2022 14:01:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=104
05/25/2022 14:01:53 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.18627450980392157 on epoch=104
05/25/2022 14:01:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/25/2022 14:01:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
05/25/2022 14:02:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
05/25/2022 14:02:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=107
05/25/2022 14:02:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
05/25/2022 14:02:10 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.23325908848162769 on epoch=108
05/25/2022 14:02:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=109
05/25/2022 14:02:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=109
05/25/2022 14:02:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=110
05/25/2022 14:02:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=111
05/25/2022 14:02:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=112
05/25/2022 14:02:28 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 14:02:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
05/25/2022 14:02:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
05/25/2022 14:02:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=114
05/25/2022 14:02:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=115
05/25/2022 14:02:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/25/2022 14:02:45 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 14:02:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
05/25/2022 14:02:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
05/25/2022 14:02:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=119
05/25/2022 14:02:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=119
05/25/2022 14:02:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=120
05/25/2022 14:03:03 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=120
05/25/2022 14:03:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
05/25/2022 14:03:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
05/25/2022 14:03:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=123
05/25/2022 14:03:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=124
05/25/2022 14:03:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=124
05/25/2022 14:03:22 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.22447768281101613 on epoch=124
05/25/2022 14:03:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=125
05/25/2022 14:03:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=126
05/25/2022 14:03:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
05/25/2022 14:03:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=128
05/25/2022 14:03:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=129
05/25/2022 14:03:41 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.23703635214892207 on epoch=129
05/25/2022 14:03:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=129
05/25/2022 14:03:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=130
05/25/2022 14:03:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=131
05/25/2022 14:03:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=132
05/25/2022 14:03:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
05/25/2022 14:04:00 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.2960971839829976 on epoch=133
05/25/2022 14:04:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=134
05/25/2022 14:04:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
05/25/2022 14:04:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
05/25/2022 14:04:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
05/25/2022 14:04:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=137
05/25/2022 14:04:19 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.24309984656055314 on epoch=137
05/25/2022 14:04:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=138
05/25/2022 14:04:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=139
05/25/2022 14:04:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=139
05/25/2022 14:04:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
05/25/2022 14:04:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=141
05/25/2022 14:04:38 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.24051088325238354 on epoch=141
05/25/2022 14:04:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=142
05/25/2022 14:04:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=143
05/25/2022 14:04:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=144
05/25/2022 14:04:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.38 on epoch=144
05/25/2022 14:04:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=145
05/25/2022 14:04:57 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.1859084026507777 on epoch=145
05/25/2022 14:05:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 14:05:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
05/25/2022 14:05:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=148
05/25/2022 14:05:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=149
05/25/2022 14:05:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=149
05/25/2022 14:05:15 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.21502571502571502 on epoch=149
05/25/2022 14:05:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=150
05/25/2022 14:05:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
05/25/2022 14:05:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=152
05/25/2022 14:05:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 14:05:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=154
05/25/2022 14:05:33 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.19849369752030363 on epoch=154
05/25/2022 14:05:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
05/25/2022 14:05:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=155
05/25/2022 14:05:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
05/25/2022 14:05:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=157
05/25/2022 14:05:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
05/25/2022 14:05:52 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.3711821532242285 on epoch=158
05/25/2022 14:05:52 - INFO - __main__ - Saving model with best Classification-F1: 0.350228958160685 -> 0.3711821532242285 on epoch=158, global_step=1900
05/25/2022 14:05:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
05/25/2022 14:05:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=159
05/25/2022 14:06:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=160
05/25/2022 14:06:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=161
05/25/2022 14:06:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
05/25/2022 14:06:11 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.2700065582161652 on epoch=162
05/25/2022 14:06:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=163
05/25/2022 14:06:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=164
05/25/2022 14:06:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
05/25/2022 14:06:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=165
05/25/2022 14:06:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
05/25/2022 14:06:30 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.25109783553096676 on epoch=166
05/25/2022 14:06:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.40 on epoch=167
05/25/2022 14:06:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=168
05/25/2022 14:06:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=169
05/25/2022 14:06:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=169
05/25/2022 14:06:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=170
05/25/2022 14:06:49 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.21085669311244493 on epoch=170
05/25/2022 14:06:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=171
05/25/2022 14:06:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
05/25/2022 14:06:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.39 on epoch=173
05/25/2022 14:07:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.38 on epoch=174
05/25/2022 14:07:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=174
05/25/2022 14:07:08 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.2193755551000888 on epoch=174
05/25/2022 14:07:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=175
05/25/2022 14:07:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=176
05/25/2022 14:07:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=177
05/25/2022 14:07:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=178
05/25/2022 14:07:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=179
05/25/2022 14:07:28 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.2360876091492752 on epoch=179
05/25/2022 14:07:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=179
05/25/2022 14:07:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=180
05/25/2022 14:07:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=181
05/25/2022 14:07:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=182
05/25/2022 14:07:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
05/25/2022 14:07:49 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.2801898990157465 on epoch=183
05/25/2022 14:07:51 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=184
05/25/2022 14:07:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
05/25/2022 14:07:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=185
05/25/2022 14:07:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=186
05/25/2022 14:08:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
05/25/2022 14:08:07 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=187
05/25/2022 14:08:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=188
05/25/2022 14:08:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.37 on epoch=189
05/25/2022 14:08:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=189
05/25/2022 14:08:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=190
05/25/2022 14:08:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=191
05/25/2022 14:08:27 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.2709252654697858 on epoch=191
05/25/2022 14:08:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=192
05/25/2022 14:08:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
05/25/2022 14:08:35 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.39 on epoch=194
05/25/2022 14:08:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=194
05/25/2022 14:08:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.38 on epoch=195
05/25/2022 14:08:45 - INFO - __main__ - Global step 2350 Train loss 0.39 Classification-F1 0.31196581196581197 on epoch=195
05/25/2022 14:08:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=196
05/25/2022 14:08:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
05/25/2022 14:08:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=198
05/25/2022 14:08:56 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=199
05/25/2022 14:08:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.35 on epoch=199
05/25/2022 14:09:04 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.2067699368904188 on epoch=199
05/25/2022 14:09:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=200
05/25/2022 14:09:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.41 on epoch=201
05/25/2022 14:09:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=202
05/25/2022 14:09:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
05/25/2022 14:09:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=204
05/25/2022 14:09:23 - INFO - __main__ - Global step 2450 Train loss 0.39 Classification-F1 0.263267350064805 on epoch=204
05/25/2022 14:09:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=204
05/25/2022 14:09:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=205
05/25/2022 14:09:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=206
05/25/2022 14:09:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=207
05/25/2022 14:09:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
05/25/2022 14:09:42 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.2260048734941131 on epoch=208
05/25/2022 14:09:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
05/25/2022 14:09:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=209
05/25/2022 14:09:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=210
05/25/2022 14:09:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=211
05/25/2022 14:09:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.35 on epoch=212
05/25/2022 14:10:01 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.1892126668246071 on epoch=212
05/25/2022 14:10:03 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 14:10:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=214
05/25/2022 14:10:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.36 on epoch=214
05/25/2022 14:10:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=215
05/25/2022 14:10:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=216
05/25/2022 14:10:19 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.28210678210678214 on epoch=216
05/25/2022 14:10:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
05/25/2022 14:10:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.38 on epoch=218
05/25/2022 14:10:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=219
05/25/2022 14:10:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=219
05/25/2022 14:10:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.35 on epoch=220
05/25/2022 14:10:38 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.30433633853606495 on epoch=220
05/25/2022 14:10:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=221
05/25/2022 14:10:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=222
05/25/2022 14:10:46 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=223
05/25/2022 14:10:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.35 on epoch=224
05/25/2022 14:10:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.32 on epoch=224
05/25/2022 14:11:04 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.3647562471959915 on epoch=224
05/25/2022 14:11:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
05/25/2022 14:11:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.38 on epoch=226
05/25/2022 14:11:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=227
05/25/2022 14:11:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=228
05/25/2022 14:11:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.37 on epoch=229
05/25/2022 14:11:24 - INFO - __main__ - Global step 2750 Train loss 0.39 Classification-F1 0.2690490252709143 on epoch=229
05/25/2022 14:11:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.35 on epoch=229
05/25/2022 14:11:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.34 on epoch=230
05/25/2022 14:11:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=231
05/25/2022 14:11:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=232
05/25/2022 14:11:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=233
05/25/2022 14:11:43 - INFO - __main__ - Global step 2800 Train loss 0.37 Classification-F1 0.2775330231503103 on epoch=233
05/25/2022 14:11:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.34 on epoch=234
05/25/2022 14:11:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.35 on epoch=234
05/25/2022 14:11:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
05/25/2022 14:11:53 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
05/25/2022 14:11:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=237
05/25/2022 14:12:02 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.29876309755787495 on epoch=237
05/25/2022 14:12:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.36 on epoch=238
05/25/2022 14:12:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=239
05/25/2022 14:12:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=239
05/25/2022 14:12:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=240
05/25/2022 14:12:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=241
05/25/2022 14:12:22 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.35833963054473 on epoch=241
05/25/2022 14:12:24 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=242
05/25/2022 14:12:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=243
05/25/2022 14:12:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=244
05/25/2022 14:12:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=244
05/25/2022 14:12:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.34 on epoch=245
05/25/2022 14:12:41 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.33200232490555076 on epoch=245
05/25/2022 14:12:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=246
05/25/2022 14:12:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.37 on epoch=247
05/25/2022 14:12:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.39 on epoch=248
05/25/2022 14:12:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.34 on epoch=249
05/25/2022 14:12:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.33 on epoch=249
05/25/2022 14:12:56 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:12:56 - INFO - __main__ - Printing 3 examples
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:12:56 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:12:56 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:12:56 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:12:56 - INFO - __main__ - Printing 3 examples
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 14:12:56 - INFO - __main__ - ['contradiction']
05/25/2022 14:12:56 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:12:56 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:12:56 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:13:02 - INFO - __main__ - Global step 3000 Train loss 0.36 Classification-F1 0.40983272133822934 on epoch=249
05/25/2022 14:13:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3711821532242285 -> 0.40983272133822934 on epoch=249, global_step=3000
05/25/2022 14:13:02 - INFO - __main__ - save last model!
05/25/2022 14:13:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 14:13:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 14:13:02 - INFO - __main__ - Printing 3 examples
05/25/2022 14:13:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 14:13:02 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 14:13:02 - INFO - __main__ - ['entailment']
05/25/2022 14:13:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 14:13:02 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:02 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:13:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:13:04 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 14:13:15 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:13:15 - INFO - __main__ - task name: anli
05/25/2022 14:13:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:13:16 - INFO - __main__ - Starting training!
05/25/2022 14:13:39 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_13_0.3_8_predictions.txt
05/25/2022 14:13:39 - INFO - __main__ - Classification-F1 on test data: 0.2943
05/25/2022 14:13:40 - INFO - __main__ - prefix=anli_64_13, lr=0.3, bsz=8, dev_performance=0.40983272133822934, test_performance=0.29428982632203354
05/25/2022 14:13:40 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.2, bsz=8 ...
05/25/2022 14:13:40 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:13:40 - INFO - __main__ - Printing 3 examples
05/25/2022 14:13:40 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/25/2022 14:13:40 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:40 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/25/2022 14:13:40 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:40 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/25/2022 14:13:40 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:40 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:13:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:13:41 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:13:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:13:41 - INFO - __main__ - Printing 3 examples
05/25/2022 14:13:41 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/25/2022 14:13:41 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:41 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/25/2022 14:13:41 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:41 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/25/2022 14:13:41 - INFO - __main__ - ['contradiction']
05/25/2022 14:13:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:13:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:13:41 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:13:56 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:13:56 - INFO - __main__ - task name: anli
05/25/2022 14:13:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:13:56 - INFO - __main__ - Starting training!
05/25/2022 14:13:59 - INFO - __main__ - Step 10 Global step 10 Train loss 6.73 on epoch=0
05/25/2022 14:14:02 - INFO - __main__ - Step 20 Global step 20 Train loss 4.83 on epoch=1
05/25/2022 14:14:05 - INFO - __main__ - Step 30 Global step 30 Train loss 2.55 on epoch=2
05/25/2022 14:14:08 - INFO - __main__ - Step 40 Global step 40 Train loss 1.62 on epoch=3
05/25/2022 14:14:10 - INFO - __main__ - Step 50 Global step 50 Train loss 1.17 on epoch=4
05/25/2022 14:14:16 - INFO - __main__ - Global step 50 Train loss 3.38 Classification-F1 0.2089107604125422 on epoch=4
05/25/2022 14:14:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2089107604125422 on epoch=4, global_step=50
05/25/2022 14:14:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=4
05/25/2022 14:14:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=5
05/25/2022 14:14:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.79 on epoch=6
05/25/2022 14:14:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=7
05/25/2022 14:14:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.75 on epoch=8
05/25/2022 14:14:34 - INFO - __main__ - Global step 100 Train loss 0.80 Classification-F1 0.2319178948392432 on epoch=8
05/25/2022 14:14:34 - INFO - __main__ - Saving model with best Classification-F1: 0.2089107604125422 -> 0.2319178948392432 on epoch=8, global_step=100
05/25/2022 14:14:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.73 on epoch=9
05/25/2022 14:14:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.67 on epoch=9
05/25/2022 14:14:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.59 on epoch=10
05/25/2022 14:14:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=11
05/25/2022 14:14:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=12
05/25/2022 14:14:53 - INFO - __main__ - Global step 150 Train loss 0.63 Classification-F1 0.18627450980392157 on epoch=12
05/25/2022 14:14:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.66 on epoch=13
05/25/2022 14:14:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.65 on epoch=14
05/25/2022 14:15:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=14
05/25/2022 14:15:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=15
05/25/2022 14:15:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.57 on epoch=16
05/25/2022 14:15:11 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.21395733250684434 on epoch=16
05/25/2022 14:15:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=17
05/25/2022 14:15:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
05/25/2022 14:15:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
05/25/2022 14:15:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=19
05/25/2022 14:15:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
05/25/2022 14:15:30 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 14:15:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=21
05/25/2022 14:15:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/25/2022 14:15:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=23
05/25/2022 14:15:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=24
05/25/2022 14:15:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=24
05/25/2022 14:15:48 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.18422782303379315 on epoch=24
05/25/2022 14:15:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=25
05/25/2022 14:15:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=26
05/25/2022 14:15:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/25/2022 14:15:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=28
05/25/2022 14:16:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=29
05/25/2022 14:16:07 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.3076400723459547 on epoch=29
05/25/2022 14:16:07 - INFO - __main__ - Saving model with best Classification-F1: 0.2319178948392432 -> 0.3076400723459547 on epoch=29, global_step=350
05/25/2022 14:16:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 14:16:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
05/25/2022 14:16:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=31
05/25/2022 14:16:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=32
05/25/2022 14:16:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=33
05/25/2022 14:16:26 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.2952590924727148 on epoch=33
05/25/2022 14:16:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=34
05/25/2022 14:16:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=34
05/25/2022 14:16:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=35
05/25/2022 14:16:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=36
05/25/2022 14:16:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=37
05/25/2022 14:16:45 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.32806982806982804 on epoch=37
05/25/2022 14:16:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3076400723459547 -> 0.32806982806982804 on epoch=37, global_step=450
05/25/2022 14:16:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=38
05/25/2022 14:16:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=39
05/25/2022 14:16:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
05/25/2022 14:16:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=40
05/25/2022 14:16:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=41
05/25/2022 14:17:03 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.1754553408096715 on epoch=41
05/25/2022 14:17:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=42
05/25/2022 14:17:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
05/25/2022 14:17:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
05/25/2022 14:17:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=44
05/25/2022 14:17:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=45
05/25/2022 14:17:22 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 14:17:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
05/25/2022 14:17:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=47
05/25/2022 14:17:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
05/25/2022 14:17:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=49
05/25/2022 14:17:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=49
05/25/2022 14:17:41 - INFO - __main__ - Global step 600 Train loss 0.48 Classification-F1 0.22492609416684695 on epoch=49
05/25/2022 14:17:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=50
05/25/2022 14:17:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=51
05/25/2022 14:17:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
05/25/2022 14:17:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=53
05/25/2022 14:17:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=54
05/25/2022 14:17:59 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.28569509951383076 on epoch=54
05/25/2022 14:18:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=54
05/25/2022 14:18:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
05/25/2022 14:18:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
05/25/2022 14:18:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=57
05/25/2022 14:18:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=58
05/25/2022 14:18:19 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.30524272870636815 on epoch=58
05/25/2022 14:18:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=59
05/25/2022 14:18:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=59
05/25/2022 14:18:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=60
05/25/2022 14:18:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 14:18:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
05/25/2022 14:18:38 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.25507183596852206 on epoch=62
05/25/2022 14:18:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
05/25/2022 14:18:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=64
05/25/2022 14:18:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
05/25/2022 14:18:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=65
05/25/2022 14:18:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=66
05/25/2022 14:18:57 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.1754553408096715 on epoch=66
05/25/2022 14:18:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
05/25/2022 14:19:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/25/2022 14:19:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=69
05/25/2022 14:19:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=69
05/25/2022 14:19:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=70
05/25/2022 14:19:16 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.2615614270786684 on epoch=70
05/25/2022 14:19:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=71
05/25/2022 14:19:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
05/25/2022 14:19:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
05/25/2022 14:19:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.48 on epoch=74
05/25/2022 14:19:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=74
05/25/2022 14:19:34 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.25160908742998295 on epoch=74
05/25/2022 14:19:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=75
05/25/2022 14:19:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=76
05/25/2022 14:19:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=77
05/25/2022 14:19:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/25/2022 14:19:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=79
05/25/2022 14:19:54 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.311251168451496 on epoch=79
05/25/2022 14:19:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
05/25/2022 14:19:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=80
05/25/2022 14:20:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=81
05/25/2022 14:20:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
05/25/2022 14:20:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
05/25/2022 14:20:13 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.321629997852036 on epoch=83
05/25/2022 14:20:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=84
05/25/2022 14:20:18 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=84
05/25/2022 14:20:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=85
05/25/2022 14:20:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=86
05/25/2022 14:20:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=87
05/25/2022 14:20:32 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.22964915584008036 on epoch=87
05/25/2022 14:20:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=88
05/25/2022 14:20:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
05/25/2022 14:20:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=89
05/25/2022 14:20:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=90
05/25/2022 14:20:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
05/25/2022 14:20:51 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.2332606141970129 on epoch=91
05/25/2022 14:20:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=92
05/25/2022 14:20:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=93
05/25/2022 14:20:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
05/25/2022 14:21:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
05/25/2022 14:21:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
05/25/2022 14:21:09 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.25431486076647364 on epoch=95
05/25/2022 14:21:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=96
05/25/2022 14:21:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
05/25/2022 14:21:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=98
05/25/2022 14:21:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=99
05/25/2022 14:21:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=99
05/25/2022 14:21:28 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.3172240153993662 on epoch=99
05/25/2022 14:21:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=100
05/25/2022 14:21:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=101
05/25/2022 14:21:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=102
05/25/2022 14:21:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/25/2022 14:21:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=104
05/25/2022 14:21:48 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.29439531050315165 on epoch=104
05/25/2022 14:21:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/25/2022 14:21:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=105
05/25/2022 14:21:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=106
05/25/2022 14:21:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=107
05/25/2022 14:22:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
05/25/2022 14:22:06 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.3091878875443983 on epoch=108
05/25/2022 14:22:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
05/25/2022 14:22:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
05/25/2022 14:22:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
05/25/2022 14:22:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=111
05/25/2022 14:22:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
05/25/2022 14:22:24 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.3025353150235059 on epoch=112
05/25/2022 14:22:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/25/2022 14:22:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=114
05/25/2022 14:22:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=114
05/25/2022 14:22:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=115
05/25/2022 14:22:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=116
05/25/2022 14:22:43 - INFO - __main__ - Global step 1400 Train loss 0.44 Classification-F1 0.30451495812320556 on epoch=116
05/25/2022 14:22:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
05/25/2022 14:22:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
05/25/2022 14:22:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=119
05/25/2022 14:22:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=119
05/25/2022 14:22:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=120
05/25/2022 14:23:02 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.1775766716943188 on epoch=120
05/25/2022 14:23:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
05/25/2022 14:23:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=122
05/25/2022 14:23:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=123
05/25/2022 14:23:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=124
05/25/2022 14:23:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.41 on epoch=124
05/25/2022 14:23:20 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.22728410513141428 on epoch=124
05/25/2022 14:23:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=125
05/25/2022 14:23:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=126
05/25/2022 14:23:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=127
05/25/2022 14:23:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
05/25/2022 14:23:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=129
05/25/2022 14:23:38 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.22914723182102859 on epoch=129
05/25/2022 14:23:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=129
05/25/2022 14:23:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=130
05/25/2022 14:23:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
05/25/2022 14:23:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=132
05/25/2022 14:23:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=133
05/25/2022 14:23:56 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=133
05/25/2022 14:23:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=134
05/25/2022 14:24:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=134
05/25/2022 14:24:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=135
05/25/2022 14:24:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=136
05/25/2022 14:24:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
05/25/2022 14:24:14 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.17186088527551943 on epoch=137
05/25/2022 14:24:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.37 on epoch=138
05/25/2022 14:24:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 14:24:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
05/25/2022 14:24:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=140
05/25/2022 14:24:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
05/25/2022 14:24:32 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.20802429893338983 on epoch=141
05/25/2022 14:24:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=142
05/25/2022 14:24:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=143
05/25/2022 14:24:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=144
05/25/2022 14:24:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=144
05/25/2022 14:24:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=145
05/25/2022 14:24:49 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.17980884109916365 on epoch=145
05/25/2022 14:24:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
05/25/2022 14:24:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=147
05/25/2022 14:24:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=148
05/25/2022 14:25:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
05/25/2022 14:25:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=149
05/25/2022 14:25:08 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.26577540106951875 on epoch=149
05/25/2022 14:25:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=150
05/25/2022 14:25:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
05/25/2022 14:25:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=152
05/25/2022 14:25:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=153
05/25/2022 14:25:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
05/25/2022 14:25:27 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.27051767676767674 on epoch=154
05/25/2022 14:25:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=154
05/25/2022 14:25:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=155
05/25/2022 14:25:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=156
05/25/2022 14:25:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=157
05/25/2022 14:25:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
05/25/2022 14:25:46 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.3268460537634939 on epoch=158
05/25/2022 14:25:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=159
05/25/2022 14:25:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=159
05/25/2022 14:25:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
05/25/2022 14:25:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=161
05/25/2022 14:25:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=162
05/25/2022 14:26:05 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.3414331762955616 on epoch=162
05/25/2022 14:26:05 - INFO - __main__ - Saving model with best Classification-F1: 0.32806982806982804 -> 0.3414331762955616 on epoch=162, global_step=1950
05/25/2022 14:26:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=163
05/25/2022 14:26:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=164
05/25/2022 14:26:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
05/25/2022 14:26:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=165
05/25/2022 14:26:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
05/25/2022 14:26:24 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.27135555029502795 on epoch=166
05/25/2022 14:26:27 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
05/25/2022 14:26:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.38 on epoch=168
05/25/2022 14:26:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=169
05/25/2022 14:26:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=169
05/25/2022 14:26:37 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
05/25/2022 14:26:43 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.3487009482692159 on epoch=170
05/25/2022 14:26:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3414331762955616 -> 0.3487009482692159 on epoch=170, global_step=2050
05/25/2022 14:26:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=171
05/25/2022 14:26:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
05/25/2022 14:26:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.46 on epoch=173
05/25/2022 14:26:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 14:26:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=174
05/25/2022 14:27:02 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.32727787369705297 on epoch=174
05/25/2022 14:27:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
05/25/2022 14:27:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
05/25/2022 14:27:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.38 on epoch=177
05/25/2022 14:27:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
05/25/2022 14:27:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=179
05/25/2022 14:27:22 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.2969434249325554 on epoch=179
05/25/2022 14:27:24 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=179
05/25/2022 14:27:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
05/25/2022 14:27:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=181
05/25/2022 14:27:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=182
05/25/2022 14:27:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
05/25/2022 14:27:41 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.34904488432714237 on epoch=183
05/25/2022 14:27:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3487009482692159 -> 0.34904488432714237 on epoch=183, global_step=2200
05/25/2022 14:27:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=184
05/25/2022 14:27:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=184
05/25/2022 14:27:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=185
05/25/2022 14:27:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=186
05/25/2022 14:27:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=187
05/25/2022 14:28:00 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.21101997417786889 on epoch=187
05/25/2022 14:28:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=188
05/25/2022 14:28:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.39 on epoch=189
05/25/2022 14:28:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
05/25/2022 14:28:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=190
05/25/2022 14:28:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=191
05/25/2022 14:28:19 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.19018873998953678 on epoch=191
05/25/2022 14:28:22 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=192
05/25/2022 14:28:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=193
05/25/2022 14:28:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=194
05/25/2022 14:28:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=194
05/25/2022 14:28:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
05/25/2022 14:28:38 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.3200403270975692 on epoch=195
05/25/2022 14:28:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.41 on epoch=196
05/25/2022 14:28:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=197
05/25/2022 14:28:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=198
05/25/2022 14:28:49 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=199
05/25/2022 14:28:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=199
05/25/2022 14:28:57 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.27945552389996836 on epoch=199
05/25/2022 14:29:00 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
05/25/2022 14:29:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=201
05/25/2022 14:29:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=202
05/25/2022 14:29:08 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.39 on epoch=203
05/25/2022 14:29:11 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=204
05/25/2022 14:29:16 - INFO - __main__ - Global step 2450 Train loss 0.39 Classification-F1 0.36977717622878914 on epoch=204
05/25/2022 14:29:16 - INFO - __main__ - Saving model with best Classification-F1: 0.34904488432714237 -> 0.36977717622878914 on epoch=204, global_step=2450
05/25/2022 14:29:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.44 on epoch=204
05/25/2022 14:29:22 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.40 on epoch=205
05/25/2022 14:29:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
05/25/2022 14:29:27 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=207
05/25/2022 14:29:30 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.40 on epoch=208
05/25/2022 14:29:36 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.3999467151050791 on epoch=208
05/25/2022 14:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.36977717622878914 -> 0.3999467151050791 on epoch=208, global_step=2500
05/25/2022 14:29:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=209
05/25/2022 14:29:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=209
05/25/2022 14:29:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=210
05/25/2022 14:29:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=211
05/25/2022 14:29:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
05/25/2022 14:29:55 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.29604950133624625 on epoch=212
05/25/2022 14:29:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=213
05/25/2022 14:30:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
05/25/2022 14:30:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=214
05/25/2022 14:30:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.38 on epoch=215
05/25/2022 14:30:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=216
05/25/2022 14:30:13 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.22780952380952382 on epoch=216
05/25/2022 14:30:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=217
05/25/2022 14:30:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=218
05/25/2022 14:30:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=219
05/25/2022 14:30:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
05/25/2022 14:30:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.37 on epoch=220
05/25/2022 14:30:32 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.33271942990818487 on epoch=220
05/25/2022 14:30:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
05/25/2022 14:30:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=222
05/25/2022 14:30:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=223
05/25/2022 14:30:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=224
05/25/2022 14:30:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.39 on epoch=224
05/25/2022 14:30:51 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.2170179547228727 on epoch=224
05/25/2022 14:30:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
05/25/2022 14:30:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=226
05/25/2022 14:30:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=227
05/25/2022 14:31:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=228
05/25/2022 14:31:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=229
05/25/2022 14:31:10 - INFO - __main__ - Global step 2750 Train loss 0.39 Classification-F1 0.3594878068984853 on epoch=229
05/25/2022 14:31:13 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=229
05/25/2022 14:31:15 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=230
05/25/2022 14:31:18 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=231
05/25/2022 14:31:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.38 on epoch=232
05/25/2022 14:31:23 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=233
05/25/2022 14:31:29 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.3304876395785486 on epoch=233
05/25/2022 14:31:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=234
05/25/2022 14:31:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=234
05/25/2022 14:31:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=235
05/25/2022 14:31:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=236
05/25/2022 14:31:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=237
05/25/2022 14:31:48 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.29858592788613375 on epoch=237
05/25/2022 14:31:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.42 on epoch=238
05/25/2022 14:31:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=239
05/25/2022 14:31:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=239
05/25/2022 14:31:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=240
05/25/2022 14:32:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.44 on epoch=241
05/25/2022 14:32:07 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.2371335917290356 on epoch=241
05/25/2022 14:32:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.41 on epoch=242
05/25/2022 14:32:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
05/25/2022 14:32:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.38 on epoch=244
05/25/2022 14:32:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.37 on epoch=244
05/25/2022 14:32:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=245
05/25/2022 14:32:27 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.39166666666666666 on epoch=245
05/25/2022 14:32:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=246
05/25/2022 14:32:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=247
05/25/2022 14:32:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.42 on epoch=248
05/25/2022 14:32:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=249
05/25/2022 14:32:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=249
05/25/2022 14:32:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:32:41 - INFO - __main__ - Printing 3 examples
05/25/2022 14:32:41 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 14:32:41 - INFO - __main__ - ['entailment']
05/25/2022 14:32:41 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 14:32:41 - INFO - __main__ - ['entailment']
05/25/2022 14:32:41 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 14:32:41 - INFO - __main__ - ['entailment']
05/25/2022 14:32:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:32:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:32:42 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:32:42 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:32:42 - INFO - __main__ - Printing 3 examples
05/25/2022 14:32:42 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 14:32:42 - INFO - __main__ - ['entailment']
05/25/2022 14:32:42 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 14:32:42 - INFO - __main__ - ['entailment']
05/25/2022 14:32:42 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 14:32:42 - INFO - __main__ - ['entailment']
05/25/2022 14:32:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:32:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:32:42 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:32:46 - INFO - __main__ - Global step 3000 Train loss 0.39 Classification-F1 0.3194405887759949 on epoch=249
05/25/2022 14:32:46 - INFO - __main__ - save last model!
05/25/2022 14:32:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 14:32:46 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 14:32:46 - INFO - __main__ - Printing 3 examples
05/25/2022 14:32:46 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 14:32:46 - INFO - __main__ - ['contradiction']
05/25/2022 14:32:46 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 14:32:46 - INFO - __main__ - ['entailment']
05/25/2022 14:32:46 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 14:32:46 - INFO - __main__ - ['contradiction']
05/25/2022 14:32:46 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:32:46 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:32:47 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 14:33:01 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:33:01 - INFO - __main__ - task name: anli
05/25/2022 14:33:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:33:01 - INFO - __main__ - Starting training!
05/25/2022 14:33:18 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_13_0.2_8_predictions.txt
05/25/2022 14:33:18 - INFO - __main__ - Classification-F1 on test data: 0.2707
05/25/2022 14:33:18 - INFO - __main__ - prefix=anli_64_13, lr=0.2, bsz=8, dev_performance=0.3999467151050791, test_performance=0.27072768545837894
05/25/2022 14:33:18 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.5, bsz=8 ...
05/25/2022 14:33:19 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:33:19 - INFO - __main__ - Printing 3 examples
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:33:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:33:19 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:33:19 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:33:19 - INFO - __main__ - Printing 3 examples
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 14:33:19 - INFO - __main__ - ['entailment']
05/25/2022 14:33:19 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:33:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:33:19 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:33:38 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:33:38 - INFO - __main__ - task name: anli
05/25/2022 14:33:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:33:39 - INFO - __main__ - Starting training!
05/25/2022 14:33:42 - INFO - __main__ - Step 10 Global step 10 Train loss 5.82 on epoch=0
05/25/2022 14:33:45 - INFO - __main__ - Step 20 Global step 20 Train loss 2.86 on epoch=1
05/25/2022 14:33:48 - INFO - __main__ - Step 30 Global step 30 Train loss 1.19 on epoch=2
05/25/2022 14:33:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=3
05/25/2022 14:33:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.75 on epoch=4
05/25/2022 14:33:57 - INFO - __main__ - Global step 50 Train loss 2.30 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 14:33:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 14:34:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=4
05/25/2022 14:34:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=5
05/25/2022 14:34:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=6
05/25/2022 14:34:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=7
05/25/2022 14:34:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.63 on epoch=8
05/25/2022 14:34:15 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 14:34:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=9
05/25/2022 14:34:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
05/25/2022 14:34:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=10
05/25/2022 14:34:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=11
05/25/2022 14:34:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=12
05/25/2022 14:34:32 - INFO - __main__ - Global step 150 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 14:34:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=13
05/25/2022 14:34:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
05/25/2022 14:34:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=14
05/25/2022 14:34:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=15
05/25/2022 14:34:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/25/2022 14:34:50 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 14:34:53 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=17
05/25/2022 14:34:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
05/25/2022 14:34:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.55 on epoch=19
05/25/2022 14:35:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=19
05/25/2022 14:35:04 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=20
05/25/2022 14:35:07 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.287236960695952 on epoch=20
05/25/2022 14:35:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.287236960695952 on epoch=20, global_step=250
05/25/2022 14:35:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=21
05/25/2022 14:35:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.61 on epoch=22
05/25/2022 14:35:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=23
05/25/2022 14:35:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=24
05/25/2022 14:35:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
05/25/2022 14:35:25 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 14:35:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=25
05/25/2022 14:35:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=26
05/25/2022 14:35:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=27
05/25/2022 14:35:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=28
05/25/2022 14:35:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.40 on epoch=29
05/25/2022 14:35:42 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 14:35:44 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=29
05/25/2022 14:35:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=30
05/25/2022 14:35:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=31
05/25/2022 14:35:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=32
05/25/2022 14:35:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=33
05/25/2022 14:36:00 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 14:36:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/25/2022 14:36:06 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=34
05/25/2022 14:36:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=35
05/25/2022 14:36:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=36
05/25/2022 14:36:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/25/2022 14:36:17 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 14:36:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=38
05/25/2022 14:36:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=39
05/25/2022 14:36:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=39
05/25/2022 14:36:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=40
05/25/2022 14:36:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
05/25/2022 14:36:36 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 14:36:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
05/25/2022 14:36:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
05/25/2022 14:36:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/25/2022 14:36:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=44
05/25/2022 14:36:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=45
05/25/2022 14:36:55 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.31749245590296743 on epoch=45
05/25/2022 14:36:55 - INFO - __main__ - Saving model with best Classification-F1: 0.287236960695952 -> 0.31749245590296743 on epoch=45, global_step=550
05/25/2022 14:36:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/25/2022 14:37:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
05/25/2022 14:37:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
05/25/2022 14:37:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=49
05/25/2022 14:37:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=49
05/25/2022 14:37:14 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 14:37:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
05/25/2022 14:37:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
05/25/2022 14:37:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
05/25/2022 14:37:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=53
05/25/2022 14:37:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=54
05/25/2022 14:37:31 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 14:37:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
05/25/2022 14:37:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=55
05/25/2022 14:37:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
05/25/2022 14:37:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
05/25/2022 14:37:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
05/25/2022 14:37:48 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.2830804178718121 on epoch=58
05/25/2022 14:37:51 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
05/25/2022 14:37:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
05/25/2022 14:37:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=60
05/25/2022 14:37:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=61
05/25/2022 14:38:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=62
05/25/2022 14:38:05 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.1824970131421744 on epoch=62
05/25/2022 14:38:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
05/25/2022 14:38:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=64
05/25/2022 14:38:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=64
05/25/2022 14:38:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=65
05/25/2022 14:38:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
05/25/2022 14:38:22 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 14:38:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
05/25/2022 14:38:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 14:38:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=69
05/25/2022 14:38:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
05/25/2022 14:38:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=70
05/25/2022 14:38:41 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.20789909678798568 on epoch=70
05/25/2022 14:38:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
05/25/2022 14:38:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
05/25/2022 14:38:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 14:38:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=74
05/25/2022 14:38:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=74
05/25/2022 14:39:00 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.28198912198912196 on epoch=74
05/25/2022 14:39:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=75
05/25/2022 14:39:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=76
05/25/2022 14:39:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/25/2022 14:39:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
05/25/2022 14:39:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=79
05/25/2022 14:39:19 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=79
05/25/2022 14:39:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=79
05/25/2022 14:39:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=80
05/25/2022 14:39:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
05/25/2022 14:39:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=82
05/25/2022 14:39:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=83
05/25/2022 14:39:36 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
05/25/2022 14:39:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/25/2022 14:39:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=84
05/25/2022 14:39:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=85
05/25/2022 14:39:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
05/25/2022 14:39:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=87
05/25/2022 14:39:53 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.18892001244942422 on epoch=87
05/25/2022 14:39:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=88
05/25/2022 14:39:59 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=89
05/25/2022 14:40:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
05/25/2022 14:40:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=90
05/25/2022 14:40:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=91
05/25/2022 14:40:12 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 14:40:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=92
05/25/2022 14:40:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
05/25/2022 14:40:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
05/25/2022 14:40:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
05/25/2022 14:40:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
05/25/2022 14:40:31 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.27001330067092627 on epoch=95
05/25/2022 14:40:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
05/25/2022 14:40:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
05/25/2022 14:40:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
05/25/2022 14:40:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=99
05/25/2022 14:40:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=99
05/25/2022 14:40:50 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 14:40:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
05/25/2022 14:40:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=101
05/25/2022 14:40:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=102
05/25/2022 14:41:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/25/2022 14:41:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=104
05/25/2022 14:41:08 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 14:41:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/25/2022 14:41:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=105
05/25/2022 14:41:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=106
05/25/2022 14:41:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=107
05/25/2022 14:41:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
05/25/2022 14:41:27 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.31196933422295486 on epoch=108
05/25/2022 14:41:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=109
05/25/2022 14:41:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=109
05/25/2022 14:41:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=110
05/25/2022 14:41:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=111
05/25/2022 14:41:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
05/25/2022 14:41:45 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.22676599885902213 on epoch=112
05/25/2022 14:41:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/25/2022 14:41:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=114
05/25/2022 14:41:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
05/25/2022 14:41:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=115
05/25/2022 14:41:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/25/2022 14:42:04 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 14:42:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=117
05/25/2022 14:42:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
05/25/2022 14:42:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
05/25/2022 14:42:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=119
05/25/2022 14:42:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=120
05/25/2022 14:42:23 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.2393936713326887 on epoch=120
05/25/2022 14:42:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=121
05/25/2022 14:42:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
05/25/2022 14:42:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
05/25/2022 14:42:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=124
05/25/2022 14:42:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=124
05/25/2022 14:42:41 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.16732026143790854 on epoch=124
05/25/2022 14:42:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=125
05/25/2022 14:42:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=126
05/25/2022 14:42:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=127
05/25/2022 14:42:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=128
05/25/2022 14:42:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=129
05/25/2022 14:42:59 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.2190435826799463 on epoch=129
05/25/2022 14:43:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
05/25/2022 14:43:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=130
05/25/2022 14:43:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
05/25/2022 14:43:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.45 on epoch=132
05/25/2022 14:43:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=133
05/25/2022 14:43:18 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.34662090251170713 on epoch=133
05/25/2022 14:43:18 - INFO - __main__ - Saving model with best Classification-F1: 0.31749245590296743 -> 0.34662090251170713 on epoch=133, global_step=1600
05/25/2022 14:43:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=134
05/25/2022 14:43:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
05/25/2022 14:43:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
05/25/2022 14:43:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=136
05/25/2022 14:43:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=137
05/25/2022 14:43:35 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 14:43:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=138
05/25/2022 14:43:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=139
05/25/2022 14:43:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=139
05/25/2022 14:43:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=140
05/25/2022 14:43:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
05/25/2022 14:43:54 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 14:43:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=142
05/25/2022 14:43:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
05/25/2022 14:44:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=144
05/25/2022 14:44:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
05/25/2022 14:44:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
05/25/2022 14:44:12 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.2880012756118951 on epoch=145
05/25/2022 14:44:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 14:44:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=147
05/25/2022 14:44:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
05/25/2022 14:44:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=149
05/25/2022 14:44:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=149
05/25/2022 14:44:31 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.19778643164582907 on epoch=149
05/25/2022 14:44:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=150
05/25/2022 14:44:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
05/25/2022 14:44:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.43 on epoch=152
05/25/2022 14:44:41 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=153
05/25/2022 14:44:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
05/25/2022 14:44:49 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.1989722270338934 on epoch=154
05/25/2022 14:44:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=154
05/25/2022 14:44:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 14:44:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
05/25/2022 14:44:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=157
05/25/2022 14:45:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
05/25/2022 14:45:08 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.32632595714787493 on epoch=158
05/25/2022 14:45:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=159
05/25/2022 14:45:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=159
05/25/2022 14:45:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=160
05/25/2022 14:45:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
05/25/2022 14:45:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=162
05/25/2022 14:45:26 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.27550542017088125 on epoch=162
05/25/2022 14:45:29 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=163
05/25/2022 14:45:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=164
05/25/2022 14:45:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
05/25/2022 14:45:37 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
05/25/2022 14:45:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=166
05/25/2022 14:45:45 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.2706228956228956 on epoch=166
05/25/2022 14:45:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=167
05/25/2022 14:45:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=168
05/25/2022 14:45:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=169
05/25/2022 14:45:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
05/25/2022 14:45:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=170
05/25/2022 14:46:03 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.34239977374506386 on epoch=170
05/25/2022 14:46:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=171
05/25/2022 14:46:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=172
05/25/2022 14:46:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
05/25/2022 14:46:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=174
05/25/2022 14:46:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
05/25/2022 14:46:21 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.2690354101174812 on epoch=174
05/25/2022 14:46:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
05/25/2022 14:46:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=176
05/25/2022 14:46:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
05/25/2022 14:46:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.40 on epoch=178
05/25/2022 14:46:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=179
05/25/2022 14:46:40 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.2087619047619048 on epoch=179
05/25/2022 14:46:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=179
05/25/2022 14:46:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
05/25/2022 14:46:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
05/25/2022 14:46:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=182
05/25/2022 14:46:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
05/25/2022 14:46:58 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.30339178634262315 on epoch=183
05/25/2022 14:47:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=184
05/25/2022 14:47:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 14:47:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=185
05/25/2022 14:47:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.38 on epoch=186
05/25/2022 14:47:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=187
05/25/2022 14:47:17 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.24511278195488725 on epoch=187
05/25/2022 14:47:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=188
05/25/2022 14:47:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=189
05/25/2022 14:47:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
05/25/2022 14:47:27 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.44 on epoch=190
05/25/2022 14:47:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.35 on epoch=191
05/25/2022 14:47:36 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.26151368760064414 on epoch=191
05/25/2022 14:47:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=192
05/25/2022 14:47:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
05/25/2022 14:47:43 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.40 on epoch=194
05/25/2022 14:47:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=194
05/25/2022 14:47:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=195
05/25/2022 14:47:54 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.2838468389027081 on epoch=195
05/25/2022 14:47:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=196
05/25/2022 14:47:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=197
05/25/2022 14:48:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.48 on epoch=198
05/25/2022 14:48:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.40 on epoch=199
05/25/2022 14:48:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=199
05/25/2022 14:48:13 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.27897777062431817 on epoch=199
05/25/2022 14:48:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=200
05/25/2022 14:48:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.40 on epoch=201
05/25/2022 14:48:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=202
05/25/2022 14:48:23 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=203
05/25/2022 14:48:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=204
05/25/2022 14:48:31 - INFO - __main__ - Global step 2450 Train loss 0.39 Classification-F1 0.257905382072912 on epoch=204
05/25/2022 14:48:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=204
05/25/2022 14:48:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.38 on epoch=205
05/25/2022 14:48:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.38 on epoch=206
05/25/2022 14:48:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.38 on epoch=207
05/25/2022 14:48:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=208
05/25/2022 14:48:50 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.3467527449099242 on epoch=208
05/25/2022 14:48:50 - INFO - __main__ - Saving model with best Classification-F1: 0.34662090251170713 -> 0.3467527449099242 on epoch=208, global_step=2500
05/25/2022 14:48:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.38 on epoch=209
05/25/2022 14:48:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=209
05/25/2022 14:48:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=210
05/25/2022 14:49:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=211
05/25/2022 14:49:03 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=212
05/25/2022 14:49:09 - INFO - __main__ - Global step 2550 Train loss 0.39 Classification-F1 0.2915050635638871 on epoch=212
05/25/2022 14:49:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 14:49:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=214
05/25/2022 14:49:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=214
05/25/2022 14:49:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=215
05/25/2022 14:49:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.35 on epoch=216
05/25/2022 14:49:28 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.3072591486183719 on epoch=216
05/25/2022 14:49:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=217
05/25/2022 14:49:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.38 on epoch=218
05/25/2022 14:49:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.37 on epoch=219
05/25/2022 14:49:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=219
05/25/2022 14:49:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
05/25/2022 14:49:47 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.3476529186387199 on epoch=220
05/25/2022 14:49:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3467527449099242 -> 0.3476529186387199 on epoch=220, global_step=2650
05/25/2022 14:49:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.36 on epoch=221
05/25/2022 14:49:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=222
05/25/2022 14:49:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=223
05/25/2022 14:49:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=224
05/25/2022 14:50:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.36 on epoch=224
05/25/2022 14:50:06 - INFO - __main__ - Global step 2700 Train loss 0.38 Classification-F1 0.3124723478439548 on epoch=224
05/25/2022 14:50:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.40 on epoch=225
05/25/2022 14:50:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=226
05/25/2022 14:50:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
05/25/2022 14:50:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.37 on epoch=228
05/25/2022 14:50:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.37 on epoch=229
05/25/2022 14:50:25 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.3491056480747203 on epoch=229
05/25/2022 14:50:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3476529186387199 -> 0.3491056480747203 on epoch=229, global_step=2750
05/25/2022 14:50:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.35 on epoch=229
05/25/2022 14:50:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.35 on epoch=230
05/25/2022 14:50:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.35 on epoch=231
05/25/2022 14:50:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.34 on epoch=232
05/25/2022 14:50:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
05/25/2022 14:50:44 - INFO - __main__ - Global step 2800 Train loss 0.36 Classification-F1 0.29008577885983056 on epoch=233
05/25/2022 14:50:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.34 on epoch=234
05/25/2022 14:50:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=234
05/25/2022 14:50:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=235
05/25/2022 14:50:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=236
05/25/2022 14:50:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=237
05/25/2022 14:51:06 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.338223538816006 on epoch=237
05/25/2022 14:51:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=238
05/25/2022 14:51:11 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.33 on epoch=239
05/25/2022 14:51:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.35 on epoch=239
05/25/2022 14:51:16 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
05/25/2022 14:51:19 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=241
05/25/2022 14:51:25 - INFO - __main__ - Global step 2900 Train loss 0.35 Classification-F1 0.3497004357298475 on epoch=241
05/25/2022 14:51:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3491056480747203 -> 0.3497004357298475 on epoch=241, global_step=2900
05/25/2022 14:51:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=242
05/25/2022 14:51:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=243
05/25/2022 14:51:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=244
05/25/2022 14:51:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.37 on epoch=244
05/25/2022 14:51:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=245
05/25/2022 14:51:44 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.2756733462615816 on epoch=245
05/25/2022 14:51:46 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.32 on epoch=246
05/25/2022 14:51:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=247
05/25/2022 14:51:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.38 on epoch=248
05/25/2022 14:51:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=249
05/25/2022 14:51:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.35 on epoch=249
05/25/2022 14:51:58 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:51:58 - INFO - __main__ - Printing 3 examples
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:51:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:51:58 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:51:58 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:51:58 - INFO - __main__ - Printing 3 examples
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 14:51:58 - INFO - __main__ - ['entailment']
05/25/2022 14:51:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:51:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:51:58 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:52:02 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.34128021579001966 on epoch=249
05/25/2022 14:52:02 - INFO - __main__ - save last model!
05/25/2022 14:52:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 14:52:02 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 14:52:02 - INFO - __main__ - Printing 3 examples
05/25/2022 14:52:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 14:52:02 - INFO - __main__ - ['contradiction']
05/25/2022 14:52:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 14:52:02 - INFO - __main__ - ['entailment']
05/25/2022 14:52:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 14:52:02 - INFO - __main__ - ['contradiction']
05/25/2022 14:52:02 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:52:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:52:04 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 14:52:14 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:52:14 - INFO - __main__ - task name: anli
05/25/2022 14:52:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:52:15 - INFO - __main__ - Starting training!
05/25/2022 14:52:34 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_21_0.5_8_predictions.txt
05/25/2022 14:52:34 - INFO - __main__ - Classification-F1 on test data: 0.3210
05/25/2022 14:52:35 - INFO - __main__ - prefix=anli_64_21, lr=0.5, bsz=8, dev_performance=0.3497004357298475, test_performance=0.32099119919736435
05/25/2022 14:52:35 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.4, bsz=8 ...
05/25/2022 14:52:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:52:36 - INFO - __main__ - Printing 3 examples
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:52:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:52:36 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 14:52:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 14:52:36 - INFO - __main__ - Printing 3 examples
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 14:52:36 - INFO - __main__ - ['entailment']
05/25/2022 14:52:36 - INFO - __main__ - Tokenizing Input ...
05/25/2022 14:52:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 14:52:36 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 14:52:51 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 14:52:51 - INFO - __main__ - task name: anli
05/25/2022 14:52:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 14:52:52 - INFO - __main__ - Starting training!
05/25/2022 14:52:55 - INFO - __main__ - Step 10 Global step 10 Train loss 6.01 on epoch=0
05/25/2022 14:52:57 - INFO - __main__ - Step 20 Global step 20 Train loss 2.34 on epoch=1
05/25/2022 14:53:00 - INFO - __main__ - Step 30 Global step 30 Train loss 1.28 on epoch=2
05/25/2022 14:53:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.94 on epoch=3
05/25/2022 14:53:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.68 on epoch=4
05/25/2022 14:53:09 - INFO - __main__ - Global step 50 Train loss 2.25 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 14:53:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 14:53:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=4
05/25/2022 14:53:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.65 on epoch=5
05/25/2022 14:53:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.61 on epoch=6
05/25/2022 14:53:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=7
05/25/2022 14:53:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.59 on epoch=8
05/25/2022 14:53:27 - INFO - __main__ - Global step 100 Train loss 0.63 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 14:53:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=9
05/25/2022 14:53:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=9
05/25/2022 14:53:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=10
05/25/2022 14:53:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=11
05/25/2022 14:53:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
05/25/2022 14:53:44 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 14:53:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.60 on epoch=13
05/25/2022 14:53:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=14
05/25/2022 14:53:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=14
05/25/2022 14:53:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=15
05/25/2022 14:53:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/25/2022 14:54:01 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 14:54:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=17
05/25/2022 14:54:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=18
05/25/2022 14:54:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=19
05/25/2022 14:54:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
05/25/2022 14:54:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
05/25/2022 14:54:19 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.1936601420405335 on epoch=20
05/25/2022 14:54:19 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1936601420405335 on epoch=20, global_step=250
05/25/2022 14:54:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/25/2022 14:54:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/25/2022 14:54:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
05/25/2022 14:54:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=24
05/25/2022 14:54:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=24
05/25/2022 14:54:37 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 14:54:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=25
05/25/2022 14:54:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
05/25/2022 14:54:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=27
05/25/2022 14:54:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=28
05/25/2022 14:54:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=29
05/25/2022 14:54:56 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 14:54:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/25/2022 14:55:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=30
05/25/2022 14:55:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/25/2022 14:55:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=32
05/25/2022 14:55:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 14:55:15 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.19775873434410016 on epoch=33
05/25/2022 14:55:15 - INFO - __main__ - Saving model with best Classification-F1: 0.1936601420405335 -> 0.19775873434410016 on epoch=33, global_step=400
05/25/2022 14:55:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/25/2022 14:55:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=34
05/25/2022 14:55:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
05/25/2022 14:55:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=36
05/25/2022 14:55:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/25/2022 14:55:34 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 14:55:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/25/2022 14:55:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=39
05/25/2022 14:55:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
05/25/2022 14:55:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=40
05/25/2022 14:55:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
05/25/2022 14:55:53 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 14:55:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
05/25/2022 14:55:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
05/25/2022 14:56:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=44
05/25/2022 14:56:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=44
05/25/2022 14:56:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
05/25/2022 14:56:11 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16272965879265092 on epoch=45
05/25/2022 14:56:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/25/2022 14:56:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=47
05/25/2022 14:56:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=48
05/25/2022 14:56:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=49
05/25/2022 14:56:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
05/25/2022 14:56:30 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 14:56:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=50
05/25/2022 14:56:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
05/25/2022 14:56:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
05/25/2022 14:56:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
05/25/2022 14:56:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=54
05/25/2022 14:56:49 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.2897328723656625 on epoch=54
05/25/2022 14:56:49 - INFO - __main__ - Saving model with best Classification-F1: 0.19775873434410016 -> 0.2897328723656625 on epoch=54, global_step=650
05/25/2022 14:56:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=54
05/25/2022 14:56:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=55
05/25/2022 14:56:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=56
05/25/2022 14:56:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=57
05/25/2022 14:57:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/25/2022 14:57:07 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.19923809523809524 on epoch=58
05/25/2022 14:57:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
05/25/2022 14:57:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/25/2022 14:57:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=60
05/25/2022 14:57:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 14:57:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=62
05/25/2022 14:57:26 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 14:57:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=63
05/25/2022 14:57:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
05/25/2022 14:57:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=64
05/25/2022 14:57:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/25/2022 14:57:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
05/25/2022 14:57:44 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 14:57:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=67
05/25/2022 14:57:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 14:57:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
05/25/2022 14:57:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
05/25/2022 14:57:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=70
05/25/2022 14:58:03 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.1885434487640847 on epoch=70
05/25/2022 14:58:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
05/25/2022 14:58:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
05/25/2022 14:58:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/25/2022 14:58:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=74
05/25/2022 14:58:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=74
05/25/2022 14:58:22 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.2318729305628869 on epoch=74
05/25/2022 14:58:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=75
05/25/2022 14:58:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=76
05/25/2022 14:58:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=77
05/25/2022 14:58:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
05/25/2022 14:58:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/25/2022 14:58:40 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.19952108905103985 on epoch=79
05/25/2022 14:58:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=79
05/25/2022 14:58:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=80
05/25/2022 14:58:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
05/25/2022 14:58:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
05/25/2022 14:58:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=83
05/25/2022 14:58:59 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.24647476011681269 on epoch=83
05/25/2022 14:59:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/25/2022 14:59:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=84
05/25/2022 14:59:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=85
05/25/2022 14:59:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
05/25/2022 14:59:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=87
05/25/2022 14:59:17 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 14:59:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=88
05/25/2022 14:59:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=89
05/25/2022 14:59:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=89
05/25/2022 14:59:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
05/25/2022 14:59:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=91
05/25/2022 14:59:36 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 14:59:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=92
05/25/2022 14:59:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=93
05/25/2022 14:59:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=94
05/25/2022 14:59:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
05/25/2022 14:59:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=95
05/25/2022 14:59:54 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.19071824861298545 on epoch=95
05/25/2022 14:59:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=96
05/25/2022 15:00:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
05/25/2022 15:00:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
05/25/2022 15:00:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
05/25/2022 15:00:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=99
05/25/2022 15:00:13 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.271760836571889 on epoch=99
05/25/2022 15:00:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=100
05/25/2022 15:00:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=101
05/25/2022 15:00:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=102
05/25/2022 15:00:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
05/25/2022 15:00:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=104
05/25/2022 15:00:32 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 15:00:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=104
05/25/2022 15:00:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=105
05/25/2022 15:00:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
05/25/2022 15:00:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
05/25/2022 15:00:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=108
05/25/2022 15:00:50 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.27939590075512405 on epoch=108
05/25/2022 15:00:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=109
05/25/2022 15:00:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.46 on epoch=109
05/25/2022 15:00:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
05/25/2022 15:01:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=111
05/25/2022 15:01:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
05/25/2022 15:01:09 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.2434884242523024 on epoch=112
05/25/2022 15:01:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/25/2022 15:01:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=114
05/25/2022 15:01:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
05/25/2022 15:01:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=115
05/25/2022 15:01:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
05/25/2022 15:01:27 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 15:01:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=117
05/25/2022 15:01:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
05/25/2022 15:01:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=119
05/25/2022 15:01:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=119
05/25/2022 15:01:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 15:01:46 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.1990221455277538 on epoch=120
05/25/2022 15:01:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=121
05/25/2022 15:01:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=122
05/25/2022 15:01:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=123
05/25/2022 15:01:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=124
05/25/2022 15:01:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=124
05/25/2022 15:02:05 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=124
05/25/2022 15:02:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=125
05/25/2022 15:02:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=126
05/25/2022 15:02:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
05/25/2022 15:02:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=128
05/25/2022 15:02:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
05/25/2022 15:02:23 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=129
05/25/2022 15:02:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=129
05/25/2022 15:02:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=130
05/25/2022 15:02:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
05/25/2022 15:02:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=132
05/25/2022 15:02:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=133
05/25/2022 15:02:41 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.2661087669948429 on epoch=133
05/25/2022 15:02:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=134
05/25/2022 15:02:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
05/25/2022 15:02:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=135
05/25/2022 15:02:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=136
05/25/2022 15:02:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=137
05/25/2022 15:03:00 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.22789871934088282 on epoch=137
05/25/2022 15:03:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=138
05/25/2022 15:03:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 15:03:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
05/25/2022 15:03:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=140
05/25/2022 15:03:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
05/25/2022 15:03:19 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.3002734277902063 on epoch=141
05/25/2022 15:03:19 - INFO - __main__ - Saving model with best Classification-F1: 0.2897328723656625 -> 0.3002734277902063 on epoch=141, global_step=1700
05/25/2022 15:03:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
05/25/2022 15:03:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
05/25/2022 15:03:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=144
05/25/2022 15:03:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=144
05/25/2022 15:03:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=145
05/25/2022 15:03:38 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.28374651442063054 on epoch=145
05/25/2022 15:03:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 15:03:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=147
05/25/2022 15:03:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
05/25/2022 15:03:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=149
05/25/2022 15:03:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=149
05/25/2022 15:03:56 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.2714536398746925 on epoch=149
05/25/2022 15:03:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=150
05/25/2022 15:04:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
05/25/2022 15:04:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=152
05/25/2022 15:04:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 15:04:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=154
05/25/2022 15:04:15 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.1885434487640847 on epoch=154
05/25/2022 15:04:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
05/25/2022 15:04:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=155
05/25/2022 15:04:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
05/25/2022 15:04:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.44 on epoch=157
05/25/2022 15:04:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
05/25/2022 15:04:34 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.2440177964801543 on epoch=158
05/25/2022 15:04:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=159
05/25/2022 15:04:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=159
05/25/2022 15:04:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=160
05/25/2022 15:04:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=161
05/25/2022 15:04:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=162
05/25/2022 15:04:52 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.35682063052534424 on epoch=162
05/25/2022 15:04:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3002734277902063 -> 0.35682063052534424 on epoch=162, global_step=1950
05/25/2022 15:04:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
05/25/2022 15:04:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=164
05/25/2022 15:05:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=164
05/25/2022 15:05:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=165
05/25/2022 15:05:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=166
05/25/2022 15:05:11 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.1775766716943188 on epoch=166
05/25/2022 15:05:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=167
05/25/2022 15:05:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=168
05/25/2022 15:05:19 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
05/25/2022 15:05:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.43 on epoch=169
05/25/2022 15:05:24 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=170
05/25/2022 15:05:30 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.276622814141086 on epoch=170
05/25/2022 15:05:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=171
05/25/2022 15:05:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=172
05/25/2022 15:05:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
05/25/2022 15:05:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 15:05:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
05/25/2022 15:05:48 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.22057899569861292 on epoch=174
05/25/2022 15:05:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
05/25/2022 15:05:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=176
05/25/2022 15:05:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
05/25/2022 15:05:59 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=178
05/25/2022 15:06:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=179
05/25/2022 15:06:07 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.1775766716943188 on epoch=179
05/25/2022 15:06:10 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=179
05/25/2022 15:06:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=180
05/25/2022 15:06:15 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=181
05/25/2022 15:06:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=182
05/25/2022 15:06:20 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
05/25/2022 15:06:26 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.2888135899521099 on epoch=183
05/25/2022 15:06:29 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
05/25/2022 15:06:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
05/25/2022 15:06:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=185
05/25/2022 15:06:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
05/25/2022 15:06:39 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.42 on epoch=187
05/25/2022 15:06:45 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.35141704326934725 on epoch=187
05/25/2022 15:06:47 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=188
05/25/2022 15:06:50 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
05/25/2022 15:06:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=189
05/25/2022 15:06:55 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=190
05/25/2022 15:06:57 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=191
05/25/2022 15:07:03 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.25895989974937345 on epoch=191
05/25/2022 15:07:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
05/25/2022 15:07:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=193
05/25/2022 15:07:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=194
05/25/2022 15:07:13 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=194
05/25/2022 15:07:16 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.43 on epoch=195
05/25/2022 15:07:22 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.27333333333333326 on epoch=195
05/25/2022 15:07:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.35 on epoch=196
05/25/2022 15:07:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
05/25/2022 15:07:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=198
05/25/2022 15:07:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=199
05/25/2022 15:07:35 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=199
05/25/2022 15:07:40 - INFO - __main__ - Global step 2400 Train loss 0.39 Classification-F1 0.38386319660682483 on epoch=199
05/25/2022 15:07:40 - INFO - __main__ - Saving model with best Classification-F1: 0.35682063052534424 -> 0.38386319660682483 on epoch=199, global_step=2400
05/25/2022 15:07:43 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
05/25/2022 15:07:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=201
05/25/2022 15:07:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=202
05/25/2022 15:07:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
05/25/2022 15:07:53 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.37 on epoch=204
05/25/2022 15:07:59 - INFO - __main__ - Global step 2450 Train loss 0.40 Classification-F1 0.17699251303962893 on epoch=204
05/25/2022 15:08:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
05/25/2022 15:08:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.36 on epoch=205
05/25/2022 15:08:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=206
05/25/2022 15:08:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
05/25/2022 15:08:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=208
05/25/2022 15:08:17 - INFO - __main__ - Global step 2500 Train loss 0.39 Classification-F1 0.2817010251646483 on epoch=208
05/25/2022 15:08:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.41 on epoch=209
05/25/2022 15:08:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=209
05/25/2022 15:08:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=210
05/25/2022 15:08:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=211
05/25/2022 15:08:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
05/25/2022 15:08:36 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.1939047619047619 on epoch=212
05/25/2022 15:08:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.42 on epoch=213
05/25/2022 15:08:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
05/25/2022 15:08:44 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.41 on epoch=214
05/25/2022 15:08:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=215
05/25/2022 15:08:49 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=216
05/25/2022 15:08:55 - INFO - __main__ - Global step 2600 Train loss 0.41 Classification-F1 0.2593231104255512 on epoch=216
05/25/2022 15:08:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
05/25/2022 15:09:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.39 on epoch=218
05/25/2022 15:09:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.35 on epoch=219
05/25/2022 15:09:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
05/25/2022 15:09:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=220
05/25/2022 15:09:14 - INFO - __main__ - Global step 2650 Train loss 0.39 Classification-F1 0.2615231568475738 on epoch=220
05/25/2022 15:09:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=221
05/25/2022 15:09:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=222
05/25/2022 15:09:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=223
05/25/2022 15:09:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=224
05/25/2022 15:09:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.41 on epoch=224
05/25/2022 15:09:33 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.36309605985453514 on epoch=224
05/25/2022 15:09:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.43 on epoch=225
05/25/2022 15:09:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=226
05/25/2022 15:09:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.39 on epoch=227
05/25/2022 15:09:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=228
05/25/2022 15:09:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=229
05/25/2022 15:09:51 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.17806437015668306 on epoch=229
05/25/2022 15:09:54 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=229
05/25/2022 15:09:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.40 on epoch=230
05/25/2022 15:09:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=231
05/25/2022 15:10:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.36 on epoch=232
05/25/2022 15:10:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=233
05/25/2022 15:10:10 - INFO - __main__ - Global step 2800 Train loss 0.39 Classification-F1 0.29130804604490085 on epoch=233
05/25/2022 15:10:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.32 on epoch=234
05/25/2022 15:10:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.41 on epoch=234
05/25/2022 15:10:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=235
05/25/2022 15:10:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=236
05/25/2022 15:10:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=237
05/25/2022 15:10:28 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.34168656250314805 on epoch=237
05/25/2022 15:10:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.40 on epoch=238
05/25/2022 15:10:33 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=239
05/25/2022 15:10:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=239
05/25/2022 15:10:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
05/25/2022 15:10:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.39 on epoch=241
05/25/2022 15:10:47 - INFO - __main__ - Global step 2900 Train loss 0.38 Classification-F1 0.30053474408675845 on epoch=241
05/25/2022 15:10:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=242
05/25/2022 15:10:52 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=243
05/25/2022 15:10:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=244
05/25/2022 15:10:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=244
05/25/2022 15:11:00 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=245
05/25/2022 15:11:08 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.21803751803751806 on epoch=245
05/25/2022 15:11:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=246
05/25/2022 15:11:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=247
05/25/2022 15:11:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=248
05/25/2022 15:11:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=249
05/25/2022 15:11:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=249
05/25/2022 15:11:22 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:11:22 - INFO - __main__ - Printing 3 examples
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:11:22 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:11:22 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:11:22 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:11:22 - INFO - __main__ - Printing 3 examples
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 15:11:22 - INFO - __main__ - ['entailment']
05/25/2022 15:11:22 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:11:22 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:11:23 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:11:26 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.2734798386708635 on epoch=249
05/25/2022 15:11:26 - INFO - __main__ - save last model!
05/25/2022 15:11:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 15:11:26 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 15:11:26 - INFO - __main__ - Printing 3 examples
05/25/2022 15:11:26 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 15:11:26 - INFO - __main__ - ['contradiction']
05/25/2022 15:11:26 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 15:11:26 - INFO - __main__ - ['entailment']
05/25/2022 15:11:26 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 15:11:26 - INFO - __main__ - ['contradiction']
05/25/2022 15:11:26 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:11:27 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:11:28 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 15:11:37 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:11:37 - INFO - __main__ - task name: anli
05/25/2022 15:11:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:11:38 - INFO - __main__ - Starting training!
05/25/2022 15:11:58 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_21_0.4_8_predictions.txt
05/25/2022 15:11:58 - INFO - __main__ - Classification-F1 on test data: 0.2599
05/25/2022 15:11:58 - INFO - __main__ - prefix=anli_64_21, lr=0.4, bsz=8, dev_performance=0.38386319660682483, test_performance=0.25991390288718685
05/25/2022 15:11:58 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.3, bsz=8 ...
05/25/2022 15:11:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:11:59 - INFO - __main__ - Printing 3 examples
05/25/2022 15:11:59 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 15:11:59 - INFO - __main__ - ['entailment']
05/25/2022 15:11:59 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 15:11:59 - INFO - __main__ - ['entailment']
05/25/2022 15:11:59 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 15:11:59 - INFO - __main__ - ['entailment']
05/25/2022 15:11:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:11:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:12:00 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:12:00 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:12:00 - INFO - __main__ - Printing 3 examples
05/25/2022 15:12:00 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 15:12:00 - INFO - __main__ - ['entailment']
05/25/2022 15:12:00 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 15:12:00 - INFO - __main__ - ['entailment']
05/25/2022 15:12:00 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 15:12:00 - INFO - __main__ - ['entailment']
05/25/2022 15:12:00 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:12:00 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:12:00 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:12:19 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:12:19 - INFO - __main__ - task name: anli
05/25/2022 15:12:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:12:20 - INFO - __main__ - Starting training!
05/25/2022 15:12:23 - INFO - __main__ - Step 10 Global step 10 Train loss 6.39 on epoch=0
05/25/2022 15:12:25 - INFO - __main__ - Step 20 Global step 20 Train loss 2.98 on epoch=1
05/25/2022 15:12:28 - INFO - __main__ - Step 30 Global step 30 Train loss 1.43 on epoch=2
05/25/2022 15:12:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=3
05/25/2022 15:12:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.84 on epoch=4
05/25/2022 15:12:38 - INFO - __main__ - Global step 50 Train loss 2.52 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 15:12:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 15:12:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.78 on epoch=4
05/25/2022 15:12:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=5
05/25/2022 15:12:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.58 on epoch=6
05/25/2022 15:12:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=7
05/25/2022 15:12:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.73 on epoch=8
05/25/2022 15:12:55 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.16535433070866143 on epoch=8
05/25/2022 15:12:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=9
05/25/2022 15:13:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=9
05/25/2022 15:13:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=10
05/25/2022 15:13:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=11
05/25/2022 15:13:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=12
05/25/2022 15:13:13 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.1647058823529412 on epoch=12
05/25/2022 15:13:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=13
05/25/2022 15:13:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=14
05/25/2022 15:13:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=14
05/25/2022 15:13:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=15
05/25/2022 15:13:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=16
05/25/2022 15:13:31 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 15:13:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=17
05/25/2022 15:13:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
05/25/2022 15:13:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=19
05/25/2022 15:13:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=19
05/25/2022 15:13:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=20
05/25/2022 15:13:47 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.19203963544266356 on epoch=20
05/25/2022 15:13:47 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19203963544266356 on epoch=20, global_step=250
05/25/2022 15:13:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=21
05/25/2022 15:13:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/25/2022 15:13:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
05/25/2022 15:13:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=24
05/25/2022 15:14:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.51 on epoch=24
05/25/2022 15:14:06 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 15:14:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=25
05/25/2022 15:14:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/25/2022 15:14:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=27
05/25/2022 15:14:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=28
05/25/2022 15:14:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=29
05/25/2022 15:14:24 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.2733320780769472 on epoch=29
05/25/2022 15:14:24 - INFO - __main__ - Saving model with best Classification-F1: 0.19203963544266356 -> 0.2733320780769472 on epoch=29, global_step=350
05/25/2022 15:14:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=29
05/25/2022 15:14:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
05/25/2022 15:14:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/25/2022 15:14:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
05/25/2022 15:14:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 15:14:43 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.22960095294818347 on epoch=33
05/25/2022 15:14:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=34
05/25/2022 15:14:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=34
05/25/2022 15:14:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
05/25/2022 15:14:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=36
05/25/2022 15:14:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=37
05/25/2022 15:15:02 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.17270961781397895 on epoch=37
05/25/2022 15:15:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=38
05/25/2022 15:15:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=39
05/25/2022 15:15:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
05/25/2022 15:15:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=40
05/25/2022 15:15:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/25/2022 15:15:18 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 15:15:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=42
05/25/2022 15:15:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/25/2022 15:15:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=44
05/25/2022 15:15:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.52 on epoch=44
05/25/2022 15:15:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=45
05/25/2022 15:15:37 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 15:15:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/25/2022 15:15:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=47
05/25/2022 15:15:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
05/25/2022 15:15:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=49
05/25/2022 15:15:50 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=49
05/25/2022 15:15:55 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.1754553408096715 on epoch=49
05/25/2022 15:15:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=50
05/25/2022 15:16:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
05/25/2022 15:16:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
05/25/2022 15:16:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.55 on epoch=53
05/25/2022 15:16:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=54
05/25/2022 15:16:14 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 15:16:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
05/25/2022 15:16:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/25/2022 15:16:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
05/25/2022 15:16:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
05/25/2022 15:16:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=58
05/25/2022 15:16:32 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.29580419580419576 on epoch=58
05/25/2022 15:16:32 - INFO - __main__ - Saving model with best Classification-F1: 0.2733320780769472 -> 0.29580419580419576 on epoch=58, global_step=700
05/25/2022 15:16:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
05/25/2022 15:16:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/25/2022 15:16:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
05/25/2022 15:16:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.44 on epoch=61
05/25/2022 15:16:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=62
05/25/2022 15:16:51 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 15:16:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
05/25/2022 15:16:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
05/25/2022 15:16:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
05/25/2022 15:17:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=65
05/25/2022 15:17:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
05/25/2022 15:17:09 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 15:17:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
05/25/2022 15:17:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 15:17:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=69
05/25/2022 15:17:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
05/25/2022 15:17:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=70
05/25/2022 15:17:28 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.27063940560966643 on epoch=70
05/25/2022 15:17:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
05/25/2022 15:17:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=72
05/25/2022 15:17:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/25/2022 15:17:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=74
05/25/2022 15:17:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
05/25/2022 15:17:46 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.2791515293566155 on epoch=74
05/25/2022 15:17:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=75
05/25/2022 15:17:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=76
05/25/2022 15:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=77
05/25/2022 15:17:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=78
05/25/2022 15:17:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=79
05/25/2022 15:18:05 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.1785932000078658 on epoch=79
05/25/2022 15:18:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=79
05/25/2022 15:18:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=80
05/25/2022 15:18:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
05/25/2022 15:18:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=82
05/25/2022 15:18:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=83
05/25/2022 15:18:22 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.20355600806983806 on epoch=83
05/25/2022 15:18:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=84
05/25/2022 15:18:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
05/25/2022 15:18:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=85
05/25/2022 15:18:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=86
05/25/2022 15:18:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=87
05/25/2022 15:18:41 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 15:18:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=88
05/25/2022 15:18:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=89
05/25/2022 15:18:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=89
05/25/2022 15:18:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=90
05/25/2022 15:18:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=91
05/25/2022 15:18:59 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 15:19:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=92
05/25/2022 15:19:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 15:19:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=94
05/25/2022 15:19:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
05/25/2022 15:19:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
05/25/2022 15:19:18 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.361429600088623 on epoch=95
05/25/2022 15:19:18 - INFO - __main__ - Saving model with best Classification-F1: 0.29580419580419576 -> 0.361429600088623 on epoch=95, global_step=1150
05/25/2022 15:19:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=96
05/25/2022 15:19:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=97
05/25/2022 15:19:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
05/25/2022 15:19:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
05/25/2022 15:19:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=99
05/25/2022 15:19:37 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.16732026143790854 on epoch=99
05/25/2022 15:19:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=100
05/25/2022 15:19:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=101
05/25/2022 15:19:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
05/25/2022 15:19:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=103
05/25/2022 15:19:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=104
05/25/2022 15:19:55 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.17595815389455882 on epoch=104
05/25/2022 15:19:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=104
05/25/2022 15:20:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
05/25/2022 15:20:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
05/25/2022 15:20:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
05/25/2022 15:20:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
05/25/2022 15:20:14 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.2734450621072457 on epoch=108
05/25/2022 15:20:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=109
05/25/2022 15:20:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=109
05/25/2022 15:20:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=110
05/25/2022 15:20:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
05/25/2022 15:20:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=112
05/25/2022 15:20:33 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.19312712377228503 on epoch=112
05/25/2022 15:20:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
05/25/2022 15:20:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
05/25/2022 15:20:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
05/25/2022 15:20:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=115
05/25/2022 15:20:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/25/2022 15:20:52 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 15:20:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=117
05/25/2022 15:20:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=118
05/25/2022 15:21:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
05/25/2022 15:21:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=119
05/25/2022 15:21:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=120
05/25/2022 15:21:11 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.22511831602740692 on epoch=120
05/25/2022 15:21:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
05/25/2022 15:21:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
05/25/2022 15:21:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=123
05/25/2022 15:21:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=124
05/25/2022 15:21:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=124
05/25/2022 15:21:30 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.2713459767967105 on epoch=124
05/25/2022 15:21:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=125
05/25/2022 15:21:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
05/25/2022 15:21:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=127
05/25/2022 15:21:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
05/25/2022 15:21:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.45 on epoch=129
05/25/2022 15:21:48 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=129
05/25/2022 15:21:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.45 on epoch=129
05/25/2022 15:21:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=130
05/25/2022 15:21:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=131
05/25/2022 15:21:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=132
05/25/2022 15:22:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
05/25/2022 15:22:07 - INFO - __main__ - Global step 1600 Train loss 0.44 Classification-F1 0.21939838490280142 on epoch=133
05/25/2022 15:22:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=134
05/25/2022 15:22:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.48 on epoch=134
05/25/2022 15:22:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
05/25/2022 15:22:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=136
05/25/2022 15:22:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=137
05/25/2022 15:22:26 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.3198494002276508 on epoch=137
05/25/2022 15:22:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=138
05/25/2022 15:22:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.38 on epoch=139
05/25/2022 15:22:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
05/25/2022 15:22:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=140
05/25/2022 15:22:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
05/25/2022 15:22:45 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.18892001244942422 on epoch=141
05/25/2022 15:22:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=142
05/25/2022 15:22:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
05/25/2022 15:22:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=144
05/25/2022 15:22:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
05/25/2022 15:22:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
05/25/2022 15:23:04 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.23648845024371046 on epoch=145
05/25/2022 15:23:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=146
05/25/2022 15:23:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.45 on epoch=147
05/25/2022 15:23:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=148
05/25/2022 15:23:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=149
05/25/2022 15:23:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=149
05/25/2022 15:23:22 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.22450901264460588 on epoch=149
05/25/2022 15:23:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=150
05/25/2022 15:23:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
05/25/2022 15:23:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=152
05/25/2022 15:23:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
05/25/2022 15:23:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
05/25/2022 15:23:41 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.16732026143790854 on epoch=154
05/25/2022 15:23:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=154
05/25/2022 15:23:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 15:23:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
05/25/2022 15:23:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
05/25/2022 15:23:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
05/25/2022 15:23:59 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.2849954505971521 on epoch=158
05/25/2022 15:24:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=159
05/25/2022 15:24:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=159
05/25/2022 15:24:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
05/25/2022 15:24:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=161
05/25/2022 15:24:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=162
05/25/2022 15:24:18 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.18399830629174121 on epoch=162
05/25/2022 15:24:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
05/25/2022 15:24:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=164
05/25/2022 15:24:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=164
05/25/2022 15:24:28 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=165
05/25/2022 15:24:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
05/25/2022 15:24:36 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=166
05/25/2022 15:24:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.44 on epoch=167
05/25/2022 15:24:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
05/25/2022 15:24:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=169
05/25/2022 15:24:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=169
05/25/2022 15:24:49 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
05/25/2022 15:24:55 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.21172465155867648 on epoch=170
05/25/2022 15:24:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=171
05/25/2022 15:25:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
05/25/2022 15:25:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=173
05/25/2022 15:25:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=174
05/25/2022 15:25:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=174
05/25/2022 15:25:14 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.24682257616310532 on epoch=174
05/25/2022 15:25:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
05/25/2022 15:25:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=176
05/25/2022 15:25:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
05/25/2022 15:25:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=178
05/25/2022 15:25:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=179
05/25/2022 15:25:32 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.24350056731291012 on epoch=179
05/25/2022 15:25:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=179
05/25/2022 15:25:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=180
05/25/2022 15:25:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.40 on epoch=181
05/25/2022 15:25:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
05/25/2022 15:25:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.45 on epoch=183
05/25/2022 15:25:51 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.27637025997681736 on epoch=183
05/25/2022 15:25:54 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=184
05/25/2022 15:25:56 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 15:25:59 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=185
05/25/2022 15:26:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=186
05/25/2022 15:26:04 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=187
05/25/2022 15:26:10 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.30467080299856214 on epoch=187
05/25/2022 15:26:12 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=188
05/25/2022 15:26:15 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
05/25/2022 15:26:17 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
05/25/2022 15:26:20 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.42 on epoch=190
05/25/2022 15:26:23 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.39 on epoch=191
05/25/2022 15:26:28 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=191
05/25/2022 15:26:31 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=192
05/25/2022 15:26:33 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=193
05/25/2022 15:26:36 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=194
05/25/2022 15:26:38 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.48 on epoch=194
05/25/2022 15:26:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=195
05/25/2022 15:26:47 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.3301766336847141 on epoch=195
05/25/2022 15:26:49 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.41 on epoch=196
05/25/2022 15:26:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.40 on epoch=197
05/25/2022 15:26:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
05/25/2022 15:26:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=199
05/25/2022 15:27:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.44 on epoch=199
05/25/2022 15:27:05 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.21306672526184722 on epoch=199
05/25/2022 15:27:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=200
05/25/2022 15:27:10 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=201
05/25/2022 15:27:13 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.48 on epoch=202
05/25/2022 15:27:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
05/25/2022 15:27:18 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=204
05/25/2022 15:27:24 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.20518374124611946 on epoch=204
05/25/2022 15:27:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=204
05/25/2022 15:27:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=205
05/25/2022 15:27:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=206
05/25/2022 15:27:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
05/25/2022 15:27:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=208
05/25/2022 15:27:43 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.35126398279357435 on epoch=208
05/25/2022 15:27:45 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=209
05/25/2022 15:27:48 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=209
05/25/2022 15:27:50 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=210
05/25/2022 15:27:53 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.37 on epoch=211
05/25/2022 15:27:55 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
05/25/2022 15:28:01 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.2345776620507358 on epoch=212
05/25/2022 15:28:04 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.42 on epoch=213
05/25/2022 15:28:06 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.40 on epoch=214
05/25/2022 15:28:09 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.40 on epoch=214
05/25/2022 15:28:11 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=215
05/25/2022 15:28:14 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=216
05/25/2022 15:28:19 - INFO - __main__ - Global step 2600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=216
05/25/2022 15:28:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=217
05/25/2022 15:28:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=218
05/25/2022 15:28:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=219
05/25/2022 15:28:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
05/25/2022 15:28:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.45 on epoch=220
05/25/2022 15:28:38 - INFO - __main__ - Global step 2650 Train loss 0.42 Classification-F1 0.2835654360464501 on epoch=220
05/25/2022 15:28:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
05/25/2022 15:28:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=222
05/25/2022 15:28:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=223
05/25/2022 15:28:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.39 on epoch=224
05/25/2022 15:28:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
05/25/2022 15:28:57 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=224
05/25/2022 15:28:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.41 on epoch=225
05/25/2022 15:29:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=226
05/25/2022 15:29:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=227
05/25/2022 15:29:07 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.46 on epoch=228
05/25/2022 15:29:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=229
05/25/2022 15:29:15 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.2111111111111111 on epoch=229
05/25/2022 15:29:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=229
05/25/2022 15:29:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.38 on epoch=230
05/25/2022 15:29:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=231
05/25/2022 15:29:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=232
05/25/2022 15:29:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=233
05/25/2022 15:29:34 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.29779869253553465 on epoch=233
05/25/2022 15:29:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=234
05/25/2022 15:29:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=234
05/25/2022 15:29:42 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=235
05/25/2022 15:29:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=236
05/25/2022 15:29:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.38 on epoch=237
05/25/2022 15:29:53 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=237
05/25/2022 15:29:55 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.46 on epoch=238
05/25/2022 15:29:58 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=239
05/25/2022 15:30:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=239
05/25/2022 15:30:03 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=240
05/25/2022 15:30:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=241
05/25/2022 15:30:11 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=241
05/25/2022 15:30:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=242
05/25/2022 15:30:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.44 on epoch=243
05/25/2022 15:30:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.37 on epoch=244
05/25/2022 15:30:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.42 on epoch=244
05/25/2022 15:30:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.42 on epoch=245
05/25/2022 15:30:30 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.2923103262086313 on epoch=245
05/25/2022 15:30:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
05/25/2022 15:30:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=247
05/25/2022 15:30:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=248
05/25/2022 15:30:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.41 on epoch=249
05/25/2022 15:30:43 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.41 on epoch=249
05/25/2022 15:30:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:30:44 - INFO - __main__ - Printing 3 examples
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:30:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:30:44 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:30:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:30:44 - INFO - __main__ - Printing 3 examples
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 15:30:44 - INFO - __main__ - ['entailment']
05/25/2022 15:30:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:30:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:30:45 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:30:48 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=249
05/25/2022 15:30:48 - INFO - __main__ - save last model!
05/25/2022 15:30:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 15:30:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 15:30:48 - INFO - __main__ - Printing 3 examples
05/25/2022 15:30:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 15:30:48 - INFO - __main__ - ['contradiction']
05/25/2022 15:30:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 15:30:48 - INFO - __main__ - ['entailment']
05/25/2022 15:30:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 15:30:48 - INFO - __main__ - ['contradiction']
05/25/2022 15:30:48 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:30:49 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:30:50 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 15:31:03 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:31:03 - INFO - __main__ - task name: anli
05/25/2022 15:31:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:31:04 - INFO - __main__ - Starting training!
05/25/2022 15:31:21 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_21_0.3_8_predictions.txt
05/25/2022 15:31:21 - INFO - __main__ - Classification-F1 on test data: 0.1665
05/25/2022 15:31:22 - INFO - __main__ - prefix=anli_64_21, lr=0.3, bsz=8, dev_performance=0.361429600088623, test_performance=0.16654163540885222
05/25/2022 15:31:22 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.2, bsz=8 ...
05/25/2022 15:31:23 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:31:23 - INFO - __main__ - Printing 3 examples
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:31:23 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:31:23 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:31:23 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:31:23 - INFO - __main__ - Printing 3 examples
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/25/2022 15:31:23 - INFO - __main__ - ['entailment']
05/25/2022 15:31:23 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:31:23 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:31:23 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:31:38 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:31:38 - INFO - __main__ - task name: anli
05/25/2022 15:31:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:31:39 - INFO - __main__ - Starting training!
05/25/2022 15:31:42 - INFO - __main__ - Step 10 Global step 10 Train loss 6.63 on epoch=0
05/25/2022 15:31:45 - INFO - __main__ - Step 20 Global step 20 Train loss 3.88 on epoch=1
05/25/2022 15:31:47 - INFO - __main__ - Step 30 Global step 30 Train loss 2.20 on epoch=2
05/25/2022 15:31:50 - INFO - __main__ - Step 40 Global step 40 Train loss 1.32 on epoch=3
05/25/2022 15:31:53 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=4
05/25/2022 15:31:58 - INFO - __main__ - Global step 50 Train loss 3.02 Classification-F1 0.20816919656144406 on epoch=4
05/25/2022 15:31:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20816919656144406 on epoch=4, global_step=50
05/25/2022 15:32:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=4
05/25/2022 15:32:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.80 on epoch=5
05/25/2022 15:32:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=6
05/25/2022 15:32:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.78 on epoch=7
05/25/2022 15:32:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.65 on epoch=8
05/25/2022 15:32:16 - INFO - __main__ - Global step 100 Train loss 0.74 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 15:32:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=9
05/25/2022 15:32:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=9
05/25/2022 15:32:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=10
05/25/2022 15:32:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=11
05/25/2022 15:32:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.64 on epoch=12
05/25/2022 15:32:34 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 15:32:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=13
05/25/2022 15:32:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=14
05/25/2022 15:32:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/25/2022 15:32:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=15
05/25/2022 15:32:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.54 on epoch=16
05/25/2022 15:32:52 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 15:32:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
05/25/2022 15:32:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=18
05/25/2022 15:32:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=19
05/25/2022 15:33:02 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=19
05/25/2022 15:33:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=20
05/25/2022 15:33:09 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 15:33:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/25/2022 15:33:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=22
05/25/2022 15:33:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
05/25/2022 15:33:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=24
05/25/2022 15:33:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/25/2022 15:33:27 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 15:33:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/25/2022 15:33:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/25/2022 15:33:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=27
05/25/2022 15:33:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=28
05/25/2022 15:33:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=29
05/25/2022 15:33:45 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.1647058823529412 on epoch=29
05/25/2022 15:33:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=29
05/25/2022 15:33:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=30
05/25/2022 15:33:53 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=31
05/25/2022 15:33:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=32
05/25/2022 15:33:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=33
05/25/2022 15:34:03 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 15:34:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=34
05/25/2022 15:34:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=34
05/25/2022 15:34:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
05/25/2022 15:34:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=36
05/25/2022 15:34:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=37
05/25/2022 15:34:22 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.21937539186280988 on epoch=37
05/25/2022 15:34:22 - INFO - __main__ - Saving model with best Classification-F1: 0.20816919656144406 -> 0.21937539186280988 on epoch=37, global_step=450
05/25/2022 15:34:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/25/2022 15:34:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=39
05/25/2022 15:34:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
05/25/2022 15:34:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=40
05/25/2022 15:34:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
05/25/2022 15:34:40 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 15:34:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=42
05/25/2022 15:34:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=43
05/25/2022 15:34:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
05/25/2022 15:34:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=44
05/25/2022 15:34:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
05/25/2022 15:34:58 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.2249456298124963 on epoch=45
05/25/2022 15:34:58 - INFO - __main__ - Saving model with best Classification-F1: 0.21937539186280988 -> 0.2249456298124963 on epoch=45, global_step=550
05/25/2022 15:35:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/25/2022 15:35:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
05/25/2022 15:35:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/25/2022 15:35:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=49
05/25/2022 15:35:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/25/2022 15:35:17 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2111111111111111 on epoch=49
05/25/2022 15:35:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=50
05/25/2022 15:35:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=51
05/25/2022 15:35:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=52
05/25/2022 15:35:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=53
05/25/2022 15:35:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
05/25/2022 15:35:35 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 15:35:38 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=54
05/25/2022 15:35:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=55
05/25/2022 15:35:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=56
05/25/2022 15:35:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=57
05/25/2022 15:35:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=58
05/25/2022 15:35:54 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.29082824362960186 on epoch=58
05/25/2022 15:35:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2249456298124963 -> 0.29082824362960186 on epoch=58, global_step=700
05/25/2022 15:35:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
05/25/2022 15:35:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
05/25/2022 15:36:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=60
05/25/2022 15:36:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
05/25/2022 15:36:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
05/25/2022 15:36:13 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 15:36:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
05/25/2022 15:36:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=64
05/25/2022 15:36:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=64
05/25/2022 15:36:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=65
05/25/2022 15:36:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
05/25/2022 15:36:31 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 15:36:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
05/25/2022 15:36:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=68
05/25/2022 15:36:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=69
05/25/2022 15:36:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
05/25/2022 15:36:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=70
05/25/2022 15:36:49 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 15:36:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=71
05/25/2022 15:36:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=72
05/25/2022 15:36:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/25/2022 15:37:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
05/25/2022 15:37:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
05/25/2022 15:37:07 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=74
05/25/2022 15:37:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
05/25/2022 15:37:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=76
05/25/2022 15:37:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=77
05/25/2022 15:37:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=78
05/25/2022 15:37:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
05/25/2022 15:37:25 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=79
05/25/2022 15:37:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
05/25/2022 15:37:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=80
05/25/2022 15:37:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=81
05/25/2022 15:37:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
05/25/2022 15:37:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
05/25/2022 15:37:43 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.23778880921738063 on epoch=83
05/25/2022 15:37:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=84
05/25/2022 15:37:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=84
05/25/2022 15:37:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=85
05/25/2022 15:37:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=86
05/25/2022 15:37:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
05/25/2022 15:38:01 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.23075052288535433 on epoch=87
05/25/2022 15:38:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/25/2022 15:38:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=89
05/25/2022 15:38:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
05/25/2022 15:38:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=90
05/25/2022 15:38:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=91
05/25/2022 15:38:20 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 15:38:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=92
05/25/2022 15:38:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=93
05/25/2022 15:38:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=94
05/25/2022 15:38:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=94
05/25/2022 15:38:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=95
05/25/2022 15:38:39 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.35474378944005114 on epoch=95
05/25/2022 15:38:39 - INFO - __main__ - Saving model with best Classification-F1: 0.29082824362960186 -> 0.35474378944005114 on epoch=95, global_step=1150
05/25/2022 15:38:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=96
05/25/2022 15:38:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=97
05/25/2022 15:38:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
05/25/2022 15:38:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=99
05/25/2022 15:38:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=99
05/25/2022 15:38:58 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.1775766716943188 on epoch=99
05/25/2022 15:39:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=100
05/25/2022 15:39:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=101
05/25/2022 15:39:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
05/25/2022 15:39:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=103
05/25/2022 15:39:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=104
05/25/2022 15:39:16 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.1965714285714286 on epoch=104
05/25/2022 15:39:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=104
05/25/2022 15:39:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=105
05/25/2022 15:39:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
05/25/2022 15:39:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.47 on epoch=107
05/25/2022 15:39:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
05/25/2022 15:39:34 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.21374764595103576 on epoch=108
05/25/2022 15:39:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.46 on epoch=109
05/25/2022 15:39:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=109
05/25/2022 15:39:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
05/25/2022 15:39:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=111
05/25/2022 15:39:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=112
05/25/2022 15:39:52 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 15:39:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=113
05/25/2022 15:39:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
05/25/2022 15:40:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=114
05/25/2022 15:40:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=115
05/25/2022 15:40:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=116
05/25/2022 15:40:11 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 15:40:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=117
05/25/2022 15:40:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=118
05/25/2022 15:40:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
05/25/2022 15:40:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
05/25/2022 15:40:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 15:40:29 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.19849369752030363 on epoch=120
05/25/2022 15:40:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
05/25/2022 15:40:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
05/25/2022 15:40:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=123
05/25/2022 15:40:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=124
05/25/2022 15:40:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=124
05/25/2022 15:40:48 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.19849369752030363 on epoch=124
05/25/2022 15:40:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=125
05/25/2022 15:40:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=126
05/25/2022 15:40:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
05/25/2022 15:40:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=128
05/25/2022 15:41:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=129
05/25/2022 15:41:06 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.178080012725682 on epoch=129
05/25/2022 15:41:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=129
05/25/2022 15:41:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=130
05/25/2022 15:41:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.39 on epoch=131
05/25/2022 15:41:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=132
05/25/2022 15:41:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
05/25/2022 15:41:24 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.27391470871798934 on epoch=133
05/25/2022 15:41:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=134
05/25/2022 15:41:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
05/25/2022 15:41:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
05/25/2022 15:41:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=136
05/25/2022 15:41:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=137
05/25/2022 15:41:42 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 15:41:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=138
05/25/2022 15:41:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=139
05/25/2022 15:41:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=139
05/25/2022 15:41:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=140
05/25/2022 15:41:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=141
05/25/2022 15:42:00 - INFO - __main__ - Global step 1700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 15:42:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
05/25/2022 15:42:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
05/25/2022 15:42:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=144
05/25/2022 15:42:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
05/25/2022 15:42:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=145
05/25/2022 15:42:19 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.2585495546121159 on epoch=145
05/25/2022 15:42:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=146
05/25/2022 15:42:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
05/25/2022 15:42:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=148
05/25/2022 15:42:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
05/25/2022 15:42:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
05/25/2022 15:42:37 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.2355632671422145 on epoch=149
05/25/2022 15:42:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.44 on epoch=150
05/25/2022 15:42:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
05/25/2022 15:42:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=152
05/25/2022 15:42:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=153
05/25/2022 15:42:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=154
05/25/2022 15:42:55 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=154
05/25/2022 15:42:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
05/25/2022 15:43:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=155
05/25/2022 15:43:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=156
05/25/2022 15:43:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=157
05/25/2022 15:43:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
05/25/2022 15:43:13 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.27649337246860467 on epoch=158
05/25/2022 15:43:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.42 on epoch=159
05/25/2022 15:43:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=159
05/25/2022 15:43:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
05/25/2022 15:43:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
05/25/2022 15:43:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=162
05/25/2022 15:43:32 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.29632317000738057 on epoch=162
05/25/2022 15:43:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
05/25/2022 15:43:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=164
05/25/2022 15:43:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
05/25/2022 15:43:42 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=165
05/25/2022 15:43:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=166
05/25/2022 15:43:50 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=166
05/25/2022 15:43:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.45 on epoch=167
05/25/2022 15:43:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.40 on epoch=168
05/25/2022 15:43:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.39 on epoch=169
05/25/2022 15:44:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
05/25/2022 15:44:03 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.47 on epoch=170
05/25/2022 15:44:08 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=170
05/25/2022 15:44:11 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=171
05/25/2022 15:44:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.44 on epoch=172
05/25/2022 15:44:16 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.48 on epoch=173
05/25/2022 15:44:19 - INFO - __main__ - Step 2090 Global step 2090 Train loss 1.14 on epoch=174
05/25/2022 15:44:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 1.89 on epoch=174
05/25/2022 15:44:27 - INFO - __main__ - Global step 2100 Train loss 0.87 Classification-F1 0.18656716417910446 on epoch=174
05/25/2022 15:44:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.65 on epoch=175
05/25/2022 15:44:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.52 on epoch=176
05/25/2022 15:44:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.89 on epoch=177
05/25/2022 15:44:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.69 on epoch=178
05/25/2022 15:44:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=179
05/25/2022 15:44:45 - INFO - __main__ - Global step 2150 Train loss 0.63 Classification-F1 0.19872393401805166 on epoch=179
05/25/2022 15:44:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.51 on epoch=179
05/25/2022 15:44:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.73 on epoch=180
05/25/2022 15:44:53 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.52 on epoch=181
05/25/2022 15:44:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.48 on epoch=182
05/25/2022 15:44:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.52 on epoch=183
05/25/2022 15:45:03 - INFO - __main__ - Global step 2200 Train loss 0.55 Classification-F1 0.19607843137254902 on epoch=183
05/25/2022 15:45:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.48 on epoch=184
05/25/2022 15:45:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.59 on epoch=184
05/25/2022 15:45:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=185
05/25/2022 15:45:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=186
05/25/2022 15:45:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.49 on epoch=187
05/25/2022 15:45:21 - INFO - __main__ - Global step 2250 Train loss 0.49 Classification-F1 0.18627450980392157 on epoch=187
05/25/2022 15:45:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.55 on epoch=188
05/25/2022 15:45:26 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.48 on epoch=189
05/25/2022 15:45:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=189
05/25/2022 15:45:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=190
05/25/2022 15:45:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=191
05/25/2022 15:45:39 - INFO - __main__ - Global step 2300 Train loss 0.47 Classification-F1 0.18962865304328722 on epoch=191
05/25/2022 15:45:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=192
05/25/2022 15:45:44 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.47 on epoch=193
05/25/2022 15:45:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=194
05/25/2022 15:45:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.49 on epoch=194
05/25/2022 15:45:52 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.51 on epoch=195
05/25/2022 15:45:57 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.1986376620522962 on epoch=195
05/25/2022 15:45:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.43 on epoch=196
05/25/2022 15:46:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
05/25/2022 15:46:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=198
05/25/2022 15:46:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.44 on epoch=199
05/25/2022 15:46:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.51 on epoch=199
05/25/2022 15:46:15 - INFO - __main__ - Global step 2400 Train loss 0.46 Classification-F1 0.1986376620522962 on epoch=199
05/25/2022 15:46:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=200
05/25/2022 15:46:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=201
05/25/2022 15:46:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=202
05/25/2022 15:46:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=203
05/25/2022 15:46:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.41 on epoch=204
05/25/2022 15:46:33 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.24401316536147996 on epoch=204
05/25/2022 15:46:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
05/25/2022 15:46:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=205
05/25/2022 15:46:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.38 on epoch=206
05/25/2022 15:46:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=207
05/25/2022 15:46:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.44 on epoch=208
05/25/2022 15:46:51 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.2395506792058516 on epoch=208
05/25/2022 15:46:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=209
05/25/2022 15:46:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.41 on epoch=209
05/25/2022 15:46:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.44 on epoch=210
05/25/2022 15:47:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.40 on epoch=211
05/25/2022 15:47:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=212
05/25/2022 15:47:09 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.22497779599644738 on epoch=212
05/25/2022 15:47:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 15:47:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=214
05/25/2022 15:47:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=214
05/25/2022 15:47:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.41 on epoch=215
05/25/2022 15:47:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.38 on epoch=216
05/25/2022 15:47:28 - INFO - __main__ - Global step 2600 Train loss 0.40 Classification-F1 0.20426303854875283 on epoch=216
05/25/2022 15:47:30 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=217
05/25/2022 15:47:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=218
05/25/2022 15:47:35 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=219
05/25/2022 15:47:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=219
05/25/2022 15:47:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.43 on epoch=220
05/25/2022 15:47:46 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.2131519274376417 on epoch=220
05/25/2022 15:47:49 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.40 on epoch=221
05/25/2022 15:47:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.42 on epoch=222
05/25/2022 15:47:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=223
05/25/2022 15:47:56 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.39 on epoch=224
05/25/2022 15:47:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.45 on epoch=224
05/25/2022 15:48:04 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.19631618453721345 on epoch=224
05/25/2022 15:48:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.40 on epoch=225
05/25/2022 15:48:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
05/25/2022 15:48:12 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.48 on epoch=227
05/25/2022 15:48:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=228
05/25/2022 15:48:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.38 on epoch=229
05/25/2022 15:48:23 - INFO - __main__ - Global step 2750 Train loss 0.42 Classification-F1 0.2596179714823783 on epoch=229
05/25/2022 15:48:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=229
05/25/2022 15:48:28 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.50 on epoch=230
05/25/2022 15:48:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=231
05/25/2022 15:48:33 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=232
05/25/2022 15:48:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=233
05/25/2022 15:48:41 - INFO - __main__ - Global step 2800 Train loss 0.42 Classification-F1 0.2531037771529114 on epoch=233
05/25/2022 15:48:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.39 on epoch=234
05/25/2022 15:48:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.41 on epoch=234
05/25/2022 15:48:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.38 on epoch=235
05/25/2022 15:48:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
05/25/2022 15:48:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.48 on epoch=237
05/25/2022 15:49:00 - INFO - __main__ - Global step 2850 Train loss 0.41 Classification-F1 0.21930058967096003 on epoch=237
05/25/2022 15:49:02 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.40 on epoch=238
05/25/2022 15:49:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
05/25/2022 15:49:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=239
05/25/2022 15:49:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.45 on epoch=240
05/25/2022 15:49:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
05/25/2022 15:49:18 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.2131519274376417 on epoch=241
05/25/2022 15:49:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=242
05/25/2022 15:49:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=243
05/25/2022 15:49:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.40 on epoch=244
05/25/2022 15:49:28 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=244
05/25/2022 15:49:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=245
05/25/2022 15:49:37 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.21930058967096003 on epoch=245
05/25/2022 15:49:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=246
05/25/2022 15:49:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=247
05/25/2022 15:49:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=248
05/25/2022 15:49:47 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.40 on epoch=249
05/25/2022 15:49:50 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=249
05/25/2022 15:49:51 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:49:51 - INFO - __main__ - Printing 3 examples
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:49:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:49:51 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:49:51 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:49:51 - INFO - __main__ - Printing 3 examples
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 15:49:51 - INFO - __main__ - ['neutral']
05/25/2022 15:49:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:49:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:49:51 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:49:55 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.20370370370370372 on epoch=249
05/25/2022 15:49:55 - INFO - __main__ - save last model!
05/25/2022 15:49:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 15:49:55 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 15:49:55 - INFO - __main__ - Printing 3 examples
05/25/2022 15:49:55 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 15:49:55 - INFO - __main__ - ['contradiction']
05/25/2022 15:49:55 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 15:49:55 - INFO - __main__ - ['entailment']
05/25/2022 15:49:55 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 15:49:55 - INFO - __main__ - ['contradiction']
05/25/2022 15:49:55 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:49:56 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:49:57 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 15:50:06 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:50:06 - INFO - __main__ - task name: anli
05/25/2022 15:50:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:50:07 - INFO - __main__ - Starting training!
05/25/2022 15:50:26 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_21_0.2_8_predictions.txt
05/25/2022 15:50:26 - INFO - __main__ - Classification-F1 on test data: 0.1807
05/25/2022 15:50:27 - INFO - __main__ - prefix=anli_64_21, lr=0.2, bsz=8, dev_performance=0.35474378944005114, test_performance=0.18069297184420016
05/25/2022 15:50:27 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.5, bsz=8 ...
05/25/2022 15:50:28 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:50:28 - INFO - __main__ - Printing 3 examples
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:50:28 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:50:28 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 15:50:28 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 15:50:28 - INFO - __main__ - Printing 3 examples
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 15:50:28 - INFO - __main__ - ['neutral']
05/25/2022 15:50:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 15:50:28 - INFO - __main__ - Tokenizing Output ...
05/25/2022 15:50:28 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 15:50:47 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 15:50:47 - INFO - __main__ - task name: anli
05/25/2022 15:50:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 15:50:48 - INFO - __main__ - Starting training!
05/25/2022 15:50:51 - INFO - __main__ - Step 10 Global step 10 Train loss 6.44 on epoch=0
05/25/2022 15:50:54 - INFO - __main__ - Step 20 Global step 20 Train loss 2.04 on epoch=1
05/25/2022 15:50:57 - INFO - __main__ - Step 30 Global step 30 Train loss 1.26 on epoch=2
05/25/2022 15:50:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.14 on epoch=3
05/25/2022 15:51:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=4
05/25/2022 15:51:06 - INFO - __main__ - Global step 50 Train loss 2.33 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 15:51:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 15:51:08 - INFO - __main__ - Step 60 Global step 60 Train loss 0.81 on epoch=4
05/25/2022 15:51:11 - INFO - __main__ - Step 70 Global step 70 Train loss 0.84 on epoch=5
05/25/2022 15:51:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.74 on epoch=6
05/25/2022 15:51:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.71 on epoch=7
05/25/2022 15:51:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=8
05/25/2022 15:51:23 - INFO - __main__ - Global step 100 Train loss 0.76 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 15:51:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.61 on epoch=9
05/25/2022 15:51:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.61 on epoch=9
05/25/2022 15:51:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=10
05/25/2022 15:51:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.57 on epoch=11
05/25/2022 15:51:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=12
05/25/2022 15:51:40 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 15:51:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 15:51:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=14
05/25/2022 15:51:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=14
05/25/2022 15:51:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=15
05/25/2022 15:51:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=16
05/25/2022 15:51:57 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 15:51:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=17
05/25/2022 15:52:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
05/25/2022 15:52:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=19
05/25/2022 15:52:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/25/2022 15:52:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=20
05/25/2022 15:52:14 - INFO - __main__ - Global step 250 Train loss 0.55 Classification-F1 0.25267765265156455 on epoch=20
05/25/2022 15:52:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.25267765265156455 on epoch=20, global_step=250
05/25/2022 15:52:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/25/2022 15:52:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=22
05/25/2022 15:52:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
05/25/2022 15:52:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=24
05/25/2022 15:52:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=24
05/25/2022 15:52:30 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 15:52:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=25
05/25/2022 15:52:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=26
05/25/2022 15:52:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.52 on epoch=27
05/25/2022 15:52:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=28
05/25/2022 15:52:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=29
05/25/2022 15:52:47 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 15:52:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=29
05/25/2022 15:52:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=30
05/25/2022 15:52:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=31
05/25/2022 15:52:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=32
05/25/2022 15:53:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=33
05/25/2022 15:53:04 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 15:53:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=34
05/25/2022 15:53:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
05/25/2022 15:53:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=35
05/25/2022 15:53:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=36
05/25/2022 15:53:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=37
05/25/2022 15:53:22 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 15:53:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/25/2022 15:53:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=39
05/25/2022 15:53:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
05/25/2022 15:53:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=40
05/25/2022 15:53:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=41
05/25/2022 15:53:39 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 15:53:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=42
05/25/2022 15:53:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/25/2022 15:53:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=44
05/25/2022 15:53:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=44
05/25/2022 15:53:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=45
05/25/2022 15:53:57 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.1862099253403601 on epoch=45
05/25/2022 15:54:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=46
05/25/2022 15:54:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=47
05/25/2022 15:54:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
05/25/2022 15:54:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=49
05/25/2022 15:54:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=49
05/25/2022 15:54:15 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 15:54:18 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=50
05/25/2022 15:54:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=51
05/25/2022 15:54:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=52
05/25/2022 15:54:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=53
05/25/2022 15:54:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=54
05/25/2022 15:54:33 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 15:54:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=54
05/25/2022 15:54:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=55
05/25/2022 15:54:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
05/25/2022 15:54:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=57
05/25/2022 15:54:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=58
05/25/2022 15:54:51 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 15:54:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=59
05/25/2022 15:54:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=59
05/25/2022 15:54:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=60
05/25/2022 15:55:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=61
05/25/2022 15:55:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=62
05/25/2022 15:55:09 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 15:55:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=63
05/25/2022 15:55:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=64
05/25/2022 15:55:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=64
05/25/2022 15:55:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=65
05/25/2022 15:55:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=66
05/25/2022 15:55:28 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.228816252411758 on epoch=66
05/25/2022 15:55:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=67
05/25/2022 15:55:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/25/2022 15:55:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=69
05/25/2022 15:55:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=69
05/25/2022 15:55:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=70
05/25/2022 15:55:46 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 15:55:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=71
05/25/2022 15:55:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=72
05/25/2022 15:55:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 15:55:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=74
05/25/2022 15:55:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=74
05/25/2022 15:56:04 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.3569094304388422 on epoch=74
05/25/2022 15:56:04 - INFO - __main__ - Saving model with best Classification-F1: 0.25267765265156455 -> 0.3569094304388422 on epoch=74, global_step=900
05/25/2022 15:56:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=75
05/25/2022 15:56:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
05/25/2022 15:56:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=77
05/25/2022 15:56:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=78
05/25/2022 15:56:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=79
05/25/2022 15:56:23 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.36958787600842175 on epoch=79
05/25/2022 15:56:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3569094304388422 -> 0.36958787600842175 on epoch=79, global_step=950
05/25/2022 15:56:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=79
05/25/2022 15:56:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=80
05/25/2022 15:56:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=81
05/25/2022 15:56:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=82
05/25/2022 15:56:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
05/25/2022 15:56:42 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.16732026143790854 on epoch=83
05/25/2022 15:56:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=84
05/25/2022 15:56:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=84
05/25/2022 15:56:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.58 on epoch=85
05/25/2022 15:56:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=86
05/25/2022 15:56:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.48 on epoch=87
05/25/2022 15:57:01 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.18627450980392157 on epoch=87
05/25/2022 15:57:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=88
05/25/2022 15:57:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.47 on epoch=89
05/25/2022 15:57:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.52 on epoch=89
05/25/2022 15:57:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
05/25/2022 15:57:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/25/2022 15:57:19 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.18892001244942422 on epoch=91
05/25/2022 15:57:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=92
05/25/2022 15:57:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 15:57:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=94
05/25/2022 15:57:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=94
05/25/2022 15:57:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.55 on epoch=95
05/25/2022 15:57:37 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.30328074162341906 on epoch=95
05/25/2022 15:57:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=96
05/25/2022 15:57:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=97
05/25/2022 15:57:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=98
05/25/2022 15:57:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=99
05/25/2022 15:57:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=99
05/25/2022 15:57:56 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.2304864309899375 on epoch=99
05/25/2022 15:57:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=100
05/25/2022 15:58:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=101
05/25/2022 15:58:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=102
05/25/2022 15:58:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/25/2022 15:58:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=104
05/25/2022 15:58:15 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.19849369752030363 on epoch=104
05/25/2022 15:58:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=104
05/25/2022 15:58:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=105
05/25/2022 15:58:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.45 on epoch=106
05/25/2022 15:58:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.55 on epoch=107
05/25/2022 15:58:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=108
05/25/2022 15:58:33 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.18739100911587436 on epoch=108
05/25/2022 15:58:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=109
05/25/2022 15:58:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=109
05/25/2022 15:58:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.49 on epoch=110
05/25/2022 15:58:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.33 on epoch=111
05/25/2022 15:58:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.93 on epoch=112
05/25/2022 15:58:52 - INFO - __main__ - Global step 1350 Train loss 0.94 Classification-F1 0.25927219796215434 on epoch=112
05/25/2022 15:58:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.06 on epoch=113
05/25/2022 15:58:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.67 on epoch=114
05/25/2022 15:59:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=114
05/25/2022 15:59:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=115
05/25/2022 15:59:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
05/25/2022 15:59:10 - INFO - __main__ - Global step 1400 Train loss 0.82 Classification-F1 0.1876078431372549 on epoch=116
05/25/2022 15:59:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=117
05/25/2022 15:59:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
05/25/2022 15:59:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=119
05/25/2022 15:59:21 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.47 on epoch=119
05/25/2022 15:59:23 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=120
05/25/2022 15:59:29 - INFO - __main__ - Global step 1450 Train loss 0.48 Classification-F1 0.23085608022846515 on epoch=120
05/25/2022 15:59:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=121
05/25/2022 15:59:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=122
05/25/2022 15:59:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.52 on epoch=123
05/25/2022 15:59:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=124
05/25/2022 15:59:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.48 on epoch=124
05/25/2022 15:59:48 - INFO - __main__ - Global step 1500 Train loss 0.48 Classification-F1 0.21946169772256727 on epoch=124
05/25/2022 15:59:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.47 on epoch=125
05/25/2022 15:59:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=126
05/25/2022 15:59:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.46 on epoch=127
05/25/2022 15:59:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.48 on epoch=128
05/25/2022 16:00:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=129
05/25/2022 16:00:07 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.21904088606103453 on epoch=129
05/25/2022 16:00:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.46 on epoch=129
05/25/2022 16:00:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=130
05/25/2022 16:00:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=131
05/25/2022 16:00:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=132
05/25/2022 16:00:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=133
05/25/2022 16:00:25 - INFO - __main__ - Global step 1600 Train loss 0.47 Classification-F1 0.20803671225728196 on epoch=133
05/25/2022 16:00:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=134
05/25/2022 16:00:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=134
05/25/2022 16:00:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=135
05/25/2022 16:00:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
05/25/2022 16:00:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=137
05/25/2022 16:00:44 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.20857052498418724 on epoch=137
05/25/2022 16:00:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=138
05/25/2022 16:00:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=139
05/25/2022 16:00:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
05/25/2022 16:00:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.45 on epoch=140
05/25/2022 16:00:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=141
05/25/2022 16:01:03 - INFO - __main__ - Global step 1700 Train loss 0.46 Classification-F1 0.23085608022846515 on epoch=141
05/25/2022 16:01:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=142
05/25/2022 16:01:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
05/25/2022 16:01:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=144
05/25/2022 16:01:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=144
05/25/2022 16:01:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=145
05/25/2022 16:01:21 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.2287760277152755 on epoch=145
05/25/2022 16:01:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
05/25/2022 16:01:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=147
05/25/2022 16:01:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=148
05/25/2022 16:01:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=149
05/25/2022 16:01:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=149
05/25/2022 16:01:40 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.24561711143989626 on epoch=149
05/25/2022 16:01:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=150
05/25/2022 16:01:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=151
05/25/2022 16:01:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=152
05/25/2022 16:01:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=153
05/25/2022 16:01:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.48 on epoch=154
05/25/2022 16:01:59 - INFO - __main__ - Global step 1850 Train loss 0.46 Classification-F1 0.23578467002583092 on epoch=154
05/25/2022 16:02:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=154
05/25/2022 16:02:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=155
05/25/2022 16:02:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=156
05/25/2022 16:02:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=157
05/25/2022 16:02:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=158
05/25/2022 16:02:17 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.23394740658489122 on epoch=158
05/25/2022 16:02:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=159
05/25/2022 16:02:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
05/25/2022 16:02:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=160
05/25/2022 16:02:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=161
05/25/2022 16:02:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.49 on epoch=162
05/25/2022 16:02:36 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.22162100612602767 on epoch=162
05/25/2022 16:02:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
05/25/2022 16:02:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=164
05/25/2022 16:02:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=164
05/25/2022 16:02:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=165
05/25/2022 16:02:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.51 on epoch=166
05/25/2022 16:02:55 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.2451378504010083 on epoch=166
05/25/2022 16:02:57 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=167
05/25/2022 16:03:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
05/25/2022 16:03:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.50 on epoch=169
05/25/2022 16:03:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=169
05/25/2022 16:03:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.45 on epoch=170
05/25/2022 16:03:13 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.2167845280188578 on epoch=170
05/25/2022 16:03:16 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=171
05/25/2022 16:03:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
05/25/2022 16:03:21 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
05/25/2022 16:03:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
05/25/2022 16:03:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=174
05/25/2022 16:03:32 - INFO - __main__ - Global step 2100 Train loss 0.45 Classification-F1 0.27188094717230943 on epoch=174
05/25/2022 16:03:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.50 on epoch=175
05/25/2022 16:03:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=176
05/25/2022 16:03:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=177
05/25/2022 16:03:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=178
05/25/2022 16:03:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.45 on epoch=179
05/25/2022 16:03:51 - INFO - __main__ - Global step 2150 Train loss 0.46 Classification-F1 0.22162100612602767 on epoch=179
05/25/2022 16:03:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=179
05/25/2022 16:03:56 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.46 on epoch=180
05/25/2022 16:03:59 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.49 on epoch=181
05/25/2022 16:04:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.46 on epoch=182
05/25/2022 16:04:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=183
05/25/2022 16:04:09 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.23620418631587659 on epoch=183
05/25/2022 16:04:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.48 on epoch=184
05/25/2022 16:04:15 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=184
05/25/2022 16:04:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=185
05/25/2022 16:04:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=186
05/25/2022 16:04:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.51 on epoch=187
05/25/2022 16:04:28 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.22760899231487466 on epoch=187
05/25/2022 16:04:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
05/25/2022 16:04:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=189
05/25/2022 16:04:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=189
05/25/2022 16:04:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=190
05/25/2022 16:04:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.49 on epoch=191
05/25/2022 16:04:47 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.26842425670633524 on epoch=191
05/25/2022 16:04:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.41 on epoch=192
05/25/2022 16:04:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=193
05/25/2022 16:04:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.48 on epoch=194
05/25/2022 16:04:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.42 on epoch=194
05/25/2022 16:05:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.49 on epoch=195
05/25/2022 16:05:06 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.2825325325325325 on epoch=195
05/25/2022 16:05:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.48 on epoch=196
05/25/2022 16:05:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.49 on epoch=197
05/25/2022 16:05:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=198
05/25/2022 16:05:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.46 on epoch=199
05/25/2022 16:05:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.47 on epoch=199
05/25/2022 16:05:25 - INFO - __main__ - Global step 2400 Train loss 0.47 Classification-F1 0.25184743543647414 on epoch=199
05/25/2022 16:05:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=200
05/25/2022 16:05:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.49 on epoch=201
05/25/2022 16:05:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.49 on epoch=202
05/25/2022 16:05:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.42 on epoch=203
05/25/2022 16:05:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=204
05/25/2022 16:05:43 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.22836920377903983 on epoch=204
05/25/2022 16:05:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.50 on epoch=204
05/25/2022 16:05:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=205
05/25/2022 16:05:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=206
05/25/2022 16:05:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.49 on epoch=207
05/25/2022 16:05:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.47 on epoch=208
05/25/2022 16:06:02 - INFO - __main__ - Global step 2500 Train loss 0.47 Classification-F1 0.2224560249876706 on epoch=208
05/25/2022 16:06:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.46 on epoch=209
05/25/2022 16:06:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.46 on epoch=209
05/25/2022 16:06:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.48 on epoch=210
05/25/2022 16:06:12 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=211
05/25/2022 16:06:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=212
05/25/2022 16:06:21 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.19839059168909237 on epoch=212
05/25/2022 16:06:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
05/25/2022 16:06:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.45 on epoch=214
05/25/2022 16:06:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
05/25/2022 16:06:31 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.48 on epoch=215
05/25/2022 16:06:34 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.49 on epoch=216
05/25/2022 16:06:39 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.297652964319631 on epoch=216
05/25/2022 16:06:42 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.46 on epoch=217
05/25/2022 16:06:45 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.45 on epoch=218
05/25/2022 16:06:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.44 on epoch=219
05/25/2022 16:06:50 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=219
05/25/2022 16:06:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.46 on epoch=220
05/25/2022 16:06:58 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.2977995837050253 on epoch=220
05/25/2022 16:07:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.47 on epoch=221
05/25/2022 16:07:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.45 on epoch=222
05/25/2022 16:07:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=223
05/25/2022 16:07:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.47 on epoch=224
05/25/2022 16:07:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.50 on epoch=224
05/25/2022 16:07:17 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.2604495367833261 on epoch=224
05/25/2022 16:07:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.49 on epoch=225
05/25/2022 16:07:22 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.48 on epoch=226
05/25/2022 16:07:25 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.45 on epoch=227
05/25/2022 16:07:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=228
05/25/2022 16:07:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.46 on epoch=229
05/25/2022 16:07:36 - INFO - __main__ - Global step 2750 Train loss 0.47 Classification-F1 0.22539151225343693 on epoch=229
05/25/2022 16:07:38 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.45 on epoch=229
05/25/2022 16:07:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.48 on epoch=230
05/25/2022 16:07:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=231
05/25/2022 16:07:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.48 on epoch=232
05/25/2022 16:07:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=233
05/25/2022 16:07:55 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.2807515739186801 on epoch=233
05/25/2022 16:07:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.45 on epoch=234
05/25/2022 16:08:00 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.50 on epoch=234
05/25/2022 16:08:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.45 on epoch=235
05/25/2022 16:08:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.48 on epoch=236
05/25/2022 16:08:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.47 on epoch=237
05/25/2022 16:08:13 - INFO - __main__ - Global step 2850 Train loss 0.47 Classification-F1 0.196116359274254 on epoch=237
05/25/2022 16:08:16 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=238
05/25/2022 16:08:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
05/25/2022 16:08:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=239
05/25/2022 16:08:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.47 on epoch=240
05/25/2022 16:08:26 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.44 on epoch=241
05/25/2022 16:08:32 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.32489335207592845 on epoch=241
05/25/2022 16:08:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.47 on epoch=242
05/25/2022 16:08:37 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=243
05/25/2022 16:08:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.46 on epoch=244
05/25/2022 16:08:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=244
05/25/2022 16:08:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.46 on epoch=245
05/25/2022 16:08:51 - INFO - __main__ - Global step 2950 Train loss 0.45 Classification-F1 0.3806815378210889 on epoch=245
05/25/2022 16:08:51 - INFO - __main__ - Saving model with best Classification-F1: 0.36958787600842175 -> 0.3806815378210889 on epoch=245, global_step=2950
05/25/2022 16:08:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
05/25/2022 16:08:56 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.48 on epoch=247
05/25/2022 16:08:59 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.47 on epoch=248
05/25/2022 16:09:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.51 on epoch=249
05/25/2022 16:09:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.46 on epoch=249
05/25/2022 16:09:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:09:05 - INFO - __main__ - Printing 3 examples
05/25/2022 16:09:05 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:09:05 - INFO - __main__ - ['neutral']
05/25/2022 16:09:05 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:09:05 - INFO - __main__ - ['neutral']
05/25/2022 16:09:05 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:09:05 - INFO - __main__ - ['neutral']
05/25/2022 16:09:05 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:09:05 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:09:06 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:09:06 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:09:06 - INFO - __main__ - Printing 3 examples
05/25/2022 16:09:06 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:09:06 - INFO - __main__ - ['neutral']
05/25/2022 16:09:06 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:09:06 - INFO - __main__ - ['neutral']
05/25/2022 16:09:06 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:09:06 - INFO - __main__ - ['neutral']
05/25/2022 16:09:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:09:06 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:09:06 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:09:10 - INFO - __main__ - Global step 3000 Train loss 0.47 Classification-F1 0.3180477130646741 on epoch=249
05/25/2022 16:09:10 - INFO - __main__ - save last model!
05/25/2022 16:09:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 16:09:10 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 16:09:10 - INFO - __main__ - Printing 3 examples
05/25/2022 16:09:10 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 16:09:10 - INFO - __main__ - ['contradiction']
05/25/2022 16:09:10 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 16:09:10 - INFO - __main__ - ['entailment']
05/25/2022 16:09:10 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 16:09:10 - INFO - __main__ - ['contradiction']
05/25/2022 16:09:10 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:09:10 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:09:11 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 16:09:21 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:09:21 - INFO - __main__ - task name: anli
05/25/2022 16:09:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:09:22 - INFO - __main__ - Starting training!
05/25/2022 16:09:42 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_42_0.5_8_predictions.txt
05/25/2022 16:09:42 - INFO - __main__ - Classification-F1 on test data: 0.2718
05/25/2022 16:09:42 - INFO - __main__ - prefix=anli_64_42, lr=0.5, bsz=8, dev_performance=0.3806815378210889, test_performance=0.2717957633995696
05/25/2022 16:09:42 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.4, bsz=8 ...
05/25/2022 16:09:43 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:09:43 - INFO - __main__ - Printing 3 examples
05/25/2022 16:09:43 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:09:43 - INFO - __main__ - ['neutral']
05/25/2022 16:09:43 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:09:43 - INFO - __main__ - ['neutral']
05/25/2022 16:09:43 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:09:43 - INFO - __main__ - ['neutral']
05/25/2022 16:09:43 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:09:43 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:09:44 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:09:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:09:44 - INFO - __main__ - Printing 3 examples
05/25/2022 16:09:44 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:09:44 - INFO - __main__ - ['neutral']
05/25/2022 16:09:44 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:09:44 - INFO - __main__ - ['neutral']
05/25/2022 16:09:44 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:09:44 - INFO - __main__ - ['neutral']
05/25/2022 16:09:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:09:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:09:44 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:10:02 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:10:02 - INFO - __main__ - task name: anli
05/25/2022 16:10:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:10:02 - INFO - __main__ - Starting training!
05/25/2022 16:10:05 - INFO - __main__ - Step 10 Global step 10 Train loss 6.55 on epoch=0
05/25/2022 16:10:08 - INFO - __main__ - Step 20 Global step 20 Train loss 2.83 on epoch=1
05/25/2022 16:10:11 - INFO - __main__ - Step 30 Global step 30 Train loss 1.54 on epoch=2
05/25/2022 16:10:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.08 on epoch=3
05/25/2022 16:10:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.80 on epoch=4
05/25/2022 16:10:20 - INFO - __main__ - Global step 50 Train loss 2.56 Classification-F1 0.1775766716943188 on epoch=4
05/25/2022 16:10:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1775766716943188 on epoch=4, global_step=50
05/25/2022 16:10:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.77 on epoch=4
05/25/2022 16:10:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.78 on epoch=5
05/25/2022 16:10:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.65 on epoch=6
05/25/2022 16:10:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=7
05/25/2022 16:10:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.61 on epoch=8
05/25/2022 16:10:37 - INFO - __main__ - Global step 100 Train loss 0.69 Classification-F1 0.1775766716943188 on epoch=8
05/25/2022 16:10:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=9
05/25/2022 16:10:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=9
05/25/2022 16:10:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=10
05/25/2022 16:10:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.61 on epoch=11
05/25/2022 16:10:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
05/25/2022 16:10:54 - INFO - __main__ - Global step 150 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 16:10:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.57 on epoch=13
05/25/2022 16:11:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
05/25/2022 16:11:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.57 on epoch=14
05/25/2022 16:11:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=15
05/25/2022 16:11:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.63 on epoch=16
05/25/2022 16:11:11 - INFO - __main__ - Global step 200 Train loss 0.58 Classification-F1 0.2276901031355469 on epoch=16
05/25/2022 16:11:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.2276901031355469 on epoch=16, global_step=200
05/25/2022 16:11:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=17
05/25/2022 16:11:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=18
05/25/2022 16:11:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=19
05/25/2022 16:11:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=19
05/25/2022 16:11:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
05/25/2022 16:11:28 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.2085278555866791 on epoch=20
05/25/2022 16:11:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=21
05/25/2022 16:11:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=22
05/25/2022 16:11:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
05/25/2022 16:11:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
05/25/2022 16:11:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
05/25/2022 16:11:45 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 16:11:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/25/2022 16:11:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/25/2022 16:11:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=27
05/25/2022 16:11:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=28
05/25/2022 16:11:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=29
05/25/2022 16:12:02 - INFO - __main__ - Global step 350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 16:12:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 16:12:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=30
05/25/2022 16:12:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=31
05/25/2022 16:12:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=32
05/25/2022 16:12:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/25/2022 16:12:21 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.19849369752030363 on epoch=33
05/25/2022 16:12:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=34
05/25/2022 16:12:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=34
05/25/2022 16:12:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=35
05/25/2022 16:12:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=36
05/25/2022 16:12:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=37
05/25/2022 16:12:38 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 16:12:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
05/25/2022 16:12:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=39
05/25/2022 16:12:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=39
05/25/2022 16:12:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=40
05/25/2022 16:12:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.57 on epoch=41
05/25/2022 16:12:55 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.19872393401805166 on epoch=41
05/25/2022 16:12:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=42
05/25/2022 16:13:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
05/25/2022 16:13:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=44
05/25/2022 16:13:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=44
05/25/2022 16:13:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=45
05/25/2022 16:13:13 - INFO - __main__ - Global step 550 Train loss 0.53 Classification-F1 0.1775766716943188 on epoch=45
05/25/2022 16:13:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=46
05/25/2022 16:13:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=47
05/25/2022 16:13:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/25/2022 16:13:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=49
05/25/2022 16:13:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=49
05/25/2022 16:13:31 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 16:13:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=50
05/25/2022 16:13:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
05/25/2022 16:13:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=52
05/25/2022 16:13:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
05/25/2022 16:13:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
05/25/2022 16:13:49 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 16:13:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=54
05/25/2022 16:13:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=55
05/25/2022 16:13:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
05/25/2022 16:13:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=57
05/25/2022 16:14:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=58
05/25/2022 16:14:06 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.2085278555866791 on epoch=58
05/25/2022 16:14:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=59
05/25/2022 16:14:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=59
05/25/2022 16:14:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=60
05/25/2022 16:14:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=61
05/25/2022 16:14:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.50 on epoch=62
05/25/2022 16:14:25 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 16:14:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
05/25/2022 16:14:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=64
05/25/2022 16:14:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=64
05/25/2022 16:14:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=65
05/25/2022 16:14:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
05/25/2022 16:14:43 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.2844155335240409 on epoch=66
05/25/2022 16:14:43 - INFO - __main__ - Saving model with best Classification-F1: 0.2276901031355469 -> 0.2844155335240409 on epoch=66, global_step=800
05/25/2022 16:14:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
05/25/2022 16:14:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 16:14:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=69
05/25/2022 16:14:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=69
05/25/2022 16:14:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.52 on epoch=70
05/25/2022 16:15:02 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.2256928668786915 on epoch=70
05/25/2022 16:15:05 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=71
05/25/2022 16:15:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.49 on epoch=72
05/25/2022 16:15:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
05/25/2022 16:15:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=74
05/25/2022 16:15:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=74
05/25/2022 16:15:21 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.225932165587338 on epoch=74
05/25/2022 16:15:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=75
05/25/2022 16:15:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.54 on epoch=76
05/25/2022 16:15:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.49 on epoch=77
05/25/2022 16:15:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
05/25/2022 16:15:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=79
05/25/2022 16:15:39 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=79
05/25/2022 16:15:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=79
05/25/2022 16:15:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=80
05/25/2022 16:15:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=81
05/25/2022 16:15:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.52 on epoch=82
05/25/2022 16:15:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
05/25/2022 16:15:57 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.29851116625310176 on epoch=83
05/25/2022 16:15:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2844155335240409 -> 0.29851116625310176 on epoch=83, global_step=1000
05/25/2022 16:16:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.54 on epoch=84
05/25/2022 16:16:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.51 on epoch=84
05/25/2022 16:16:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=85
05/25/2022 16:16:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.48 on epoch=86
05/25/2022 16:16:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=87
05/25/2022 16:16:16 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 16:16:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=88
05/25/2022 16:16:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
05/25/2022 16:16:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.53 on epoch=89
05/25/2022 16:16:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=90
05/25/2022 16:16:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=91
05/25/2022 16:16:34 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 16:16:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=92
05/25/2022 16:16:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.52 on epoch=93
05/25/2022 16:16:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=94
05/25/2022 16:16:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=94
05/25/2022 16:16:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=95
05/25/2022 16:16:53 - INFO - __main__ - Global step 1150 Train loss 0.50 Classification-F1 0.30581375978677333 on epoch=95
05/25/2022 16:16:53 - INFO - __main__ - Saving model with best Classification-F1: 0.29851116625310176 -> 0.30581375978677333 on epoch=95, global_step=1150
05/25/2022 16:16:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.50 on epoch=96
05/25/2022 16:16:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=97
05/25/2022 16:17:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=98
05/25/2022 16:17:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=99
05/25/2022 16:17:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.49 on epoch=99
05/25/2022 16:17:11 - INFO - __main__ - Global step 1200 Train loss 0.48 Classification-F1 0.19843776900307117 on epoch=99
05/25/2022 16:17:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=100
05/25/2022 16:17:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=101
05/25/2022 16:17:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=102
05/25/2022 16:17:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=103
05/25/2022 16:17:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.47 on epoch=104
05/25/2022 16:17:30 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 16:17:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=104
05/25/2022 16:17:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=105
05/25/2022 16:17:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.47 on epoch=106
05/25/2022 16:17:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=107
05/25/2022 16:17:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
05/25/2022 16:17:48 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.20578995698612923 on epoch=108
05/25/2022 16:17:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=109
05/25/2022 16:17:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=109
05/25/2022 16:17:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=110
05/25/2022 16:17:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=111
05/25/2022 16:18:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.54 on epoch=112
05/25/2022 16:18:07 - INFO - __main__ - Global step 1350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 16:18:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=113
05/25/2022 16:18:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=114
05/25/2022 16:18:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.55 on epoch=114
05/25/2022 16:18:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.50 on epoch=115
05/25/2022 16:18:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
05/25/2022 16:18:25 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.30901960784313726 on epoch=116
05/25/2022 16:18:25 - INFO - __main__ - Saving model with best Classification-F1: 0.30581375978677333 -> 0.30901960784313726 on epoch=116, global_step=1400
05/25/2022 16:18:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=117
05/25/2022 16:18:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=118
05/25/2022 16:18:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.47 on epoch=119
05/25/2022 16:18:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=119
05/25/2022 16:18:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=120
05/25/2022 16:18:44 - INFO - __main__ - Global step 1450 Train loss 0.48 Classification-F1 0.27873701039168663 on epoch=120
05/25/2022 16:18:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=121
05/25/2022 16:18:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=122
05/25/2022 16:18:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=123
05/25/2022 16:18:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.47 on epoch=124
05/25/2022 16:18:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=124
05/25/2022 16:19:03 - INFO - __main__ - Global step 1500 Train loss 0.47 Classification-F1 0.1945253810628134 on epoch=124
05/25/2022 16:19:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=125
05/25/2022 16:19:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
05/25/2022 16:19:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=127
05/25/2022 16:19:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=128
05/25/2022 16:19:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=129
05/25/2022 16:19:20 - INFO - __main__ - Global step 1550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=129
05/25/2022 16:19:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=129
05/25/2022 16:19:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=130
05/25/2022 16:19:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=131
05/25/2022 16:19:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=132
05/25/2022 16:19:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
05/25/2022 16:19:38 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=133
05/25/2022 16:19:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=134
05/25/2022 16:19:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
05/25/2022 16:19:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=135
05/25/2022 16:19:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=136
05/25/2022 16:19:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=137
05/25/2022 16:19:57 - INFO - __main__ - Global step 1650 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 16:19:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=138
05/25/2022 16:20:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.51 on epoch=139
05/25/2022 16:20:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.48 on epoch=139
05/25/2022 16:20:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=140
05/25/2022 16:20:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=141
05/25/2022 16:20:15 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.25324963612473433 on epoch=141
05/25/2022 16:20:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=142
05/25/2022 16:20:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=143
05/25/2022 16:20:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=144
05/25/2022 16:20:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=144
05/25/2022 16:20:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=145
05/25/2022 16:20:32 - INFO - __main__ - Global step 1750 Train loss 0.47 Classification-F1 0.3083453780746644 on epoch=145
05/25/2022 16:20:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.49 on epoch=146
05/25/2022 16:20:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.44 on epoch=147
05/25/2022 16:20:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=148
05/25/2022 16:20:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.48 on epoch=149
05/25/2022 16:20:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=149
05/25/2022 16:20:49 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.29442191265502854 on epoch=149
05/25/2022 16:20:52 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.47 on epoch=150
05/25/2022 16:20:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.50 on epoch=151
05/25/2022 16:20:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.51 on epoch=152
05/25/2022 16:21:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
05/25/2022 16:21:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=154
05/25/2022 16:21:08 - INFO - __main__ - Global step 1850 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=154
05/25/2022 16:21:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=154
05/25/2022 16:21:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=155
05/25/2022 16:21:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
05/25/2022 16:21:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=157
05/25/2022 16:21:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=158
05/25/2022 16:21:26 - INFO - __main__ - Global step 1900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=158
05/25/2022 16:21:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.47 on epoch=159
05/25/2022 16:21:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
05/25/2022 16:21:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=160
05/25/2022 16:21:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=161
05/25/2022 16:21:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=162
05/25/2022 16:21:44 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.2896490051419629 on epoch=162
05/25/2022 16:21:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
05/25/2022 16:21:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.45 on epoch=164
05/25/2022 16:21:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
05/25/2022 16:21:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=165
05/25/2022 16:21:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=166
05/25/2022 16:22:02 - INFO - __main__ - Global step 2000 Train loss 0.45 Classification-F1 0.21443151663604818 on epoch=166
05/25/2022 16:22:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.49 on epoch=167
05/25/2022 16:22:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
05/25/2022 16:22:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=169
05/25/2022 16:22:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.49 on epoch=169
05/25/2022 16:22:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.50 on epoch=170
05/25/2022 16:22:20 - INFO - __main__ - Global step 2050 Train loss 0.48 Classification-F1 0.3182332643202208 on epoch=170
05/25/2022 16:22:20 - INFO - __main__ - Saving model with best Classification-F1: 0.30901960784313726 -> 0.3182332643202208 on epoch=170, global_step=2050
05/25/2022 16:22:22 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.45 on epoch=171
05/25/2022 16:22:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=172
05/25/2022 16:22:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=173
05/25/2022 16:22:30 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.49 on epoch=174
05/25/2022 16:22:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=174
05/25/2022 16:22:39 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.26436837952482 on epoch=174
05/25/2022 16:22:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.47 on epoch=175
05/25/2022 16:22:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=176
05/25/2022 16:22:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.48 on epoch=177
05/25/2022 16:22:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=178
05/25/2022 16:22:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=179
05/25/2022 16:22:56 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=179
05/25/2022 16:22:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.45 on epoch=179
05/25/2022 16:23:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.45 on epoch=180
05/25/2022 16:23:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
05/25/2022 16:23:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=182
05/25/2022 16:23:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=183
05/25/2022 16:23:15 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.30027078441421196 on epoch=183
05/25/2022 16:23:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.43 on epoch=184
05/25/2022 16:23:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.49 on epoch=184
05/25/2022 16:23:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.46 on epoch=185
05/25/2022 16:23:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=186
05/25/2022 16:23:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=187
05/25/2022 16:23:34 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.2774282733437663 on epoch=187
05/25/2022 16:23:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.45 on epoch=188
05/25/2022 16:23:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.43 on epoch=189
05/25/2022 16:23:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=189
05/25/2022 16:23:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=190
05/25/2022 16:23:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=191
05/25/2022 16:23:52 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.23185840707964603 on epoch=191
05/25/2022 16:23:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.47 on epoch=192
05/25/2022 16:23:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=193
05/25/2022 16:24:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.48 on epoch=194
05/25/2022 16:24:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=194
05/25/2022 16:24:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.45 on epoch=195
05/25/2022 16:24:10 - INFO - __main__ - Global step 2350 Train loss 0.44 Classification-F1 0.29071149600950924 on epoch=195
05/25/2022 16:24:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.48 on epoch=196
05/25/2022 16:24:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.43 on epoch=197
05/25/2022 16:24:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=198
05/25/2022 16:24:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=199
05/25/2022 16:24:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=199
05/25/2022 16:24:28 - INFO - __main__ - Global step 2400 Train loss 0.45 Classification-F1 0.2669740887132192 on epoch=199
05/25/2022 16:24:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=200
05/25/2022 16:24:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=201
05/25/2022 16:24:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.47 on epoch=202
05/25/2022 16:24:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.44 on epoch=203
05/25/2022 16:24:41 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=204
05/25/2022 16:24:46 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.1775766716943188 on epoch=204
05/25/2022 16:24:48 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.42 on epoch=204
05/25/2022 16:24:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=205
05/25/2022 16:24:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
05/25/2022 16:24:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
05/25/2022 16:24:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.48 on epoch=208
05/25/2022 16:25:04 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.2908875993982378 on epoch=208
05/25/2022 16:25:07 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=209
05/25/2022 16:25:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=209
05/25/2022 16:25:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.45 on epoch=210
05/25/2022 16:25:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.48 on epoch=211
05/25/2022 16:25:18 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.48 on epoch=212
05/25/2022 16:25:23 - INFO - __main__ - Global step 2550 Train loss 0.47 Classification-F1 0.30633593459680414 on epoch=212
05/25/2022 16:25:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
05/25/2022 16:25:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.43 on epoch=214
05/25/2022 16:25:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
05/25/2022 16:25:34 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=215
05/25/2022 16:25:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=216
05/25/2022 16:25:42 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.33782357226757065 on epoch=216
05/25/2022 16:25:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3182332643202208 -> 0.33782357226757065 on epoch=216, global_step=2600
05/25/2022 16:25:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=217
05/25/2022 16:25:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=218
05/25/2022 16:25:50 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=219
05/25/2022 16:25:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.46 on epoch=219
05/25/2022 16:25:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.41 on epoch=220
05/25/2022 16:26:01 - INFO - __main__ - Global step 2650 Train loss 0.43 Classification-F1 0.3041857907399224 on epoch=220
05/25/2022 16:26:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.46 on epoch=221
05/25/2022 16:26:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.45 on epoch=222
05/25/2022 16:26:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.44 on epoch=223
05/25/2022 16:26:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=224
05/25/2022 16:26:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
05/25/2022 16:26:20 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.21660059493756234 on epoch=224
05/25/2022 16:26:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.48 on epoch=225
05/25/2022 16:26:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.46 on epoch=226
05/25/2022 16:26:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.50 on epoch=227
05/25/2022 16:26:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=228
05/25/2022 16:26:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=229
05/25/2022 16:26:39 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.20387111086462203 on epoch=229
05/25/2022 16:26:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=229
05/25/2022 16:26:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=230
05/25/2022 16:26:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=231
05/25/2022 16:26:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
05/25/2022 16:26:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.48 on epoch=233
05/25/2022 16:26:58 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.3195615454928071 on epoch=233
05/25/2022 16:27:01 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.49 on epoch=234
05/25/2022 16:27:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=234
05/25/2022 16:27:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.46 on epoch=235
05/25/2022 16:27:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=236
05/25/2022 16:27:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.47 on epoch=237
05/25/2022 16:27:17 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.29107718077567324 on epoch=237
05/25/2022 16:27:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=238
05/25/2022 16:27:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
05/25/2022 16:27:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=239
05/25/2022 16:27:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=240
05/25/2022 16:27:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=241
05/25/2022 16:27:37 - INFO - __main__ - Global step 2900 Train loss 0.42 Classification-F1 0.340739012505306 on epoch=241
05/25/2022 16:27:37 - INFO - __main__ - Saving model with best Classification-F1: 0.33782357226757065 -> 0.340739012505306 on epoch=241, global_step=2900
05/25/2022 16:27:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=242
05/25/2022 16:27:42 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
05/25/2022 16:27:45 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=244
05/25/2022 16:27:48 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.41 on epoch=244
05/25/2022 16:27:51 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.38 on epoch=245
05/25/2022 16:27:56 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.24443636657377113 on epoch=245
05/25/2022 16:27:59 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
05/25/2022 16:28:01 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.38 on epoch=247
05/25/2022 16:28:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
05/25/2022 16:28:07 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=249
05/25/2022 16:28:09 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=249
05/25/2022 16:28:11 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:28:11 - INFO - __main__ - Printing 3 examples
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:28:11 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:28:11 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:28:11 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:28:11 - INFO - __main__ - Printing 3 examples
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:28:11 - INFO - __main__ - ['neutral']
05/25/2022 16:28:11 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:28:11 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:28:11 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:28:15 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.34270674138223806 on epoch=249
05/25/2022 16:28:15 - INFO - __main__ - Saving model with best Classification-F1: 0.340739012505306 -> 0.34270674138223806 on epoch=249, global_step=3000
05/25/2022 16:28:15 - INFO - __main__ - save last model!
05/25/2022 16:28:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 16:28:15 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 16:28:15 - INFO - __main__ - Printing 3 examples
05/25/2022 16:28:15 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 16:28:15 - INFO - __main__ - ['contradiction']
05/25/2022 16:28:15 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 16:28:15 - INFO - __main__ - ['entailment']
05/25/2022 16:28:15 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 16:28:15 - INFO - __main__ - ['contradiction']
05/25/2022 16:28:15 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:28:16 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:28:17 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 16:28:26 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:28:26 - INFO - __main__ - task name: anli
05/25/2022 16:28:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:28:27 - INFO - __main__ - Starting training!
05/25/2022 16:28:47 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_42_0.4_8_predictions.txt
05/25/2022 16:28:47 - INFO - __main__ - Classification-F1 on test data: 0.2798
05/25/2022 16:28:48 - INFO - __main__ - prefix=anli_64_42, lr=0.4, bsz=8, dev_performance=0.34270674138223806, test_performance=0.2798348830280978
05/25/2022 16:28:48 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.3, bsz=8 ...
05/25/2022 16:28:49 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:28:49 - INFO - __main__ - Printing 3 examples
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:28:49 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:28:49 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:28:49 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:28:49 - INFO - __main__ - Printing 3 examples
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:28:49 - INFO - __main__ - ['neutral']
05/25/2022 16:28:49 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:28:49 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:28:49 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:29:08 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:29:08 - INFO - __main__ - task name: anli
05/25/2022 16:29:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:29:10 - INFO - __main__ - Starting training!
05/25/2022 16:29:13 - INFO - __main__ - Step 10 Global step 10 Train loss 7.18 on epoch=0
05/25/2022 16:29:15 - INFO - __main__ - Step 20 Global step 20 Train loss 3.40 on epoch=1
05/25/2022 16:29:18 - INFO - __main__ - Step 30 Global step 30 Train loss 1.52 on epoch=2
05/25/2022 16:29:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=3
05/25/2022 16:29:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.91 on epoch=4
05/25/2022 16:29:27 - INFO - __main__ - Global step 50 Train loss 2.79 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 16:29:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 16:29:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.82 on epoch=4
05/25/2022 16:29:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=5
05/25/2022 16:29:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.75 on epoch=6
05/25/2022 16:29:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.74 on epoch=7
05/25/2022 16:29:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=8
05/25/2022 16:29:46 - INFO - __main__ - Global step 100 Train loss 0.73 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 16:29:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.63 on epoch=9
05/25/2022 16:29:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=9
05/25/2022 16:29:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.69 on epoch=10
05/25/2022 16:29:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.66 on epoch=11
05/25/2022 16:29:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=12
05/25/2022 16:30:03 - INFO - __main__ - Global step 150 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 16:30:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=13
05/25/2022 16:30:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.60 on epoch=14
05/25/2022 16:30:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=14
05/25/2022 16:30:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=15
05/25/2022 16:30:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=16
05/25/2022 16:30:21 - INFO - __main__ - Global step 200 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 16:30:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.60 on epoch=17
05/25/2022 16:30:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=18
05/25/2022 16:30:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/25/2022 16:30:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
05/25/2022 16:30:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=20
05/25/2022 16:30:39 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.274879654189999 on epoch=20
05/25/2022 16:30:39 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.274879654189999 on epoch=20, global_step=250
05/25/2022 16:30:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=21
05/25/2022 16:30:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=22
05/25/2022 16:30:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=23
05/25/2022 16:30:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=24
05/25/2022 16:30:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=24
05/25/2022 16:30:57 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 16:30:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=25
05/25/2022 16:31:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.56 on epoch=26
05/25/2022 16:31:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.60 on epoch=27
05/25/2022 16:31:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=28
05/25/2022 16:31:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=29
05/25/2022 16:31:14 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 16:31:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=29
05/25/2022 16:31:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=30
05/25/2022 16:31:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=31
05/25/2022 16:31:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=32
05/25/2022 16:31:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=33
05/25/2022 16:31:32 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 16:31:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=34
05/25/2022 16:31:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.58 on epoch=34
05/25/2022 16:31:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
05/25/2022 16:31:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=36
05/25/2022 16:31:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=37
05/25/2022 16:31:50 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 16:31:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/25/2022 16:31:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=39
05/25/2022 16:31:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
05/25/2022 16:32:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=40
05/25/2022 16:32:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=41
05/25/2022 16:32:08 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 16:32:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=42
05/25/2022 16:32:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=43
05/25/2022 16:32:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=44
05/25/2022 16:32:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=44
05/25/2022 16:32:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=45
05/25/2022 16:32:27 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 16:32:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
05/25/2022 16:32:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=47
05/25/2022 16:32:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
05/25/2022 16:32:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.57 on epoch=49
05/25/2022 16:32:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=49
05/25/2022 16:32:45 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=49
05/25/2022 16:32:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=50
05/25/2022 16:32:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
05/25/2022 16:32:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=52
05/25/2022 16:32:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=53
05/25/2022 16:32:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=54
05/25/2022 16:33:04 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=54
05/25/2022 16:33:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=54
05/25/2022 16:33:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=55
05/25/2022 16:33:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=56
05/25/2022 16:33:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=57
05/25/2022 16:33:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=58
05/25/2022 16:33:22 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 16:33:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=59
05/25/2022 16:33:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=59
05/25/2022 16:33:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=60
05/25/2022 16:33:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=61
05/25/2022 16:33:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=62
05/25/2022 16:33:40 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.1775766716943188 on epoch=62
05/25/2022 16:33:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
05/25/2022 16:33:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.51 on epoch=64
05/25/2022 16:33:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=64
05/25/2022 16:33:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=65
05/25/2022 16:33:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=66
05/25/2022 16:33:59 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.30689680689680693 on epoch=66
05/25/2022 16:33:59 - INFO - __main__ - Saving model with best Classification-F1: 0.274879654189999 -> 0.30689680689680693 on epoch=66, global_step=800
05/25/2022 16:34:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.53 on epoch=67
05/25/2022 16:34:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
05/25/2022 16:34:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=69
05/25/2022 16:34:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=69
05/25/2022 16:34:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=70
05/25/2022 16:34:18 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.2867692307692307 on epoch=70
05/25/2022 16:34:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=71
05/25/2022 16:34:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=72
05/25/2022 16:34:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 16:34:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=74
05/25/2022 16:34:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=74
05/25/2022 16:34:37 - INFO - __main__ - Global step 900 Train loss 0.50 Classification-F1 0.23466666666666666 on epoch=74
05/25/2022 16:34:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=75
05/25/2022 16:34:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
05/25/2022 16:34:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=77
05/25/2022 16:34:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.52 on epoch=78
05/25/2022 16:34:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=79
05/25/2022 16:34:56 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.1775766716943188 on epoch=79
05/25/2022 16:34:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=79
05/25/2022 16:35:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=80
05/25/2022 16:35:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=81
05/25/2022 16:35:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=82
05/25/2022 16:35:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
05/25/2022 16:35:15 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.17054932412999713 on epoch=83
05/25/2022 16:35:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=84
05/25/2022 16:35:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
05/25/2022 16:35:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=85
05/25/2022 16:35:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.50 on epoch=86
05/25/2022 16:35:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=87
05/25/2022 16:35:33 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 16:35:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=88
05/25/2022 16:35:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.49 on epoch=89
05/25/2022 16:35:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
05/25/2022 16:35:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=90
05/25/2022 16:35:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=91
05/25/2022 16:35:52 - INFO - __main__ - Global step 1100 Train loss 0.48 Classification-F1 0.3373005465627393 on epoch=91
05/25/2022 16:35:52 - INFO - __main__ - Saving model with best Classification-F1: 0.30689680689680693 -> 0.3373005465627393 on epoch=91, global_step=1100
05/25/2022 16:35:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=92
05/25/2022 16:35:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 16:36:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.51 on epoch=94
05/25/2022 16:36:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=94
05/25/2022 16:36:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
05/25/2022 16:36:11 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.23653959437852393 on epoch=95
05/25/2022 16:36:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=96
05/25/2022 16:36:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=97
05/25/2022 16:36:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=98
05/25/2022 16:36:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=99
05/25/2022 16:36:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=99
05/25/2022 16:36:30 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.2272576691181342 on epoch=99
05/25/2022 16:36:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=100
05/25/2022 16:36:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=101
05/25/2022 16:36:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.48 on epoch=102
05/25/2022 16:36:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/25/2022 16:36:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=104
05/25/2022 16:36:49 - INFO - __main__ - Global step 1250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 16:36:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=104
05/25/2022 16:36:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=105
05/25/2022 16:36:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.52 on epoch=106
05/25/2022 16:37:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=107
05/25/2022 16:37:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
05/25/2022 16:37:08 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.18209928917548127 on epoch=108
05/25/2022 16:37:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.48 on epoch=109
05/25/2022 16:37:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=109
05/25/2022 16:37:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=110
05/25/2022 16:37:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.49 on epoch=111
05/25/2022 16:37:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.54 on epoch=112
05/25/2022 16:37:27 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.2637935418255784 on epoch=112
05/25/2022 16:37:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=113
05/25/2022 16:37:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=114
05/25/2022 16:37:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
05/25/2022 16:37:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.46 on epoch=115
05/25/2022 16:37:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.52 on epoch=116
05/25/2022 16:37:46 - INFO - __main__ - Global step 1400 Train loss 0.47 Classification-F1 0.29628820759729657 on epoch=116
05/25/2022 16:37:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=117
05/25/2022 16:37:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=118
05/25/2022 16:37:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=119
05/25/2022 16:37:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=119
05/25/2022 16:37:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=120
05/25/2022 16:38:05 - INFO - __main__ - Global step 1450 Train loss 0.50 Classification-F1 0.22654176424668226 on epoch=120
05/25/2022 16:38:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=121
05/25/2022 16:38:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=122
05/25/2022 16:38:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=123
05/25/2022 16:38:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=124
05/25/2022 16:38:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=124
05/25/2022 16:38:24 - INFO - __main__ - Global step 1500 Train loss 0.51 Classification-F1 0.16732026143790854 on epoch=124
05/25/2022 16:38:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=125
05/25/2022 16:38:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=126
05/25/2022 16:38:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.49 on epoch=127
05/25/2022 16:38:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
05/25/2022 16:38:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.48 on epoch=129
05/25/2022 16:38:43 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=129
05/25/2022 16:38:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=129
05/25/2022 16:38:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=130
05/25/2022 16:38:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=131
05/25/2022 16:38:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=132
05/25/2022 16:38:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=133
05/25/2022 16:39:01 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=133
05/25/2022 16:39:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.46 on epoch=134
05/25/2022 16:39:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.50 on epoch=134
05/25/2022 16:39:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=135
05/25/2022 16:39:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=136
05/25/2022 16:39:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.47 on epoch=137
05/25/2022 16:39:20 - INFO - __main__ - Global step 1650 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 16:39:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.48 on epoch=138
05/25/2022 16:39:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=139
05/25/2022 16:39:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=139
05/25/2022 16:39:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.47 on epoch=140
05/25/2022 16:39:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=141
05/25/2022 16:39:38 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.16732026143790854 on epoch=141
05/25/2022 16:39:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.47 on epoch=142
05/25/2022 16:39:43 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=143
05/25/2022 16:39:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.47 on epoch=144
05/25/2022 16:39:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.53 on epoch=144
05/25/2022 16:39:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.48 on epoch=145
05/25/2022 16:39:56 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.3049800031774745 on epoch=145
05/25/2022 16:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.48 on epoch=146
05/25/2022 16:40:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=147
05/25/2022 16:40:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=148
05/25/2022 16:40:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=149
05/25/2022 16:40:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
05/25/2022 16:40:14 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.32859927596769706 on epoch=149
05/25/2022 16:40:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=150
05/25/2022 16:40:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=151
05/25/2022 16:40:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=152
05/25/2022 16:40:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=153
05/25/2022 16:40:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=154
05/25/2022 16:40:33 - INFO - __main__ - Global step 1850 Train loss 0.46 Classification-F1 0.1775766716943188 on epoch=154
05/25/2022 16:40:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.47 on epoch=154
05/25/2022 16:40:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=155
05/25/2022 16:40:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=156
05/25/2022 16:40:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=157
05/25/2022 16:40:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
05/25/2022 16:40:52 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.18477666534683335 on epoch=158
05/25/2022 16:40:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=159
05/25/2022 16:40:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
05/25/2022 16:41:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=160
05/25/2022 16:41:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=161
05/25/2022 16:41:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.51 on epoch=162
05/25/2022 16:41:11 - INFO - __main__ - Global step 1950 Train loss 0.46 Classification-F1 0.19696969696969693 on epoch=162
05/25/2022 16:41:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=163
05/25/2022 16:41:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=164
05/25/2022 16:41:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=164
05/25/2022 16:41:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.48 on epoch=165
05/25/2022 16:41:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=166
05/25/2022 16:41:29 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.27632903149419846 on epoch=166
05/25/2022 16:41:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.47 on epoch=167
05/25/2022 16:41:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.49 on epoch=168
05/25/2022 16:41:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.45 on epoch=169
05/25/2022 16:41:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.45 on epoch=169
05/25/2022 16:41:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.45 on epoch=170
05/25/2022 16:41:49 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.2999137532853458 on epoch=170
05/25/2022 16:41:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.51 on epoch=171
05/25/2022 16:41:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.49 on epoch=172
05/25/2022 16:41:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=173
05/25/2022 16:41:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.48 on epoch=174
05/25/2022 16:42:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=174
05/25/2022 16:42:08 - INFO - __main__ - Global step 2100 Train loss 0.48 Classification-F1 0.2951691524321333 on epoch=174
05/25/2022 16:42:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.45 on epoch=175
05/25/2022 16:42:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.44 on epoch=176
05/25/2022 16:42:15 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.47 on epoch=177
05/25/2022 16:42:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=178
05/25/2022 16:42:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=179
05/25/2022 16:42:26 - INFO - __main__ - Global step 2150 Train loss 0.45 Classification-F1 0.1647058823529412 on epoch=179
05/25/2022 16:42:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.46 on epoch=179
05/25/2022 16:42:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.42 on epoch=180
05/25/2022 16:42:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.50 on epoch=181
05/25/2022 16:42:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.48 on epoch=182
05/25/2022 16:42:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=183
05/25/2022 16:42:45 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.20720969882763254 on epoch=183
05/25/2022 16:42:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=184
05/25/2022 16:42:51 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 16:42:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.47 on epoch=185
05/25/2022 16:42:56 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=186
05/25/2022 16:42:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=187
05/25/2022 16:43:04 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.18422782303379315 on epoch=187
05/25/2022 16:43:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=188
05/25/2022 16:43:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.47 on epoch=189
05/25/2022 16:43:12 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.47 on epoch=189
05/25/2022 16:43:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=190
05/25/2022 16:43:17 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=191
05/25/2022 16:43:23 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.3040570540570541 on epoch=191
05/25/2022 16:43:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.49 on epoch=192
05/25/2022 16:43:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.46 on epoch=193
05/25/2022 16:43:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=194
05/25/2022 16:43:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=194
05/25/2022 16:43:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.46 on epoch=195
05/25/2022 16:43:41 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.20841537740271918 on epoch=195
05/25/2022 16:43:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.46 on epoch=196
05/25/2022 16:43:47 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.49 on epoch=197
05/25/2022 16:43:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.45 on epoch=198
05/25/2022 16:43:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=199
05/25/2022 16:43:55 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.46 on epoch=199
05/25/2022 16:44:00 - INFO - __main__ - Global step 2400 Train loss 0.46 Classification-F1 0.257855833077072 on epoch=199
05/25/2022 16:44:03 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.49 on epoch=200
05/25/2022 16:44:05 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.48 on epoch=201
05/25/2022 16:44:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=202
05/25/2022 16:44:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
05/25/2022 16:44:13 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=204
05/25/2022 16:44:19 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.22173280845480015 on epoch=204
05/25/2022 16:44:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.48 on epoch=204
05/25/2022 16:44:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.48 on epoch=205
05/25/2022 16:44:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.42 on epoch=206
05/25/2022 16:44:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.45 on epoch=207
05/25/2022 16:44:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=208
05/25/2022 16:44:38 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.22964355026085562 on epoch=208
05/25/2022 16:44:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=209
05/25/2022 16:44:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.46 on epoch=209
05/25/2022 16:44:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.50 on epoch=210
05/25/2022 16:44:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.46 on epoch=211
05/25/2022 16:44:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=212
05/25/2022 16:44:57 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.31808121742183143 on epoch=212
05/25/2022 16:45:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
05/25/2022 16:45:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.47 on epoch=214
05/25/2022 16:45:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=214
05/25/2022 16:45:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=215
05/25/2022 16:45:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.51 on epoch=216
05/25/2022 16:45:16 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.1843809523809524 on epoch=216
05/25/2022 16:45:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.47 on epoch=217
05/25/2022 16:45:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.40 on epoch=218
05/25/2022 16:45:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.42 on epoch=219
05/25/2022 16:45:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.50 on epoch=219
05/25/2022 16:45:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.49 on epoch=220
05/25/2022 16:45:35 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.3675626021473318 on epoch=220
05/25/2022 16:45:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3373005465627393 -> 0.3675626021473318 on epoch=220, global_step=2650
05/25/2022 16:45:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.44 on epoch=221
05/25/2022 16:45:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.41 on epoch=222
05/25/2022 16:45:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=223
05/25/2022 16:45:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.39 on epoch=224
05/25/2022 16:45:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.41 on epoch=224
05/25/2022 16:45:54 - INFO - __main__ - Global step 2700 Train loss 0.41 Classification-F1 0.32393176367996507 on epoch=224
05/25/2022 16:45:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=225
05/25/2022 16:45:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=226
05/25/2022 16:46:02 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=227
05/25/2022 16:46:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=228
05/25/2022 16:46:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=229
05/25/2022 16:46:13 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.19530135192785794 on epoch=229
05/25/2022 16:46:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=229
05/25/2022 16:46:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.46 on epoch=230
05/25/2022 16:46:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.38 on epoch=231
05/25/2022 16:46:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=232
05/25/2022 16:46:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.46 on epoch=233
05/25/2022 16:46:31 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.21209774373823823 on epoch=233
05/25/2022 16:46:34 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
05/25/2022 16:46:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.43 on epoch=234
05/25/2022 16:46:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.45 on epoch=235
05/25/2022 16:46:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=236
05/25/2022 16:46:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=237
05/25/2022 16:46:50 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.2158133565831916 on epoch=237
05/25/2022 16:46:53 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.40 on epoch=238
05/25/2022 16:46:56 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.40 on epoch=239
05/25/2022 16:46:58 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.48 on epoch=239
05/25/2022 16:47:01 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=240
05/25/2022 16:47:04 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=241
05/25/2022 16:47:09 - INFO - __main__ - Global step 2900 Train loss 0.42 Classification-F1 0.3529291352242172 on epoch=241
05/25/2022 16:47:12 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=242
05/25/2022 16:47:14 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
05/25/2022 16:47:17 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
05/25/2022 16:47:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=244
05/25/2022 16:47:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.45 on epoch=245
05/25/2022 16:47:28 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.3167531544888725 on epoch=245
05/25/2022 16:47:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.47 on epoch=246
05/25/2022 16:47:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=247
05/25/2022 16:47:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
05/25/2022 16:47:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=249
05/25/2022 16:47:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.42 on epoch=249
05/25/2022 16:47:42 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:47:42 - INFO - __main__ - Printing 3 examples
05/25/2022 16:47:42 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:47:42 - INFO - __main__ - ['neutral']
05/25/2022 16:47:42 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:47:42 - INFO - __main__ - ['neutral']
05/25/2022 16:47:42 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:47:42 - INFO - __main__ - ['neutral']
05/25/2022 16:47:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:47:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:47:43 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:47:43 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:47:43 - INFO - __main__ - Printing 3 examples
05/25/2022 16:47:43 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:47:43 - INFO - __main__ - ['neutral']
05/25/2022 16:47:43 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:47:43 - INFO - __main__ - ['neutral']
05/25/2022 16:47:43 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:47:43 - INFO - __main__ - ['neutral']
05/25/2022 16:47:43 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:47:43 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:47:43 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:47:46 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.13042622582096267 on epoch=249
05/25/2022 16:47:46 - INFO - __main__ - save last model!
05/25/2022 16:47:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 16:47:46 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 16:47:46 - INFO - __main__ - Printing 3 examples
05/25/2022 16:47:46 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 16:47:46 - INFO - __main__ - ['contradiction']
05/25/2022 16:47:46 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 16:47:46 - INFO - __main__ - ['entailment']
05/25/2022 16:47:46 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 16:47:46 - INFO - __main__ - ['contradiction']
05/25/2022 16:47:46 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:47:47 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:47:48 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 16:47:58 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:47:58 - INFO - __main__ - task name: anli
05/25/2022 16:47:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:47:59 - INFO - __main__ - Starting training!
05/25/2022 16:48:17 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_42_0.3_8_predictions.txt
05/25/2022 16:48:17 - INFO - __main__ - Classification-F1 on test data: 0.2033
05/25/2022 16:48:18 - INFO - __main__ - prefix=anli_64_42, lr=0.3, bsz=8, dev_performance=0.3675626021473318, test_performance=0.20329272591743427
05/25/2022 16:48:18 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.2, bsz=8 ...
05/25/2022 16:48:19 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:48:19 - INFO - __main__ - Printing 3 examples
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:48:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:48:19 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 16:48:19 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 16:48:19 - INFO - __main__ - Printing 3 examples
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/25/2022 16:48:19 - INFO - __main__ - ['neutral']
05/25/2022 16:48:19 - INFO - __main__ - Tokenizing Input ...
05/25/2022 16:48:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 16:48:19 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 16:48:34 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 16:48:34 - INFO - __main__ - task name: anli
05/25/2022 16:48:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 16:48:35 - INFO - __main__ - Starting training!
05/25/2022 16:48:38 - INFO - __main__ - Step 10 Global step 10 Train loss 7.76 on epoch=0
05/25/2022 16:48:40 - INFO - __main__ - Step 20 Global step 20 Train loss 5.23 on epoch=1
05/25/2022 16:48:43 - INFO - __main__ - Step 30 Global step 30 Train loss 3.09 on epoch=2
05/25/2022 16:48:46 - INFO - __main__ - Step 40 Global step 40 Train loss 2.08 on epoch=3
05/25/2022 16:48:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.46 on epoch=4
05/25/2022 16:48:52 - INFO - __main__ - Global step 50 Train loss 3.92 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 16:48:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 16:48:55 - INFO - __main__ - Step 60 Global step 60 Train loss 1.16 on epoch=4
05/25/2022 16:48:58 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=5
05/25/2022 16:49:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=6
05/25/2022 16:49:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=7
05/25/2022 16:49:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=8
05/25/2022 16:49:09 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 16:49:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.78 on epoch=9
05/25/2022 16:49:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.75 on epoch=9
05/25/2022 16:49:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.78 on epoch=10
05/25/2022 16:49:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.72 on epoch=11
05/25/2022 16:49:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.72 on epoch=12
05/25/2022 16:49:26 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 16:49:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=13
05/25/2022 16:49:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=14
05/25/2022 16:49:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=14
05/25/2022 16:49:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=15
05/25/2022 16:49:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.67 on epoch=16
05/25/2022 16:49:53 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.10575223972814336 on epoch=16
05/25/2022 16:49:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.99 on epoch=17
05/25/2022 16:49:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=18
05/25/2022 16:50:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=19
05/25/2022 16:50:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.61 on epoch=19
05/25/2022 16:50:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.68 on epoch=20
05/25/2022 16:50:11 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.26895030913051127 on epoch=20
05/25/2022 16:50:11 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26895030913051127 on epoch=20, global_step=250
05/25/2022 16:50:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.57 on epoch=21
05/25/2022 16:50:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=22
05/25/2022 16:50:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.61 on epoch=23
05/25/2022 16:50:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=24
05/25/2022 16:50:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=24
05/25/2022 16:50:28 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 16:50:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.65 on epoch=25
05/25/2022 16:50:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=26
05/25/2022 16:50:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=27
05/25/2022 16:50:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=28
05/25/2022 16:50:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=29
05/25/2022 16:50:46 - INFO - __main__ - Global step 350 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=29
05/25/2022 16:50:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=29
05/25/2022 16:50:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=30
05/25/2022 16:50:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=31
05/25/2022 16:50:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=32
05/25/2022 16:50:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=33
05/25/2022 16:51:04 - INFO - __main__ - Global step 400 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 16:51:07 - INFO - __main__ - Step 410 Global step 410 Train loss 0.59 on epoch=34
05/25/2022 16:51:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=34
05/25/2022 16:51:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=35
05/25/2022 16:51:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.60 on epoch=36
05/25/2022 16:51:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=37
05/25/2022 16:51:22 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 16:51:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=38
05/25/2022 16:51:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=39
05/25/2022 16:51:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=39
05/25/2022 16:51:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=40
05/25/2022 16:51:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=41
05/25/2022 16:51:39 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.26666666666666666 on epoch=41
05/25/2022 16:51:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=42
05/25/2022 16:51:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=43
05/25/2022 16:51:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=44
05/25/2022 16:51:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=44
05/25/2022 16:51:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=45
05/25/2022 16:51:57 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 16:52:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.58 on epoch=46
05/25/2022 16:52:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.57 on epoch=47
05/25/2022 16:52:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=48
05/25/2022 16:52:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=49
05/25/2022 16:52:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=49
05/25/2022 16:52:16 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.20833333333333334 on epoch=49
05/25/2022 16:52:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.63 on epoch=50
05/25/2022 16:52:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.54 on epoch=51
05/25/2022 16:52:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=52
05/25/2022 16:52:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=53
05/25/2022 16:52:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=54
05/25/2022 16:52:35 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.18931039128510116 on epoch=54
05/25/2022 16:52:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=54
05/25/2022 16:52:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.54 on epoch=55
05/25/2022 16:52:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=56
05/25/2022 16:52:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=57
05/25/2022 16:52:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=58
05/25/2022 16:52:53 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 16:52:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=59
05/25/2022 16:52:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.59 on epoch=59
05/25/2022 16:53:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=60
05/25/2022 16:53:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=61
05/25/2022 16:53:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=62
05/25/2022 16:53:11 - INFO - __main__ - Global step 750 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 16:53:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=63
05/25/2022 16:53:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=64
05/25/2022 16:53:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=64
05/25/2022 16:53:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=65
05/25/2022 16:53:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.58 on epoch=66
05/25/2022 16:53:29 - INFO - __main__ - Global step 800 Train loss 0.53 Classification-F1 0.22204407389592581 on epoch=66
05/25/2022 16:53:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=67
05/25/2022 16:53:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=68
05/25/2022 16:53:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=69
05/25/2022 16:53:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=69
05/25/2022 16:53:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.60 on epoch=70
05/25/2022 16:53:47 - INFO - __main__ - Global step 850 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 16:53:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.56 on epoch=71
05/25/2022 16:53:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=72
05/25/2022 16:53:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=73
05/25/2022 16:53:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.54 on epoch=74
05/25/2022 16:54:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=74
05/25/2022 16:54:05 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.1881810228266921 on epoch=74
05/25/2022 16:54:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=75
05/25/2022 16:54:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
05/25/2022 16:54:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=77
05/25/2022 16:54:15 - INFO - __main__ - Step 940 Global step 940 Train loss 0.51 on epoch=78
05/25/2022 16:54:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=79
05/25/2022 16:54:23 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=79
05/25/2022 16:54:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=79
05/25/2022 16:54:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.54 on epoch=80
05/25/2022 16:54:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.59 on epoch=81
05/25/2022 16:54:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=82
05/25/2022 16:54:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
05/25/2022 16:54:41 - INFO - __main__ - Global step 1000 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=83
05/25/2022 16:54:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=84
05/25/2022 16:54:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=84
05/25/2022 16:54:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=85
05/25/2022 16:54:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=86
05/25/2022 16:54:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=87
05/25/2022 16:54:59 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 16:55:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=88
05/25/2022 16:55:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=89
05/25/2022 16:55:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.49 on epoch=89
05/25/2022 16:55:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=90
05/25/2022 16:55:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/25/2022 16:55:17 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.1775766716943188 on epoch=91
05/25/2022 16:55:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.52 on epoch=92
05/25/2022 16:55:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.51 on epoch=93
05/25/2022 16:55:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=94
05/25/2022 16:55:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.54 on epoch=94
05/25/2022 16:55:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.53 on epoch=95
05/25/2022 16:55:35 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.18627450980392157 on epoch=95
05/25/2022 16:55:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=96
05/25/2022 16:55:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=97
05/25/2022 16:55:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.49 on epoch=98
05/25/2022 16:55:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.58 on epoch=99
05/25/2022 16:55:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=99
05/25/2022 16:55:53 - INFO - __main__ - Global step 1200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 16:55:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.54 on epoch=100
05/25/2022 16:55:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=101
05/25/2022 16:56:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=102
05/25/2022 16:56:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.50 on epoch=103
05/25/2022 16:56:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=104
05/25/2022 16:56:12 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=104
05/25/2022 16:56:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=104
05/25/2022 16:56:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.52 on epoch=105
05/25/2022 16:56:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=106
05/25/2022 16:56:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=107
05/25/2022 16:56:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=108
05/25/2022 16:56:30 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.16732026143790854 on epoch=108
05/25/2022 16:56:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.51 on epoch=109
05/25/2022 16:56:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=109
05/25/2022 16:56:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.51 on epoch=110
05/25/2022 16:56:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.54 on epoch=111
05/25/2022 16:56:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=112
05/25/2022 16:56:49 - INFO - __main__ - Global step 1350 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=112
05/25/2022 16:56:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=113
05/25/2022 16:56:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=114
05/25/2022 16:56:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=114
05/25/2022 16:57:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=115
05/25/2022 16:57:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=116
05/25/2022 16:57:08 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.21384176339793756 on epoch=116
05/25/2022 16:57:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=117
05/25/2022 16:57:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
05/25/2022 16:57:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=119
05/25/2022 16:57:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=119
05/25/2022 16:57:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.53 on epoch=120
05/25/2022 16:57:27 - INFO - __main__ - Global step 1450 Train loss 0.50 Classification-F1 0.2606100412996965 on epoch=120
05/25/2022 16:57:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.50 on epoch=121
05/25/2022 16:57:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=122
05/25/2022 16:57:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=123
05/25/2022 16:57:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.56 on epoch=124
05/25/2022 16:57:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.54 on epoch=124
05/25/2022 16:57:45 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.19056716417910446 on epoch=124
05/25/2022 16:57:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.49 on epoch=125
05/25/2022 16:57:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=126
05/25/2022 16:57:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.54 on epoch=127
05/25/2022 16:57:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=128
05/25/2022 16:57:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.51 on epoch=129
05/25/2022 16:58:04 - INFO - __main__ - Global step 1550 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=129
05/25/2022 16:58:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=129
05/25/2022 16:58:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=130
05/25/2022 16:58:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=131
05/25/2022 16:58:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=132
05/25/2022 16:58:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=133
05/25/2022 16:58:23 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.17911609088079675 on epoch=133
05/25/2022 16:58:25 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.54 on epoch=134
05/25/2022 16:58:28 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.48 on epoch=134
05/25/2022 16:58:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.50 on epoch=135
05/25/2022 16:58:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=136
05/25/2022 16:58:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=137
05/25/2022 16:58:41 - INFO - __main__ - Global step 1650 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=137
05/25/2022 16:58:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.51 on epoch=138
05/25/2022 16:58:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=139
05/25/2022 16:58:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=139
05/25/2022 16:58:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.49 on epoch=140
05/25/2022 16:58:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.49 on epoch=141
05/25/2022 16:59:00 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.20151667775919033 on epoch=141
05/25/2022 16:59:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=142
05/25/2022 16:59:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=143
05/25/2022 16:59:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=144
05/25/2022 16:59:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.52 on epoch=144
05/25/2022 16:59:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=145
05/25/2022 16:59:19 - INFO - __main__ - Global step 1750 Train loss 0.49 Classification-F1 0.2720272732616235 on epoch=145
05/25/2022 16:59:19 - INFO - __main__ - Saving model with best Classification-F1: 0.26895030913051127 -> 0.2720272732616235 on epoch=145, global_step=1750
05/25/2022 16:59:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=146
05/25/2022 16:59:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=147
05/25/2022 16:59:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.53 on epoch=148
05/25/2022 16:59:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=149
05/25/2022 16:59:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.50 on epoch=149
05/25/2022 16:59:38 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.18971428571428572 on epoch=149
05/25/2022 16:59:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=150
05/25/2022 16:59:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=151
05/25/2022 16:59:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=152
05/25/2022 16:59:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=153
05/25/2022 16:59:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=154
05/25/2022 16:59:57 - INFO - __main__ - Global step 1850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=154
05/25/2022 16:59:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=154
05/25/2022 17:00:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=155
05/25/2022 17:00:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.50 on epoch=156
05/25/2022 17:00:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.57 on epoch=157
05/25/2022 17:00:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=158
05/25/2022 17:00:16 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=158
05/25/2022 17:00:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=159
05/25/2022 17:00:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=159
05/25/2022 17:00:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=160
05/25/2022 17:00:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.47 on epoch=161
05/25/2022 17:00:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=162
05/25/2022 17:00:35 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=162
05/25/2022 17:00:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=163
05/25/2022 17:00:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=164
05/25/2022 17:00:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=164
05/25/2022 17:00:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.54 on epoch=165
05/25/2022 17:00:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.47 on epoch=166
05/25/2022 17:00:54 - INFO - __main__ - Global step 2000 Train loss 0.48 Classification-F1 0.24228082037412668 on epoch=166
05/25/2022 17:00:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=167
05/25/2022 17:00:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
05/25/2022 17:01:02 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=169
05/25/2022 17:01:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.48 on epoch=169
05/25/2022 17:01:07 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.46 on epoch=170
05/25/2022 17:01:12 - INFO - __main__ - Global step 2050 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=170
05/25/2022 17:01:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.50 on epoch=171
05/25/2022 17:01:18 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.48 on epoch=172
05/25/2022 17:01:20 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.49 on epoch=173
05/25/2022 17:01:23 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.50 on epoch=174
05/25/2022 17:01:26 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=174
05/25/2022 17:01:31 - INFO - __main__ - Global step 2100 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=174
05/25/2022 17:01:34 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.48 on epoch=175
05/25/2022 17:01:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.52 on epoch=176
05/25/2022 17:01:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.52 on epoch=177
05/25/2022 17:01:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=178
05/25/2022 17:01:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.46 on epoch=179
05/25/2022 17:01:50 - INFO - __main__ - Global step 2150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=179
05/25/2022 17:01:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.55 on epoch=179
05/25/2022 17:01:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.50 on epoch=180
05/25/2022 17:01:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=181
05/25/2022 17:02:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.50 on epoch=182
05/25/2022 17:02:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.48 on epoch=183
05/25/2022 17:02:09 - INFO - __main__ - Global step 2200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=183
05/25/2022 17:02:12 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=184
05/25/2022 17:02:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.48 on epoch=184
05/25/2022 17:02:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.48 on epoch=185
05/25/2022 17:02:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.50 on epoch=186
05/25/2022 17:02:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=187
05/25/2022 17:02:28 - INFO - __main__ - Global step 2250 Train loss 0.48 Classification-F1 0.1775766716943188 on epoch=187
05/25/2022 17:02:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.48 on epoch=188
05/25/2022 17:02:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.46 on epoch=189
05/25/2022 17:02:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=189
05/25/2022 17:02:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.52 on epoch=190
05/25/2022 17:02:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.48 on epoch=191
05/25/2022 17:02:47 - INFO - __main__ - Global step 2300 Train loss 0.48 Classification-F1 0.3393464052287582 on epoch=191
05/25/2022 17:02:47 - INFO - __main__ - Saving model with best Classification-F1: 0.2720272732616235 -> 0.3393464052287582 on epoch=191, global_step=2300
05/25/2022 17:02:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.49 on epoch=192
05/25/2022 17:02:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.52 on epoch=193
05/25/2022 17:02:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.49 on epoch=194
05/25/2022 17:02:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.49 on epoch=194
05/25/2022 17:03:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.51 on epoch=195
05/25/2022 17:03:06 - INFO - __main__ - Global step 2350 Train loss 0.50 Classification-F1 0.2899497891172408 on epoch=195
05/25/2022 17:03:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=196
05/25/2022 17:03:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.49 on epoch=197
05/25/2022 17:03:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.53 on epoch=198
05/25/2022 17:03:17 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.49 on epoch=199
05/25/2022 17:03:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.50 on epoch=199
05/25/2022 17:03:25 - INFO - __main__ - Global step 2400 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=199
05/25/2022 17:03:28 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.50 on epoch=200
05/25/2022 17:03:31 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.51 on epoch=201
05/25/2022 17:03:33 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.51 on epoch=202
05/25/2022 17:03:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=203
05/25/2022 17:03:39 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.50 on epoch=204
05/25/2022 17:03:44 - INFO - __main__ - Global step 2450 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=204
05/25/2022 17:03:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.47 on epoch=204
05/25/2022 17:03:50 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=205
05/25/2022 17:03:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.45 on epoch=206
05/25/2022 17:03:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.45 on epoch=207
05/25/2022 17:03:58 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=208
05/25/2022 17:04:03 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.16732026143790854 on epoch=208
05/25/2022 17:04:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.47 on epoch=209
05/25/2022 17:04:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.48 on epoch=209
05/25/2022 17:04:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=210
05/25/2022 17:04:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.47 on epoch=211
05/25/2022 17:04:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=212
05/25/2022 17:04:22 - INFO - __main__ - Global step 2550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=212
05/25/2022 17:04:25 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
05/25/2022 17:04:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.50 on epoch=214
05/25/2022 17:04:30 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=214
05/25/2022 17:04:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.46 on epoch=215
05/25/2022 17:04:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=216
05/25/2022 17:04:41 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=216
05/25/2022 17:04:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.54 on epoch=217
05/25/2022 17:04:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.50 on epoch=218
05/25/2022 17:04:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.45 on epoch=219
05/25/2022 17:04:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=219
05/25/2022 17:04:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.50 on epoch=220
05/25/2022 17:04:59 - INFO - __main__ - Global step 2650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=220
05/25/2022 17:05:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.44 on epoch=221
05/25/2022 17:05:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=222
05/25/2022 17:05:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.47 on epoch=223
05/25/2022 17:05:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.46 on epoch=224
05/25/2022 17:05:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=224
05/25/2022 17:05:18 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=224
05/25/2022 17:05:20 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.47 on epoch=225
05/25/2022 17:05:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.46 on epoch=226
05/25/2022 17:05:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.45 on epoch=227
05/25/2022 17:05:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.49 on epoch=228
05/25/2022 17:05:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.48 on epoch=229
05/25/2022 17:05:37 - INFO - __main__ - Global step 2750 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=229
05/25/2022 17:05:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=229
05/25/2022 17:05:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.46 on epoch=230
05/25/2022 17:05:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.50 on epoch=231
05/25/2022 17:05:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=232
05/25/2022 17:05:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=233
05/25/2022 17:05:56 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.2637182072899312 on epoch=233
05/25/2022 17:05:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.46 on epoch=234
05/25/2022 17:06:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.42 on epoch=234
05/25/2022 17:06:04 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.49 on epoch=235
05/25/2022 17:06:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.52 on epoch=236
05/25/2022 17:06:09 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.52 on epoch=237
05/25/2022 17:06:14 - INFO - __main__ - Global step 2850 Train loss 0.48 Classification-F1 0.19946907708101735 on epoch=237
05/25/2022 17:06:17 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.45 on epoch=238
05/25/2022 17:06:20 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.47 on epoch=239
05/25/2022 17:06:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.42 on epoch=239
05/25/2022 17:06:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.47 on epoch=240
05/25/2022 17:06:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.51 on epoch=241
05/25/2022 17:06:34 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.28425665101721437 on epoch=241
05/25/2022 17:06:36 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.47 on epoch=242
05/25/2022 17:06:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.46 on epoch=243
05/25/2022 17:06:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.49 on epoch=244
05/25/2022 17:06:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=244
05/25/2022 17:06:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.49 on epoch=245
05/25/2022 17:06:53 - INFO - __main__ - Global step 2950 Train loss 0.47 Classification-F1 0.34658587493276505 on epoch=245
05/25/2022 17:06:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3393464052287582 -> 0.34658587493276505 on epoch=245, global_step=2950
05/25/2022 17:06:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.45 on epoch=246
05/25/2022 17:06:58 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.49 on epoch=247
05/25/2022 17:07:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
05/25/2022 17:07:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.52 on epoch=249
05/25/2022 17:07:06 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.47 on epoch=249
05/25/2022 17:07:07 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:07:07 - INFO - __main__ - Printing 3 examples
05/25/2022 17:07:07 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:07:07 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:07 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:07:07 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:07 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:07:07 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:07 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:07:07 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:07:08 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:07:08 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:07:08 - INFO - __main__ - Printing 3 examples
05/25/2022 17:07:08 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:07:08 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:08 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:07:08 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:08 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:07:08 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:08 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:07:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:07:08 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:07:11 - INFO - __main__ - Global step 3000 Train loss 0.47 Classification-F1 0.1859084026507777 on epoch=249
05/25/2022 17:07:11 - INFO - __main__ - save last model!
05/25/2022 17:07:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 17:07:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 17:07:12 - INFO - __main__ - Printing 3 examples
05/25/2022 17:07:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 17:07:12 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 17:07:12 - INFO - __main__ - ['entailment']
05/25/2022 17:07:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 17:07:12 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:12 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:07:12 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:07:13 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 17:07:23 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:07:23 - INFO - __main__ - task name: anli
05/25/2022 17:07:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:07:24 - INFO - __main__ - Starting training!
05/25/2022 17:07:43 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_42_0.2_8_predictions.txt
05/25/2022 17:07:43 - INFO - __main__ - Classification-F1 on test data: 0.1737
05/25/2022 17:07:43 - INFO - __main__ - prefix=anli_64_42, lr=0.2, bsz=8, dev_performance=0.34658587493276505, test_performance=0.17365296308348702
05/25/2022 17:07:43 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.5, bsz=8 ...
05/25/2022 17:07:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:07:44 - INFO - __main__ - Printing 3 examples
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:07:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:07:44 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:07:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:07:44 - INFO - __main__ - Printing 3 examples
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:07:44 - INFO - __main__ - ['contradiction']
05/25/2022 17:07:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:07:45 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:07:45 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:07:59 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:07:59 - INFO - __main__ - task name: anli
05/25/2022 17:08:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:08:00 - INFO - __main__ - Starting training!
05/25/2022 17:08:03 - INFO - __main__ - Step 10 Global step 10 Train loss 5.67 on epoch=0
05/25/2022 17:08:06 - INFO - __main__ - Step 20 Global step 20 Train loss 2.48 on epoch=1
05/25/2022 17:08:08 - INFO - __main__ - Step 30 Global step 30 Train loss 1.13 on epoch=2
05/25/2022 17:08:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.84 on epoch=3
05/25/2022 17:08:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=4
05/25/2022 17:08:17 - INFO - __main__ - Global step 50 Train loss 2.17 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 17:08:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 17:08:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=4
05/25/2022 17:08:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.57 on epoch=5
05/25/2022 17:08:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=6
05/25/2022 17:08:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=7
05/25/2022 17:08:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=8
05/25/2022 17:08:36 - INFO - __main__ - Global step 100 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 17:08:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=9
05/25/2022 17:08:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=9
05/25/2022 17:08:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=10
05/25/2022 17:08:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=11
05/25/2022 17:08:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.53 on epoch=12
05/25/2022 17:08:55 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 17:08:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=13
05/25/2022 17:09:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=14
05/25/2022 17:09:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/25/2022 17:09:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=15
05/25/2022 17:09:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
05/25/2022 17:09:14 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 17:09:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=17
05/25/2022 17:09:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
05/25/2022 17:09:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
05/25/2022 17:09:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
05/25/2022 17:09:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=20
05/25/2022 17:09:32 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 17:09:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/25/2022 17:09:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
05/25/2022 17:09:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
05/25/2022 17:09:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=24
05/25/2022 17:09:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=24
05/25/2022 17:09:51 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 17:09:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
05/25/2022 17:09:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/25/2022 17:09:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/25/2022 17:10:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/25/2022 17:10:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
05/25/2022 17:10:10 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.18399830629174121 on epoch=29
05/25/2022 17:10:10 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.18399830629174121 on epoch=29, global_step=350
05/25/2022 17:10:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 17:10:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=30
05/25/2022 17:10:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
05/25/2022 17:10:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=32
05/25/2022 17:10:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/25/2022 17:10:29 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=33
05/25/2022 17:10:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=34
05/25/2022 17:10:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
05/25/2022 17:10:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
05/25/2022 17:10:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
05/25/2022 17:10:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
05/25/2022 17:10:48 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.30529100529100534 on epoch=37
05/25/2022 17:10:48 - INFO - __main__ - Saving model with best Classification-F1: 0.18399830629174121 -> 0.30529100529100534 on epoch=37, global_step=450
05/25/2022 17:10:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=38
05/25/2022 17:10:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
05/25/2022 17:10:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/25/2022 17:10:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=40
05/25/2022 17:11:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
05/25/2022 17:11:07 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.1926530612244898 on epoch=41
05/25/2022 17:11:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=42
05/25/2022 17:11:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/25/2022 17:11:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/25/2022 17:11:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=44
05/25/2022 17:11:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
05/25/2022 17:11:26 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 17:11:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/25/2022 17:11:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=47
05/25/2022 17:11:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
05/25/2022 17:11:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=49
05/25/2022 17:11:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=49
05/25/2022 17:11:45 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.20311539249846278 on epoch=49
05/25/2022 17:11:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=50
05/25/2022 17:11:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
05/25/2022 17:11:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
05/25/2022 17:11:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
05/25/2022 17:11:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=54
05/25/2022 17:12:04 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=54
05/25/2022 17:12:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=54
05/25/2022 17:12:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=55
05/25/2022 17:12:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=56
05/25/2022 17:12:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=57
05/25/2022 17:12:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/25/2022 17:12:23 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.28928372289283727 on epoch=58
05/25/2022 17:12:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
05/25/2022 17:12:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
05/25/2022 17:12:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=60
05/25/2022 17:12:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/25/2022 17:12:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=62
05/25/2022 17:12:42 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.27536231884057966 on epoch=62
05/25/2022 17:12:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=63
05/25/2022 17:12:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=64
05/25/2022 17:12:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
05/25/2022 17:12:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
05/25/2022 17:12:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
05/25/2022 17:13:01 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 17:13:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
05/25/2022 17:13:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=68
05/25/2022 17:13:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=69
05/25/2022 17:13:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=69
05/25/2022 17:13:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.40 on epoch=70
05/25/2022 17:13:20 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.29309334367855633 on epoch=70
05/25/2022 17:13:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=71
05/25/2022 17:13:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=72
05/25/2022 17:13:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/25/2022 17:13:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=74
05/25/2022 17:13:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
05/25/2022 17:13:39 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=74
05/25/2022 17:13:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=75
05/25/2022 17:13:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=76
05/25/2022 17:13:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=77
05/25/2022 17:13:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/25/2022 17:13:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=79
05/25/2022 17:13:58 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.1775766716943188 on epoch=79
05/25/2022 17:14:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=79
05/25/2022 17:14:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.48 on epoch=80
05/25/2022 17:14:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=81
05/25/2022 17:14:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=82
05/25/2022 17:14:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=83
05/25/2022 17:14:17 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.21656600517687663 on epoch=83
05/25/2022 17:14:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=84
05/25/2022 17:14:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=84
05/25/2022 17:14:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=85
05/25/2022 17:14:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=86
05/25/2022 17:14:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
05/25/2022 17:14:37 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.32603158430972995 on epoch=87
05/25/2022 17:14:37 - INFO - __main__ - Saving model with best Classification-F1: 0.30529100529100534 -> 0.32603158430972995 on epoch=87, global_step=1050
05/25/2022 17:14:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/25/2022 17:14:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=89
05/25/2022 17:14:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
05/25/2022 17:14:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=90
05/25/2022 17:14:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=91
05/25/2022 17:14:55 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 17:14:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=92
05/25/2022 17:15:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.70 on epoch=93
05/25/2022 17:15:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.57 on epoch=94
05/25/2022 17:15:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.84 on epoch=94
05/25/2022 17:15:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=95
05/25/2022 17:15:14 - INFO - __main__ - Global step 1150 Train loss 0.60 Classification-F1 0.3117276080039167 on epoch=95
05/25/2022 17:15:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
05/25/2022 17:15:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
05/25/2022 17:15:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
05/25/2022 17:15:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=99
05/25/2022 17:15:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=99
05/25/2022 17:15:33 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.2694685180008076 on epoch=99
05/25/2022 17:15:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=100
05/25/2022 17:15:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
05/25/2022 17:15:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=102
05/25/2022 17:15:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=103
05/25/2022 17:15:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=104
05/25/2022 17:15:52 - INFO - __main__ - Global step 1250 Train loss 0.44 Classification-F1 0.24755338959546735 on epoch=104
05/25/2022 17:15:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/25/2022 17:15:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=105
05/25/2022 17:16:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
05/25/2022 17:16:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=107
05/25/2022 17:16:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=108
05/25/2022 17:16:11 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.20163659666765257 on epoch=108
05/25/2022 17:16:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=109
05/25/2022 17:16:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
05/25/2022 17:16:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=110
05/25/2022 17:16:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=111
05/25/2022 17:16:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
05/25/2022 17:16:30 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.21645756069333755 on epoch=112
05/25/2022 17:16:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
05/25/2022 17:16:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=114
05/25/2022 17:16:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=114
05/25/2022 17:16:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=115
05/25/2022 17:16:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
05/25/2022 17:16:49 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.3322643439900077 on epoch=116
05/25/2022 17:16:49 - INFO - __main__ - Saving model with best Classification-F1: 0.32603158430972995 -> 0.3322643439900077 on epoch=116, global_step=1400
05/25/2022 17:16:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
05/25/2022 17:16:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.40 on epoch=118
05/25/2022 17:16:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
05/25/2022 17:17:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=119
05/25/2022 17:17:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 17:17:08 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.26720647773279355 on epoch=120
05/25/2022 17:17:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=121
05/25/2022 17:17:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=122
05/25/2022 17:17:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=123
05/25/2022 17:17:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=124
05/25/2022 17:17:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=124
05/25/2022 17:17:27 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.23826726688586897 on epoch=124
05/25/2022 17:17:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=125
05/25/2022 17:17:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=126
05/25/2022 17:17:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=127
05/25/2022 17:17:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
05/25/2022 17:17:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=129
05/25/2022 17:17:47 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.20707752286699654 on epoch=129
05/25/2022 17:17:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=129
05/25/2022 17:17:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=130
05/25/2022 17:17:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.45 on epoch=131
05/25/2022 17:17:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
05/25/2022 17:18:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=133
05/25/2022 17:18:07 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=133
05/25/2022 17:18:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=134
05/25/2022 17:18:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
05/25/2022 17:18:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=135
05/25/2022 17:18:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=136
05/25/2022 17:18:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
05/25/2022 17:18:26 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.24469072255195476 on epoch=137
05/25/2022 17:18:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=138
05/25/2022 17:18:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 17:18:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=139
05/25/2022 17:18:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=140
05/25/2022 17:18:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=141
05/25/2022 17:18:45 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.30938534278959806 on epoch=141
05/25/2022 17:18:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=142
05/25/2022 17:18:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
05/25/2022 17:18:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=144
05/25/2022 17:18:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=144
05/25/2022 17:18:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
05/25/2022 17:19:05 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.2848555800191126 on epoch=145
05/25/2022 17:19:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 17:19:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=147
05/25/2022 17:19:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
05/25/2022 17:19:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=149
05/25/2022 17:19:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=149
05/25/2022 17:19:24 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.3243413101281121 on epoch=149
05/25/2022 17:19:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=150
05/25/2022 17:19:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=151
05/25/2022 17:19:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=152
05/25/2022 17:19:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
05/25/2022 17:19:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=154
05/25/2022 17:19:43 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.2326951399116348 on epoch=154
05/25/2022 17:19:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
05/25/2022 17:19:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 17:19:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=156
05/25/2022 17:19:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
05/25/2022 17:19:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
05/25/2022 17:20:03 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.2322903629536921 on epoch=158
05/25/2022 17:20:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=159
05/25/2022 17:20:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=159
05/25/2022 17:20:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=160
05/25/2022 17:20:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=161
05/25/2022 17:20:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
05/25/2022 17:20:22 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.25496031746031744 on epoch=162
05/25/2022 17:20:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=163
05/25/2022 17:20:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=164
05/25/2022 17:20:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
05/25/2022 17:20:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
05/25/2022 17:20:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=166
05/25/2022 17:20:41 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.3210242587601078 on epoch=166
05/25/2022 17:20:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.39 on epoch=167
05/25/2022 17:20:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.48 on epoch=168
05/25/2022 17:20:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=169
05/25/2022 17:20:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=169
05/25/2022 17:20:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=170
05/25/2022 17:21:00 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.1647058823529412 on epoch=170
05/25/2022 17:21:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=171
05/25/2022 17:21:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
05/25/2022 17:21:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=173
05/25/2022 17:21:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 17:21:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=174
05/25/2022 17:21:20 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.3115371527923443 on epoch=174
05/25/2022 17:21:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
05/25/2022 17:21:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=176
05/25/2022 17:21:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
05/25/2022 17:21:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.40 on epoch=178
05/25/2022 17:21:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=179
05/25/2022 17:21:39 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.17861409796893668 on epoch=179
05/25/2022 17:21:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=179
05/25/2022 17:21:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=180
05/25/2022 17:21:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
05/25/2022 17:21:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=182
05/25/2022 17:21:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.39 on epoch=183
05/25/2022 17:21:59 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.23262670936912208 on epoch=183
05/25/2022 17:22:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
05/25/2022 17:22:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 17:22:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=185
05/25/2022 17:22:10 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=186
05/25/2022 17:22:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=187
05/25/2022 17:22:18 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.3031650349849657 on epoch=187
05/25/2022 17:22:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=188
05/25/2022 17:22:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
05/25/2022 17:22:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=189
05/25/2022 17:22:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=190
05/25/2022 17:22:32 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=191
05/25/2022 17:22:37 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.19872393401805166 on epoch=191
05/25/2022 17:22:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
05/25/2022 17:22:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
05/25/2022 17:22:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=194
05/25/2022 17:22:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.40 on epoch=194
05/25/2022 17:22:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
05/25/2022 17:22:57 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.23447282337904293 on epoch=195
05/25/2022 17:22:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.46 on epoch=196
05/25/2022 17:23:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
05/25/2022 17:23:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=198
05/25/2022 17:23:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=199
05/25/2022 17:23:10 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=199
05/25/2022 17:23:16 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.3147401908801697 on epoch=199
05/25/2022 17:23:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=200
05/25/2022 17:23:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=201
05/25/2022 17:23:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.37 on epoch=202
05/25/2022 17:23:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
05/25/2022 17:23:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=204
05/25/2022 17:23:35 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=204
05/25/2022 17:23:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.42 on epoch=204
05/25/2022 17:23:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=205
05/25/2022 17:23:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
05/25/2022 17:23:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.38 on epoch=207
05/25/2022 17:23:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=208
05/25/2022 17:23:55 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.25114339521119183 on epoch=208
05/25/2022 17:23:58 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=209
05/25/2022 17:24:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=209
05/25/2022 17:24:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=210
05/25/2022 17:24:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=211
05/25/2022 17:24:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
05/25/2022 17:24:14 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.2822886989553656 on epoch=212
05/25/2022 17:24:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=213
05/25/2022 17:24:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.40 on epoch=214
05/25/2022 17:24:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
05/25/2022 17:24:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.41 on epoch=215
05/25/2022 17:24:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.40 on epoch=216
05/25/2022 17:24:33 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.2956784225757765 on epoch=216
05/25/2022 17:24:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=217
05/25/2022 17:24:39 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
05/25/2022 17:24:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=219
05/25/2022 17:24:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=219
05/25/2022 17:24:47 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=220
05/25/2022 17:24:53 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.22274980741631 on epoch=220
05/25/2022 17:24:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.43 on epoch=221
05/25/2022 17:24:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=222
05/25/2022 17:25:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.39 on epoch=223
05/25/2022 17:25:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.43 on epoch=224
05/25/2022 17:25:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.43 on epoch=224
05/25/2022 17:25:12 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.3118058079890141 on epoch=224
05/25/2022 17:25:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.36 on epoch=225
05/25/2022 17:25:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=226
05/25/2022 17:25:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
05/25/2022 17:25:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.42 on epoch=228
05/25/2022 17:25:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=229
05/25/2022 17:25:31 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.29713804713804715 on epoch=229
05/25/2022 17:25:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=229
05/25/2022 17:25:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.41 on epoch=230
05/25/2022 17:25:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 1.08 on epoch=231
05/25/2022 17:25:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 1.10 on epoch=232
05/25/2022 17:25:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.49 on epoch=233
05/25/2022 17:25:51 - INFO - __main__ - Global step 2800 Train loss 0.70 Classification-F1 0.3193811718235681 on epoch=233
05/25/2022 17:25:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.49 on epoch=234
05/25/2022 17:25:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.47 on epoch=234
05/25/2022 17:25:59 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.44 on epoch=235
05/25/2022 17:26:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=236
05/25/2022 17:26:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=237
05/25/2022 17:26:10 - INFO - __main__ - Global step 2850 Train loss 0.46 Classification-F1 0.3195627958785853 on epoch=237
05/25/2022 17:26:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.47 on epoch=238
05/25/2022 17:26:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.48 on epoch=239
05/25/2022 17:26:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=239
05/25/2022 17:26:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
05/25/2022 17:26:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=241
05/25/2022 17:26:29 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.2117758784425451 on epoch=241
05/25/2022 17:26:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=242
05/25/2022 17:26:34 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
05/25/2022 17:26:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.47 on epoch=244
05/25/2022 17:26:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.42 on epoch=244
05/25/2022 17:26:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.44 on epoch=245
05/25/2022 17:26:48 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.3133261403063383 on epoch=245
05/25/2022 17:26:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.53 on epoch=246
05/25/2022 17:26:54 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=247
05/25/2022 17:26:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=248
05/25/2022 17:26:59 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.45 on epoch=249
05/25/2022 17:27:02 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=249
05/25/2022 17:27:03 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:27:03 - INFO - __main__ - Printing 3 examples
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:27:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:27:03 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:27:03 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:27:03 - INFO - __main__ - Printing 3 examples
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:27:03 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:27:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:27:04 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:27:07 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.2639666017042066 on epoch=249
05/25/2022 17:27:07 - INFO - __main__ - save last model!
05/25/2022 17:27:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 17:27:07 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 17:27:07 - INFO - __main__ - Printing 3 examples
05/25/2022 17:27:07 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 17:27:07 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:07 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 17:27:07 - INFO - __main__ - ['entailment']
05/25/2022 17:27:07 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 17:27:07 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:07 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:27:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:27:09 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 17:27:18 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:27:18 - INFO - __main__ - task name: anli
05/25/2022 17:27:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:27:19 - INFO - __main__ - Starting training!
05/25/2022 17:27:39 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_87_0.5_8_predictions.txt
05/25/2022 17:27:39 - INFO - __main__ - Classification-F1 on test data: 0.2512
05/25/2022 17:27:39 - INFO - __main__ - prefix=anli_64_87, lr=0.5, bsz=8, dev_performance=0.3322643439900077, test_performance=0.2511702707015207
05/25/2022 17:27:39 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.4, bsz=8 ...
05/25/2022 17:27:40 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:27:40 - INFO - __main__ - Printing 3 examples
05/25/2022 17:27:40 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:27:40 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:40 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:27:40 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:40 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:27:40 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:40 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:27:40 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:27:41 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:27:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:27:41 - INFO - __main__ - Printing 3 examples
05/25/2022 17:27:41 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:27:41 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:41 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:27:41 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:41 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:27:41 - INFO - __main__ - ['contradiction']
05/25/2022 17:27:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:27:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:27:41 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:27:55 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:27:55 - INFO - __main__ - task name: anli
05/25/2022 17:27:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:27:56 - INFO - __main__ - Starting training!
05/25/2022 17:27:59 - INFO - __main__ - Step 10 Global step 10 Train loss 5.61 on epoch=0
05/25/2022 17:28:02 - INFO - __main__ - Step 20 Global step 20 Train loss 2.64 on epoch=1
05/25/2022 17:28:04 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=2
05/25/2022 17:28:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.92 on epoch=3
05/25/2022 17:28:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.81 on epoch=4
05/25/2022 17:28:13 - INFO - __main__ - Global step 50 Train loss 2.24 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 17:28:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 17:28:16 - INFO - __main__ - Step 60 Global step 60 Train loss 0.70 on epoch=4
05/25/2022 17:28:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=5
05/25/2022 17:28:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=6
05/25/2022 17:28:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=7
05/25/2022 17:28:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.69 on epoch=8
05/25/2022 17:28:31 - INFO - __main__ - Global step 100 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=8
05/25/2022 17:28:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.64 on epoch=9
05/25/2022 17:28:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=9
05/25/2022 17:28:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=10
05/25/2022 17:28:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=11
05/25/2022 17:28:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.57 on epoch=12
05/25/2022 17:28:48 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 17:28:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=13
05/25/2022 17:28:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
05/25/2022 17:28:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=14
05/25/2022 17:28:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.57 on epoch=15
05/25/2022 17:29:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=16
05/25/2022 17:29:06 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 17:29:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
05/25/2022 17:29:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=18
05/25/2022 17:29:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
05/25/2022 17:29:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=19
05/25/2022 17:29:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=20
05/25/2022 17:29:24 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 17:29:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/25/2022 17:29:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=22
05/25/2022 17:29:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
05/25/2022 17:29:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=24
05/25/2022 17:29:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=24
05/25/2022 17:29:43 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 17:29:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=25
05/25/2022 17:29:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
05/25/2022 17:29:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=27
05/25/2022 17:29:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/25/2022 17:29:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
05/25/2022 17:30:01 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.2691768180140273 on epoch=29
05/25/2022 17:30:01 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2691768180140273 on epoch=29, global_step=350
05/25/2022 17:30:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 17:30:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=30
05/25/2022 17:30:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=31
05/25/2022 17:30:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
05/25/2022 17:30:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 17:30:20 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.1881810228266921 on epoch=33
05/25/2022 17:30:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=34
05/25/2022 17:30:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=34
05/25/2022 17:30:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=35
05/25/2022 17:30:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=36
05/25/2022 17:30:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=37
05/25/2022 17:30:39 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 17:30:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/25/2022 17:30:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=39
05/25/2022 17:30:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=39
05/25/2022 17:30:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=40
05/25/2022 17:30:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=41
05/25/2022 17:30:57 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/25/2022 17:30:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
05/25/2022 17:31:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=43
05/25/2022 17:31:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=44
05/25/2022 17:31:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=44
05/25/2022 17:31:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=45
05/25/2022 17:31:15 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 17:31:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=46
05/25/2022 17:31:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=47
05/25/2022 17:31:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=48
05/25/2022 17:31:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=49
05/25/2022 17:31:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/25/2022 17:31:34 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2593241551939925 on epoch=49
05/25/2022 17:31:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
05/25/2022 17:31:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
05/25/2022 17:31:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=52
05/25/2022 17:31:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=53
05/25/2022 17:31:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=54
05/25/2022 17:31:53 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.25826788063511913 on epoch=54
05/25/2022 17:31:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=54
05/25/2022 17:31:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
05/25/2022 17:32:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=56
05/25/2022 17:32:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
05/25/2022 17:32:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
05/25/2022 17:32:11 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 17:32:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=59
05/25/2022 17:32:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
05/25/2022 17:32:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=60
05/25/2022 17:32:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
05/25/2022 17:32:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
05/25/2022 17:32:30 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.29558260085643456 on epoch=62
05/25/2022 17:32:30 - INFO - __main__ - Saving model with best Classification-F1: 0.2691768180140273 -> 0.29558260085643456 on epoch=62, global_step=750
05/25/2022 17:32:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
05/25/2022 17:32:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
05/25/2022 17:32:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=64
05/25/2022 17:32:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
05/25/2022 17:32:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=66
05/25/2022 17:32:49 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 17:32:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
05/25/2022 17:32:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/25/2022 17:32:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=69
05/25/2022 17:32:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=69
05/25/2022 17:33:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
05/25/2022 17:33:07 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.19175627240143367 on epoch=70
05/25/2022 17:33:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
05/25/2022 17:33:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
05/25/2022 17:33:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 17:33:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=74
05/25/2022 17:33:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=74
05/25/2022 17:33:26 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.29944095038434665 on epoch=74
05/25/2022 17:33:27 - INFO - __main__ - Saving model with best Classification-F1: 0.29558260085643456 -> 0.29944095038434665 on epoch=74, global_step=900
05/25/2022 17:33:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=75
05/25/2022 17:33:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=76
05/25/2022 17:33:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=77
05/25/2022 17:33:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=78
05/25/2022 17:33:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
05/25/2022 17:33:45 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.20591651294664204 on epoch=79
05/25/2022 17:33:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
05/25/2022 17:33:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
05/25/2022 17:33:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=81
05/25/2022 17:33:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=82
05/25/2022 17:33:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
05/25/2022 17:34:04 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3192903941489463 on epoch=83
05/25/2022 17:34:04 - INFO - __main__ - Saving model with best Classification-F1: 0.29944095038434665 -> 0.3192903941489463 on epoch=83, global_step=1000
05/25/2022 17:34:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
05/25/2022 17:34:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=84
05/25/2022 17:34:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
05/25/2022 17:34:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
05/25/2022 17:34:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
05/25/2022 17:34:23 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 17:34:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=88
05/25/2022 17:34:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
05/25/2022 17:34:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
05/25/2022 17:34:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=90
05/25/2022 17:34:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=91
05/25/2022 17:34:41 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 17:34:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
05/25/2022 17:34:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 17:34:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
05/25/2022 17:34:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=94
05/25/2022 17:34:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.41 on epoch=95
05/25/2022 17:35:00 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=95
05/25/2022 17:35:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=96
05/25/2022 17:35:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
05/25/2022 17:35:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.46 on epoch=98
05/25/2022 17:35:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=99
05/25/2022 17:35:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
05/25/2022 17:35:18 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 17:35:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=100
05/25/2022 17:35:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=101
05/25/2022 17:35:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
05/25/2022 17:35:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
05/25/2022 17:35:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=104
05/25/2022 17:35:37 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.16890060792499817 on epoch=104
05/25/2022 17:35:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=104
05/25/2022 17:35:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=105
05/25/2022 17:35:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
05/25/2022 17:35:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=107
05/25/2022 17:35:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
05/25/2022 17:35:56 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.2222222222222222 on epoch=108
05/25/2022 17:35:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
05/25/2022 17:36:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=109
05/25/2022 17:36:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=110
05/25/2022 17:36:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
05/25/2022 17:36:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=112
05/25/2022 17:36:15 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.2156190476190476 on epoch=112
05/25/2022 17:36:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/25/2022 17:36:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=114
05/25/2022 17:36:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
05/25/2022 17:36:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=115
05/25/2022 17:36:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
05/25/2022 17:36:33 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.24422001053155248 on epoch=116
05/25/2022 17:36:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
05/25/2022 17:36:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=118
05/25/2022 17:36:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=119
05/25/2022 17:36:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
05/25/2022 17:36:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=120
05/25/2022 17:36:52 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=120
05/25/2022 17:36:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=121
05/25/2022 17:36:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
05/25/2022 17:37:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=123
05/25/2022 17:37:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=124
05/25/2022 17:37:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=124
05/25/2022 17:37:11 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.24722222222222223 on epoch=124
05/25/2022 17:37:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=125
05/25/2022 17:37:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=126
05/25/2022 17:37:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
05/25/2022 17:37:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
05/25/2022 17:37:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=129
05/25/2022 17:37:30 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.1775766716943188 on epoch=129
05/25/2022 17:37:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=129
05/25/2022 17:37:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=130
05/25/2022 17:37:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
05/25/2022 17:37:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
05/25/2022 17:37:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
05/25/2022 17:37:49 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.24214130096483036 on epoch=133
05/25/2022 17:37:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=134
05/25/2022 17:37:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
05/25/2022 17:37:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=135
05/25/2022 17:37:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=136
05/25/2022 17:38:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=137
05/25/2022 17:38:07 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.2063100137174211 on epoch=137
05/25/2022 17:38:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
05/25/2022 17:38:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.47 on epoch=139
05/25/2022 17:38:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
05/25/2022 17:38:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
05/25/2022 17:38:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=141
05/25/2022 17:38:26 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 17:38:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=142
05/25/2022 17:38:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.40 on epoch=143
05/25/2022 17:38:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=144
05/25/2022 17:38:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=144
05/25/2022 17:38:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=145
05/25/2022 17:38:45 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.22439281074770198 on epoch=145
05/25/2022 17:38:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/25/2022 17:38:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
05/25/2022 17:38:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=148
05/25/2022 17:38:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=149
05/25/2022 17:38:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=149
05/25/2022 17:39:04 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.2529529372902867 on epoch=149
05/25/2022 17:39:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=150
05/25/2022 17:39:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
05/25/2022 17:39:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=152
05/25/2022 17:39:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=153
05/25/2022 17:39:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=154
05/25/2022 17:39:23 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.18020816002070505 on epoch=154
05/25/2022 17:39:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
05/25/2022 17:39:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=155
05/25/2022 17:39:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
05/25/2022 17:39:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=157
05/25/2022 17:39:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=158
05/25/2022 17:39:42 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.23192730231927305 on epoch=158
05/25/2022 17:39:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
05/25/2022 17:39:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=159
05/25/2022 17:39:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
05/25/2022 17:39:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=161
05/25/2022 17:39:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=162
05/25/2022 17:40:01 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3123700054734537 on epoch=162
05/25/2022 17:40:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=163
05/25/2022 17:40:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.41 on epoch=164
05/25/2022 17:40:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
05/25/2022 17:40:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=165
05/25/2022 17:40:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
05/25/2022 17:40:19 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.3218815513626834 on epoch=166
05/25/2022 17:40:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3192903941489463 -> 0.3218815513626834 on epoch=166, global_step=2000
05/25/2022 17:40:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
05/25/2022 17:40:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
05/25/2022 17:40:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
05/25/2022 17:40:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.39 on epoch=169
05/25/2022 17:40:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
05/25/2022 17:40:39 - INFO - __main__ - Global step 2050 Train loss 0.39 Classification-F1 0.26217834848594096 on epoch=170
05/25/2022 17:40:41 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=171
05/25/2022 17:40:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
05/25/2022 17:40:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=173
05/25/2022 17:40:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
05/25/2022 17:40:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=174
05/25/2022 17:40:58 - INFO - __main__ - Global step 2100 Train loss 0.40 Classification-F1 0.3210962785415996 on epoch=174
05/25/2022 17:41:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=175
05/25/2022 17:41:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
05/25/2022 17:41:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=177
05/25/2022 17:41:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
05/25/2022 17:41:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.41 on epoch=179
05/25/2022 17:41:17 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.30624320148545375 on epoch=179
05/25/2022 17:41:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.38 on epoch=179
05/25/2022 17:41:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=180
05/25/2022 17:41:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=181
05/25/2022 17:41:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.37 on epoch=182
05/25/2022 17:41:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.38 on epoch=183
05/25/2022 17:41:36 - INFO - __main__ - Global step 2200 Train loss 0.37 Classification-F1 0.4000000000000001 on epoch=183
05/25/2022 17:41:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3218815513626834 -> 0.4000000000000001 on epoch=183, global_step=2200
05/25/2022 17:41:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.42 on epoch=184
05/25/2022 17:41:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.38 on epoch=184
05/25/2022 17:41:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=185
05/25/2022 17:41:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=186
05/25/2022 17:41:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=187
05/25/2022 17:41:55 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.31001289305160423 on epoch=187
05/25/2022 17:41:58 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=188
05/25/2022 17:42:01 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.39 on epoch=189
05/25/2022 17:42:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.35 on epoch=189
05/25/2022 17:42:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=190
05/25/2022 17:42:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
05/25/2022 17:42:14 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.3036231884057971 on epoch=191
05/25/2022 17:42:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=192
05/25/2022 17:42:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
05/25/2022 17:42:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=194
05/25/2022 17:42:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=194
05/25/2022 17:42:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.34 on epoch=195
05/25/2022 17:42:33 - INFO - __main__ - Global step 2350 Train loss 0.39 Classification-F1 0.23731884057971012 on epoch=195
05/25/2022 17:42:36 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=196
05/25/2022 17:42:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
05/25/2022 17:42:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=198
05/25/2022 17:42:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=199
05/25/2022 17:42:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=199
05/25/2022 17:42:52 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.3595238095238096 on epoch=199
05/25/2022 17:42:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.39 on epoch=200
05/25/2022 17:42:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.36 on epoch=201
05/25/2022 17:43:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.36 on epoch=202
05/25/2022 17:43:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=203
05/25/2022 17:43:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=204
05/25/2022 17:43:12 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.2660777818672556 on epoch=204
05/25/2022 17:43:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.37 on epoch=204
05/25/2022 17:43:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.35 on epoch=205
05/25/2022 17:43:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.39 on epoch=206
05/25/2022 17:43:23 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=207
05/25/2022 17:43:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.38 on epoch=208
05/25/2022 17:43:31 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.36613615030277263 on epoch=208
05/25/2022 17:43:34 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.34 on epoch=209
05/25/2022 17:43:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
05/25/2022 17:43:39 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=210
05/25/2022 17:43:42 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=211
05/25/2022 17:43:44 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=212
05/25/2022 17:43:50 - INFO - __main__ - Global step 2550 Train loss 0.35 Classification-F1 0.3281830024606683 on epoch=212
05/25/2022 17:43:53 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=213
05/25/2022 17:43:56 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.36 on epoch=214
05/25/2022 17:43:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.38 on epoch=214
05/25/2022 17:44:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=215
05/25/2022 17:44:04 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.36 on epoch=216
05/25/2022 17:44:09 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.22923225566703828 on epoch=216
05/25/2022 17:44:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=217
05/25/2022 17:44:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.35 on epoch=218
05/25/2022 17:44:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=219
05/25/2022 17:44:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.35 on epoch=219
05/25/2022 17:44:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.35 on epoch=220
05/25/2022 17:44:28 - INFO - __main__ - Global step 2650 Train loss 0.35 Classification-F1 0.41081704467222213 on epoch=220
05/25/2022 17:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4000000000000001 -> 0.41081704467222213 on epoch=220, global_step=2650
05/25/2022 17:44:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.34 on epoch=221
05/25/2022 17:44:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.35 on epoch=222
05/25/2022 17:44:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=223
05/25/2022 17:44:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.34 on epoch=224
05/25/2022 17:44:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
05/25/2022 17:44:48 - INFO - __main__ - Global step 2700 Train loss 0.35 Classification-F1 0.32279982412366437 on epoch=224
05/25/2022 17:44:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=225
05/25/2022 17:44:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.34 on epoch=226
05/25/2022 17:44:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=227
05/25/2022 17:44:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=228
05/25/2022 17:45:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.33 on epoch=229
05/25/2022 17:45:07 - INFO - __main__ - Global step 2750 Train loss 0.33 Classification-F1 0.3737892370825784 on epoch=229
05/25/2022 17:45:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.38 on epoch=229
05/25/2022 17:45:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.33 on epoch=230
05/25/2022 17:45:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.29 on epoch=231
05/25/2022 17:45:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.34 on epoch=232
05/25/2022 17:45:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.35 on epoch=233
05/25/2022 17:45:26 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.364322698899391 on epoch=233
05/25/2022 17:45:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.35 on epoch=234
05/25/2022 17:45:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=234
05/25/2022 17:45:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=235
05/25/2022 17:45:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=236
05/25/2022 17:45:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.33 on epoch=237
05/25/2022 17:45:45 - INFO - __main__ - Global step 2850 Train loss 0.34 Classification-F1 0.3671614100185529 on epoch=237
05/25/2022 17:45:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.33 on epoch=238
05/25/2022 17:45:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=239
05/25/2022 17:45:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=239
05/25/2022 17:45:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.31 on epoch=240
05/25/2022 17:45:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.31 on epoch=241
05/25/2022 17:46:04 - INFO - __main__ - Global step 2900 Train loss 0.31 Classification-F1 0.3172302737520129 on epoch=241
05/25/2022 17:46:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.34 on epoch=242
05/25/2022 17:46:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.33 on epoch=243
05/25/2022 17:46:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=244
05/25/2022 17:46:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=244
05/25/2022 17:46:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.27 on epoch=245
05/25/2022 17:46:24 - INFO - __main__ - Global step 2950 Train loss 0.32 Classification-F1 0.4048454469507101 on epoch=245
05/25/2022 17:46:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.31 on epoch=246
05/25/2022 17:46:29 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.32 on epoch=247
05/25/2022 17:46:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.32 on epoch=248
05/25/2022 17:46:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.35 on epoch=249
05/25/2022 17:46:37 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=249
05/25/2022 17:46:38 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:46:38 - INFO - __main__ - Printing 3 examples
05/25/2022 17:46:38 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:46:38 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:38 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:46:38 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:38 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:46:38 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:38 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:46:38 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:46:39 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:46:39 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:46:39 - INFO - __main__ - Printing 3 examples
05/25/2022 17:46:39 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:46:39 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:39 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:46:39 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:39 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:46:39 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:39 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:46:39 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:46:39 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:46:43 - INFO - __main__ - Global step 3000 Train loss 0.32 Classification-F1 0.3140485355742109 on epoch=249
05/25/2022 17:46:43 - INFO - __main__ - save last model!
05/25/2022 17:46:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 17:46:43 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 17:46:43 - INFO - __main__ - Printing 3 examples
05/25/2022 17:46:43 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 17:46:43 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:43 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 17:46:43 - INFO - __main__ - ['entailment']
05/25/2022 17:46:43 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 17:46:43 - INFO - __main__ - ['contradiction']
05/25/2022 17:46:43 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:46:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:46:45 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 17:46:54 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:46:54 - INFO - __main__ - task name: anli
05/25/2022 17:46:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:46:55 - INFO - __main__ - Starting training!
05/25/2022 17:47:15 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_87_0.4_8_predictions.txt
05/25/2022 17:47:15 - INFO - __main__ - Classification-F1 on test data: 0.3365
05/25/2022 17:47:16 - INFO - __main__ - prefix=anli_64_87, lr=0.4, bsz=8, dev_performance=0.41081704467222213, test_performance=0.33651590286846617
05/25/2022 17:47:16 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.3, bsz=8 ...
05/25/2022 17:47:17 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:47:17 - INFO - __main__ - Printing 3 examples
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:47:17 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:47:17 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 17:47:17 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 17:47:17 - INFO - __main__ - Printing 3 examples
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 17:47:17 - INFO - __main__ - ['contradiction']
05/25/2022 17:47:17 - INFO - __main__ - Tokenizing Input ...
05/25/2022 17:47:17 - INFO - __main__ - Tokenizing Output ...
05/25/2022 17:47:17 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 17:47:36 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 17:47:36 - INFO - __main__ - task name: anli
05/25/2022 17:47:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 17:47:37 - INFO - __main__ - Starting training!
05/25/2022 17:47:40 - INFO - __main__ - Step 10 Global step 10 Train loss 6.21 on epoch=0
05/25/2022 17:47:43 - INFO - __main__ - Step 20 Global step 20 Train loss 3.38 on epoch=1
05/25/2022 17:47:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.54 on epoch=2
05/25/2022 17:47:48 - INFO - __main__ - Step 40 Global step 40 Train loss 1.03 on epoch=3
05/25/2022 17:47:51 - INFO - __main__ - Step 50 Global step 50 Train loss 0.94 on epoch=4
05/25/2022 17:47:57 - INFO - __main__ - Global step 50 Train loss 2.62 Classification-F1 0.16666666666666666 on epoch=4
05/25/2022 17:47:57 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/25/2022 17:48:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.72 on epoch=4
05/25/2022 17:48:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.75 on epoch=5
05/25/2022 17:48:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.72 on epoch=6
05/25/2022 17:48:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=7
05/25/2022 17:48:10 - INFO - __main__ - Step 100 Global step 100 Train loss 0.68 on epoch=8
05/25/2022 17:48:16 - INFO - __main__ - Global step 100 Train loss 0.70 Classification-F1 0.20639187574671447 on epoch=8
05/25/2022 17:48:16 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20639187574671447 on epoch=8, global_step=100
05/25/2022 17:48:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.62 on epoch=9
05/25/2022 17:48:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=9
05/25/2022 17:48:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=10
05/25/2022 17:48:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=11
05/25/2022 17:48:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=12
05/25/2022 17:48:35 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.2483060502488457 on epoch=12
05/25/2022 17:48:35 - INFO - __main__ - Saving model with best Classification-F1: 0.20639187574671447 -> 0.2483060502488457 on epoch=12, global_step=150
05/25/2022 17:48:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=13
05/25/2022 17:48:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=14
05/25/2022 17:48:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=14
05/25/2022 17:48:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
05/25/2022 17:48:48 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/25/2022 17:48:53 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
05/25/2022 17:48:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=17
05/25/2022 17:48:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=18
05/25/2022 17:49:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/25/2022 17:49:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=19
05/25/2022 17:49:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
05/25/2022 17:49:10 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=20
05/25/2022 17:49:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=21
05/25/2022 17:49:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/25/2022 17:49:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
05/25/2022 17:49:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
05/25/2022 17:49:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
05/25/2022 17:49:28 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
05/25/2022 17:49:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/25/2022 17:49:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
05/25/2022 17:49:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=27
05/25/2022 17:49:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=28
05/25/2022 17:49:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=29
05/25/2022 17:49:47 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.20795676058833953 on epoch=29
05/25/2022 17:49:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=29
05/25/2022 17:49:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=30
05/25/2022 17:49:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
05/25/2022 17:49:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
05/25/2022 17:50:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/25/2022 17:50:06 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.26026729099600043 on epoch=33
05/25/2022 17:50:06 - INFO - __main__ - Saving model with best Classification-F1: 0.2483060502488457 -> 0.26026729099600043 on epoch=33, global_step=400
05/25/2022 17:50:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=34
05/25/2022 17:50:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
05/25/2022 17:50:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=35
05/25/2022 17:50:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=36
05/25/2022 17:50:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=37
05/25/2022 17:50:25 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
05/25/2022 17:50:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/25/2022 17:50:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=39
05/25/2022 17:50:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
05/25/2022 17:50:36 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
05/25/2022 17:50:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/25/2022 17:50:43 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.18684018775524036 on epoch=41
05/25/2022 17:50:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
05/25/2022 17:50:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=43
05/25/2022 17:50:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=44
05/25/2022 17:50:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=44
05/25/2022 17:50:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
05/25/2022 17:51:01 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=45
05/25/2022 17:51:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/25/2022 17:51:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
05/25/2022 17:51:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=48
05/25/2022 17:51:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
05/25/2022 17:51:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=49
05/25/2022 17:51:19 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.24347073892936755 on epoch=49
05/25/2022 17:51:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=50
05/25/2022 17:51:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
05/25/2022 17:51:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=52
05/25/2022 17:51:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=53
05/25/2022 17:51:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=54
05/25/2022 17:51:37 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.17823541288108216 on epoch=54
05/25/2022 17:51:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
05/25/2022 17:51:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
05/25/2022 17:51:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
05/25/2022 17:51:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
05/25/2022 17:51:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/25/2022 17:51:54 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=58
05/25/2022 17:51:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=59
05/25/2022 17:52:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/25/2022 17:52:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=60
05/25/2022 17:52:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=61
05/25/2022 17:52:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=62
05/25/2022 17:52:12 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
05/25/2022 17:52:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
05/25/2022 17:52:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=64
05/25/2022 17:52:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
05/25/2022 17:52:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=65
05/25/2022 17:52:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
05/25/2022 17:52:31 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=66
05/25/2022 17:52:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
05/25/2022 17:52:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
05/25/2022 17:52:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=69
05/25/2022 17:52:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=69
05/25/2022 17:52:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=70
05/25/2022 17:52:49 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.21300353095594624 on epoch=70
05/25/2022 17:52:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
05/25/2022 17:52:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=72
05/25/2022 17:52:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 17:53:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
05/25/2022 17:53:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=74
05/25/2022 17:53:07 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=74
05/25/2022 17:53:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
05/25/2022 17:53:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
05/25/2022 17:53:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=77
05/25/2022 17:53:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/25/2022 17:53:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=79
05/25/2022 17:53:26 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.23038319704986368 on epoch=79
05/25/2022 17:53:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=79
05/25/2022 17:53:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/25/2022 17:53:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=81
05/25/2022 17:53:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=82
05/25/2022 17:53:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
05/25/2022 17:53:46 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.22098973753240578 on epoch=83
05/25/2022 17:53:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=84
05/25/2022 17:53:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=84
05/25/2022 17:53:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=85
05/25/2022 17:53:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
05/25/2022 17:53:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=87
05/25/2022 17:54:05 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=87
05/25/2022 17:54:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/25/2022 17:54:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
05/25/2022 17:54:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
05/25/2022 17:54:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=90
05/25/2022 17:54:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=91
05/25/2022 17:54:24 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
05/25/2022 17:54:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=92
05/25/2022 17:54:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 17:54:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=94
05/25/2022 17:54:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=94
05/25/2022 17:54:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.45 on epoch=95
05/25/2022 17:54:43 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.2472642447943467 on epoch=95
05/25/2022 17:54:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=96
05/25/2022 17:54:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=97
05/25/2022 17:54:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=98
05/25/2022 17:54:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=99
05/25/2022 17:54:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.43 on epoch=99
05/25/2022 17:55:02 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 17:55:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=100
05/25/2022 17:55:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=101
05/25/2022 17:55:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
05/25/2022 17:55:13 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=103
05/25/2022 17:55:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=104
05/25/2022 17:55:21 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.2500128176957445 on epoch=104
05/25/2022 17:55:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=104
05/25/2022 17:55:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=105
05/25/2022 17:55:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=106
05/25/2022 17:55:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
05/25/2022 17:55:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=108
05/25/2022 17:55:39 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.27210227605172316 on epoch=108
05/25/2022 17:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.26026729099600043 -> 0.27210227605172316 on epoch=108, global_step=1300
05/25/2022 17:55:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=109
05/25/2022 17:55:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=109
05/25/2022 17:55:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=110
05/25/2022 17:55:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=111
05/25/2022 17:55:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=112
05/25/2022 17:55:58 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.2704682149126593 on epoch=112
05/25/2022 17:56:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
05/25/2022 17:56:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=114
05/25/2022 17:56:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
05/25/2022 17:56:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=115
05/25/2022 17:56:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
05/25/2022 17:56:16 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 17:56:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
05/25/2022 17:56:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=118
05/25/2022 17:56:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=119
05/25/2022 17:56:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
05/25/2022 17:56:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=120
05/25/2022 17:56:35 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.28630952380952385 on epoch=120
05/25/2022 17:56:35 - INFO - __main__ - Saving model with best Classification-F1: 0.27210227605172316 -> 0.28630952380952385 on epoch=120, global_step=1450
05/25/2022 17:56:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=121
05/25/2022 17:56:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
05/25/2022 17:56:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
05/25/2022 17:56:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=124
05/25/2022 17:56:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=124
05/25/2022 17:56:54 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=124
05/25/2022 17:56:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=125
05/25/2022 17:57:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=126
05/25/2022 17:57:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=127
05/25/2022 17:57:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=128
05/25/2022 17:57:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=129
05/25/2022 17:57:12 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.2615326680474487 on epoch=129
05/25/2022 17:57:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
05/25/2022 17:57:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=130
05/25/2022 17:57:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
05/25/2022 17:57:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
05/25/2022 17:57:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=133
05/25/2022 17:57:31 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.27326013610110367 on epoch=133
05/25/2022 17:57:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
05/25/2022 17:57:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=134
05/25/2022 17:57:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=135
05/25/2022 17:57:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
05/25/2022 17:57:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
05/25/2022 17:57:50 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.28314994606256744 on epoch=137
05/25/2022 17:57:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=138
05/25/2022 17:57:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 17:57:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
05/25/2022 17:58:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=140
05/25/2022 17:58:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=141
05/25/2022 17:58:08 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
05/25/2022 17:58:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=142
05/25/2022 17:58:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
05/25/2022 17:58:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=144
05/25/2022 17:58:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=144
05/25/2022 17:58:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=145
05/25/2022 17:58:27 - INFO - __main__ - Global step 1750 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=145
05/25/2022 17:58:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
05/25/2022 17:58:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=147
05/25/2022 17:58:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.44 on epoch=148
05/25/2022 17:58:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.43 on epoch=149
05/25/2022 17:58:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=149
05/25/2022 17:58:45 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.22248243559718972 on epoch=149
05/25/2022 17:58:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=150
05/25/2022 17:58:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=151
05/25/2022 17:58:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=152
05/25/2022 17:58:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=153
05/25/2022 17:58:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=154
05/25/2022 17:59:05 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.31163826998689387 on epoch=154
05/25/2022 17:59:05 - INFO - __main__ - Saving model with best Classification-F1: 0.28630952380952385 -> 0.31163826998689387 on epoch=154, global_step=1850
05/25/2022 17:59:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
05/25/2022 17:59:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=155
05/25/2022 17:59:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
05/25/2022 17:59:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=157
05/25/2022 17:59:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
05/25/2022 17:59:23 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.18544600938967137 on epoch=158
05/25/2022 17:59:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=159
05/25/2022 17:59:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=159
05/25/2022 17:59:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
05/25/2022 17:59:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
05/25/2022 17:59:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=162
05/25/2022 17:59:43 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.28682406481441225 on epoch=162
05/25/2022 17:59:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
05/25/2022 17:59:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=164
05/25/2022 17:59:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=164
05/25/2022 17:59:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
05/25/2022 17:59:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=166
05/25/2022 18:00:02 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=166
05/25/2022 18:00:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=167
05/25/2022 18:00:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=168
05/25/2022 18:00:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
05/25/2022 18:00:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=169
05/25/2022 18:00:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
05/25/2022 18:00:20 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.2822886989553656 on epoch=170
05/25/2022 18:00:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=171
05/25/2022 18:00:25 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
05/25/2022 18:00:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=173
05/25/2022 18:00:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=174
05/25/2022 18:00:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
05/25/2022 18:00:38 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.17168042804626904 on epoch=174
05/25/2022 18:00:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.44 on epoch=175
05/25/2022 18:00:43 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=176
05/25/2022 18:00:46 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
05/25/2022 18:00:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.46 on epoch=178
05/25/2022 18:00:51 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=179
05/25/2022 18:00:56 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.2619360902255639 on epoch=179
05/25/2022 18:00:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=179
05/25/2022 18:01:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
05/25/2022 18:01:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=181
05/25/2022 18:01:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
05/25/2022 18:01:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
05/25/2022 18:01:15 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.27830836180963225 on epoch=183
05/25/2022 18:01:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.40 on epoch=184
05/25/2022 18:01:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
05/25/2022 18:01:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=185
05/25/2022 18:01:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=186
05/25/2022 18:01:28 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=187
05/25/2022 18:01:33 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.1647058823529412 on epoch=187
05/25/2022 18:01:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
05/25/2022 18:01:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=189
05/25/2022 18:01:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=189
05/25/2022 18:01:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=190
05/25/2022 18:01:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=191
05/25/2022 18:01:51 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=191
05/25/2022 18:01:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.42 on epoch=192
05/25/2022 18:01:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
05/25/2022 18:01:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=194
05/25/2022 18:02:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=194
05/25/2022 18:02:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=195
05/25/2022 18:02:10 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.18627450980392157 on epoch=195
05/25/2022 18:02:12 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=196
05/25/2022 18:02:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.39 on epoch=197
05/25/2022 18:02:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.43 on epoch=198
05/25/2022 18:02:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.44 on epoch=199
05/25/2022 18:02:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=199
05/25/2022 18:02:28 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.178080012725682 on epoch=199
05/25/2022 18:02:31 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
05/25/2022 18:02:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=201
05/25/2022 18:02:36 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=202
05/25/2022 18:02:39 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
05/25/2022 18:02:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=204
05/25/2022 18:02:47 - INFO - __main__ - Global step 2450 Train loss 0.39 Classification-F1 0.4129324171541102 on epoch=204
05/25/2022 18:02:47 - INFO - __main__ - Saving model with best Classification-F1: 0.31163826998689387 -> 0.4129324171541102 on epoch=204, global_step=2450
05/25/2022 18:02:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=204
05/25/2022 18:02:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.38 on epoch=205
05/25/2022 18:02:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=206
05/25/2022 18:02:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=207
05/25/2022 18:03:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=208
05/25/2022 18:03:07 - INFO - __main__ - Global step 2500 Train loss 0.40 Classification-F1 0.3962180628084518 on epoch=208
05/25/2022 18:03:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=209
05/25/2022 18:03:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
05/25/2022 18:03:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.39 on epoch=210
05/25/2022 18:03:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=211
05/25/2022 18:03:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
05/25/2022 18:03:25 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.2796221322537112 on epoch=212
05/25/2022 18:03:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
05/25/2022 18:03:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.42 on epoch=214
05/25/2022 18:03:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.41 on epoch=214
05/25/2022 18:03:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=215
05/25/2022 18:03:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.42 on epoch=216
05/25/2022 18:03:44 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=216
05/25/2022 18:03:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=217
05/25/2022 18:03:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
05/25/2022 18:03:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=219
05/25/2022 18:03:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.35 on epoch=219
05/25/2022 18:03:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
05/25/2022 18:04:03 - INFO - __main__ - Global step 2650 Train loss 0.39 Classification-F1 0.3377952867087446 on epoch=220
05/25/2022 18:04:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=221
05/25/2022 18:04:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=222
05/25/2022 18:04:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=223
05/25/2022 18:04:14 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.40 on epoch=224
05/25/2022 18:04:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=224
05/25/2022 18:04:22 - INFO - __main__ - Global step 2700 Train loss 0.42 Classification-F1 0.19357686453576864 on epoch=224
05/25/2022 18:04:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
05/25/2022 18:04:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=226
05/25/2022 18:04:30 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.41 on epoch=227
05/25/2022 18:04:33 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=228
05/25/2022 18:04:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=229
05/25/2022 18:04:41 - INFO - __main__ - Global step 2750 Train loss 0.39 Classification-F1 0.33491180844891383 on epoch=229
05/25/2022 18:04:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.46 on epoch=229
05/25/2022 18:04:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.38 on epoch=230
05/25/2022 18:04:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.44 on epoch=231
05/25/2022 18:04:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.40 on epoch=232
05/25/2022 18:04:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.45 on epoch=233
05/25/2022 18:05:01 - INFO - __main__ - Global step 2800 Train loss 0.43 Classification-F1 0.3008668497280999 on epoch=233
05/25/2022 18:05:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=234
05/25/2022 18:05:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
05/25/2022 18:05:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=235
05/25/2022 18:05:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=236
05/25/2022 18:05:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.40 on epoch=237
05/25/2022 18:05:20 - INFO - __main__ - Global step 2850 Train loss 0.40 Classification-F1 0.29583333333333334 on epoch=237
05/25/2022 18:05:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
05/25/2022 18:05:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.35 on epoch=239
05/25/2022 18:05:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=239
05/25/2022 18:05:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=240
05/25/2022 18:05:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
05/25/2022 18:05:39 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.2980845035723085 on epoch=241
05/25/2022 18:05:42 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.39 on epoch=242
05/25/2022 18:05:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=243
05/25/2022 18:05:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=244
05/25/2022 18:05:50 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.40 on epoch=244
05/25/2022 18:05:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=245
05/25/2022 18:05:59 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.3502511648702315 on epoch=245
05/25/2022 18:06:01 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=246
05/25/2022 18:06:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.42 on epoch=247
05/25/2022 18:06:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=248
05/25/2022 18:06:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.38 on epoch=249
05/25/2022 18:06:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=249
05/25/2022 18:06:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 18:06:13 - INFO - __main__ - Printing 3 examples
05/25/2022 18:06:13 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 18:06:13 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:13 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 18:06:13 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:13 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 18:06:13 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:13 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:06:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:06:14 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 18:06:14 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 18:06:14 - INFO - __main__ - Printing 3 examples
05/25/2022 18:06:14 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 18:06:14 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:14 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 18:06:14 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:14 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 18:06:14 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:14 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:06:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:06:14 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 18:06:18 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.25450632230293246 on epoch=249
05/25/2022 18:06:18 - INFO - __main__ - save last model!
05/25/2022 18:06:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 18:06:18 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 18:06:18 - INFO - __main__ - Printing 3 examples
05/25/2022 18:06:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 18:06:18 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 18:06:18 - INFO - __main__ - ['entailment']
05/25/2022 18:06:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 18:06:18 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:06:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:06:20 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 18:06:29 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 18:06:29 - INFO - __main__ - task name: anli
05/25/2022 18:06:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:06:30 - INFO - __main__ - Starting training!
05/25/2022 18:06:50 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_87_0.3_8_predictions.txt
05/25/2022 18:06:50 - INFO - __main__ - Classification-F1 on test data: 0.2348
05/25/2022 18:06:50 - INFO - __main__ - prefix=anli_64_87, lr=0.3, bsz=8, dev_performance=0.4129324171541102, test_performance=0.23480621363534948
05/25/2022 18:06:50 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.2, bsz=8 ...
05/25/2022 18:06:51 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 18:06:51 - INFO - __main__ - Printing 3 examples
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:06:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:06:51 - INFO - __main__ - Loaded 192 examples from train data
05/25/2022 18:06:51 - INFO - __main__ - Start tokenizing ... 192 instances
05/25/2022 18:06:51 - INFO - __main__ - Printing 3 examples
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/25/2022 18:06:51 - INFO - __main__ - ['contradiction']
05/25/2022 18:06:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:06:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:06:52 - INFO - __main__ - Loaded 192 examples from dev data
05/25/2022 18:07:11 - INFO - __main__ - try to initialize prompt embeddings
05/25/2022 18:07:11 - INFO - __main__ - task name: anli
05/25/2022 18:07:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:07:12 - INFO - __main__ - Starting training!
05/25/2022 18:07:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=0
05/25/2022 18:07:18 - INFO - __main__ - Step 20 Global step 20 Train loss 4.47 on epoch=1
05/25/2022 18:07:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=2
05/25/2022 18:07:23 - INFO - __main__ - Step 40 Global step 40 Train loss 1.71 on epoch=3
05/25/2022 18:07:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.27 on epoch=4
05/25/2022 18:07:32 - INFO - __main__ - Global step 50 Train loss 3.30 Classification-F1 0.2577276524644945 on epoch=4
05/25/2022 18:07:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2577276524644945 on epoch=4, global_step=50
05/25/2022 18:07:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=4
05/25/2022 18:07:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=5
05/25/2022 18:07:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=6
05/25/2022 18:07:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.66 on epoch=7
05/25/2022 18:07:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.72 on epoch=8
05/25/2022 18:07:50 - INFO - __main__ - Global step 100 Train loss 0.82 Classification-F1 0.2692383868854457 on epoch=8
05/25/2022 18:07:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2577276524644945 -> 0.2692383868854457 on epoch=8, global_step=100
05/25/2022 18:07:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.70 on epoch=9
05/25/2022 18:07:56 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=9
05/25/2022 18:07:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.60 on epoch=10
05/25/2022 18:08:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.60 on epoch=11
05/25/2022 18:08:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
05/25/2022 18:08:08 - INFO - __main__ - Global step 150 Train loss 0.62 Classification-F1 0.16666666666666666 on epoch=12
05/25/2022 18:08:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=13
05/25/2022 18:08:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=14
05/25/2022 18:08:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.56 on epoch=14
05/25/2022 18:08:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=15
05/25/2022 18:08:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=16
05/25/2022 18:08:27 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.30958825941974716 on epoch=16
05/25/2022 18:08:27 - INFO - __main__ - Saving model with best Classification-F1: 0.2692383868854457 -> 0.30958825941974716 on epoch=16, global_step=200
05/25/2022 18:08:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=17
05/25/2022 18:08:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=18
05/25/2022 18:08:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=19
05/25/2022 18:08:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=19
05/25/2022 18:08:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=20
05/25/2022 18:08:47 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.31105379396987015 on epoch=20
05/25/2022 18:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.30958825941974716 -> 0.31105379396987015 on epoch=20, global_step=250
05/25/2022 18:08:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.54 on epoch=21
05/25/2022 18:08:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=22
05/25/2022 18:08:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=23
05/25/2022 18:08:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
05/25/2022 18:09:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=24
05/25/2022 18:09:06 - INFO - __main__ - Global step 300 Train loss 0.52 Classification-F1 0.2903225806451613 on epoch=24
05/25/2022 18:09:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=25
05/25/2022 18:09:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/25/2022 18:09:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=27
05/25/2022 18:09:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/25/2022 18:09:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=29
05/25/2022 18:09:26 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.245631918736699 on epoch=29
05/25/2022 18:09:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/25/2022 18:09:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=30
05/25/2022 18:09:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/25/2022 18:09:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=32
05/25/2022 18:09:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=33
05/25/2022 18:09:45 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.2873941468123382 on epoch=33
05/25/2022 18:09:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=34
05/25/2022 18:09:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=34
05/25/2022 18:09:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
05/25/2022 18:09:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=36
05/25/2022 18:09:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=37
05/25/2022 18:10:05 - INFO - __main__ - Global step 450 Train loss 0.49 Classification-F1 0.2858307333389549 on epoch=37
05/25/2022 18:10:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/25/2022 18:10:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=39
05/25/2022 18:10:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.54 on epoch=39
05/25/2022 18:10:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=40
05/25/2022 18:10:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=41
05/25/2022 18:10:24 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.19607843137254902 on epoch=41
05/25/2022 18:10:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
05/25/2022 18:10:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/25/2022 18:10:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=44
05/25/2022 18:10:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.53 on epoch=44
05/25/2022 18:10:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
05/25/2022 18:10:43 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=45
05/25/2022 18:10:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/25/2022 18:10:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=47
05/25/2022 18:10:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=48
05/25/2022 18:10:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=49
05/25/2022 18:10:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=49
05/25/2022 18:11:02 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2873289321197443 on epoch=49
05/25/2022 18:11:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=50
05/25/2022 18:11:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=51
05/25/2022 18:11:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
05/25/2022 18:11:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=53
05/25/2022 18:11:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=54
05/25/2022 18:11:21 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.33946884749637046 on epoch=54
05/25/2022 18:11:21 - INFO - __main__ - Saving model with best Classification-F1: 0.31105379396987015 -> 0.33946884749637046 on epoch=54, global_step=650
05/25/2022 18:11:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=54
05/25/2022 18:11:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/25/2022 18:11:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
05/25/2022 18:11:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=57
05/25/2022 18:11:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/25/2022 18:11:41 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.2872420754164548 on epoch=58
05/25/2022 18:11:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=59
05/25/2022 18:11:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=59
05/25/2022 18:11:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=60
05/25/2022 18:11:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=61
05/25/2022 18:11:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=62
05/25/2022 18:12:00 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.29920265225743864 on epoch=62
05/25/2022 18:12:03 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=63
05/25/2022 18:12:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
05/25/2022 18:12:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
05/25/2022 18:12:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=65
05/25/2022 18:12:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
05/25/2022 18:12:19 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.2365374089512021 on epoch=66
05/25/2022 18:12:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
05/25/2022 18:12:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
05/25/2022 18:12:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=69
05/25/2022 18:12:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
05/25/2022 18:12:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=70
05/25/2022 18:12:38 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=70
05/25/2022 18:12:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
05/25/2022 18:12:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
05/25/2022 18:12:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/25/2022 18:12:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=74
05/25/2022 18:12:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
05/25/2022 18:12:57 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.2739393939393939 on epoch=74
05/25/2022 18:13:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=75
05/25/2022 18:13:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=76
05/25/2022 18:13:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=77
05/25/2022 18:13:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=78
05/25/2022 18:13:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=79
05/25/2022 18:13:16 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.20677820677820677 on epoch=79
05/25/2022 18:13:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=79
05/25/2022 18:13:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/25/2022 18:13:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=81
05/25/2022 18:13:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=82
05/25/2022 18:13:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
05/25/2022 18:13:35 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.22511831602740692 on epoch=83
05/25/2022 18:13:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.46 on epoch=84
05/25/2022 18:13:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=84
05/25/2022 18:13:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=85
05/25/2022 18:13:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=86
05/25/2022 18:13:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=87
05/25/2022 18:13:54 - INFO - __main__ - Global step 1050 Train loss 0.46 Classification-F1 0.33379558091313705 on epoch=87
05/25/2022 18:13:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=88
05/25/2022 18:14:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.50 on epoch=89
05/25/2022 18:14:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.45 on epoch=89
05/25/2022 18:14:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=90
05/25/2022 18:14:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.46 on epoch=91
05/25/2022 18:14:13 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.19368510277601184 on epoch=91
05/25/2022 18:14:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=92
05/25/2022 18:14:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
05/25/2022 18:14:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=94
05/25/2022 18:14:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.46 on epoch=94
05/25/2022 18:14:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=95
05/25/2022 18:14:33 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.1847349666191944 on epoch=95
05/25/2022 18:14:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=96
05/25/2022 18:14:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=97
05/25/2022 18:14:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
05/25/2022 18:14:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
05/25/2022 18:14:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=99
05/25/2022 18:14:52 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=99
05/25/2022 18:14:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=100
05/25/2022 18:14:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
05/25/2022 18:15:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=102
05/25/2022 18:15:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=103
05/25/2022 18:15:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=104
05/25/2022 18:15:11 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.2156511350059737 on epoch=104
05/25/2022 18:15:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=104
05/25/2022 18:15:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=105
05/25/2022 18:15:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
05/25/2022 18:15:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=107
05/25/2022 18:15:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.40 on epoch=108
05/25/2022 18:15:31 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.2682261208576998 on epoch=108
05/25/2022 18:15:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=109
05/25/2022 18:15:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=109
05/25/2022 18:15:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
05/25/2022 18:15:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
05/25/2022 18:15:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=112
05/25/2022 18:15:50 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.26435786435786435 on epoch=112
05/25/2022 18:15:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=113
05/25/2022 18:15:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=114
05/25/2022 18:15:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
05/25/2022 18:16:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=115
05/25/2022 18:16:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
05/25/2022 18:16:08 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
05/25/2022 18:16:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
05/25/2022 18:16:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=118
05/25/2022 18:16:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=119
05/25/2022 18:16:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=119
05/25/2022 18:16:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
05/25/2022 18:16:27 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.30280621709885863 on epoch=120
05/25/2022 18:16:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.48 on epoch=121
05/25/2022 18:16:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
05/25/2022 18:16:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=123
05/25/2022 18:16:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.43 on epoch=124
05/25/2022 18:16:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=124
05/25/2022 18:16:45 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.18061964403427822 on epoch=124
05/25/2022 18:16:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=125
05/25/2022 18:16:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=126
05/25/2022 18:16:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=127
05/25/2022 18:16:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=128
05/25/2022 18:16:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=129
05/25/2022 18:17:05 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.31482391482391475 on epoch=129
05/25/2022 18:17:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
05/25/2022 18:17:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=130
05/25/2022 18:17:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
05/25/2022 18:17:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=132
05/25/2022 18:17:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
05/25/2022 18:17:24 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.31068734847804613 on epoch=133
05/25/2022 18:17:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
05/25/2022 18:17:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=134
05/25/2022 18:17:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=135
05/25/2022 18:17:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=136
05/25/2022 18:17:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
05/25/2022 18:17:43 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3659117840684661 on epoch=137
05/25/2022 18:17:43 - INFO - __main__ - Saving model with best Classification-F1: 0.33946884749637046 -> 0.3659117840684661 on epoch=137, global_step=1650
05/25/2022 18:17:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=138
05/25/2022 18:17:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
05/25/2022 18:17:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=139
05/25/2022 18:17:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
05/25/2022 18:17:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=141
05/25/2022 18:18:01 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.17485714285714285 on epoch=141
05/25/2022 18:18:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
05/25/2022 18:18:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
05/25/2022 18:18:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=144
05/25/2022 18:18:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=144
05/25/2022 18:18:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=145
05/25/2022 18:18:20 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.2026639665102591 on epoch=145
05/25/2022 18:18:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=146
05/25/2022 18:18:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
05/25/2022 18:18:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
05/25/2022 18:18:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
05/25/2022 18:18:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=149
05/25/2022 18:18:39 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.3101023017902813 on epoch=149
05/25/2022 18:18:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=150
05/25/2022 18:18:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=151
05/25/2022 18:18:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=152
05/25/2022 18:18:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
05/25/2022 18:18:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
05/25/2022 18:18:59 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.2124497002545783 on epoch=154
05/25/2022 18:19:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=154
05/25/2022 18:19:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.42 on epoch=155
05/25/2022 18:19:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
05/25/2022 18:19:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=157
05/25/2022 18:19:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=158
05/25/2022 18:19:18 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=158
05/25/2022 18:19:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
05/25/2022 18:19:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
05/25/2022 18:19:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=160
05/25/2022 18:19:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=161
05/25/2022 18:19:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=162
05/25/2022 18:19:37 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.1881810228266921 on epoch=162
05/25/2022 18:19:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
05/25/2022 18:19:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=164
05/25/2022 18:19:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=164
05/25/2022 18:19:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=165
05/25/2022 18:19:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=166
05/25/2022 18:19:56 - INFO - __main__ - Global step 2000 Train loss 0.44 Classification-F1 0.2022321113230204 on epoch=166
05/25/2022 18:19:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=167
05/25/2022 18:20:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=168
05/25/2022 18:20:04 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.42 on epoch=169
05/25/2022 18:20:07 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=169
05/25/2022 18:20:10 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=170
05/25/2022 18:20:15 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.26479689999592393 on epoch=170
05/25/2022 18:20:18 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=171
05/25/2022 18:20:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=172
05/25/2022 18:20:23 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
05/25/2022 18:20:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
05/25/2022 18:20:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=174
05/25/2022 18:20:34 - INFO - __main__ - Global step 2100 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=174
05/25/2022 18:20:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=175
05/25/2022 18:20:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=176
05/25/2022 18:20:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.40 on epoch=177
05/25/2022 18:20:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.40 on epoch=178
05/25/2022 18:20:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=179
05/25/2022 18:20:54 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.24291306468846718 on epoch=179
05/25/2022 18:20:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.38 on epoch=179
05/25/2022 18:20:59 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
05/25/2022 18:21:02 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=181
05/25/2022 18:21:04 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
05/25/2022 18:21:07 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
05/25/2022 18:21:13 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.3100505863790887 on epoch=183
05/25/2022 18:21:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
05/25/2022 18:21:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
05/25/2022 18:21:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=185
05/25/2022 18:21:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.42 on epoch=186
05/25/2022 18:21:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.42 on epoch=187
05/25/2022 18:21:32 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.3238238238238238 on epoch=187
05/25/2022 18:21:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=188
05/25/2022 18:21:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=189
05/25/2022 18:21:40 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
05/25/2022 18:21:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.42 on epoch=190
05/25/2022 18:21:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.39 on epoch=191
05/25/2022 18:21:51 - INFO - __main__ - Global step 2300 Train loss 0.39 Classification-F1 0.16864295125164688 on epoch=191
05/25/2022 18:21:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=192
05/25/2022 18:21:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=193
05/25/2022 18:21:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.41 on epoch=194
05/25/2022 18:22:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.40 on epoch=194
05/25/2022 18:22:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=195
05/25/2022 18:22:10 - INFO - __main__ - Global step 2350 Train loss 0.40 Classification-F1 0.4210412946465274 on epoch=195
05/25/2022 18:22:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3659117840684661 -> 0.4210412946465274 on epoch=195, global_step=2350
05/25/2022 18:22:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=196
05/25/2022 18:22:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.38 on epoch=197
05/25/2022 18:22:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=198
05/25/2022 18:22:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.39 on epoch=199
05/25/2022 18:22:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=199
05/25/2022 18:22:29 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.30285841396952506 on epoch=199
05/25/2022 18:22:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.41 on epoch=200
05/25/2022 18:22:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=201
05/25/2022 18:22:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.37 on epoch=202
05/25/2022 18:22:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.42 on epoch=203
05/25/2022 18:22:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=204
05/25/2022 18:22:48 - INFO - __main__ - Global step 2450 Train loss 0.40 Classification-F1 0.3507060270982621 on epoch=204
05/25/2022 18:22:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
05/25/2022 18:22:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=205
05/25/2022 18:22:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.40 on epoch=206
05/25/2022 18:22:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=207
05/25/2022 18:23:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
05/25/2022 18:23:08 - INFO - __main__ - Global step 2500 Train loss 0.40 Classification-F1 0.38578758216295456 on epoch=208
05/25/2022 18:23:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=209
05/25/2022 18:23:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.38 on epoch=209
05/25/2022 18:23:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=210
05/25/2022 18:23:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=211
05/25/2022 18:23:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=212
05/25/2022 18:23:26 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=212
05/25/2022 18:23:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
05/25/2022 18:23:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
05/25/2022 18:23:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=214
05/25/2022 18:23:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.38 on epoch=215
05/25/2022 18:23:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=216
05/25/2022 18:23:45 - INFO - __main__ - Global step 2600 Train loss 0.39 Classification-F1 0.3366646511761826 on epoch=216
05/25/2022 18:23:48 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
05/25/2022 18:23:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=218
05/25/2022 18:23:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=219
05/25/2022 18:23:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.37 on epoch=219
05/25/2022 18:23:58 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
05/25/2022 18:24:04 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.35061252268602544 on epoch=220
05/25/2022 18:24:07 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
05/25/2022 18:24:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.36 on epoch=222
05/25/2022 18:24:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.37 on epoch=223
05/25/2022 18:24:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=224
05/25/2022 18:24:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
05/25/2022 18:24:23 - INFO - __main__ - Global step 2700 Train loss 0.39 Classification-F1 0.31168617278613264 on epoch=224
05/25/2022 18:24:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
05/25/2022 18:24:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=226
05/25/2022 18:24:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.37 on epoch=227
05/25/2022 18:24:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=228
05/25/2022 18:24:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.37 on epoch=229
05/25/2022 18:24:42 - INFO - __main__ - Global step 2750 Train loss 0.39 Classification-F1 0.31190847409469447 on epoch=229
05/25/2022 18:24:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=229
05/25/2022 18:24:48 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.36 on epoch=230
05/25/2022 18:24:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.39 on epoch=231
05/25/2022 18:24:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.36 on epoch=232
05/25/2022 18:24:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.38 on epoch=233
05/25/2022 18:25:02 - INFO - __main__ - Global step 2800 Train loss 0.38 Classification-F1 0.4141225610895764 on epoch=233
05/25/2022 18:25:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.38 on epoch=234
05/25/2022 18:25:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
05/25/2022 18:25:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
05/25/2022 18:25:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
05/25/2022 18:25:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.40 on epoch=237
05/25/2022 18:25:21 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.35904892449349707 on epoch=237
05/25/2022 18:25:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.38 on epoch=238
05/25/2022 18:25:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
05/25/2022 18:25:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=239
05/25/2022 18:25:31 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=240
05/25/2022 18:25:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.35 on epoch=241
05/25/2022 18:25:40 - INFO - __main__ - Global step 2900 Train loss 0.39 Classification-F1 0.4315461706281371 on epoch=241
05/25/2022 18:25:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4210412946465274 -> 0.4315461706281371 on epoch=241, global_step=2900
05/25/2022 18:25:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.34 on epoch=242
05/25/2022 18:25:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=243
05/25/2022 18:25:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=244
05/25/2022 18:25:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=244
05/25/2022 18:25:53 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=245
05/25/2022 18:25:59 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.3606220111104896 on epoch=245
05/25/2022 18:26:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=246
05/25/2022 18:26:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.35 on epoch=247
05/25/2022 18:26:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=248
05/25/2022 18:26:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.36 on epoch=249
05/25/2022 18:26:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=249
05/25/2022 18:26:18 - INFO - __main__ - Global step 3000 Train loss 0.36 Classification-F1 0.2866734249659583 on epoch=249
05/25/2022 18:26:18 - INFO - __main__ - save last model!
05/25/2022 18:26:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 18:26:18 - INFO - __main__ - Start tokenizing ... 1000 instances
05/25/2022 18:26:18 - INFO - __main__ - Printing 3 examples
05/25/2022 18:26:18 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/25/2022 18:26:18 - INFO - __main__ - ['contradiction']
05/25/2022 18:26:18 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/25/2022 18:26:18 - INFO - __main__ - ['entailment']
05/25/2022 18:26:18 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/25/2022 18:26:18 - INFO - __main__ - ['contradiction']
05/25/2022 18:26:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:26:19 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:26:20 - INFO - __main__ - Loaded 1000 examples from test data
05/25/2022 18:26:49 - INFO - __main__ - Saved prediction in models/T5-large-cls2cls-down64shot/singletask-anli/anli_64_87_0.2_8_predictions.txt
05/25/2022 18:26:49 - INFO - __main__ - Classification-F1 on test data: 0.2494
05/25/2022 18:26:50 - INFO - __main__ - prefix=anli_64_87, lr=0.2, bsz=8, dev_performance=0.4315461706281371, test_performance=0.24944422861089524
