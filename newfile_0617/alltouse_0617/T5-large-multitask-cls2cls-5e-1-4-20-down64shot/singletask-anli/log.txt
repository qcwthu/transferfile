05/27/2022 10:09:34 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/27/2022 10:09:34 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli
05/27/2022 10:09:34 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/27/2022 10:09:34 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli
05/27/2022 10:09:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/27/2022 10:09:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/27/2022 10:09:35 - INFO - __main__ - args.device: cuda:0
05/27/2022 10:09:35 - INFO - __main__ - Using 2 gpus
05/27/2022 10:09:35 - INFO - __main__ - args.device: cuda:1
05/27/2022 10:09:35 - INFO - __main__ - Using 2 gpus
05/27/2022 10:09:35 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/27/2022 10:09:35 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/27/2022 10:09:40 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.5, bsz=8 ...
05/27/2022 10:09:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:09:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:09:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:09:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:09:41 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:09:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:09:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:09:41 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:09:41 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:09:41 - INFO - __main__ - Printing 3 examples
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:09:41 - INFO - __main__ - ['neutral']
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:09:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:09:41 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:09:41 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:09:59 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:09:59 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:10:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:10:00 - INFO - __main__ - Starting training!
05/27/2022 10:10:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:10:04 - INFO - __main__ - Starting training!
05/27/2022 10:10:08 - INFO - __main__ - Step 10 Global step 10 Train loss 0.46 on epoch=0
05/27/2022 10:10:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=1
05/27/2022 10:10:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=2
05/27/2022 10:10:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=3
05/27/2022 10:10:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=4
05/27/2022 10:10:24 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 10:10:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 10:10:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=4
05/27/2022 10:10:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=5
05/27/2022 10:10:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=6
05/27/2022 10:10:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=7
05/27/2022 10:10:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=8
05/27/2022 10:10:42 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 10:10:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=9
05/27/2022 10:10:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.44 on epoch=9
05/27/2022 10:10:49 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=10
05/27/2022 10:10:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=11
05/27/2022 10:10:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=12
05/27/2022 10:11:00 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 10:11:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=13
05/27/2022 10:11:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
05/27/2022 10:11:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=14
05/27/2022 10:11:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=15
05/27/2022 10:11:13 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=16
05/27/2022 10:11:18 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 10:11:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=17
05/27/2022 10:11:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.47 on epoch=18
05/27/2022 10:11:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=19
05/27/2022 10:11:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
05/27/2022 10:11:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=20
05/27/2022 10:11:36 - INFO - __main__ - Global step 250 Train loss 0.44 Classification-F1 0.178743961352657 on epoch=20
05/27/2022 10:11:36 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.178743961352657 on epoch=20, global_step=250
05/27/2022 10:11:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/27/2022 10:11:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
05/27/2022 10:11:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=23
05/27/2022 10:11:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=24
05/27/2022 10:11:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=24
05/27/2022 10:11:54 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 10:11:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=25
05/27/2022 10:11:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
05/27/2022 10:12:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=27
05/27/2022 10:12:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/27/2022 10:12:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=29
05/27/2022 10:12:12 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.23315339631129103 on epoch=29
05/27/2022 10:12:12 - INFO - __main__ - Saving model with best Classification-F1: 0.178743961352657 -> 0.23315339631129103 on epoch=29, global_step=350
05/27/2022 10:12:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
05/27/2022 10:12:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=30
05/27/2022 10:12:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
05/27/2022 10:12:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=32
05/27/2022 10:12:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=33
05/27/2022 10:12:30 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.17823541288108216 on epoch=33
05/27/2022 10:12:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/27/2022 10:12:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=34
05/27/2022 10:12:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
05/27/2022 10:12:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=36
05/27/2022 10:12:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=37
05/27/2022 10:12:48 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.31799838579499595 on epoch=37
05/27/2022 10:12:48 - INFO - __main__ - Saving model with best Classification-F1: 0.23315339631129103 -> 0.31799838579499595 on epoch=37, global_step=450
05/27/2022 10:12:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=38
05/27/2022 10:12:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=39
05/27/2022 10:12:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
05/27/2022 10:12:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=40
05/27/2022 10:13:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
05/27/2022 10:13:06 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.28121224854246546 on epoch=41
05/27/2022 10:13:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=42
05/27/2022 10:13:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=43
05/27/2022 10:13:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=44
05/27/2022 10:13:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=44
05/27/2022 10:13:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=45
05/27/2022 10:13:24 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3084211223746107 on epoch=45
05/27/2022 10:13:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/27/2022 10:13:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=47
05/27/2022 10:13:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=48
05/27/2022 10:13:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=49
05/27/2022 10:13:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=49
05/27/2022 10:13:41 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.3805112214298676 on epoch=49
05/27/2022 10:13:41 - INFO - __main__ - Saving model with best Classification-F1: 0.31799838579499595 -> 0.3805112214298676 on epoch=49, global_step=600
05/27/2022 10:13:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=50
05/27/2022 10:13:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=51
05/27/2022 10:13:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=52
05/27/2022 10:13:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=53
05/27/2022 10:13:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=54
05/27/2022 10:14:00 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3818703466590791 on epoch=54
05/27/2022 10:14:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3805112214298676 -> 0.3818703466590791 on epoch=54, global_step=650
05/27/2022 10:14:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=54
05/27/2022 10:14:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=55
05/27/2022 10:14:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=56
05/27/2022 10:14:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=57
05/27/2022 10:14:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=58
05/27/2022 10:14:17 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.36088281466432726 on epoch=58
05/27/2022 10:14:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=59
05/27/2022 10:14:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=59
05/27/2022 10:14:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=60
05/27/2022 10:14:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=61
05/27/2022 10:14:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=62
05/27/2022 10:14:35 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.34432065091766445 on epoch=62
05/27/2022 10:14:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=63
05/27/2022 10:14:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=64
05/27/2022 10:14:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=64
05/27/2022 10:14:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=65
05/27/2022 10:14:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=66
05/27/2022 10:14:53 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.3711615487316422 on epoch=66
05/27/2022 10:14:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=67
05/27/2022 10:14:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=68
05/27/2022 10:15:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=69
05/27/2022 10:15:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=69
05/27/2022 10:15:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=70
05/27/2022 10:15:11 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.3818181818181818 on epoch=70
05/27/2022 10:15:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=71
05/27/2022 10:15:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=72
05/27/2022 10:15:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=73
05/27/2022 10:15:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=74
05/27/2022 10:15:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=74
05/27/2022 10:15:28 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.3353174603174603 on epoch=74
05/27/2022 10:15:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=75
05/27/2022 10:15:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=76
05/27/2022 10:15:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=77
05/27/2022 10:15:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=78
05/27/2022 10:15:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=79
05/27/2022 10:15:46 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.30877260484801844 on epoch=79
05/27/2022 10:15:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=79
05/27/2022 10:15:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=80
05/27/2022 10:15:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=81
05/27/2022 10:15:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=82
05/27/2022 10:15:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=83
05/27/2022 10:16:04 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.3745006079555324 on epoch=83
05/27/2022 10:16:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=84
05/27/2022 10:16:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=84
05/27/2022 10:16:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=85
05/27/2022 10:16:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.30 on epoch=86
05/27/2022 10:16:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=87
05/27/2022 10:16:22 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.336267126395064 on epoch=87
05/27/2022 10:16:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=88
05/27/2022 10:16:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=89
05/27/2022 10:16:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=89
05/27/2022 10:16:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=90
05/27/2022 10:16:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=91
05/27/2022 10:16:40 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.3080188980399668 on epoch=91
05/27/2022 10:16:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=92
05/27/2022 10:16:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=93
05/27/2022 10:16:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=94
05/27/2022 10:16:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=94
05/27/2022 10:16:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=95
05/27/2022 10:16:58 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.3827377675509947 on epoch=95
05/27/2022 10:16:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3818703466590791 -> 0.3827377675509947 on epoch=95, global_step=1150
05/27/2022 10:17:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=96
05/27/2022 10:17:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=97
05/27/2022 10:17:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=98
05/27/2022 10:17:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=99
05/27/2022 10:17:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=99
05/27/2022 10:17:16 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.4197100464462409 on epoch=99
05/27/2022 10:17:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3827377675509947 -> 0.4197100464462409 on epoch=99, global_step=1200
05/27/2022 10:17:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=100
05/27/2022 10:17:21 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=101
05/27/2022 10:17:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=102
05/27/2022 10:17:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=103
05/27/2022 10:17:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=104
05/27/2022 10:17:34 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.3832666860138622 on epoch=104
05/27/2022 10:17:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=104
05/27/2022 10:17:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=105
05/27/2022 10:17:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=106
05/27/2022 10:17:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=107
05/27/2022 10:17:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=108
05/27/2022 10:17:52 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.3066079295154185 on epoch=108
05/27/2022 10:17:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=109
05/27/2022 10:17:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=109
05/27/2022 10:18:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=110
05/27/2022 10:18:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=111
05/27/2022 10:18:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=112
05/27/2022 10:18:11 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.37677621283255086 on epoch=112
05/27/2022 10:18:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=113
05/27/2022 10:18:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=114
05/27/2022 10:18:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=114
05/27/2022 10:18:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=115
05/27/2022 10:18:24 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=116
05/27/2022 10:18:29 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.34563099992992785 on epoch=116
05/27/2022 10:18:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=117
05/27/2022 10:18:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=118
05/27/2022 10:18:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=119
05/27/2022 10:18:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=119
05/27/2022 10:18:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=120
05/27/2022 10:18:47 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.3422100105562113 on epoch=120
05/27/2022 10:18:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=121
05/27/2022 10:18:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=122
05/27/2022 10:18:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=123
05/27/2022 10:18:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=124
05/27/2022 10:19:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=124
05/27/2022 10:19:05 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.33547806835478067 on epoch=124
05/27/2022 10:19:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=125
05/27/2022 10:19:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=126
05/27/2022 10:19:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=127
05/27/2022 10:19:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=128
05/27/2022 10:19:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=129
05/27/2022 10:19:23 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.40674655457749687 on epoch=129
05/27/2022 10:19:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=129
05/27/2022 10:19:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=130
05/27/2022 10:19:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=131
05/27/2022 10:19:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=132
05/27/2022 10:19:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=133
05/27/2022 10:19:42 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.43208470169677066 on epoch=133
05/27/2022 10:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4197100464462409 -> 0.43208470169677066 on epoch=133, global_step=1600
05/27/2022 10:19:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=134
05/27/2022 10:19:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=134
05/27/2022 10:19:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=135
05/27/2022 10:19:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=136
05/27/2022 10:19:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.22 on epoch=137
05/27/2022 10:20:01 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.4009671372736699 on epoch=137
05/27/2022 10:20:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=138
05/27/2022 10:20:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=139
05/27/2022 10:20:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=139
05/27/2022 10:20:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=140
05/27/2022 10:20:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=141
05/27/2022 10:20:19 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.42391510681666117 on epoch=141
05/27/2022 10:20:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=142
05/27/2022 10:20:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=143
05/27/2022 10:20:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=144
05/27/2022 10:20:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=144
05/27/2022 10:20:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=145
05/27/2022 10:20:38 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.43388370888370886 on epoch=145
05/27/2022 10:20:38 - INFO - __main__ - Saving model with best Classification-F1: 0.43208470169677066 -> 0.43388370888370886 on epoch=145, global_step=1750
05/27/2022 10:20:41 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=146
05/27/2022 10:20:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=147
05/27/2022 10:20:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=148
05/27/2022 10:20:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=149
05/27/2022 10:20:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=149
05/27/2022 10:20:57 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.4121687811241645 on epoch=149
05/27/2022 10:21:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=150
05/27/2022 10:21:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=151
05/27/2022 10:21:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=152
05/27/2022 10:21:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=153
05/27/2022 10:21:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=154
05/27/2022 10:21:16 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.40848114169215083 on epoch=154
05/27/2022 10:21:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=154
05/27/2022 10:21:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=155
05/27/2022 10:21:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=156
05/27/2022 10:21:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=157
05/27/2022 10:21:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=158
05/27/2022 10:21:34 - INFO - __main__ - Global step 1900 Train loss 0.18 Classification-F1 0.40950590762620837 on epoch=158
05/27/2022 10:21:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=159
05/27/2022 10:21:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=159
05/27/2022 10:21:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=160
05/27/2022 10:21:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=161
05/27/2022 10:21:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=162
05/27/2022 10:21:53 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.41469288914498686 on epoch=162
05/27/2022 10:21:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=163
05/27/2022 10:21:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=164
05/27/2022 10:22:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=164
05/27/2022 10:22:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=165
05/27/2022 10:22:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=166
05/27/2022 10:22:12 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.38146425646425647 on epoch=166
05/27/2022 10:22:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=167
05/27/2022 10:22:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=168
05/27/2022 10:22:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=169
05/27/2022 10:22:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=169
05/27/2022 10:22:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=170
05/27/2022 10:22:30 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.40604834644081467 on epoch=170
05/27/2022 10:22:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=171
05/27/2022 10:22:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=172
05/27/2022 10:22:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=173
05/27/2022 10:22:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=174
05/27/2022 10:22:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=174
05/27/2022 10:22:49 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.4123801220575414 on epoch=174
05/27/2022 10:22:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=175
05/27/2022 10:22:54 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=176
05/27/2022 10:22:57 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=177
05/27/2022 10:23:00 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=178
05/27/2022 10:23:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=179
05/27/2022 10:23:08 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.3919900762006025 on epoch=179
05/27/2022 10:23:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=179
05/27/2022 10:23:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=180
05/27/2022 10:23:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=181
05/27/2022 10:23:19 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=182
05/27/2022 10:23:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=183
05/27/2022 10:23:27 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.4110126100486385 on epoch=183
05/27/2022 10:23:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=184
05/27/2022 10:23:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=184
05/27/2022 10:23:35 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=185
05/27/2022 10:23:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=186
05/27/2022 10:23:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=187
05/27/2022 10:23:46 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.41576917217810205 on epoch=187
05/27/2022 10:23:48 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=188
05/27/2022 10:23:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=189
05/27/2022 10:23:54 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.17 on epoch=189
05/27/2022 10:23:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=190
05/27/2022 10:23:59 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=191
05/27/2022 10:24:04 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.4025505183110645 on epoch=191
05/27/2022 10:24:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=192
05/27/2022 10:24:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=193
05/27/2022 10:24:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=194
05/27/2022 10:24:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=194
05/27/2022 10:24:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=195
05/27/2022 10:24:23 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.34251376054133215 on epoch=195
05/27/2022 10:24:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=196
05/27/2022 10:24:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=197
05/27/2022 10:24:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=198
05/27/2022 10:24:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=199
05/27/2022 10:24:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=199
05/27/2022 10:24:42 - INFO - __main__ - Global step 2400 Train loss 0.15 Classification-F1 0.42127091443082887 on epoch=199
05/27/2022 10:24:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=200
05/27/2022 10:24:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=201
05/27/2022 10:24:50 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=202
05/27/2022 10:24:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=203
05/27/2022 10:24:55 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=204
05/27/2022 10:25:01 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.39191919191919194 on epoch=204
05/27/2022 10:25:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=204
05/27/2022 10:25:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=205
05/27/2022 10:25:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=206
05/27/2022 10:25:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=207
05/27/2022 10:25:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=208
05/27/2022 10:25:20 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.4084572432398519 on epoch=208
05/27/2022 10:25:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=209
05/27/2022 10:25:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=209
05/27/2022 10:25:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=210
05/27/2022 10:25:31 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=211
05/27/2022 10:25:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.17 on epoch=212
05/27/2022 10:25:39 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.4460594315245478 on epoch=212
05/27/2022 10:25:39 - INFO - __main__ - Saving model with best Classification-F1: 0.43388370888370886 -> 0.4460594315245478 on epoch=212, global_step=2550
05/27/2022 10:25:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=213
05/27/2022 10:25:44 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=214
05/27/2022 10:25:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=214
05/27/2022 10:25:50 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=215
05/27/2022 10:25:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=216
05/27/2022 10:25:58 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.3836421341568022 on epoch=216
05/27/2022 10:26:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=217
05/27/2022 10:26:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=218
05/27/2022 10:26:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=219
05/27/2022 10:26:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=219
05/27/2022 10:26:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=220
05/27/2022 10:26:17 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.4581459758879114 on epoch=220
05/27/2022 10:26:17 - INFO - __main__ - Saving model with best Classification-F1: 0.4460594315245478 -> 0.4581459758879114 on epoch=220, global_step=2650
05/27/2022 10:26:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=221
05/27/2022 10:26:22 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=222
05/27/2022 10:26:25 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=223
05/27/2022 10:26:27 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=224
05/27/2022 10:26:30 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=224
05/27/2022 10:26:35 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.4370326643053916 on epoch=224
05/27/2022 10:26:38 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=225
05/27/2022 10:26:41 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=226
05/27/2022 10:26:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=227
05/27/2022 10:26:46 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=228
05/27/2022 10:26:49 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=229
05/27/2022 10:26:54 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.43805517971645397 on epoch=229
05/27/2022 10:26:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=229
05/27/2022 10:27:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=230
05/27/2022 10:27:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=231
05/27/2022 10:27:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=232
05/27/2022 10:27:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=233
05/27/2022 10:27:13 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.25635631315855717 on epoch=233
05/27/2022 10:27:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=234
05/27/2022 10:27:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=234
05/27/2022 10:27:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=235
05/27/2022 10:27:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=236
05/27/2022 10:27:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=237
05/27/2022 10:27:32 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.2862687300417588 on epoch=237
05/27/2022 10:27:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=238
05/27/2022 10:27:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=239
05/27/2022 10:27:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=239
05/27/2022 10:27:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=240
05/27/2022 10:27:46 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=241
05/27/2022 10:27:51 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.4294233835463221 on epoch=241
05/27/2022 10:27:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=242
05/27/2022 10:27:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.13 on epoch=243
05/27/2022 10:27:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=244
05/27/2022 10:28:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=244
05/27/2022 10:28:05 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=245
05/27/2022 10:28:10 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.45999478259943677 on epoch=245
05/27/2022 10:28:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4581459758879114 -> 0.45999478259943677 on epoch=245, global_step=2950
05/27/2022 10:28:13 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=246
05/27/2022 10:28:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=247
05/27/2022 10:28:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=248
05/27/2022 10:28:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=249
05/27/2022 10:28:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=249
05/27/2022 10:28:25 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:28:25 - INFO - __main__ - Printing 3 examples
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:28:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:28:25 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:28:25 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:28:25 - INFO - __main__ - Printing 3 examples
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:28:25 - INFO - __main__ - ['neutral']
05/27/2022 10:28:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:28:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:28:26 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:28:29 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.30095238095238097 on epoch=249
05/27/2022 10:28:29 - INFO - __main__ - save last model!
05/27/2022 10:28:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 10:28:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 10:28:29 - INFO - __main__ - Printing 3 examples
05/27/2022 10:28:29 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 10:28:29 - INFO - __main__ - ['contradiction']
05/27/2022 10:28:29 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 10:28:29 - INFO - __main__ - ['entailment']
05/27/2022 10:28:29 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 10:28:29 - INFO - __main__ - ['contradiction']
05/27/2022 10:28:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:28:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:28:31 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 10:28:44 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:28:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:28:45 - INFO - __main__ - Starting training!
05/27/2022 10:29:00 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_100_0.5_8_predictions.txt
05/27/2022 10:29:00 - INFO - __main__ - Classification-F1 on test data: 0.0969
05/27/2022 10:29:00 - INFO - __main__ - prefix=anli_64_100, lr=0.5, bsz=8, dev_performance=0.45999478259943677, test_performance=0.096934861640744
05/27/2022 10:29:00 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.4, bsz=8 ...
05/27/2022 10:29:01 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:29:01 - INFO - __main__ - Printing 3 examples
05/27/2022 10:29:01 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:29:01 - INFO - __main__ - ['neutral']
05/27/2022 10:29:01 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:29:01 - INFO - __main__ - ['neutral']
05/27/2022 10:29:01 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:29:01 - INFO - __main__ - ['neutral']
05/27/2022 10:29:01 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:29:01 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:29:02 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:29:02 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:29:02 - INFO - __main__ - Printing 3 examples
05/27/2022 10:29:02 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:29:02 - INFO - __main__ - ['neutral']
05/27/2022 10:29:02 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:29:02 - INFO - __main__ - ['neutral']
05/27/2022 10:29:02 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:29:02 - INFO - __main__ - ['neutral']
05/27/2022 10:29:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:29:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:29:02 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:29:17 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:29:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:29:18 - INFO - __main__ - Starting training!
05/27/2022 10:29:22 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=0
05/27/2022 10:29:24 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=1
05/27/2022 10:29:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=2
05/27/2022 10:29:29 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=3
05/27/2022 10:29:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=4
05/27/2022 10:29:37 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 10:29:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 10:29:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.50 on epoch=4
05/27/2022 10:29:42 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=5
05/27/2022 10:29:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=6
05/27/2022 10:29:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=7
05/27/2022 10:29:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=8
05/27/2022 10:29:55 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 10:29:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
05/27/2022 10:30:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.46 on epoch=9
05/27/2022 10:30:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
05/27/2022 10:30:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=11
05/27/2022 10:30:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=12
05/27/2022 10:30:13 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 10:30:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=13
05/27/2022 10:30:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=14
05/27/2022 10:30:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=14
05/27/2022 10:30:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=15
05/27/2022 10:30:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/27/2022 10:30:31 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 10:30:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=17
05/27/2022 10:30:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.42 on epoch=18
05/27/2022 10:30:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/27/2022 10:30:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/27/2022 10:30:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=20
05/27/2022 10:30:49 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 10:30:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.44 on epoch=21
05/27/2022 10:30:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
05/27/2022 10:30:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
05/27/2022 10:31:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=24
05/27/2022 10:31:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=24
05/27/2022 10:31:08 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 10:31:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=25
05/27/2022 10:31:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
05/27/2022 10:31:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=27
05/27/2022 10:31:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=28
05/27/2022 10:31:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
05/27/2022 10:31:26 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 10:31:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=29
05/27/2022 10:31:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
05/27/2022 10:31:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
05/27/2022 10:31:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=32
05/27/2022 10:31:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
05/27/2022 10:31:45 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.17823541288108216 on epoch=33
05/27/2022 10:31:45 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17823541288108216 on epoch=33, global_step=400
05/27/2022 10:31:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
05/27/2022 10:31:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=34
05/27/2022 10:31:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=35
05/27/2022 10:31:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=36
05/27/2022 10:31:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=37
05/27/2022 10:32:03 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.18884497145366708 on epoch=37
05/27/2022 10:32:03 - INFO - __main__ - Saving model with best Classification-F1: 0.17823541288108216 -> 0.18884497145366708 on epoch=37, global_step=450
05/27/2022 10:32:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=38
05/27/2022 10:32:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
05/27/2022 10:32:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
05/27/2022 10:32:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
05/27/2022 10:32:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
05/27/2022 10:32:22 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.20664734299516907 on epoch=41
05/27/2022 10:32:22 - INFO - __main__ - Saving model with best Classification-F1: 0.18884497145366708 -> 0.20664734299516907 on epoch=41, global_step=500
05/27/2022 10:32:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=42
05/27/2022 10:32:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=43
05/27/2022 10:32:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=44
05/27/2022 10:32:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.39 on epoch=44
05/27/2022 10:32:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
05/27/2022 10:32:40 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.2660369933097206 on epoch=45
05/27/2022 10:32:40 - INFO - __main__ - Saving model with best Classification-F1: 0.20664734299516907 -> 0.2660369933097206 on epoch=45, global_step=550
05/27/2022 10:32:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
05/27/2022 10:32:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=47
05/27/2022 10:32:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=48
05/27/2022 10:32:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=49
05/27/2022 10:32:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=49
05/27/2022 10:32:58 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.29639960741773846 on epoch=49
05/27/2022 10:32:58 - INFO - __main__ - Saving model with best Classification-F1: 0.2660369933097206 -> 0.29639960741773846 on epoch=49, global_step=600
05/27/2022 10:33:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
05/27/2022 10:33:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
05/27/2022 10:33:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=52
05/27/2022 10:33:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=53
05/27/2022 10:33:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=54
05/27/2022 10:33:16 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.3178146472264119 on epoch=54
05/27/2022 10:33:16 - INFO - __main__ - Saving model with best Classification-F1: 0.29639960741773846 -> 0.3178146472264119 on epoch=54, global_step=650
05/27/2022 10:33:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=54
05/27/2022 10:33:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=55
05/27/2022 10:33:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=56
05/27/2022 10:33:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=57
05/27/2022 10:33:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=58
05/27/2022 10:33:34 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.28466483011937554 on epoch=58
05/27/2022 10:33:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=59
05/27/2022 10:33:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=59
05/27/2022 10:33:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=60
05/27/2022 10:33:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=61
05/27/2022 10:33:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=62
05/27/2022 10:33:52 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.27744982290436837 on epoch=62
05/27/2022 10:33:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=63
05/27/2022 10:33:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=64
05/27/2022 10:34:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
05/27/2022 10:34:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=65
05/27/2022 10:34:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=66
05/27/2022 10:34:10 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.310061877858488 on epoch=66
05/27/2022 10:34:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=67
05/27/2022 10:34:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=68
05/27/2022 10:34:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=69
05/27/2022 10:34:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=69
05/27/2022 10:34:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=70
05/27/2022 10:34:28 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.34242424242424246 on epoch=70
05/27/2022 10:34:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3178146472264119 -> 0.34242424242424246 on epoch=70, global_step=850
05/27/2022 10:34:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=71
05/27/2022 10:34:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=72
05/27/2022 10:34:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=73
05/27/2022 10:34:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=74
05/27/2022 10:34:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=74
05/27/2022 10:34:46 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.336578454225513 on epoch=74
05/27/2022 10:34:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=75
05/27/2022 10:34:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=76
05/27/2022 10:34:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=77
05/27/2022 10:34:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=78
05/27/2022 10:34:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=79
05/27/2022 10:35:04 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.31741047356234336 on epoch=79
05/27/2022 10:35:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=79
05/27/2022 10:35:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=80
05/27/2022 10:35:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=81
05/27/2022 10:35:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.29 on epoch=82
05/27/2022 10:35:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=83
05/27/2022 10:35:22 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.3144630192502533 on epoch=83
05/27/2022 10:35:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=84
05/27/2022 10:35:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=84
05/27/2022 10:35:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=85
05/27/2022 10:35:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=86
05/27/2022 10:35:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=87
05/27/2022 10:35:40 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.34259259259259256 on epoch=87
05/27/2022 10:35:40 - INFO - __main__ - Saving model with best Classification-F1: 0.34242424242424246 -> 0.34259259259259256 on epoch=87, global_step=1050
05/27/2022 10:35:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=88
05/27/2022 10:35:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=89
05/27/2022 10:35:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=89
05/27/2022 10:35:50 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=90
05/27/2022 10:35:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=91
05/27/2022 10:35:58 - INFO - __main__ - Global step 1100 Train loss 0.31 Classification-F1 0.32539682539682535 on epoch=91
05/27/2022 10:36:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=92
05/27/2022 10:36:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=93
05/27/2022 10:36:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=94
05/27/2022 10:36:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=94
05/27/2022 10:36:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=95
05/27/2022 10:36:16 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.39268620498298307 on epoch=95
05/27/2022 10:36:16 - INFO - __main__ - Saving model with best Classification-F1: 0.34259259259259256 -> 0.39268620498298307 on epoch=95, global_step=1150
05/27/2022 10:36:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=96
05/27/2022 10:36:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=97
05/27/2022 10:36:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=98
05/27/2022 10:36:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=99
05/27/2022 10:36:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=99
05/27/2022 10:36:34 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.34632905221140514 on epoch=99
05/27/2022 10:36:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=100
05/27/2022 10:36:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=101
05/27/2022 10:36:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=102
05/27/2022 10:36:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=103
05/27/2022 10:36:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=104
05/27/2022 10:36:52 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.34725515712357824 on epoch=104
05/27/2022 10:36:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.30 on epoch=104
05/27/2022 10:36:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=105
05/27/2022 10:37:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=106
05/27/2022 10:37:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=107
05/27/2022 10:37:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=108
05/27/2022 10:37:10 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.32929000641293105 on epoch=108
05/27/2022 10:37:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=109
05/27/2022 10:37:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=109
05/27/2022 10:37:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=110
05/27/2022 10:37:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=111
05/27/2022 10:37:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=112
05/27/2022 10:37:28 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.33669667819651994 on epoch=112
05/27/2022 10:37:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=113
05/27/2022 10:37:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=114
05/27/2022 10:37:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=114
05/27/2022 10:37:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=115
05/27/2022 10:37:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=116
05/27/2022 10:37:46 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.29148980834225496 on epoch=116
05/27/2022 10:37:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=117
05/27/2022 10:37:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=118
05/27/2022 10:37:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=119
05/27/2022 10:37:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=119
05/27/2022 10:37:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=120
05/27/2022 10:38:05 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.39162043125949025 on epoch=120
05/27/2022 10:38:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=121
05/27/2022 10:38:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=122
05/27/2022 10:38:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=123
05/27/2022 10:38:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=124
05/27/2022 10:38:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=124
05/27/2022 10:38:23 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.3804288546344621 on epoch=124
05/27/2022 10:38:26 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=125
05/27/2022 10:38:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=126
05/27/2022 10:38:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=127
05/27/2022 10:38:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=128
05/27/2022 10:38:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=129
05/27/2022 10:38:42 - INFO - __main__ - Global step 1550 Train loss 0.25 Classification-F1 0.3806299899489864 on epoch=129
05/27/2022 10:38:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=129
05/27/2022 10:38:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=130
05/27/2022 10:38:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=131
05/27/2022 10:38:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=132
05/27/2022 10:38:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=133
05/27/2022 10:39:00 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.3911584055601822 on epoch=133
05/27/2022 10:39:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=134
05/27/2022 10:39:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=134
05/27/2022 10:39:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=135
05/27/2022 10:39:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=136
05/27/2022 10:39:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=137
05/27/2022 10:39:19 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.4360200022046518 on epoch=137
05/27/2022 10:39:19 - INFO - __main__ - Saving model with best Classification-F1: 0.39268620498298307 -> 0.4360200022046518 on epoch=137, global_step=1650
05/27/2022 10:39:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=138
05/27/2022 10:39:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=139
05/27/2022 10:39:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=139
05/27/2022 10:39:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=140
05/27/2022 10:39:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=141
05/27/2022 10:39:37 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.38490908501880705 on epoch=141
05/27/2022 10:39:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=142
05/27/2022 10:39:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=143
05/27/2022 10:39:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=144
05/27/2022 10:39:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=144
05/27/2022 10:39:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=145
05/27/2022 10:39:55 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.3747102544721592 on epoch=145
05/27/2022 10:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=146
05/27/2022 10:40:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=147
05/27/2022 10:40:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=148
05/27/2022 10:40:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=149
05/27/2022 10:40:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=149
05/27/2022 10:40:14 - INFO - __main__ - Global step 1800 Train loss 0.22 Classification-F1 0.3870862402972495 on epoch=149
05/27/2022 10:40:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=150
05/27/2022 10:40:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=151
05/27/2022 10:40:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=152
05/27/2022 10:40:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=153
05/27/2022 10:40:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=154
05/27/2022 10:40:32 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.39326809177555444 on epoch=154
05/27/2022 10:40:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=154
05/27/2022 10:40:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=155
05/27/2022 10:40:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=156
05/27/2022 10:40:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=157
05/27/2022 10:40:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=158
05/27/2022 10:40:50 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.4081266865402557 on epoch=158
05/27/2022 10:40:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=159
05/27/2022 10:40:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=159
05/27/2022 10:40:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=160
05/27/2022 10:41:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=161
05/27/2022 10:41:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=162
05/27/2022 10:41:08 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.36770223205383273 on epoch=162
05/27/2022 10:41:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=163
05/27/2022 10:41:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=164
05/27/2022 10:41:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=164
05/27/2022 10:41:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=165
05/27/2022 10:41:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=166
05/27/2022 10:41:27 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.4262511965482263 on epoch=166
05/27/2022 10:41:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=167
05/27/2022 10:41:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=168
05/27/2022 10:41:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=169
05/27/2022 10:41:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=169
05/27/2022 10:41:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=170
05/27/2022 10:41:45 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.408987052551409 on epoch=170
05/27/2022 10:41:48 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=171
05/27/2022 10:41:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=172
05/27/2022 10:41:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=173
05/27/2022 10:41:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=174
05/27/2022 10:41:58 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=174
05/27/2022 10:42:03 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.40448925400381713 on epoch=174
05/27/2022 10:42:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=175
05/27/2022 10:42:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=176
05/27/2022 10:42:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=177
05/27/2022 10:42:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=178
05/27/2022 10:42:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=179
05/27/2022 10:42:22 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.42593401857870766 on epoch=179
05/27/2022 10:42:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=179
05/27/2022 10:42:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.22 on epoch=180
05/27/2022 10:42:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=181
05/27/2022 10:42:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=182
05/27/2022 10:42:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=183
05/27/2022 10:42:40 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.40554529030138786 on epoch=183
05/27/2022 10:42:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=184
05/27/2022 10:42:45 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=184
05/27/2022 10:42:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=185
05/27/2022 10:42:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=186
05/27/2022 10:42:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=187
05/27/2022 10:42:59 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.39981327244153125 on epoch=187
05/27/2022 10:43:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=188
05/27/2022 10:43:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=189
05/27/2022 10:43:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=189
05/27/2022 10:43:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=190
05/27/2022 10:43:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=191
05/27/2022 10:43:17 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.42089997526852435 on epoch=191
05/27/2022 10:43:19 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=192
05/27/2022 10:43:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=193
05/27/2022 10:43:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=194
05/27/2022 10:43:27 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=194
05/27/2022 10:43:30 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=195
05/27/2022 10:43:35 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.4375322646170699 on epoch=195
05/27/2022 10:43:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4360200022046518 -> 0.4375322646170699 on epoch=195, global_step=2350
05/27/2022 10:43:38 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.20 on epoch=196
05/27/2022 10:43:40 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=197
05/27/2022 10:43:43 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=198
05/27/2022 10:43:46 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=199
05/27/2022 10:43:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=199
05/27/2022 10:43:54 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.4678726369071729 on epoch=199
05/27/2022 10:43:54 - INFO - __main__ - Saving model with best Classification-F1: 0.4375322646170699 -> 0.4678726369071729 on epoch=199, global_step=2400
05/27/2022 10:43:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=200
05/27/2022 10:43:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=201
05/27/2022 10:44:02 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=202
05/27/2022 10:44:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=203
05/27/2022 10:44:07 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=204
05/27/2022 10:44:12 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.4222858593574445 on epoch=204
05/27/2022 10:44:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=204
05/27/2022 10:44:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=205
05/27/2022 10:44:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=206
05/27/2022 10:44:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=207
05/27/2022 10:44:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=208
05/27/2022 10:44:30 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.41420510105061054 on epoch=208
05/27/2022 10:44:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=209
05/27/2022 10:44:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=209
05/27/2022 10:44:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=210
05/27/2022 10:44:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=211
05/27/2022 10:44:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=212
05/27/2022 10:44:49 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.4299317047543998 on epoch=212
05/27/2022 10:44:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=213
05/27/2022 10:44:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=214
05/27/2022 10:44:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=214
05/27/2022 10:44:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=215
05/27/2022 10:45:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=216
05/27/2022 10:45:07 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.34035672898232144 on epoch=216
05/27/2022 10:45:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=217
05/27/2022 10:45:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=218
05/27/2022 10:45:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=219
05/27/2022 10:45:17 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=219
05/27/2022 10:45:20 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=220
05/27/2022 10:45:25 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.41538147694818445 on epoch=220
05/27/2022 10:45:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=221
05/27/2022 10:45:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=222
05/27/2022 10:45:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=223
05/27/2022 10:45:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=224
05/27/2022 10:45:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=224
05/27/2022 10:45:43 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.37602973678923046 on epoch=224
05/27/2022 10:45:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=225
05/27/2022 10:45:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.14 on epoch=226
05/27/2022 10:45:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=227
05/27/2022 10:45:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=228
05/27/2022 10:45:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=229
05/27/2022 10:46:02 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.4243971385715934 on epoch=229
05/27/2022 10:46:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=229
05/27/2022 10:46:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=230
05/27/2022 10:46:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=231
05/27/2022 10:46:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=232
05/27/2022 10:46:15 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=233
05/27/2022 10:46:20 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.32588553301139156 on epoch=233
05/27/2022 10:46:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=234
05/27/2022 10:46:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=234
05/27/2022 10:46:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=235
05/27/2022 10:46:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=236
05/27/2022 10:46:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=237
05/27/2022 10:46:38 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.44253154253154253 on epoch=237
05/27/2022 10:46:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=238
05/27/2022 10:46:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=239
05/27/2022 10:46:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=239
05/27/2022 10:46:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=240
05/27/2022 10:46:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.15 on epoch=241
05/27/2022 10:46:57 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.45596153946010737 on epoch=241
05/27/2022 10:46:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=242
05/27/2022 10:47:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=243
05/27/2022 10:47:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=244
05/27/2022 10:47:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=244
05/27/2022 10:47:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=245
05/27/2022 10:47:15 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.37651685621034275 on epoch=245
05/27/2022 10:47:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=246
05/27/2022 10:47:20 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=247
05/27/2022 10:47:23 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=248
05/27/2022 10:47:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=249
05/27/2022 10:47:28 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=249
05/27/2022 10:47:29 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:47:29 - INFO - __main__ - Printing 3 examples
05/27/2022 10:47:29 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:47:29 - INFO - __main__ - ['neutral']
05/27/2022 10:47:29 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:47:29 - INFO - __main__ - ['neutral']
05/27/2022 10:47:29 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:47:29 - INFO - __main__ - ['neutral']
05/27/2022 10:47:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:47:29 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:47:30 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:47:30 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:47:30 - INFO - __main__ - Printing 3 examples
05/27/2022 10:47:30 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:47:30 - INFO - __main__ - ['neutral']
05/27/2022 10:47:30 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:47:30 - INFO - __main__ - ['neutral']
05/27/2022 10:47:30 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:47:30 - INFO - __main__ - ['neutral']
05/27/2022 10:47:30 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:47:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:47:30 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:47:33 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.4460846171372486 on epoch=249
05/27/2022 10:47:33 - INFO - __main__ - save last model!
05/27/2022 10:47:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 10:47:33 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 10:47:33 - INFO - __main__ - Printing 3 examples
05/27/2022 10:47:33 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 10:47:33 - INFO - __main__ - ['contradiction']
05/27/2022 10:47:33 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 10:47:33 - INFO - __main__ - ['entailment']
05/27/2022 10:47:33 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 10:47:33 - INFO - __main__ - ['contradiction']
05/27/2022 10:47:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:47:34 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:47:35 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 10:47:46 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:47:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:47:47 - INFO - __main__ - Starting training!
05/27/2022 10:48:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_100_0.4_8_predictions.txt
05/27/2022 10:48:03 - INFO - __main__ - Classification-F1 on test data: 0.1411
05/27/2022 10:48:04 - INFO - __main__ - prefix=anli_64_100, lr=0.4, bsz=8, dev_performance=0.4678726369071729, test_performance=0.14107301349259788
05/27/2022 10:48:04 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.3, bsz=8 ...
05/27/2022 10:48:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:48:05 - INFO - __main__ - Printing 3 examples
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:48:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:48:05 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 10:48:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 10:48:05 - INFO - __main__ - Printing 3 examples
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 10:48:05 - INFO - __main__ - ['neutral']
05/27/2022 10:48:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 10:48:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 10:48:05 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 10:48:24 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 10:48:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 10:48:24 - INFO - __main__ - Starting training!
05/27/2022 10:48:28 - INFO - __main__ - Step 10 Global step 10 Train loss 0.44 on epoch=0
05/27/2022 10:48:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=1
05/27/2022 10:48:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=2
05/27/2022 10:48:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=3
05/27/2022 10:48:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=4
05/27/2022 10:48:44 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 10:48:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 10:48:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=4
05/27/2022 10:48:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=5
05/27/2022 10:48:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=6
05/27/2022 10:48:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.44 on epoch=7
05/27/2022 10:48:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=8
05/27/2022 10:49:02 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 10:49:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=9
05/27/2022 10:49:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=9
05/27/2022 10:49:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=10
05/27/2022 10:49:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=11
05/27/2022 10:49:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=12
05/27/2022 10:49:20 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 10:49:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=13
05/27/2022 10:49:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.54 on epoch=14
05/27/2022 10:49:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=14
05/27/2022 10:49:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=15
05/27/2022 10:49:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/27/2022 10:49:38 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 10:49:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=17
05/27/2022 10:49:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=18
05/27/2022 10:49:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=19
05/27/2022 10:49:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=19
05/27/2022 10:49:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=20
05/27/2022 10:49:57 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.21828571428571428 on epoch=20
05/27/2022 10:49:57 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21828571428571428 on epoch=20, global_step=250
05/27/2022 10:49:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/27/2022 10:50:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=22
05/27/2022 10:50:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=23
05/27/2022 10:50:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=24
05/27/2022 10:50:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/27/2022 10:50:15 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 10:50:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
05/27/2022 10:50:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
05/27/2022 10:50:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.37 on epoch=27
05/27/2022 10:50:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
05/27/2022 10:50:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
05/27/2022 10:50:34 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.1679790026246719 on epoch=29
05/27/2022 10:50:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=29
05/27/2022 10:50:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
05/27/2022 10:50:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=31
05/27/2022 10:50:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=32
05/27/2022 10:50:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
05/27/2022 10:50:53 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 10:50:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
05/27/2022 10:50:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=34
05/27/2022 10:51:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
05/27/2022 10:51:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=36
05/27/2022 10:51:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=37
05/27/2022 10:51:12 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16864295125164688 on epoch=37
05/27/2022 10:51:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=38
05/27/2022 10:51:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=39
05/27/2022 10:51:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
05/27/2022 10:51:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
05/27/2022 10:51:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
05/27/2022 10:51:30 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.24009375228887422 on epoch=41
05/27/2022 10:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21828571428571428 -> 0.24009375228887422 on epoch=41, global_step=500
05/27/2022 10:51:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=42
05/27/2022 10:51:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/27/2022 10:51:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=44
05/27/2022 10:51:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=44
05/27/2022 10:51:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=45
05/27/2022 10:51:49 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.3326893975981197 on epoch=45
05/27/2022 10:51:49 - INFO - __main__ - Saving model with best Classification-F1: 0.24009375228887422 -> 0.3326893975981197 on epoch=45, global_step=550
05/27/2022 10:51:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/27/2022 10:51:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=47
05/27/2022 10:51:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=48
05/27/2022 10:51:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=49
05/27/2022 10:52:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=49
05/27/2022 10:52:07 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.2222222222222222 on epoch=49
05/27/2022 10:52:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=50
05/27/2022 10:52:12 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
05/27/2022 10:52:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=52
05/27/2022 10:52:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=53
05/27/2022 10:52:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=54
05/27/2022 10:52:25 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.26533411718596905 on epoch=54
05/27/2022 10:52:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=54
05/27/2022 10:52:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=55
05/27/2022 10:52:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=56
05/27/2022 10:52:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
05/27/2022 10:52:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=58
05/27/2022 10:52:43 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.29639960741773846 on epoch=58
05/27/2022 10:52:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=59
05/27/2022 10:52:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
05/27/2022 10:52:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=60
05/27/2022 10:52:54 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=61
05/27/2022 10:52:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=62
05/27/2022 10:53:01 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3170629954109303 on epoch=62
05/27/2022 10:53:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=63
05/27/2022 10:53:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=64
05/27/2022 10:53:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
05/27/2022 10:53:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/27/2022 10:53:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=66
05/27/2022 10:53:19 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.28121224854246546 on epoch=66
05/27/2022 10:53:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=67
05/27/2022 10:53:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=68
05/27/2022 10:53:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=69
05/27/2022 10:53:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=69
05/27/2022 10:53:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=70
05/27/2022 10:53:37 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.36649277447982254 on epoch=70
05/27/2022 10:53:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3326893975981197 -> 0.36649277447982254 on epoch=70, global_step=850
05/27/2022 10:53:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=71
05/27/2022 10:53:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=72
05/27/2022 10:53:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=73
05/27/2022 10:53:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=74
05/27/2022 10:53:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=74
05/27/2022 10:53:55 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.3430615703257391 on epoch=74
05/27/2022 10:53:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=75
05/27/2022 10:54:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
05/27/2022 10:54:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=77
05/27/2022 10:54:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=78
05/27/2022 10:54:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=79
05/27/2022 10:54:14 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3601044226044226 on epoch=79
05/27/2022 10:54:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=79
05/27/2022 10:54:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=80
05/27/2022 10:54:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=81
05/27/2022 10:54:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=82
05/27/2022 10:54:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=83
05/27/2022 10:54:32 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.3210635699997402 on epoch=83
05/27/2022 10:54:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=84
05/27/2022 10:54:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=84
05/27/2022 10:54:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=85
05/27/2022 10:54:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
05/27/2022 10:54:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=87
05/27/2022 10:54:49 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.3084211223746107 on epoch=87
05/27/2022 10:54:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=88
05/27/2022 10:54:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=89
05/27/2022 10:54:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=89
05/27/2022 10:55:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.36 on epoch=90
05/27/2022 10:55:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=91
05/27/2022 10:55:07 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.32920077972709555 on epoch=91
05/27/2022 10:55:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.35 on epoch=92
05/27/2022 10:55:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=93
05/27/2022 10:55:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=94
05/27/2022 10:55:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=94
05/27/2022 10:55:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=95
05/27/2022 10:55:25 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.3182814274517331 on epoch=95
05/27/2022 10:55:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=96
05/27/2022 10:55:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=97
05/27/2022 10:55:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=98
05/27/2022 10:55:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=99
05/27/2022 10:55:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=99
05/27/2022 10:55:43 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.32381223059068737 on epoch=99
05/27/2022 10:55:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=100
05/27/2022 10:55:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=101
05/27/2022 10:55:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=102
05/27/2022 10:55:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=103
05/27/2022 10:55:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=104
05/27/2022 10:56:01 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.3404424832996262 on epoch=104
05/27/2022 10:56:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=104
05/27/2022 10:56:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=105
05/27/2022 10:56:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=106
05/27/2022 10:56:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=107
05/27/2022 10:56:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=108
05/27/2022 10:56:19 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.33743948029662313 on epoch=108
05/27/2022 10:56:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=109
05/27/2022 10:56:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=109
05/27/2022 10:56:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=110
05/27/2022 10:56:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=111
05/27/2022 10:56:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.28 on epoch=112
05/27/2022 10:56:38 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.37000658591204744 on epoch=112
05/27/2022 10:56:38 - INFO - __main__ - Saving model with best Classification-F1: 0.36649277447982254 -> 0.37000658591204744 on epoch=112, global_step=1350
05/27/2022 10:56:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=113
05/27/2022 10:56:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=114
05/27/2022 10:56:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.33 on epoch=114
05/27/2022 10:56:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.32 on epoch=115
05/27/2022 10:56:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=116
05/27/2022 10:56:56 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.34632905221140514 on epoch=116
05/27/2022 10:56:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=117
05/27/2022 10:57:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=118
05/27/2022 10:57:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=119
05/27/2022 10:57:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=119
05/27/2022 10:57:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=120
05/27/2022 10:57:14 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.3517321227961574 on epoch=120
05/27/2022 10:57:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=121
05/27/2022 10:57:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=122
05/27/2022 10:57:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=123
05/27/2022 10:57:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=124
05/27/2022 10:57:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=124
05/27/2022 10:57:32 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.37725148414803583 on epoch=124
05/27/2022 10:57:32 - INFO - __main__ - Saving model with best Classification-F1: 0.37000658591204744 -> 0.37725148414803583 on epoch=124, global_step=1500
05/27/2022 10:57:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=125
05/27/2022 10:57:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=126
05/27/2022 10:57:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=127
05/27/2022 10:57:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=128
05/27/2022 10:57:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=129
05/27/2022 10:57:50 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.37641758112574547 on epoch=129
05/27/2022 10:57:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=129
05/27/2022 10:57:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=130
05/27/2022 10:57:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=131
05/27/2022 10:58:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=132
05/27/2022 10:58:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=133
05/27/2022 10:58:08 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.3804693018618079 on epoch=133
05/27/2022 10:58:08 - INFO - __main__ - Saving model with best Classification-F1: 0.37725148414803583 -> 0.3804693018618079 on epoch=133, global_step=1600
05/27/2022 10:58:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=134
05/27/2022 10:58:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=134
05/27/2022 10:58:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=135
05/27/2022 10:58:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=136
05/27/2022 10:58:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=137
05/27/2022 10:58:26 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.3722156377878251 on epoch=137
05/27/2022 10:58:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=138
05/27/2022 10:58:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=139
05/27/2022 10:58:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=139
05/27/2022 10:58:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=140
05/27/2022 10:58:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=141
05/27/2022 10:58:44 - INFO - __main__ - Global step 1700 Train loss 0.29 Classification-F1 0.3306397306397307 on epoch=141
05/27/2022 10:58:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.27 on epoch=142
05/27/2022 10:58:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.30 on epoch=143
05/27/2022 10:58:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=144
05/27/2022 10:58:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.29 on epoch=144
05/27/2022 10:58:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=145
05/27/2022 10:59:02 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.3537696272324104 on epoch=145
05/27/2022 10:59:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=146
05/27/2022 10:59:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=147
05/27/2022 10:59:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=148
05/27/2022 10:59:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.29 on epoch=149
05/27/2022 10:59:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=149
05/27/2022 10:59:20 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.3505554575912537 on epoch=149
05/27/2022 10:59:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=150
05/27/2022 10:59:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=151
05/27/2022 10:59:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=152
05/27/2022 10:59:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=153
05/27/2022 10:59:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=154
05/27/2022 10:59:38 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.34197765127556395 on epoch=154
05/27/2022 10:59:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=154
05/27/2022 10:59:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=155
05/27/2022 10:59:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=156
05/27/2022 10:59:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=157
05/27/2022 10:59:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=158
05/27/2022 10:59:56 - INFO - __main__ - Global step 1900 Train loss 0.28 Classification-F1 0.369884488448845 on epoch=158
05/27/2022 10:59:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=159
05/27/2022 11:00:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=159
05/27/2022 11:00:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=160
05/27/2022 11:00:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=161
05/27/2022 11:00:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=162
05/27/2022 11:00:15 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.37201362704834723 on epoch=162
05/27/2022 11:00:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=163
05/27/2022 11:00:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=164
05/27/2022 11:00:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=164
05/27/2022 11:00:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=165
05/27/2022 11:00:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=166
05/27/2022 11:00:33 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.36203208556149735 on epoch=166
05/27/2022 11:00:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=167
05/27/2022 11:00:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=168
05/27/2022 11:00:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.26 on epoch=169
05/27/2022 11:00:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=169
05/27/2022 11:00:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=170
05/27/2022 11:00:51 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.36675060625409756 on epoch=170
05/27/2022 11:00:54 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=171
05/27/2022 11:00:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=172
05/27/2022 11:00:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=173
05/27/2022 11:01:02 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.31 on epoch=174
05/27/2022 11:01:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=174
05/27/2022 11:01:10 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.39009418448578215 on epoch=174
05/27/2022 11:01:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3804693018618079 -> 0.39009418448578215 on epoch=174, global_step=2100
05/27/2022 11:01:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.24 on epoch=175
05/27/2022 11:01:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=176
05/27/2022 11:01:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=177
05/27/2022 11:01:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=178
05/27/2022 11:01:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=179
05/27/2022 11:01:28 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.3933921373982407 on epoch=179
05/27/2022 11:01:28 - INFO - __main__ - Saving model with best Classification-F1: 0.39009418448578215 -> 0.3933921373982407 on epoch=179, global_step=2150
05/27/2022 11:01:31 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=179
05/27/2022 11:01:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=180
05/27/2022 11:01:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=181
05/27/2022 11:01:39 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=182
05/27/2022 11:01:41 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=183
05/27/2022 11:01:46 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.36689113355780023 on epoch=183
05/27/2022 11:01:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=184
05/27/2022 11:01:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=184
05/27/2022 11:01:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=185
05/27/2022 11:01:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.31 on epoch=186
05/27/2022 11:01:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=187
05/27/2022 11:02:05 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.38536231884057964 on epoch=187
05/27/2022 11:02:07 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=188
05/27/2022 11:02:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.24 on epoch=189
05/27/2022 11:02:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.26 on epoch=189
05/27/2022 11:02:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.25 on epoch=190
05/27/2022 11:02:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=191
05/27/2022 11:02:23 - INFO - __main__ - Global step 2300 Train loss 0.23 Classification-F1 0.3610995802776624 on epoch=191
05/27/2022 11:02:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=192
05/27/2022 11:02:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=193
05/27/2022 11:02:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.25 on epoch=194
05/27/2022 11:02:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=194
05/27/2022 11:02:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=195
05/27/2022 11:02:41 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.3902993579112983 on epoch=195
05/27/2022 11:02:44 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=196
05/27/2022 11:02:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=197
05/27/2022 11:02:49 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.32 on epoch=198
05/27/2022 11:02:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=199
05/27/2022 11:02:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.25 on epoch=199
05/27/2022 11:02:59 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.4137241286097331 on epoch=199
05/27/2022 11:02:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3933921373982407 -> 0.4137241286097331 on epoch=199, global_step=2400
05/27/2022 11:03:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=200
05/27/2022 11:03:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=201
05/27/2022 11:03:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=202
05/27/2022 11:03:10 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=203
05/27/2022 11:03:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.22 on epoch=204
05/27/2022 11:03:18 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.37675160941332736 on epoch=204
05/27/2022 11:03:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=204
05/27/2022 11:03:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=205
05/27/2022 11:03:26 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=206
05/27/2022 11:03:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=207
05/27/2022 11:03:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=208
05/27/2022 11:03:36 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.382701337445344 on epoch=208
05/27/2022 11:03:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=209
05/27/2022 11:03:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=209
05/27/2022 11:03:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.21 on epoch=210
05/27/2022 11:03:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=211
05/27/2022 11:03:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=212
05/27/2022 11:03:54 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.40116887942974905 on epoch=212
05/27/2022 11:03:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=213
05/27/2022 11:03:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.26 on epoch=214
05/27/2022 11:04:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.18 on epoch=214
05/27/2022 11:04:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=215
05/27/2022 11:04:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.20 on epoch=216
05/27/2022 11:04:12 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.3944597395099547 on epoch=216
05/27/2022 11:04:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=217
05/27/2022 11:04:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=218
05/27/2022 11:04:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.17 on epoch=219
05/27/2022 11:04:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=219
05/27/2022 11:04:26 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=220
05/27/2022 11:04:31 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.4152046783625731 on epoch=220
05/27/2022 11:04:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4137241286097331 -> 0.4152046783625731 on epoch=220, global_step=2650
05/27/2022 11:04:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.19 on epoch=221
05/27/2022 11:04:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=222
05/27/2022 11:04:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=223
05/27/2022 11:04:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.23 on epoch=224
05/27/2022 11:04:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=224
05/27/2022 11:04:49 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.42754366862808685 on epoch=224
05/27/2022 11:04:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4152046783625731 -> 0.42754366862808685 on epoch=224, global_step=2700
05/27/2022 11:04:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=225
05/27/2022 11:04:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=226
05/27/2022 11:04:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.16 on epoch=227
05/27/2022 11:05:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.22 on epoch=228
05/27/2022 11:05:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=229
05/27/2022 11:05:08 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.4394812553069552 on epoch=229
05/27/2022 11:05:08 - INFO - __main__ - Saving model with best Classification-F1: 0.42754366862808685 -> 0.4394812553069552 on epoch=229, global_step=2750
05/27/2022 11:05:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=229
05/27/2022 11:05:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=230
05/27/2022 11:05:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=231
05/27/2022 11:05:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=232
05/27/2022 11:05:21 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.18 on epoch=233
05/27/2022 11:05:26 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.39914709806996546 on epoch=233
05/27/2022 11:05:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=234
05/27/2022 11:05:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=234
05/27/2022 11:05:34 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=235
05/27/2022 11:05:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.19 on epoch=236
05/27/2022 11:05:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=237
05/27/2022 11:05:45 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.4028992224780197 on epoch=237
05/27/2022 11:05:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=238
05/27/2022 11:05:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.25 on epoch=239
05/27/2022 11:05:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=239
05/27/2022 11:05:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.21 on epoch=240
05/27/2022 11:05:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=241
05/27/2022 11:06:03 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.36787744007907097 on epoch=241
05/27/2022 11:06:06 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=242
05/27/2022 11:06:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=243
05/27/2022 11:06:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.28 on epoch=244
05/27/2022 11:06:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=244
05/27/2022 11:06:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=245
05/27/2022 11:06:21 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.3913545823993585 on epoch=245
05/27/2022 11:06:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=246
05/27/2022 11:06:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=247
05/27/2022 11:06:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=248
05/27/2022 11:06:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=249
05/27/2022 11:06:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.17 on epoch=249
05/27/2022 11:06:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:06:36 - INFO - __main__ - Printing 3 examples
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:06:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:06:36 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:06:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:06:36 - INFO - __main__ - Printing 3 examples
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 11:06:36 - INFO - __main__ - ['neutral']
05/27/2022 11:06:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:06:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:06:37 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:06:40 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.451826932062424 on epoch=249
05/27/2022 11:06:40 - INFO - __main__ - Saving model with best Classification-F1: 0.4394812553069552 -> 0.451826932062424 on epoch=249, global_step=3000
05/27/2022 11:06:40 - INFO - __main__ - save last model!
05/27/2022 11:06:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 11:06:40 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 11:06:40 - INFO - __main__ - Printing 3 examples
05/27/2022 11:06:40 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 11:06:40 - INFO - __main__ - ['contradiction']
05/27/2022 11:06:40 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 11:06:40 - INFO - __main__ - ['entailment']
05/27/2022 11:06:40 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 11:06:40 - INFO - __main__ - ['contradiction']
05/27/2022 11:06:40 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:06:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:06:42 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 11:06:54 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:06:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:06:55 - INFO - __main__ - Starting training!
05/27/2022 11:07:11 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_100_0.3_8_predictions.txt
05/27/2022 11:07:11 - INFO - __main__ - Classification-F1 on test data: 0.3186
05/27/2022 11:07:12 - INFO - __main__ - prefix=anli_64_100, lr=0.3, bsz=8, dev_performance=0.451826932062424, test_performance=0.3186249735714297
05/27/2022 11:07:12 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.2, bsz=8 ...
05/27/2022 11:07:12 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:07:12 - INFO - __main__ - Printing 3 examples
05/27/2022 11:07:12 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
05/27/2022 11:07:12 - INFO - __main__ - ['neutral']
05/27/2022 11:07:12 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
05/27/2022 11:07:12 - INFO - __main__ - ['neutral']
05/27/2022 11:07:12 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
05/27/2022 11:07:12 - INFO - __main__ - ['neutral']
05/27/2022 11:07:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:07:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:07:13 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:07:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:07:13 - INFO - __main__ - Printing 3 examples
05/27/2022 11:07:13 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
05/27/2022 11:07:13 - INFO - __main__ - ['neutral']
05/27/2022 11:07:13 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
05/27/2022 11:07:13 - INFO - __main__ - ['neutral']
05/27/2022 11:07:13 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
05/27/2022 11:07:13 - INFO - __main__ - ['neutral']
05/27/2022 11:07:13 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:07:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:07:13 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:07:32 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:07:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:07:32 - INFO - __main__ - Starting training!
05/27/2022 11:07:36 - INFO - __main__ - Step 10 Global step 10 Train loss 0.46 on epoch=0
05/27/2022 11:07:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
05/27/2022 11:07:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=2
05/27/2022 11:07:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=3
05/27/2022 11:07:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=4
05/27/2022 11:07:52 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 11:07:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 11:07:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=4
05/27/2022 11:07:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=5
05/27/2022 11:08:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=6
05/27/2022 11:08:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=7
05/27/2022 11:08:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=8
05/27/2022 11:08:10 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 11:08:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
05/27/2022 11:08:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=9
05/27/2022 11:08:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=10
05/27/2022 11:08:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=11
05/27/2022 11:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=12
05/27/2022 11:08:28 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 11:08:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=13
05/27/2022 11:08:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=14
05/27/2022 11:08:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/27/2022 11:08:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=15
05/27/2022 11:08:41 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=16
05/27/2022 11:08:46 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 11:08:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=17
05/27/2022 11:08:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=18
05/27/2022 11:08:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=19
05/27/2022 11:08:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=19
05/27/2022 11:08:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=20
05/27/2022 11:09:04 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 11:09:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/27/2022 11:09:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/27/2022 11:09:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
05/27/2022 11:09:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=24
05/27/2022 11:09:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
05/27/2022 11:09:22 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 11:09:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
05/27/2022 11:09:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/27/2022 11:09:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=27
05/27/2022 11:09:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/27/2022 11:09:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=29
05/27/2022 11:09:40 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 11:09:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/27/2022 11:09:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=30
05/27/2022 11:09:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=31
05/27/2022 11:09:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=32
05/27/2022 11:09:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/27/2022 11:09:58 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 11:10:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/27/2022 11:10:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
05/27/2022 11:10:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=35
05/27/2022 11:10:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.51 on epoch=36
05/27/2022 11:10:11 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=37
05/27/2022 11:10:16 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=37
05/27/2022 11:10:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/27/2022 11:10:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=39
05/27/2022 11:10:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
05/27/2022 11:10:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=40
05/27/2022 11:10:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=41
05/27/2022 11:10:34 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
05/27/2022 11:10:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
05/27/2022 11:10:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/27/2022 11:10:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
05/27/2022 11:10:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
05/27/2022 11:10:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=45
05/27/2022 11:10:52 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=45
05/27/2022 11:10:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=46
05/27/2022 11:10:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=47
05/27/2022 11:11:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=48
05/27/2022 11:11:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=49
05/27/2022 11:11:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=49
05/27/2022 11:11:10 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=49
05/27/2022 11:11:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
05/27/2022 11:11:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=51
05/27/2022 11:11:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=52
05/27/2022 11:11:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
05/27/2022 11:11:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=54
05/27/2022 11:11:28 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.1754553408096715 on epoch=54
05/27/2022 11:11:28 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1754553408096715 on epoch=54, global_step=650
05/27/2022 11:11:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
05/27/2022 11:11:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=55
05/27/2022 11:11:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=56
05/27/2022 11:11:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=57
05/27/2022 11:11:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=58
05/27/2022 11:11:47 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.3021253699219801 on epoch=58
05/27/2022 11:11:47 - INFO - __main__ - Saving model with best Classification-F1: 0.1754553408096715 -> 0.3021253699219801 on epoch=58, global_step=700
05/27/2022 11:11:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
05/27/2022 11:11:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/27/2022 11:11:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=60
05/27/2022 11:11:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=61
05/27/2022 11:12:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=62
05/27/2022 11:12:05 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=62
05/27/2022 11:12:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
05/27/2022 11:12:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
05/27/2022 11:12:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=64
05/27/2022 11:12:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
05/27/2022 11:12:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
05/27/2022 11:12:23 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.1859084026507777 on epoch=66
05/27/2022 11:12:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
05/27/2022 11:12:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=68
05/27/2022 11:12:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=69
05/27/2022 11:12:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=69
05/27/2022 11:12:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
05/27/2022 11:12:42 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.260405224369526 on epoch=70
05/27/2022 11:12:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=71
05/27/2022 11:12:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
05/27/2022 11:12:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/27/2022 11:12:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=74
05/27/2022 11:12:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=74
05/27/2022 11:13:00 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.24009375228887422 on epoch=74
05/27/2022 11:13:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
05/27/2022 11:13:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
05/27/2022 11:13:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=77
05/27/2022 11:13:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=78
05/27/2022 11:13:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/27/2022 11:13:18 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3349395123624088 on epoch=79
05/27/2022 11:13:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3021253699219801 -> 0.3349395123624088 on epoch=79, global_step=950
05/27/2022 11:13:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=79
05/27/2022 11:13:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=80
05/27/2022 11:13:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
05/27/2022 11:13:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
05/27/2022 11:13:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=83
05/27/2022 11:13:37 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.25711820534943913 on epoch=83
05/27/2022 11:13:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/27/2022 11:13:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=84
05/27/2022 11:13:44 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
05/27/2022 11:13:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=86
05/27/2022 11:13:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=87
05/27/2022 11:13:55 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.2493989071038251 on epoch=87
05/27/2022 11:13:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
05/27/2022 11:14:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=89
05/27/2022 11:14:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=89
05/27/2022 11:14:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=90
05/27/2022 11:14:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=91
05/27/2022 11:14:13 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.28121224854246546 on epoch=91
05/27/2022 11:14:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=92
05/27/2022 11:14:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
05/27/2022 11:14:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
05/27/2022 11:14:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
05/27/2022 11:14:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=95
05/27/2022 11:14:31 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.2727734299770646 on epoch=95
05/27/2022 11:14:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
05/27/2022 11:14:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=97
05/27/2022 11:14:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=98
05/27/2022 11:14:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=99
05/27/2022 11:14:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=99
05/27/2022 11:14:49 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.310061877858488 on epoch=99
05/27/2022 11:14:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=100
05/27/2022 11:14:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
05/27/2022 11:14:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
05/27/2022 11:15:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.38 on epoch=103
05/27/2022 11:15:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=104
05/27/2022 11:15:07 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.28889580523567454 on epoch=104
05/27/2022 11:15:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=104
05/27/2022 11:15:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.35 on epoch=105
05/27/2022 11:15:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=106
05/27/2022 11:15:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
05/27/2022 11:15:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
05/27/2022 11:15:26 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.3455982298087561 on epoch=108
05/27/2022 11:15:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3349395123624088 -> 0.3455982298087561 on epoch=108, global_step=1300
05/27/2022 11:15:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=109
05/27/2022 11:15:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=109
05/27/2022 11:15:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=110
05/27/2022 11:15:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=111
05/27/2022 11:15:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=112
05/27/2022 11:15:44 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.32525133057362504 on epoch=112
05/27/2022 11:15:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=113
05/27/2022 11:15:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=114
05/27/2022 11:15:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=114
05/27/2022 11:15:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=115
05/27/2022 11:15:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=116
05/27/2022 11:16:02 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.35095350669818753 on epoch=116
05/27/2022 11:16:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3455982298087561 -> 0.35095350669818753 on epoch=116, global_step=1400
05/27/2022 11:16:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
05/27/2022 11:16:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=118
05/27/2022 11:16:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=119
05/27/2022 11:16:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=119
05/27/2022 11:16:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=120
05/27/2022 11:16:21 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.3498207885304659 on epoch=120
05/27/2022 11:16:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.38 on epoch=121
05/27/2022 11:16:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=122
05/27/2022 11:16:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.38 on epoch=123
05/27/2022 11:16:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=124
05/27/2022 11:16:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=124
05/27/2022 11:16:39 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.32930288102701893 on epoch=124
05/27/2022 11:16:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=125
05/27/2022 11:16:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
05/27/2022 11:16:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.34 on epoch=127
05/27/2022 11:16:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=128
05/27/2022 11:16:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=129
05/27/2022 11:16:57 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.3489984830191015 on epoch=129
05/27/2022 11:17:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=129
05/27/2022 11:17:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=130
05/27/2022 11:17:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=131
05/27/2022 11:17:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=132
05/27/2022 11:17:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=133
05/27/2022 11:17:15 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.3285186429291233 on epoch=133
05/27/2022 11:17:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=134
05/27/2022 11:17:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=134
05/27/2022 11:17:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=135
05/27/2022 11:17:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=136
05/27/2022 11:17:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=137
05/27/2022 11:17:33 - INFO - __main__ - Global step 1650 Train loss 0.36 Classification-F1 0.3314298510805061 on epoch=137
05/27/2022 11:17:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=138
05/27/2022 11:17:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=139
05/27/2022 11:17:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=139
05/27/2022 11:17:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=140
05/27/2022 11:17:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=141
05/27/2022 11:17:51 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.3589569160997732 on epoch=141
05/27/2022 11:17:51 - INFO - __main__ - Saving model with best Classification-F1: 0.35095350669818753 -> 0.3589569160997732 on epoch=141, global_step=1700
05/27/2022 11:17:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=142
05/27/2022 11:17:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=143
05/27/2022 11:17:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=144
05/27/2022 11:18:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=144
05/27/2022 11:18:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=145
05/27/2022 11:18:09 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.3698606720524529 on epoch=145
05/27/2022 11:18:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3589569160997732 -> 0.3698606720524529 on epoch=145, global_step=1750
05/27/2022 11:18:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=146
05/27/2022 11:18:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.35 on epoch=147
05/27/2022 11:18:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=148
05/27/2022 11:18:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=149
05/27/2022 11:18:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=149
05/27/2022 11:18:27 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3634346705042873 on epoch=149
05/27/2022 11:18:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=150
05/27/2022 11:18:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=151
05/27/2022 11:18:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=152
05/27/2022 11:18:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=153
05/27/2022 11:18:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=154
05/27/2022 11:18:45 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.3569023569023569 on epoch=154
05/27/2022 11:18:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=154
05/27/2022 11:18:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=155
05/27/2022 11:18:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=156
05/27/2022 11:18:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=157
05/27/2022 11:18:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=158
05/27/2022 11:19:03 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.356898722791869 on epoch=158
05/27/2022 11:19:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=159
05/27/2022 11:19:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=159
05/27/2022 11:19:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=160
05/27/2022 11:19:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=161
05/27/2022 11:19:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.32 on epoch=162
05/27/2022 11:19:20 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.36936460353847056 on epoch=162
05/27/2022 11:19:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.30 on epoch=163
05/27/2022 11:19:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=164
05/27/2022 11:19:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=164
05/27/2022 11:19:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=165
05/27/2022 11:19:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=166
05/27/2022 11:19:38 - INFO - __main__ - Global step 2000 Train loss 0.31 Classification-F1 0.3404424832996262 on epoch=166
05/27/2022 11:19:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.29 on epoch=167
05/27/2022 11:19:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=168
05/27/2022 11:19:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=169
05/27/2022 11:19:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.30 on epoch=169
05/27/2022 11:19:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=170
05/27/2022 11:19:56 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.35212121212121206 on epoch=170
05/27/2022 11:19:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.29 on epoch=171
05/27/2022 11:20:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=172
05/27/2022 11:20:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.31 on epoch=173
05/27/2022 11:20:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=174
05/27/2022 11:20:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.26 on epoch=174
05/27/2022 11:20:14 - INFO - __main__ - Global step 2100 Train loss 0.29 Classification-F1 0.3578220835782208 on epoch=174
05/27/2022 11:20:17 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.38 on epoch=175
05/27/2022 11:20:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=176
05/27/2022 11:20:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.31 on epoch=177
05/27/2022 11:20:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=178
05/27/2022 11:20:27 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.34 on epoch=179
05/27/2022 11:20:32 - INFO - __main__ - Global step 2150 Train loss 0.34 Classification-F1 0.3578220835782208 on epoch=179
05/27/2022 11:20:35 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.31 on epoch=179
05/27/2022 11:20:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=180
05/27/2022 11:20:40 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=181
05/27/2022 11:20:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=182
05/27/2022 11:20:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=183
05/27/2022 11:20:50 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.36158594491927826 on epoch=183
05/27/2022 11:20:53 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=184
05/27/2022 11:20:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.34 on epoch=184
05/27/2022 11:20:58 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=185
05/27/2022 11:21:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.34 on epoch=186
05/27/2022 11:21:03 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=187
05/27/2022 11:21:08 - INFO - __main__ - Global step 2250 Train loss 0.31 Classification-F1 0.3607641657070304 on epoch=187
05/27/2022 11:21:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=188
05/27/2022 11:21:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=189
05/27/2022 11:21:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.30 on epoch=189
05/27/2022 11:21:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.29 on epoch=190
05/27/2022 11:21:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=191
05/27/2022 11:21:26 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.3412441899836858 on epoch=191
05/27/2022 11:21:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=192
05/27/2022 11:21:32 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.30 on epoch=193
05/27/2022 11:21:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=194
05/27/2022 11:21:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.35 on epoch=194
05/27/2022 11:21:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.33 on epoch=195
05/27/2022 11:21:45 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.37447365426197793 on epoch=195
05/27/2022 11:21:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3698606720524529 -> 0.37447365426197793 on epoch=195, global_step=2350
05/27/2022 11:21:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.30 on epoch=196
05/27/2022 11:21:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.30 on epoch=197
05/27/2022 11:21:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=198
05/27/2022 11:21:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.28 on epoch=199
05/27/2022 11:21:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.33 on epoch=199
05/27/2022 11:22:03 - INFO - __main__ - Global step 2400 Train loss 0.30 Classification-F1 0.36313932980599645 on epoch=199
05/27/2022 11:22:05 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=200
05/27/2022 11:22:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.32 on epoch=201
05/27/2022 11:22:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.30 on epoch=202
05/27/2022 11:22:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.32 on epoch=203
05/27/2022 11:22:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=204
05/27/2022 11:22:21 - INFO - __main__ - Global step 2450 Train loss 0.30 Classification-F1 0.34402884732917743 on epoch=204
05/27/2022 11:22:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.28 on epoch=204
05/27/2022 11:22:26 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.30 on epoch=205
05/27/2022 11:22:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.26 on epoch=206
05/27/2022 11:22:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=207
05/27/2022 11:22:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.28 on epoch=208
05/27/2022 11:22:39 - INFO - __main__ - Global step 2500 Train loss 0.28 Classification-F1 0.3545496153902939 on epoch=208
05/27/2022 11:22:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.32 on epoch=209
05/27/2022 11:22:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.32 on epoch=209
05/27/2022 11:22:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.26 on epoch=210
05/27/2022 11:22:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.33 on epoch=211
05/27/2022 11:22:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.26 on epoch=212
05/27/2022 11:22:58 - INFO - __main__ - Global step 2550 Train loss 0.30 Classification-F1 0.35737331430139235 on epoch=212
05/27/2022 11:23:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=213
05/27/2022 11:23:03 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.27 on epoch=214
05/27/2022 11:23:06 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=214
05/27/2022 11:23:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=215
05/27/2022 11:23:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.32 on epoch=216
05/27/2022 11:23:16 - INFO - __main__ - Global step 2600 Train loss 0.30 Classification-F1 0.3766028230028661 on epoch=216
05/27/2022 11:23:16 - INFO - __main__ - Saving model with best Classification-F1: 0.37447365426197793 -> 0.3766028230028661 on epoch=216, global_step=2600
05/27/2022 11:23:19 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.29 on epoch=217
05/27/2022 11:23:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.29 on epoch=218
05/27/2022 11:23:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.27 on epoch=219
05/27/2022 11:23:27 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.25 on epoch=219
05/27/2022 11:23:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=220
05/27/2022 11:23:35 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.35668276972624796 on epoch=220
05/27/2022 11:23:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.27 on epoch=221
05/27/2022 11:23:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.23 on epoch=222
05/27/2022 11:23:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=223
05/27/2022 11:23:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=224
05/27/2022 11:23:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=224
05/27/2022 11:23:53 - INFO - __main__ - Global step 2700 Train loss 0.26 Classification-F1 0.37861952861952863 on epoch=224
05/27/2022 11:23:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3766028230028661 -> 0.37861952861952863 on epoch=224, global_step=2700
05/27/2022 11:23:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=225
05/27/2022 11:23:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=226
05/27/2022 11:24:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.27 on epoch=227
05/27/2022 11:24:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=228
05/27/2022 11:24:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=229
05/27/2022 11:24:11 - INFO - __main__ - Global step 2750 Train loss 0.27 Classification-F1 0.3683385218494178 on epoch=229
05/27/2022 11:24:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.29 on epoch=229
05/27/2022 11:24:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=230
05/27/2022 11:24:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.29 on epoch=231
05/27/2022 11:24:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.23 on epoch=232
05/27/2022 11:24:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=233
05/27/2022 11:24:30 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.36608512489410905 on epoch=233
05/27/2022 11:24:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=234
05/27/2022 11:24:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=234
05/27/2022 11:24:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.31 on epoch=235
05/27/2022 11:24:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.30 on epoch=236
05/27/2022 11:24:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=237
05/27/2022 11:24:48 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.36853002070393376 on epoch=237
05/27/2022 11:24:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.27 on epoch=238
05/27/2022 11:24:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.28 on epoch=239
05/27/2022 11:24:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.32 on epoch=239
05/27/2022 11:24:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.28 on epoch=240
05/27/2022 11:25:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.23 on epoch=241
05/27/2022 11:25:06 - INFO - __main__ - Global step 2900 Train loss 0.27 Classification-F1 0.35051643192488263 on epoch=241
05/27/2022 11:25:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.29 on epoch=242
05/27/2022 11:25:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.29 on epoch=243
05/27/2022 11:25:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.25 on epoch=244
05/27/2022 11:25:17 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=244
05/27/2022 11:25:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.24 on epoch=245
05/27/2022 11:25:25 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.36301758890879027 on epoch=245
05/27/2022 11:25:27 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.21 on epoch=246
05/27/2022 11:25:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.30 on epoch=247
05/27/2022 11:25:32 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=248
05/27/2022 11:25:35 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.26 on epoch=249
05/27/2022 11:25:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.25 on epoch=249
05/27/2022 11:25:39 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:25:39 - INFO - __main__ - Printing 3 examples
05/27/2022 11:25:39 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 11:25:39 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:39 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 11:25:39 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:39 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 11:25:39 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:39 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:25:39 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:25:40 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:25:40 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:25:40 - INFO - __main__ - Printing 3 examples
05/27/2022 11:25:40 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 11:25:40 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:40 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 11:25:40 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:40 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 11:25:40 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:40 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:25:40 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:25:40 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:25:43 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.3630313031303131 on epoch=249
05/27/2022 11:25:43 - INFO - __main__ - save last model!
05/27/2022 11:25:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 11:25:43 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 11:25:43 - INFO - __main__ - Printing 3 examples
05/27/2022 11:25:43 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 11:25:43 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:43 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 11:25:43 - INFO - __main__ - ['entailment']
05/27/2022 11:25:43 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 11:25:43 - INFO - __main__ - ['contradiction']
05/27/2022 11:25:43 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:25:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:25:45 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 11:25:58 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:25:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:25:58 - INFO - __main__ - Starting training!
05/27/2022 11:26:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_100_0.2_8_predictions.txt
05/27/2022 11:26:13 - INFO - __main__ - Classification-F1 on test data: 0.2992
05/27/2022 11:26:13 - INFO - __main__ - prefix=anli_64_100, lr=0.2, bsz=8, dev_performance=0.37861952861952863, test_performance=0.2992253925686273
05/27/2022 11:26:13 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.5, bsz=8 ...
05/27/2022 11:26:14 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:26:14 - INFO - __main__ - Printing 3 examples
05/27/2022 11:26:14 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 11:26:14 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:14 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 11:26:14 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:14 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 11:26:14 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:14 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:26:14 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:26:15 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:26:15 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:26:15 - INFO - __main__ - Printing 3 examples
05/27/2022 11:26:15 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 11:26:15 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:15 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 11:26:15 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:15 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 11:26:15 - INFO - __main__ - ['contradiction']
05/27/2022 11:26:15 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:26:15 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:26:15 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:26:34 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:26:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:26:34 - INFO - __main__ - Starting training!
05/27/2022 11:26:38 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=0
05/27/2022 11:26:41 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
05/27/2022 11:26:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=2
05/27/2022 11:26:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=3
05/27/2022 11:26:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=4
05/27/2022 11:26:54 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 11:26:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 11:26:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=4
05/27/2022 11:26:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=5
05/27/2022 11:27:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=6
05/27/2022 11:27:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=7
05/27/2022 11:27:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=8
05/27/2022 11:27:12 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.22969556243550052 on epoch=8
05/27/2022 11:27:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.22969556243550052 on epoch=8, global_step=100
05/27/2022 11:27:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=9
05/27/2022 11:27:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=9
05/27/2022 11:27:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
05/27/2022 11:27:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=11
05/27/2022 11:27:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=12
05/27/2022 11:27:31 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.2866880513231756 on epoch=12
05/27/2022 11:27:31 - INFO - __main__ - Saving model with best Classification-F1: 0.22969556243550052 -> 0.2866880513231756 on epoch=12, global_step=150
05/27/2022 11:27:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=13
05/27/2022 11:27:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
05/27/2022 11:27:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=14
05/27/2022 11:27:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
05/27/2022 11:27:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
05/27/2022 11:27:49 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.2087619047619048 on epoch=16
05/27/2022 11:27:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=17
05/27/2022 11:27:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
05/27/2022 11:27:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
05/27/2022 11:28:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=19
05/27/2022 11:28:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=20
05/27/2022 11:28:07 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 11:28:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=21
05/27/2022 11:28:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/27/2022 11:28:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=23
05/27/2022 11:28:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=24
05/27/2022 11:28:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
05/27/2022 11:28:26 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.3471377650749849 on epoch=24
05/27/2022 11:28:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2866880513231756 -> 0.3471377650749849 on epoch=24, global_step=300
05/27/2022 11:28:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=25
05/27/2022 11:28:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=26
05/27/2022 11:28:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=27
05/27/2022 11:28:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/27/2022 11:28:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
05/27/2022 11:28:45 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.20105649404759807 on epoch=29
05/27/2022 11:28:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=29
05/27/2022 11:28:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=30
05/27/2022 11:28:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=31
05/27/2022 11:28:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
05/27/2022 11:28:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=33
05/27/2022 11:29:03 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.2510785159620362 on epoch=33
05/27/2022 11:29:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=34
05/27/2022 11:29:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=34
05/27/2022 11:29:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=35
05/27/2022 11:29:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=36
05/27/2022 11:29:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=37
05/27/2022 11:29:21 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.24496715405806313 on epoch=37
05/27/2022 11:29:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=38
05/27/2022 11:29:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=39
05/27/2022 11:29:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=39
05/27/2022 11:29:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=40
05/27/2022 11:29:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=41
05/27/2022 11:29:38 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.2477219749947023 on epoch=41
05/27/2022 11:29:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=42
05/27/2022 11:29:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/27/2022 11:29:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/27/2022 11:29:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=44
05/27/2022 11:29:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=45
05/27/2022 11:29:56 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.2423065861374748 on epoch=45
05/27/2022 11:29:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=46
05/27/2022 11:30:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=47
05/27/2022 11:30:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=48
05/27/2022 11:30:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=49
05/27/2022 11:30:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=49
05/27/2022 11:30:13 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.3268370411227554 on epoch=49
05/27/2022 11:30:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=50
05/27/2022 11:30:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=51
05/27/2022 11:30:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=52
05/27/2022 11:30:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=53
05/27/2022 11:30:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=54
05/27/2022 11:30:30 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.2507615840949174 on epoch=54
05/27/2022 11:30:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=54
05/27/2022 11:30:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=55
05/27/2022 11:30:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=56
05/27/2022 11:30:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=57
05/27/2022 11:30:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=58
05/27/2022 11:30:47 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.2507615840949174 on epoch=58
05/27/2022 11:30:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=59
05/27/2022 11:30:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=59
05/27/2022 11:30:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=60
05/27/2022 11:30:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=61
05/27/2022 11:31:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=62
05/27/2022 11:31:04 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.352616586879036 on epoch=62
05/27/2022 11:31:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3471377650749849 -> 0.352616586879036 on epoch=62, global_step=750
05/27/2022 11:31:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=63
05/27/2022 11:31:09 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=64
05/27/2022 11:31:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=64
05/27/2022 11:31:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=65
05/27/2022 11:31:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=66
05/27/2022 11:31:21 - INFO - __main__ - Global step 800 Train loss 0.33 Classification-F1 0.35122142351221425 on epoch=66
05/27/2022 11:31:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=67
05/27/2022 11:31:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=68
05/27/2022 11:31:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=69
05/27/2022 11:31:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=69
05/27/2022 11:31:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=70
05/27/2022 11:31:38 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.29291724640561845 on epoch=70
05/27/2022 11:31:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=71
05/27/2022 11:31:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=72
05/27/2022 11:31:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=73
05/27/2022 11:31:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=74
05/27/2022 11:31:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=74
05/27/2022 11:31:56 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.3275854098284869 on epoch=74
05/27/2022 11:31:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=75
05/27/2022 11:32:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=76
05/27/2022 11:32:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=77
05/27/2022 11:32:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=78
05/27/2022 11:32:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=79
05/27/2022 11:32:13 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.3040196410005887 on epoch=79
05/27/2022 11:32:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=79
05/27/2022 11:32:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=80
05/27/2022 11:32:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=81
05/27/2022 11:32:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=82
05/27/2022 11:32:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=83
05/27/2022 11:32:31 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.2924315619967794 on epoch=83
05/27/2022 11:32:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=84
05/27/2022 11:32:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=84
05/27/2022 11:32:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=85
05/27/2022 11:32:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=86
05/27/2022 11:32:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=87
05/27/2022 11:32:48 - INFO - __main__ - Global step 1050 Train loss 0.27 Classification-F1 0.2931910076014879 on epoch=87
05/27/2022 11:32:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=88
05/27/2022 11:32:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=89
05/27/2022 11:32:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=89
05/27/2022 11:32:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=90
05/27/2022 11:33:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=91
05/27/2022 11:33:05 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.3276968279666607 on epoch=91
05/27/2022 11:33:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=92
05/27/2022 11:33:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=93
05/27/2022 11:33:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=94
05/27/2022 11:33:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=94
05/27/2022 11:33:18 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=95
05/27/2022 11:33:23 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.2899498968464486 on epoch=95
05/27/2022 11:33:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=96
05/27/2022 11:33:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=97
05/27/2022 11:33:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=98
05/27/2022 11:33:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=99
05/27/2022 11:33:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=99
05/27/2022 11:33:40 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.28551972372197093 on epoch=99
05/27/2022 11:33:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=100
05/27/2022 11:33:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=101
05/27/2022 11:33:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=102
05/27/2022 11:33:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=103
05/27/2022 11:33:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=104
05/27/2022 11:33:58 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.3228793813736746 on epoch=104
05/27/2022 11:34:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=104
05/27/2022 11:34:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=105
05/27/2022 11:34:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=106
05/27/2022 11:34:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=107
05/27/2022 11:34:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=108
05/27/2022 11:34:16 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.3141077651710329 on epoch=108
05/27/2022 11:34:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=109
05/27/2022 11:34:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=109
05/27/2022 11:34:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=110
05/27/2022 11:34:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=111
05/27/2022 11:34:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=112
05/27/2022 11:34:33 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.33142123483962677 on epoch=112
05/27/2022 11:34:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=113
05/27/2022 11:34:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=114
05/27/2022 11:34:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=114
05/27/2022 11:34:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=115
05/27/2022 11:34:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=116
05/27/2022 11:34:51 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.3604658252545576 on epoch=116
05/27/2022 11:34:51 - INFO - __main__ - Saving model with best Classification-F1: 0.352616586879036 -> 0.3604658252545576 on epoch=116, global_step=1400
05/27/2022 11:34:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=117
05/27/2022 11:34:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=118
05/27/2022 11:34:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=119
05/27/2022 11:35:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=119
05/27/2022 11:35:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=120
05/27/2022 11:35:09 - INFO - __main__ - Global step 1450 Train loss 0.21 Classification-F1 0.3378241825721626 on epoch=120
05/27/2022 11:35:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=121
05/27/2022 11:35:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=122
05/27/2022 11:35:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=123
05/27/2022 11:35:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=124
05/27/2022 11:35:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=124
05/27/2022 11:35:26 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.33504432075860646 on epoch=124
05/27/2022 11:35:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=125
05/27/2022 11:35:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=126
05/27/2022 11:35:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=127
05/27/2022 11:35:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=128
05/27/2022 11:35:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=129
05/27/2022 11:35:44 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.33337046939988113 on epoch=129
05/27/2022 11:35:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=129
05/27/2022 11:35:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=130
05/27/2022 11:35:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=131
05/27/2022 11:35:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=132
05/27/2022 11:35:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.28 on epoch=133
05/27/2022 11:36:02 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.32742812742812744 on epoch=133
05/27/2022 11:36:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=134
05/27/2022 11:36:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=134
05/27/2022 11:36:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=135
05/27/2022 11:36:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=136
05/27/2022 11:36:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.22 on epoch=137
05/27/2022 11:36:20 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.34122042341220427 on epoch=137
05/27/2022 11:36:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=138
05/27/2022 11:36:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=139
05/27/2022 11:36:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=139
05/27/2022 11:36:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=140
05/27/2022 11:36:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=141
05/27/2022 11:36:38 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.360423920346955 on epoch=141
05/27/2022 11:36:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=142
05/27/2022 11:36:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=143
05/27/2022 11:36:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=144
05/27/2022 11:36:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=144
05/27/2022 11:36:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=145
05/27/2022 11:36:56 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.3436039456480211 on epoch=145
05/27/2022 11:36:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=146
05/27/2022 11:37:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=147
05/27/2022 11:37:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=148
05/27/2022 11:37:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=149
05/27/2022 11:37:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=149
05/27/2022 11:37:14 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.3662852168535038 on epoch=149
05/27/2022 11:37:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3604658252545576 -> 0.3662852168535038 on epoch=149, global_step=1800
05/27/2022 11:37:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=150
05/27/2022 11:37:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=151
05/27/2022 11:37:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=152
05/27/2022 11:37:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=153
05/27/2022 11:37:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=154
05/27/2022 11:37:32 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.33284096228959525 on epoch=154
05/27/2022 11:37:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=154
05/27/2022 11:37:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=155
05/27/2022 11:37:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=156
05/27/2022 11:37:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=157
05/27/2022 11:37:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=158
05/27/2022 11:37:50 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.3264330232415339 on epoch=158
05/27/2022 11:37:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=159
05/27/2022 11:37:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=159
05/27/2022 11:37:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=160
05/27/2022 11:38:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.15 on epoch=161
05/27/2022 11:38:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=162
05/27/2022 11:38:07 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.2627803341209634 on epoch=162
05/27/2022 11:38:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=163
05/27/2022 11:38:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=164
05/27/2022 11:38:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=164
05/27/2022 11:38:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=165
05/27/2022 11:38:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=166
05/27/2022 11:38:25 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.2489260739260739 on epoch=166
05/27/2022 11:38:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=167
05/27/2022 11:38:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=168
05/27/2022 11:38:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=169
05/27/2022 11:38:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=169
05/27/2022 11:38:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=170
05/27/2022 11:38:43 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.2674388111888112 on epoch=170
05/27/2022 11:38:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=171
05/27/2022 11:38:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=172
05/27/2022 11:38:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=173
05/27/2022 11:38:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=174
05/27/2022 11:38:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=174
05/27/2022 11:39:00 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.26768823077234294 on epoch=174
05/27/2022 11:39:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=175
05/27/2022 11:39:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=176
05/27/2022 11:39:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=177
05/27/2022 11:39:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=178
05/27/2022 11:39:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=179
05/27/2022 11:39:19 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.3007930802788441 on epoch=179
05/27/2022 11:39:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=179
05/27/2022 11:39:24 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=180
05/27/2022 11:39:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=181
05/27/2022 11:39:29 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=182
05/27/2022 11:39:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=183
05/27/2022 11:39:37 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.2870046530653986 on epoch=183
05/27/2022 11:39:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=184
05/27/2022 11:39:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=184
05/27/2022 11:39:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=185
05/27/2022 11:39:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=186
05/27/2022 11:39:50 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=187
05/27/2022 11:39:55 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.17175945762805295 on epoch=187
05/27/2022 11:39:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=188
05/27/2022 11:40:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=189
05/27/2022 11:40:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=189
05/27/2022 11:40:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=190
05/27/2022 11:40:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=191
05/27/2022 11:40:13 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.15744853210509757 on epoch=191
05/27/2022 11:40:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=192
05/27/2022 11:40:18 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=193
05/27/2022 11:40:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=194
05/27/2022 11:40:23 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=194
05/27/2022 11:40:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=195
05/27/2022 11:40:31 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.159720452867661 on epoch=195
05/27/2022 11:40:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=196
05/27/2022 11:40:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=197
05/27/2022 11:40:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=198
05/27/2022 11:40:42 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=199
05/27/2022 11:40:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=199
05/27/2022 11:40:49 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.23051114023591088 on epoch=199
05/27/2022 11:40:52 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=200
05/27/2022 11:40:54 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=201
05/27/2022 11:40:57 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=202
05/27/2022 11:41:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=203
05/27/2022 11:41:02 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=204
05/27/2022 11:41:07 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.21541231126596982 on epoch=204
05/27/2022 11:41:10 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=204
05/27/2022 11:41:13 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=205
05/27/2022 11:41:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=206
05/27/2022 11:41:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=207
05/27/2022 11:41:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=208
05/27/2022 11:41:25 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.32134126654674594 on epoch=208
05/27/2022 11:41:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=209
05/27/2022 11:41:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=209
05/27/2022 11:41:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=210
05/27/2022 11:41:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=211
05/27/2022 11:41:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=212
05/27/2022 11:41:43 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.13139587554782794 on epoch=212
05/27/2022 11:41:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=213
05/27/2022 11:41:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=214
05/27/2022 11:41:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=214
05/27/2022 11:41:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=215
05/27/2022 11:41:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=216
05/27/2022 11:42:01 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.17882493387244217 on epoch=216
05/27/2022 11:42:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=217
05/27/2022 11:42:06 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=218
05/27/2022 11:42:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=219
05/27/2022 11:42:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=219
05/27/2022 11:42:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=220
05/27/2022 11:42:19 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.16188972708034985 on epoch=220
05/27/2022 11:42:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=221
05/27/2022 11:42:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=222
05/27/2022 11:42:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=223
05/27/2022 11:42:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=224
05/27/2022 11:42:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=224
05/27/2022 11:42:37 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.18873733068106469 on epoch=224
05/27/2022 11:42:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=225
05/27/2022 11:42:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=226
05/27/2022 11:42:45 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=227
05/27/2022 11:42:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=228
05/27/2022 11:42:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=229
05/27/2022 11:42:55 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.134786889905525 on epoch=229
05/27/2022 11:42:58 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=229
05/27/2022 11:43:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=230
05/27/2022 11:43:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=231
05/27/2022 11:43:06 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=232
05/27/2022 11:43:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=233
05/27/2022 11:43:13 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.12096239461968361 on epoch=233
05/27/2022 11:43:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=234
05/27/2022 11:43:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=234
05/27/2022 11:43:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=235
05/27/2022 11:43:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=236
05/27/2022 11:43:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=237
05/27/2022 11:43:31 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.22043094160741217 on epoch=237
05/27/2022 11:43:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=238
05/27/2022 11:43:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=239
05/27/2022 11:43:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=239
05/27/2022 11:43:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=240
05/27/2022 11:43:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=241
05/27/2022 11:43:49 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.18780701385309617 on epoch=241
05/27/2022 11:43:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=242
05/27/2022 11:43:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=243
05/27/2022 11:43:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=244
05/27/2022 11:43:59 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=244
05/27/2022 11:44:02 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=245
05/27/2022 11:44:07 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.15786560168621905 on epoch=245
05/27/2022 11:44:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=246
05/27/2022 11:44:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=247
05/27/2022 11:44:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=248
05/27/2022 11:44:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=249
05/27/2022 11:44:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=249
05/27/2022 11:44:22 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:44:22 - INFO - __main__ - Printing 3 examples
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:44:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:44:22 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:44:22 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:44:22 - INFO - __main__ - Printing 3 examples
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 11:44:22 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:44:23 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:44:23 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:44:25 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.1882089358869545 on epoch=249
05/27/2022 11:44:25 - INFO - __main__ - save last model!
05/27/2022 11:44:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 11:44:25 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 11:44:25 - INFO - __main__ - Printing 3 examples
05/27/2022 11:44:25 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 11:44:25 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:25 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 11:44:25 - INFO - __main__ - ['entailment']
05/27/2022 11:44:25 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 11:44:25 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:44:26 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:44:27 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 11:44:41 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:44:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:44:42 - INFO - __main__ - Starting training!
05/27/2022 11:44:53 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_13_0.5_8_predictions.txt
05/27/2022 11:44:53 - INFO - __main__ - Classification-F1 on test data: 0.0322
05/27/2022 11:44:53 - INFO - __main__ - prefix=anli_64_13, lr=0.5, bsz=8, dev_performance=0.3662852168535038, test_performance=0.03223340242447083
05/27/2022 11:44:53 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.4, bsz=8 ...
05/27/2022 11:44:54 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:44:54 - INFO - __main__ - Printing 3 examples
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:44:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:44:54 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 11:44:54 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 11:44:54 - INFO - __main__ - Printing 3 examples
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 11:44:54 - INFO - __main__ - ['contradiction']
05/27/2022 11:44:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 11:44:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 11:44:55 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 11:45:13 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 11:45:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 11:45:14 - INFO - __main__ - Starting training!
05/27/2022 11:45:18 - INFO - __main__ - Step 10 Global step 10 Train loss 0.52 on epoch=0
05/27/2022 11:45:20 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=1
05/27/2022 11:45:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=2
05/27/2022 11:45:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=3
05/27/2022 11:45:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=4
05/27/2022 11:45:34 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 11:45:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 11:45:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=4
05/27/2022 11:45:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=5
05/27/2022 11:45:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=6
05/27/2022 11:45:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=7
05/27/2022 11:45:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.49 on epoch=8
05/27/2022 11:45:52 - INFO - __main__ - Global step 100 Train loss 0.47 Classification-F1 0.16732026143790854 on epoch=8
05/27/2022 11:45:52 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16732026143790854 on epoch=8, global_step=100
05/27/2022 11:45:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
05/27/2022 11:45:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=9
05/27/2022 11:46:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=10
05/27/2022 11:46:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=11
05/27/2022 11:46:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=12
05/27/2022 11:46:10 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.18892001244942422 on epoch=12
05/27/2022 11:46:11 - INFO - __main__ - Saving model with best Classification-F1: 0.16732026143790854 -> 0.18892001244942422 on epoch=12, global_step=150
05/27/2022 11:46:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=13
05/27/2022 11:46:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.46 on epoch=14
05/27/2022 11:46:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=14
05/27/2022 11:46:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=15
05/27/2022 11:46:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=16
05/27/2022 11:46:29 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.24253968253968253 on epoch=16
05/27/2022 11:46:29 - INFO - __main__ - Saving model with best Classification-F1: 0.18892001244942422 -> 0.24253968253968253 on epoch=16, global_step=200
05/27/2022 11:46:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
05/27/2022 11:46:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=18
05/27/2022 11:46:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=19
05/27/2022 11:46:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
05/27/2022 11:46:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.43 on epoch=20
05/27/2022 11:46:48 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16732026143790854 on epoch=20
05/27/2022 11:46:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/27/2022 11:46:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/27/2022 11:46:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=23
05/27/2022 11:46:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=24
05/27/2022 11:47:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
05/27/2022 11:47:07 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.21830043177404457 on epoch=24
05/27/2022 11:47:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=25
05/27/2022 11:47:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=26
05/27/2022 11:47:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=27
05/27/2022 11:47:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=28
05/27/2022 11:47:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=29
05/27/2022 11:47:26 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.3278066962277489 on epoch=29
05/27/2022 11:47:26 - INFO - __main__ - Saving model with best Classification-F1: 0.24253968253968253 -> 0.3278066962277489 on epoch=29, global_step=350
05/27/2022 11:47:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=29
05/27/2022 11:47:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=30
05/27/2022 11:47:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=31
05/27/2022 11:47:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=32
05/27/2022 11:47:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=33
05/27/2022 11:47:44 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.1895141895141895 on epoch=33
05/27/2022 11:47:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/27/2022 11:47:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
05/27/2022 11:47:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
05/27/2022 11:47:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=36
05/27/2022 11:47:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=37
05/27/2022 11:48:03 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.3113026819923372 on epoch=37
05/27/2022 11:48:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=38
05/27/2022 11:48:08 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/27/2022 11:48:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
05/27/2022 11:48:14 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=40
05/27/2022 11:48:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/27/2022 11:48:22 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.3405847993613951 on epoch=41
05/27/2022 11:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3278066962277489 -> 0.3405847993613951 on epoch=41, global_step=500
05/27/2022 11:48:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
05/27/2022 11:48:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=43
05/27/2022 11:48:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=44
05/27/2022 11:48:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
05/27/2022 11:48:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=45
05/27/2022 11:48:40 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.1989722270338934 on epoch=45
05/27/2022 11:48:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=46
05/27/2022 11:48:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=47
05/27/2022 11:48:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=48
05/27/2022 11:48:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=49
05/27/2022 11:48:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=49
05/27/2022 11:48:58 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.29916640938561284 on epoch=49
05/27/2022 11:49:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=50
05/27/2022 11:49:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=51
05/27/2022 11:49:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=52
05/27/2022 11:49:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=53
05/27/2022 11:49:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=54
05/27/2022 11:49:16 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.2722222222222222 on epoch=54
05/27/2022 11:49:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=54
05/27/2022 11:49:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/27/2022 11:49:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=56
05/27/2022 11:49:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
05/27/2022 11:49:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=58
05/27/2022 11:49:34 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.2717082493930471 on epoch=58
05/27/2022 11:49:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=59
05/27/2022 11:49:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=59
05/27/2022 11:49:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=60
05/27/2022 11:49:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=61
05/27/2022 11:49:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=62
05/27/2022 11:49:52 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.3328610387433917 on epoch=62
05/27/2022 11:49:54 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=63
05/27/2022 11:49:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=64
05/27/2022 11:49:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=64
05/27/2022 11:50:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=65
05/27/2022 11:50:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=66
05/27/2022 11:50:09 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.34848484848484845 on epoch=66
05/27/2022 11:50:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3405847993613951 -> 0.34848484848484845 on epoch=66, global_step=800
05/27/2022 11:50:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=67
05/27/2022 11:50:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=68
05/27/2022 11:50:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.31 on epoch=69
05/27/2022 11:50:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=69
05/27/2022 11:50:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=70
05/27/2022 11:50:27 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.23957434570031633 on epoch=70
05/27/2022 11:50:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=71
05/27/2022 11:50:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=72
05/27/2022 11:50:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=73
05/27/2022 11:50:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=74
05/27/2022 11:50:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=74
05/27/2022 11:50:45 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.34848484848484845 on epoch=74
05/27/2022 11:50:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=75
05/27/2022 11:50:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=76
05/27/2022 11:50:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=77
05/27/2022 11:50:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=78
05/27/2022 11:50:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
05/27/2022 11:51:02 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.3065406209511013 on epoch=79
05/27/2022 11:51:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=79
05/27/2022 11:51:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=80
05/27/2022 11:51:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=81
05/27/2022 11:51:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=82
05/27/2022 11:51:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=83
05/27/2022 11:51:20 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.32968594571550375 on epoch=83
05/27/2022 11:51:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=84
05/27/2022 11:51:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=84
05/27/2022 11:51:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=85
05/27/2022 11:51:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=86
05/27/2022 11:51:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=87
05/27/2022 11:51:38 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.3631782945736434 on epoch=87
05/27/2022 11:51:38 - INFO - __main__ - Saving model with best Classification-F1: 0.34848484848484845 -> 0.3631782945736434 on epoch=87, global_step=1050
05/27/2022 11:51:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=88
05/27/2022 11:51:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=89
05/27/2022 11:51:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=89
05/27/2022 11:51:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=90
05/27/2022 11:51:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=91
05/27/2022 11:51:56 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.3518425070607234 on epoch=91
05/27/2022 11:51:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=92
05/27/2022 11:52:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=93
05/27/2022 11:52:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=94
05/27/2022 11:52:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=94
05/27/2022 11:52:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=95
05/27/2022 11:52:13 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.3482726464296873 on epoch=95
05/27/2022 11:52:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=96
05/27/2022 11:52:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=97
05/27/2022 11:52:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=98
05/27/2022 11:52:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=99
05/27/2022 11:52:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=99
05/27/2022 11:52:31 - INFO - __main__ - Global step 1200 Train loss 0.31 Classification-F1 0.3399997017284835 on epoch=99
05/27/2022 11:52:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=100
05/27/2022 11:52:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=101
05/27/2022 11:52:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=102
05/27/2022 11:52:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=103
05/27/2022 11:52:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=104
05/27/2022 11:52:49 - INFO - __main__ - Global step 1250 Train loss 0.27 Classification-F1 0.3644436802164294 on epoch=104
05/27/2022 11:52:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3631782945736434 -> 0.3644436802164294 on epoch=104, global_step=1250
05/27/2022 11:52:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=104
05/27/2022 11:52:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=105
05/27/2022 11:52:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=106
05/27/2022 11:53:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=107
05/27/2022 11:53:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=108
05/27/2022 11:53:07 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.34567901234567905 on epoch=108
05/27/2022 11:53:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=109
05/27/2022 11:53:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=109
05/27/2022 11:53:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=110
05/27/2022 11:53:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=111
05/27/2022 11:53:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=112
05/27/2022 11:53:25 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.38700200479606206 on epoch=112
05/27/2022 11:53:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3644436802164294 -> 0.38700200479606206 on epoch=112, global_step=1350
05/27/2022 11:53:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=113
05/27/2022 11:53:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=114
05/27/2022 11:53:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=114
05/27/2022 11:53:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=115
05/27/2022 11:53:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=116
05/27/2022 11:53:43 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.377747978166936 on epoch=116
05/27/2022 11:53:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=117
05/27/2022 11:53:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=118
05/27/2022 11:53:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=119
05/27/2022 11:53:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=119
05/27/2022 11:53:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=120
05/27/2022 11:54:01 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.3816130225675176 on epoch=120
05/27/2022 11:54:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=121
05/27/2022 11:54:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=122
05/27/2022 11:54:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=123
05/27/2022 11:54:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=124
05/27/2022 11:54:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=124
05/27/2022 11:54:18 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.370365353284914 on epoch=124
05/27/2022 11:54:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=125
05/27/2022 11:54:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=126
05/27/2022 11:54:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=127
05/27/2022 11:54:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=128
05/27/2022 11:54:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=129
05/27/2022 11:54:36 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.3651610357073865 on epoch=129
05/27/2022 11:54:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=129
05/27/2022 11:54:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.27 on epoch=130
05/27/2022 11:54:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=131
05/27/2022 11:54:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=132
05/27/2022 11:54:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=133
05/27/2022 11:54:54 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.370365353284914 on epoch=133
05/27/2022 11:54:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=134
05/27/2022 11:54:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=134
05/27/2022 11:55:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=135
05/27/2022 11:55:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=136
05/27/2022 11:55:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=137
05/27/2022 11:55:12 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.36496005437075657 on epoch=137
05/27/2022 11:55:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=138
05/27/2022 11:55:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=139
05/27/2022 11:55:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=139
05/27/2022 11:55:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=140
05/27/2022 11:55:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=141
05/27/2022 11:55:30 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.3879176379176379 on epoch=141
05/27/2022 11:55:30 - INFO - __main__ - Saving model with best Classification-F1: 0.38700200479606206 -> 0.3879176379176379 on epoch=141, global_step=1700
05/27/2022 11:55:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=142
05/27/2022 11:55:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=143
05/27/2022 11:55:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=144
05/27/2022 11:55:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=144
05/27/2022 11:55:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=145
05/27/2022 11:55:48 - INFO - __main__ - Global step 1750 Train loss 0.22 Classification-F1 0.38725423818816623 on epoch=145
05/27/2022 11:55:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=146
05/27/2022 11:55:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=147
05/27/2022 11:55:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=148
05/27/2022 11:55:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=149
05/27/2022 11:56:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=149
05/27/2022 11:56:06 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.39387106642932784 on epoch=149
05/27/2022 11:56:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3879176379176379 -> 0.39387106642932784 on epoch=149, global_step=1800
05/27/2022 11:56:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=150
05/27/2022 11:56:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=151
05/27/2022 11:56:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=152
05/27/2022 11:56:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=153
05/27/2022 11:56:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=154
05/27/2022 11:56:24 - INFO - __main__ - Global step 1850 Train loss 0.18 Classification-F1 0.2749098370849397 on epoch=154
05/27/2022 11:56:27 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=154
05/27/2022 11:56:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=155
05/27/2022 11:56:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=156
05/27/2022 11:56:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=157
05/27/2022 11:56:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=158
05/27/2022 11:56:42 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.27926812585499317 on epoch=158
05/27/2022 11:56:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=159
05/27/2022 11:56:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=159
05/27/2022 11:56:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=160
05/27/2022 11:56:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=161
05/27/2022 11:56:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=162
05/27/2022 11:57:00 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.375887599216723 on epoch=162
05/27/2022 11:57:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=163
05/27/2022 11:57:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=164
05/27/2022 11:57:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=164
05/27/2022 11:57:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=165
05/27/2022 11:57:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=166
05/27/2022 11:57:18 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.39328390332232993 on epoch=166
05/27/2022 11:57:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=167
05/27/2022 11:57:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=168
05/27/2022 11:57:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=169
05/27/2022 11:57:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=169
05/27/2022 11:57:31 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=170
05/27/2022 11:57:36 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.3932834498427586 on epoch=170
05/27/2022 11:57:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=171
05/27/2022 11:57:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=172
05/27/2022 11:57:44 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=173
05/27/2022 11:57:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=174
05/27/2022 11:57:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=174
05/27/2022 11:57:54 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.3892841719419838 on epoch=174
05/27/2022 11:57:57 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=175
05/27/2022 11:57:59 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=176
05/27/2022 11:58:02 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=177
05/27/2022 11:58:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=178
05/27/2022 11:58:07 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=179
05/27/2022 11:58:12 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.2604875856645321 on epoch=179
05/27/2022 11:58:15 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=179
05/27/2022 11:58:18 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=180
05/27/2022 11:58:20 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=181
05/27/2022 11:58:23 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=182
05/27/2022 11:58:26 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=183
05/27/2022 11:58:31 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.2742754921873963 on epoch=183
05/27/2022 11:58:33 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=184
05/27/2022 11:58:36 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=184
05/27/2022 11:58:39 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=185
05/27/2022 11:58:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=186
05/27/2022 11:58:44 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=187
05/27/2022 11:58:49 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.3721655427047748 on epoch=187
05/27/2022 11:58:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.23 on epoch=188
05/27/2022 11:58:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=189
05/27/2022 11:58:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=189
05/27/2022 11:59:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=190
05/27/2022 11:59:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=191
05/27/2022 11:59:07 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.39605227797019754 on epoch=191
05/27/2022 11:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.39387106642932784 -> 0.39605227797019754 on epoch=191, global_step=2300
05/27/2022 11:59:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=192
05/27/2022 11:59:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=193
05/27/2022 11:59:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=194
05/27/2022 11:59:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=194
05/27/2022 11:59:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.16 on epoch=195
05/27/2022 11:59:25 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.3693052220788216 on epoch=195
05/27/2022 11:59:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=196
05/27/2022 11:59:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=197
05/27/2022 11:59:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=198
05/27/2022 11:59:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=199
05/27/2022 11:59:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=199
05/27/2022 11:59:44 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.27520319848243335 on epoch=199
05/27/2022 11:59:46 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=200
05/27/2022 11:59:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=201
05/27/2022 11:59:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=202
05/27/2022 11:59:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=203
05/27/2022 11:59:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.13 on epoch=204
05/27/2022 12:00:02 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.346694641159216 on epoch=204
05/27/2022 12:00:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=204
05/27/2022 12:00:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=205
05/27/2022 12:00:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=206
05/27/2022 12:00:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=207
05/27/2022 12:00:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=208
05/27/2022 12:00:20 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.39999517458479206 on epoch=208
05/27/2022 12:00:20 - INFO - __main__ - Saving model with best Classification-F1: 0.39605227797019754 -> 0.39999517458479206 on epoch=208, global_step=2500
05/27/2022 12:00:23 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=209
05/27/2022 12:00:25 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=209
05/27/2022 12:00:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=210
05/27/2022 12:00:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=211
05/27/2022 12:00:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.15 on epoch=212
05/27/2022 12:00:38 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.32685493086924744 on epoch=212
05/27/2022 12:00:41 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=213
05/27/2022 12:00:44 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=214
05/27/2022 12:00:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=214
05/27/2022 12:00:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=215
05/27/2022 12:00:51 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=216
05/27/2022 12:00:56 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.2852206273258905 on epoch=216
05/27/2022 12:00:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=217
05/27/2022 12:01:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=218
05/27/2022 12:01:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=219
05/27/2022 12:01:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=219
05/27/2022 12:01:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=220
05/27/2022 12:01:14 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.2845224754908995 on epoch=220
05/27/2022 12:01:17 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=221
05/27/2022 12:01:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=222
05/27/2022 12:01:22 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=223
05/27/2022 12:01:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.12 on epoch=224
05/27/2022 12:01:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=224
05/27/2022 12:01:32 - INFO - __main__ - Global step 2700 Train loss 0.12 Classification-F1 0.3761509791532149 on epoch=224
05/27/2022 12:01:35 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=225
05/27/2022 12:01:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=226
05/27/2022 12:01:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=227
05/27/2022 12:01:43 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=228
05/27/2022 12:01:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=229
05/27/2022 12:01:50 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.2970196177062374 on epoch=229
05/27/2022 12:01:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=229
05/27/2022 12:01:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=230
05/27/2022 12:01:58 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=231
05/27/2022 12:02:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=232
05/27/2022 12:02:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=233
05/27/2022 12:02:09 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.285032902730392 on epoch=233
05/27/2022 12:02:11 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=234
05/27/2022 12:02:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=234
05/27/2022 12:02:17 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=235
05/27/2022 12:02:19 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=236
05/27/2022 12:02:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=237
05/27/2022 12:02:27 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.26938520273828326 on epoch=237
05/27/2022 12:02:29 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=238
05/27/2022 12:02:32 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=239
05/27/2022 12:02:35 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=239
05/27/2022 12:02:37 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=240
05/27/2022 12:02:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=241
05/27/2022 12:02:45 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.3883444261260711 on epoch=241
05/27/2022 12:02:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.14 on epoch=242
05/27/2022 12:02:50 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=243
05/27/2022 12:02:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=244
05/27/2022 12:02:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=244
05/27/2022 12:02:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=245
05/27/2022 12:03:03 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.20275247140918784 on epoch=245
05/27/2022 12:03:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=246
05/27/2022 12:03:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=247
05/27/2022 12:03:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=248
05/27/2022 12:03:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=249
05/27/2022 12:03:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=249
05/27/2022 12:03:18 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:03:18 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:18 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:18 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:03:18 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:03:18 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 12:03:18 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:18 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:18 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:19 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:03:21 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.2469633005815619 on epoch=249
05/27/2022 12:03:21 - INFO - __main__ - save last model!
05/27/2022 12:03:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 12:03:22 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 12:03:22 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:22 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 12:03:22 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:22 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 12:03:22 - INFO - __main__ - ['entailment']
05/27/2022 12:03:22 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 12:03:22 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:22 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:22 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:23 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 12:03:35 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:03:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:03:36 - INFO - __main__ - Starting training!
05/27/2022 12:03:50 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_13_0.4_8_predictions.txt
05/27/2022 12:03:50 - INFO - __main__ - Classification-F1 on test data: 0.1111
05/27/2022 12:03:51 - INFO - __main__ - prefix=anli_64_13, lr=0.4, bsz=8, dev_performance=0.39999517458479206, test_performance=0.1110559478954038
05/27/2022 12:03:51 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.3, bsz=8 ...
05/27/2022 12:03:52 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:03:52 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:52 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:52 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:03:52 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:03:52 - INFO - __main__ - Printing 3 examples
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 12:03:52 - INFO - __main__ - ['contradiction']
05/27/2022 12:03:52 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:03:52 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:03:52 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:04:11 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:04:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:04:12 - INFO - __main__ - Starting training!
05/27/2022 12:04:15 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=0
05/27/2022 12:04:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=1
05/27/2022 12:04:21 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=2
05/27/2022 12:04:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
05/27/2022 12:04:26 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=4
05/27/2022 12:04:32 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 12:04:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 12:04:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=4
05/27/2022 12:04:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.45 on epoch=5
05/27/2022 12:04:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=6
05/27/2022 12:04:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=7
05/27/2022 12:04:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=8
05/27/2022 12:04:50 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 12:04:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
05/27/2022 12:04:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=9
05/27/2022 12:04:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
05/27/2022 12:05:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
05/27/2022 12:05:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=12
05/27/2022 12:05:08 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.21833930704898444 on epoch=12
05/27/2022 12:05:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21833930704898444 on epoch=12, global_step=150
05/27/2022 12:05:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.48 on epoch=13
05/27/2022 12:05:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=14
05/27/2022 12:05:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
05/27/2022 12:05:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=15
05/27/2022 12:05:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
05/27/2022 12:05:27 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 12:05:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=17
05/27/2022 12:05:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=18
05/27/2022 12:05:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=19
05/27/2022 12:05:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=19
05/27/2022 12:05:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=20
05/27/2022 12:05:46 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 12:05:49 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=21
05/27/2022 12:05:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
05/27/2022 12:05:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
05/27/2022 12:05:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=24
05/27/2022 12:05:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=24
05/27/2022 12:06:05 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 12:06:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
05/27/2022 12:06:10 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/27/2022 12:06:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=27
05/27/2022 12:06:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/27/2022 12:06:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
05/27/2022 12:06:24 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 12:06:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
05/27/2022 12:06:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=30
05/27/2022 12:06:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
05/27/2022 12:06:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
05/27/2022 12:06:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
05/27/2022 12:06:42 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 12:06:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
05/27/2022 12:06:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=34
05/27/2022 12:06:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=35
05/27/2022 12:06:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=36
05/27/2022 12:06:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=37
05/27/2022 12:07:01 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16732026143790854 on epoch=37
05/27/2022 12:07:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=38
05/27/2022 12:07:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=39
05/27/2022 12:07:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=39
05/27/2022 12:07:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
05/27/2022 12:07:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=41
05/27/2022 12:07:20 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.18892001244942422 on epoch=41
05/27/2022 12:07:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=42
05/27/2022 12:07:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
05/27/2022 12:07:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=44
05/27/2022 12:07:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=44
05/27/2022 12:07:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
05/27/2022 12:07:38 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.2087619047619048 on epoch=45
05/27/2022 12:07:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/27/2022 12:07:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=47
05/27/2022 12:07:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=48
05/27/2022 12:07:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
05/27/2022 12:07:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=49
05/27/2022 12:07:57 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.2087619047619048 on epoch=49
05/27/2022 12:08:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=50
05/27/2022 12:08:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
05/27/2022 12:08:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=52
05/27/2022 12:08:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=53
05/27/2022 12:08:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=54
05/27/2022 12:08:16 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.21830043177404457 on epoch=54
05/27/2022 12:08:18 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
05/27/2022 12:08:21 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=55
05/27/2022 12:08:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
05/27/2022 12:08:26 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=57
05/27/2022 12:08:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
05/27/2022 12:08:35 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.2366664817998632 on epoch=58
05/27/2022 12:08:35 - INFO - __main__ - Saving model with best Classification-F1: 0.21833930704898444 -> 0.2366664817998632 on epoch=58, global_step=700
05/27/2022 12:08:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=59
05/27/2022 12:08:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
05/27/2022 12:08:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=60
05/27/2022 12:08:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=61
05/27/2022 12:08:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=62
05/27/2022 12:08:53 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.24802712053961484 on epoch=62
05/27/2022 12:08:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2366664817998632 -> 0.24802712053961484 on epoch=62, global_step=750
05/27/2022 12:08:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
05/27/2022 12:08:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=64
05/27/2022 12:09:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=64
05/27/2022 12:09:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=65
05/27/2022 12:09:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
05/27/2022 12:09:11 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.2423065861374748 on epoch=66
05/27/2022 12:09:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=67
05/27/2022 12:09:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=68
05/27/2022 12:09:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=69
05/27/2022 12:09:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=69
05/27/2022 12:09:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=70
05/27/2022 12:09:29 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.2507615840949174 on epoch=70
05/27/2022 12:09:29 - INFO - __main__ - Saving model with best Classification-F1: 0.24802712053961484 -> 0.2507615840949174 on epoch=70, global_step=850
05/27/2022 12:09:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
05/27/2022 12:09:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=72
05/27/2022 12:09:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=73
05/27/2022 12:09:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=74
05/27/2022 12:09:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=74
05/27/2022 12:09:47 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.2477219749947023 on epoch=74
05/27/2022 12:09:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=75
05/27/2022 12:09:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=76
05/27/2022 12:09:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/27/2022 12:09:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=78
05/27/2022 12:10:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.40 on epoch=79
05/27/2022 12:10:05 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.28625235404896415 on epoch=79
05/27/2022 12:10:05 - INFO - __main__ - Saving model with best Classification-F1: 0.2507615840949174 -> 0.28625235404896415 on epoch=79, global_step=950
05/27/2022 12:10:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=79
05/27/2022 12:10:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=80
05/27/2022 12:10:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=81
05/27/2022 12:10:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=82
05/27/2022 12:10:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=83
05/27/2022 12:10:23 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.2675708130253585 on epoch=83
05/27/2022 12:10:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=84
05/27/2022 12:10:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=84
05/27/2022 12:10:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=85
05/27/2022 12:10:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=86
05/27/2022 12:10:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=87
05/27/2022 12:10:41 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.30694879832810873 on epoch=87
05/27/2022 12:10:41 - INFO - __main__ - Saving model with best Classification-F1: 0.28625235404896415 -> 0.30694879832810873 on epoch=87, global_step=1050
05/27/2022 12:10:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=88
05/27/2022 12:10:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=89
05/27/2022 12:10:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=89
05/27/2022 12:10:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=90
05/27/2022 12:10:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=91
05/27/2022 12:10:59 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.31376363960633624 on epoch=91
05/27/2022 12:10:59 - INFO - __main__ - Saving model with best Classification-F1: 0.30694879832810873 -> 0.31376363960633624 on epoch=91, global_step=1100
05/27/2022 12:11:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=92
05/27/2022 12:11:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=93
05/27/2022 12:11:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.37 on epoch=94
05/27/2022 12:11:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=94
05/27/2022 12:11:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=95
05/27/2022 12:11:16 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.2638888888888889 on epoch=95
05/27/2022 12:11:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=96
05/27/2022 12:11:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.32 on epoch=97
05/27/2022 12:11:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=98
05/27/2022 12:11:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=99
05/27/2022 12:11:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=99
05/27/2022 12:11:33 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.3270150519058816 on epoch=99
05/27/2022 12:11:34 - INFO - __main__ - Saving model with best Classification-F1: 0.31376363960633624 -> 0.3270150519058816 on epoch=99, global_step=1200
05/27/2022 12:11:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=100
05/27/2022 12:11:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=101
05/27/2022 12:11:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=102
05/27/2022 12:11:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=103
05/27/2022 12:11:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=104
05/27/2022 12:11:51 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.29291724640561845 on epoch=104
05/27/2022 12:11:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=104
05/27/2022 12:11:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=105
05/27/2022 12:11:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=106
05/27/2022 12:12:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=107
05/27/2022 12:12:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=108
05/27/2022 12:12:09 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.2964994775339603 on epoch=108
05/27/2022 12:12:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=109
05/27/2022 12:12:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=109
05/27/2022 12:12:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=110
05/27/2022 12:12:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=111
05/27/2022 12:12:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=112
05/27/2022 12:12:26 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.3288771731626185 on epoch=112
05/27/2022 12:12:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3270150519058816 -> 0.3288771731626185 on epoch=112, global_step=1350
05/27/2022 12:12:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=113
05/27/2022 12:12:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=114
05/27/2022 12:12:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=114
05/27/2022 12:12:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=115
05/27/2022 12:12:39 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=116
05/27/2022 12:12:44 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.34567901234567905 on epoch=116
05/27/2022 12:12:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3288771731626185 -> 0.34567901234567905 on epoch=116, global_step=1400
05/27/2022 12:12:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=117
05/27/2022 12:12:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=118
05/27/2022 12:12:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=119
05/27/2022 12:12:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=119
05/27/2022 12:12:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=120
05/27/2022 12:13:02 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.2964994775339603 on epoch=120
05/27/2022 12:13:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=121
05/27/2022 12:13:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=122
05/27/2022 12:13:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=123
05/27/2022 12:13:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=124
05/27/2022 12:13:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.32 on epoch=124
05/27/2022 12:13:20 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.3087930709847487 on epoch=124
05/27/2022 12:13:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=125
05/27/2022 12:13:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=126
05/27/2022 12:13:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=127
05/27/2022 12:13:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=128
05/27/2022 12:13:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=129
05/27/2022 12:13:38 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.3198229981208705 on epoch=129
05/27/2022 12:13:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=129
05/27/2022 12:13:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=130
05/27/2022 12:13:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=131
05/27/2022 12:13:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.28 on epoch=132
05/27/2022 12:13:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=133
05/27/2022 12:13:56 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.3029494024917366 on epoch=133
05/27/2022 12:13:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=134
05/27/2022 12:14:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=134
05/27/2022 12:14:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=135
05/27/2022 12:14:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.32 on epoch=136
05/27/2022 12:14:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=137
05/27/2022 12:14:14 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.32316486918733417 on epoch=137
05/27/2022 12:14:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=138
05/27/2022 12:14:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=139
05/27/2022 12:14:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=139
05/27/2022 12:14:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=140
05/27/2022 12:14:27 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=141
05/27/2022 12:14:31 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.32242424242424245 on epoch=141
05/27/2022 12:14:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=142
05/27/2022 12:14:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=143
05/27/2022 12:14:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=144
05/27/2022 12:14:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=144
05/27/2022 12:14:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.27 on epoch=145
05/27/2022 12:14:49 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.3079177308252198 on epoch=145
05/27/2022 12:14:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=146
05/27/2022 12:14:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=147
05/27/2022 12:14:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=148
05/27/2022 12:15:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=149
05/27/2022 12:15:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=149
05/27/2022 12:15:07 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.32242424242424245 on epoch=149
05/27/2022 12:15:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.29 on epoch=150
05/27/2022 12:15:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=151
05/27/2022 12:15:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=152
05/27/2022 12:15:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=153
05/27/2022 12:15:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.29 on epoch=154
05/27/2022 12:15:25 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.3073131546815757 on epoch=154
05/27/2022 12:15:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=154
05/27/2022 12:15:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=155
05/27/2022 12:15:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=156
05/27/2022 12:15:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=157
05/27/2022 12:15:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=158
05/27/2022 12:15:43 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.35403297682709445 on epoch=158
05/27/2022 12:15:43 - INFO - __main__ - Saving model with best Classification-F1: 0.34567901234567905 -> 0.35403297682709445 on epoch=158, global_step=1900
05/27/2022 12:15:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=159
05/27/2022 12:15:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.27 on epoch=159
05/27/2022 12:15:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=160
05/27/2022 12:15:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=161
05/27/2022 12:15:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=162
05/27/2022 12:16:01 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.3392093885452519 on epoch=162
05/27/2022 12:16:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=163
05/27/2022 12:16:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=164
05/27/2022 12:16:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=164
05/27/2022 12:16:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=165
05/27/2022 12:16:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=166
05/27/2022 12:16:19 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.3564812952848763 on epoch=166
05/27/2022 12:16:19 - INFO - __main__ - Saving model with best Classification-F1: 0.35403297682709445 -> 0.3564812952848763 on epoch=166, global_step=2000
05/27/2022 12:16:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.24 on epoch=167
05/27/2022 12:16:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=168
05/27/2022 12:16:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=169
05/27/2022 12:16:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=169
05/27/2022 12:16:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.28 on epoch=170
05/27/2022 12:16:37 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.33719243719243713 on epoch=170
05/27/2022 12:16:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=171
05/27/2022 12:16:43 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=172
05/27/2022 12:16:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=173
05/27/2022 12:16:48 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=174
05/27/2022 12:16:51 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=174
05/27/2022 12:16:56 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.363015913902721 on epoch=174
05/27/2022 12:16:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3564812952848763 -> 0.363015913902721 on epoch=174, global_step=2100
05/27/2022 12:16:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=175
05/27/2022 12:17:01 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.26 on epoch=176
05/27/2022 12:17:04 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=177
05/27/2022 12:17:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=178
05/27/2022 12:17:09 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.29 on epoch=179
05/27/2022 12:17:14 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.33078291217826106 on epoch=179
05/27/2022 12:17:17 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=179
05/27/2022 12:17:19 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.26 on epoch=180
05/27/2022 12:17:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=181
05/27/2022 12:17:24 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=182
05/27/2022 12:17:27 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=183
05/27/2022 12:17:32 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.37397056891688996 on epoch=183
05/27/2022 12:17:32 - INFO - __main__ - Saving model with best Classification-F1: 0.363015913902721 -> 0.37397056891688996 on epoch=183, global_step=2200
05/27/2022 12:17:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=184
05/27/2022 12:17:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=184
05/27/2022 12:17:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=185
05/27/2022 12:17:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=186
05/27/2022 12:17:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=187
05/27/2022 12:17:51 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.3570025204881069 on epoch=187
05/27/2022 12:17:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.25 on epoch=188
05/27/2022 12:17:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=189
05/27/2022 12:17:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=189
05/27/2022 12:18:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=190
05/27/2022 12:18:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=191
05/27/2022 12:18:10 - INFO - __main__ - Global step 2300 Train loss 0.24 Classification-F1 0.36975614369980564 on epoch=191
05/27/2022 12:18:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=192
05/27/2022 12:18:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=193
05/27/2022 12:18:18 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=194
05/27/2022 12:18:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=194
05/27/2022 12:18:23 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=195
05/27/2022 12:18:28 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.3584248713371947 on epoch=195
05/27/2022 12:18:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=196
05/27/2022 12:18:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=197
05/27/2022 12:18:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=198
05/27/2022 12:18:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.27 on epoch=199
05/27/2022 12:18:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=199
05/27/2022 12:18:47 - INFO - __main__ - Global step 2400 Train loss 0.22 Classification-F1 0.3748792270531401 on epoch=199
05/27/2022 12:18:47 - INFO - __main__ - Saving model with best Classification-F1: 0.37397056891688996 -> 0.3748792270531401 on epoch=199, global_step=2400
05/27/2022 12:18:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=200
05/27/2022 12:18:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=201
05/27/2022 12:18:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=202
05/27/2022 12:18:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=203
05/27/2022 12:19:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=204
05/27/2022 12:19:05 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.34080437401538316 on epoch=204
05/27/2022 12:19:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=204
05/27/2022 12:19:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.23 on epoch=205
05/27/2022 12:19:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.24 on epoch=206
05/27/2022 12:19:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=207
05/27/2022 12:19:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.20 on epoch=208
05/27/2022 12:19:23 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.36448683738898396 on epoch=208
05/27/2022 12:19:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=209
05/27/2022 12:19:28 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.22 on epoch=209
05/27/2022 12:19:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=210
05/27/2022 12:19:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=211
05/27/2022 12:19:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=212
05/27/2022 12:19:42 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.3918442277597207 on epoch=212
05/27/2022 12:19:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3748792270531401 -> 0.3918442277597207 on epoch=212, global_step=2550
05/27/2022 12:19:44 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.21 on epoch=213
05/27/2022 12:19:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=214
05/27/2022 12:19:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=214
05/27/2022 12:19:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=215
05/27/2022 12:19:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=216
05/27/2022 12:20:00 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.37083604373819035 on epoch=216
05/27/2022 12:20:03 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=217
05/27/2022 12:20:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=218
05/27/2022 12:20:08 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=219
05/27/2022 12:20:11 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=219
05/27/2022 12:20:13 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.22 on epoch=220
05/27/2022 12:20:18 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.34080437401538316 on epoch=220
05/27/2022 12:20:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=221
05/27/2022 12:20:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=222
05/27/2022 12:20:26 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=223
05/27/2022 12:20:29 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=224
05/27/2022 12:20:32 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=224
05/27/2022 12:20:36 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.3701179636409149 on epoch=224
05/27/2022 12:20:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.20 on epoch=225
05/27/2022 12:20:42 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.20 on epoch=226
05/27/2022 12:20:44 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.20 on epoch=227
05/27/2022 12:20:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=228
05/27/2022 12:20:50 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=229
05/27/2022 12:20:55 - INFO - __main__ - Global step 2750 Train loss 0.19 Classification-F1 0.3428401303224357 on epoch=229
05/27/2022 12:20:57 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=229
05/27/2022 12:21:00 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=230
05/27/2022 12:21:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=231
05/27/2022 12:21:05 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=232
05/27/2022 12:21:08 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=233
05/27/2022 12:21:13 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.32380143025445923 on epoch=233
05/27/2022 12:21:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=234
05/27/2022 12:21:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=234
05/27/2022 12:21:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=235
05/27/2022 12:21:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=236
05/27/2022 12:21:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=237
05/27/2022 12:21:32 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.2860755837729522 on epoch=237
05/27/2022 12:21:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=238
05/27/2022 12:21:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=239
05/27/2022 12:21:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=239
05/27/2022 12:21:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=240
05/27/2022 12:21:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.25 on epoch=241
05/27/2022 12:21:50 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.379660233997308 on epoch=241
05/27/2022 12:21:52 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=242
05/27/2022 12:21:55 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.21 on epoch=243
05/27/2022 12:21:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=244
05/27/2022 12:22:00 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.18 on epoch=244
05/27/2022 12:22:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=245
05/27/2022 12:22:08 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.38991137574963836 on epoch=245
05/27/2022 12:22:11 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=246
05/27/2022 12:22:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=247
05/27/2022 12:22:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=248
05/27/2022 12:22:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=249
05/27/2022 12:22:21 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=249
05/27/2022 12:22:23 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:22:23 - INFO - __main__ - Printing 3 examples
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:22:23 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:22:23 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:22:23 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:22:23 - INFO - __main__ - Printing 3 examples
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 12:22:23 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:23 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:22:23 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:22:23 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:22:26 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.390690464219876 on epoch=249
05/27/2022 12:22:26 - INFO - __main__ - save last model!
05/27/2022 12:22:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 12:22:26 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 12:22:26 - INFO - __main__ - Printing 3 examples
05/27/2022 12:22:26 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 12:22:26 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:26 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 12:22:26 - INFO - __main__ - ['entailment']
05/27/2022 12:22:26 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 12:22:26 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:26 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:22:27 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:22:28 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 12:22:39 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:22:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:22:40 - INFO - __main__ - Starting training!
05/27/2022 12:22:53 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_13_0.3_8_predictions.txt
05/27/2022 12:22:53 - INFO - __main__ - Classification-F1 on test data: 0.3192
05/27/2022 12:22:53 - INFO - __main__ - prefix=anli_64_13, lr=0.3, bsz=8, dev_performance=0.3918442277597207, test_performance=0.31916421811332113
05/27/2022 12:22:53 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.2, bsz=8 ...
05/27/2022 12:22:54 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:22:54 - INFO - __main__ - Printing 3 examples
05/27/2022 12:22:54 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
05/27/2022 12:22:54 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:54 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
05/27/2022 12:22:54 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:54 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
05/27/2022 12:22:54 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:54 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:22:54 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:22:55 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:22:55 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:22:55 - INFO - __main__ - Printing 3 examples
05/27/2022 12:22:55 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
05/27/2022 12:22:55 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:55 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
05/27/2022 12:22:55 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:55 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
05/27/2022 12:22:55 - INFO - __main__ - ['contradiction']
05/27/2022 12:22:55 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:22:55 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:22:55 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:23:10 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:23:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:23:11 - INFO - __main__ - Starting training!
05/27/2022 12:23:15 - INFO - __main__ - Step 10 Global step 10 Train loss 0.46 on epoch=0
05/27/2022 12:23:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
05/27/2022 12:23:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=2
05/27/2022 12:23:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=3
05/27/2022 12:23:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=4
05/27/2022 12:23:31 - INFO - __main__ - Global step 50 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 12:23:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 12:23:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=4
05/27/2022 12:23:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=5
05/27/2022 12:23:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=6
05/27/2022 12:23:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=7
05/27/2022 12:23:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.48 on epoch=8
05/27/2022 12:23:49 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 12:23:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=9
05/27/2022 12:23:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=9
05/27/2022 12:23:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.45 on epoch=10
05/27/2022 12:24:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
05/27/2022 12:24:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=12
05/27/2022 12:24:08 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16732026143790854 on epoch=12
05/27/2022 12:24:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16732026143790854 on epoch=12, global_step=150
05/27/2022 12:24:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=13
05/27/2022 12:24:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=14
05/27/2022 12:24:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=14
05/27/2022 12:24:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=15
05/27/2022 12:24:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/27/2022 12:24:25 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 12:24:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=17
05/27/2022 12:24:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=18
05/27/2022 12:24:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=19
05/27/2022 12:24:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=19
05/27/2022 12:24:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
05/27/2022 12:24:43 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 12:24:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=21
05/27/2022 12:24:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/27/2022 12:24:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=23
05/27/2022 12:24:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=24
05/27/2022 12:24:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=24
05/27/2022 12:25:01 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 12:25:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
05/27/2022 12:25:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/27/2022 12:25:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/27/2022 12:25:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
05/27/2022 12:25:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.49 on epoch=29
05/27/2022 12:25:18 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 12:25:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=29
05/27/2022 12:25:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
05/27/2022 12:25:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/27/2022 12:25:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
05/27/2022 12:25:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=33
05/27/2022 12:25:37 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 12:25:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=34
05/27/2022 12:25:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=34
05/27/2022 12:25:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
05/27/2022 12:25:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=36
05/27/2022 12:25:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=37
05/27/2022 12:25:55 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=37
05/27/2022 12:25:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.53 on epoch=38
05/27/2022 12:26:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/27/2022 12:26:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
05/27/2022 12:26:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=40
05/27/2022 12:26:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=41
05/27/2022 12:26:13 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.1679790026246719 on epoch=41
05/27/2022 12:26:14 - INFO - __main__ - Saving model with best Classification-F1: 0.16732026143790854 -> 0.1679790026246719 on epoch=41, global_step=500
05/27/2022 12:26:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=42
05/27/2022 12:26:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/27/2022 12:26:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
05/27/2022 12:26:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=44
05/27/2022 12:26:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=45
05/27/2022 12:26:32 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=45
05/27/2022 12:26:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=46
05/27/2022 12:26:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=47
05/27/2022 12:26:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
05/27/2022 12:26:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=49
05/27/2022 12:26:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
05/27/2022 12:26:50 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=49
05/27/2022 12:26:50 - INFO - __main__ - Saving model with best Classification-F1: 0.1679790026246719 -> 0.1775766716943188 on epoch=49, global_step=600
05/27/2022 12:26:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
05/27/2022 12:26:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/27/2022 12:26:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
05/27/2022 12:27:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=53
05/27/2022 12:27:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=54
05/27/2022 12:27:09 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=54
05/27/2022 12:27:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=54
05/27/2022 12:27:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=55
05/27/2022 12:27:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
05/27/2022 12:27:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=57
05/27/2022 12:27:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
05/27/2022 12:27:27 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=58
05/27/2022 12:27:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
05/27/2022 12:27:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=59
05/27/2022 12:27:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=60
05/27/2022 12:27:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
05/27/2022 12:27:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=62
05/27/2022 12:27:46 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.1989722270338934 on epoch=62
05/27/2022 12:27:46 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.1989722270338934 on epoch=62, global_step=750
05/27/2022 12:27:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=63
05/27/2022 12:27:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=64
05/27/2022 12:27:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
05/27/2022 12:27:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=65
05/27/2022 12:27:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
05/27/2022 12:28:05 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.178080012725682 on epoch=66
05/27/2022 12:28:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=67
05/27/2022 12:28:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=68
05/27/2022 12:28:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=69
05/27/2022 12:28:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=69
05/27/2022 12:28:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
05/27/2022 12:28:23 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=70
05/27/2022 12:28:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=71
05/27/2022 12:28:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=72
05/27/2022 12:28:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
05/27/2022 12:28:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=74
05/27/2022 12:28:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=74
05/27/2022 12:28:41 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.24551380648941626 on epoch=74
05/27/2022 12:28:41 - INFO - __main__ - Saving model with best Classification-F1: 0.1989722270338934 -> 0.24551380648941626 on epoch=74, global_step=900
05/27/2022 12:28:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
05/27/2022 12:28:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=76
05/27/2022 12:28:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=77
05/27/2022 12:28:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=78
05/27/2022 12:28:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=79
05/27/2022 12:28:59 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.21830043177404457 on epoch=79
05/27/2022 12:29:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
05/27/2022 12:29:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=80
05/27/2022 12:29:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=81
05/27/2022 12:29:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=82
05/27/2022 12:29:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=83
05/27/2022 12:29:17 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.2366664817998632 on epoch=83
05/27/2022 12:29:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=84
05/27/2022 12:29:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=84
05/27/2022 12:29:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.42 on epoch=85
05/27/2022 12:29:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
05/27/2022 12:29:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=87
05/27/2022 12:29:35 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.25414965986394555 on epoch=87
05/27/2022 12:29:35 - INFO - __main__ - Saving model with best Classification-F1: 0.24551380648941626 -> 0.25414965986394555 on epoch=87, global_step=1050
05/27/2022 12:29:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=88
05/27/2022 12:29:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
05/27/2022 12:29:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
05/27/2022 12:29:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=90
05/27/2022 12:29:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
05/27/2022 12:29:53 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.26666666666666666 on epoch=91
05/27/2022 12:29:53 - INFO - __main__ - Saving model with best Classification-F1: 0.25414965986394555 -> 0.26666666666666666 on epoch=91, global_step=1100
05/27/2022 12:29:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=92
05/27/2022 12:29:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=93
05/27/2022 12:30:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=94
05/27/2022 12:30:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
05/27/2022 12:30:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
05/27/2022 12:30:12 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.2457989018911874 on epoch=95
05/27/2022 12:30:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
05/27/2022 12:30:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=97
05/27/2022 12:30:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=98
05/27/2022 12:30:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
05/27/2022 12:30:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=99
05/27/2022 12:30:30 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.2744976496719872 on epoch=99
05/27/2022 12:30:30 - INFO - __main__ - Saving model with best Classification-F1: 0.26666666666666666 -> 0.2744976496719872 on epoch=99, global_step=1200
05/27/2022 12:30:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=100
05/27/2022 12:30:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=101
05/27/2022 12:30:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=102
05/27/2022 12:30:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=103
05/27/2022 12:30:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=104
05/27/2022 12:30:48 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.2744976496719872 on epoch=104
05/27/2022 12:30:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=104
05/27/2022 12:30:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
05/27/2022 12:30:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=106
05/27/2022 12:30:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=107
05/27/2022 12:31:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=108
05/27/2022 12:31:06 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.26710086314057113 on epoch=108
05/27/2022 12:31:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=109
05/27/2022 12:31:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=109
05/27/2022 12:31:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=110
05/27/2022 12:31:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=111
05/27/2022 12:31:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=112
05/27/2022 12:31:24 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.32785829307568437 on epoch=112
05/27/2022 12:31:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2744976496719872 -> 0.32785829307568437 on epoch=112, global_step=1350
05/27/2022 12:31:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=113
05/27/2022 12:31:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=114
05/27/2022 12:31:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=114
05/27/2022 12:31:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
05/27/2022 12:31:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=116
05/27/2022 12:31:42 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.29353358364622445 on epoch=116
05/27/2022 12:31:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=117
05/27/2022 12:31:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=118
05/27/2022 12:31:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
05/27/2022 12:31:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=119
05/27/2022 12:31:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=120
05/27/2022 12:32:00 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.2717082493930471 on epoch=120
05/27/2022 12:32:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=121
05/27/2022 12:32:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.39 on epoch=122
05/27/2022 12:32:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=123
05/27/2022 12:32:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=124
05/27/2022 12:32:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
05/27/2022 12:32:17 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3145245559038663 on epoch=124
05/27/2022 12:32:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=125
05/27/2022 12:32:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=126
05/27/2022 12:32:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.36 on epoch=127
05/27/2022 12:32:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.40 on epoch=128
05/27/2022 12:32:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=129
05/27/2022 12:32:34 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3006691843901146 on epoch=129
05/27/2022 12:32:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=129
05/27/2022 12:32:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=130
05/27/2022 12:32:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=131
05/27/2022 12:32:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=132
05/27/2022 12:32:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=133
05/27/2022 12:32:52 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.2717082493930471 on epoch=133
05/27/2022 12:32:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=134
05/27/2022 12:32:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=134
05/27/2022 12:33:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=135
05/27/2022 12:33:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=136
05/27/2022 12:33:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.35 on epoch=137
05/27/2022 12:33:10 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.3533049157450307 on epoch=137
05/27/2022 12:33:10 - INFO - __main__ - Saving model with best Classification-F1: 0.32785829307568437 -> 0.3533049157450307 on epoch=137, global_step=1650
05/27/2022 12:33:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=138
05/27/2022 12:33:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=139
05/27/2022 12:33:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=139
05/27/2022 12:33:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=140
05/27/2022 12:33:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.33 on epoch=141
05/27/2022 12:33:28 - INFO - __main__ - Global step 1700 Train loss 0.32 Classification-F1 0.33685440449686577 on epoch=141
05/27/2022 12:33:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.33 on epoch=142
05/27/2022 12:33:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=143
05/27/2022 12:33:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=144
05/27/2022 12:33:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=144
05/27/2022 12:33:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=145
05/27/2022 12:33:45 - INFO - __main__ - Global step 1750 Train loss 0.33 Classification-F1 0.32785829307568437 on epoch=145
05/27/2022 12:33:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=146
05/27/2022 12:33:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=147
05/27/2022 12:33:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=148
05/27/2022 12:33:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=149
05/27/2022 12:33:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=149
05/27/2022 12:34:02 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3241038437544988 on epoch=149
05/27/2022 12:34:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=150
05/27/2022 12:34:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=151
05/27/2022 12:34:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=152
05/27/2022 12:34:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=153
05/27/2022 12:34:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=154
05/27/2022 12:34:20 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.310955755602375 on epoch=154
05/27/2022 12:34:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
05/27/2022 12:34:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.31 on epoch=155
05/27/2022 12:34:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=156
05/27/2022 12:34:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=157
05/27/2022 12:34:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=158
05/27/2022 12:34:38 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.30434782608695654 on epoch=158
05/27/2022 12:34:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=159
05/27/2022 12:34:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=159
05/27/2022 12:34:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=160
05/27/2022 12:34:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=161
05/27/2022 12:34:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=162
05/27/2022 12:34:55 - INFO - __main__ - Global step 1950 Train loss 0.30 Classification-F1 0.3116509926854754 on epoch=162
05/27/2022 12:34:58 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=163
05/27/2022 12:35:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=164
05/27/2022 12:35:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=164
05/27/2022 12:35:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=165
05/27/2022 12:35:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=166
05/27/2022 12:35:13 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.3166437718093253 on epoch=166
05/27/2022 12:35:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=167
05/27/2022 12:35:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=168
05/27/2022 12:35:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=169
05/27/2022 12:35:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=169
05/27/2022 12:35:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.25 on epoch=170
05/27/2022 12:35:30 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.31588526956630897 on epoch=170
05/27/2022 12:35:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.30 on epoch=171
05/27/2022 12:35:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.25 on epoch=172
05/27/2022 12:35:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=173
05/27/2022 12:35:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.29 on epoch=174
05/27/2022 12:35:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=174
05/27/2022 12:35:48 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.3131197559115179 on epoch=174
05/27/2022 12:35:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=175
05/27/2022 12:35:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.28 on epoch=176
05/27/2022 12:35:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.32 on epoch=177
05/27/2022 12:35:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.30 on epoch=178
05/27/2022 12:36:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.31 on epoch=179
05/27/2022 12:36:05 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.3138666282771086 on epoch=179
05/27/2022 12:36:08 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.30 on epoch=179
05/27/2022 12:36:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=180
05/27/2022 12:36:13 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=181
05/27/2022 12:36:16 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.33 on epoch=182
05/27/2022 12:36:18 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=183
05/27/2022 12:36:23 - INFO - __main__ - Global step 2200 Train loss 0.30 Classification-F1 0.3094753130279633 on epoch=183
05/27/2022 12:36:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=184
05/27/2022 12:36:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.32 on epoch=184
05/27/2022 12:36:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=185
05/27/2022 12:36:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=186
05/27/2022 12:36:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=187
05/27/2022 12:36:40 - INFO - __main__ - Global step 2250 Train loss 0.28 Classification-F1 0.31637643402349286 on epoch=187
05/27/2022 12:36:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=188
05/27/2022 12:36:45 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=189
05/27/2022 12:36:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=189
05/27/2022 12:36:50 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.28 on epoch=190
05/27/2022 12:36:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.29 on epoch=191
05/27/2022 12:36:57 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.31637643402349286 on epoch=191
05/27/2022 12:37:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=192
05/27/2022 12:37:03 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.31 on epoch=193
05/27/2022 12:37:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=194
05/27/2022 12:37:08 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=194
05/27/2022 12:37:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=195
05/27/2022 12:37:15 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.30456349206349204 on epoch=195
05/27/2022 12:37:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=196
05/27/2022 12:37:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=197
05/27/2022 12:37:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.28 on epoch=198
05/27/2022 12:37:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=199
05/27/2022 12:37:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.28 on epoch=199
05/27/2022 12:37:32 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.32242424242424245 on epoch=199
05/27/2022 12:37:35 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.31 on epoch=200
05/27/2022 12:37:38 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=201
05/27/2022 12:37:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=202
05/27/2022 12:37:43 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.22 on epoch=203
05/27/2022 12:37:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.28 on epoch=204
05/27/2022 12:37:50 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.3138666282771086 on epoch=204
05/27/2022 12:37:52 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.29 on epoch=204
05/27/2022 12:37:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=205
05/27/2022 12:37:58 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.25 on epoch=206
05/27/2022 12:38:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=207
05/27/2022 12:38:03 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=208
05/27/2022 12:38:07 - INFO - __main__ - Global step 2500 Train loss 0.26 Classification-F1 0.32071255759480993 on epoch=208
05/27/2022 12:38:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.29 on epoch=209
05/27/2022 12:38:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=209
05/27/2022 12:38:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.28 on epoch=210
05/27/2022 12:38:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=211
05/27/2022 12:38:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.29 on epoch=212
05/27/2022 12:38:25 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.33802010338020105 on epoch=212
05/27/2022 12:38:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.25 on epoch=213
05/27/2022 12:38:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=214
05/27/2022 12:38:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=214
05/27/2022 12:38:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.27 on epoch=215
05/27/2022 12:38:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.25 on epoch=216
05/27/2022 12:38:43 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.3321212121212121 on epoch=216
05/27/2022 12:38:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.28 on epoch=217
05/27/2022 12:38:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=218
05/27/2022 12:38:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=219
05/27/2022 12:38:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=219
05/27/2022 12:38:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=220
05/27/2022 12:39:01 - INFO - __main__ - Global step 2650 Train loss 0.23 Classification-F1 0.3081481481481481 on epoch=220
05/27/2022 12:39:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.25 on epoch=221
05/27/2022 12:39:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.29 on epoch=222
05/27/2022 12:39:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=223
05/27/2022 12:39:11 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.22 on epoch=224
05/27/2022 12:39:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.22 on epoch=224
05/27/2022 12:39:18 - INFO - __main__ - Global step 2700 Train loss 0.24 Classification-F1 0.32545454545454544 on epoch=224
05/27/2022 12:39:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.27 on epoch=225
05/27/2022 12:39:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.25 on epoch=226
05/27/2022 12:39:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.26 on epoch=227
05/27/2022 12:39:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=228
05/27/2022 12:39:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=229
05/27/2022 12:39:36 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.32545454545454544 on epoch=229
05/27/2022 12:39:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.22 on epoch=229
05/27/2022 12:39:41 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=230
05/27/2022 12:39:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.31 on epoch=231
05/27/2022 12:39:46 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.27 on epoch=232
05/27/2022 12:39:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=233
05/27/2022 12:39:54 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.326127032009385 on epoch=233
05/27/2022 12:39:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.22 on epoch=234
05/27/2022 12:39:59 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.23 on epoch=234
05/27/2022 12:40:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.24 on epoch=235
05/27/2022 12:40:04 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.28 on epoch=236
05/27/2022 12:40:07 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.23 on epoch=237
05/27/2022 12:40:11 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.32003432003432003 on epoch=237
05/27/2022 12:40:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.23 on epoch=238
05/27/2022 12:40:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=239
05/27/2022 12:40:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.21 on epoch=239
05/27/2022 12:40:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.26 on epoch=240
05/27/2022 12:40:25 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=241
05/27/2022 12:40:29 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.32851420137947657 on epoch=241
05/27/2022 12:40:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=242
05/27/2022 12:40:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.24 on epoch=243
05/27/2022 12:40:37 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=244
05/27/2022 12:40:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.25 on epoch=244
05/27/2022 12:40:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.27 on epoch=245
05/27/2022 12:40:47 - INFO - __main__ - Global step 2950 Train loss 0.24 Classification-F1 0.3268370411227554 on epoch=245
05/27/2022 12:40:50 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=246
05/27/2022 12:40:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=247
05/27/2022 12:40:55 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.24 on epoch=248
05/27/2022 12:40:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.24 on epoch=249
05/27/2022 12:41:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=249
05/27/2022 12:41:02 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:41:02 - INFO - __main__ - Printing 3 examples
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:41:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:41:02 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:41:02 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:41:02 - INFO - __main__ - Printing 3 examples
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 12:41:02 - INFO - __main__ - ['entailment']
05/27/2022 12:41:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:41:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:41:02 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:41:05 - INFO - __main__ - Global step 3000 Train loss 0.22 Classification-F1 0.33224331607472674 on epoch=249
05/27/2022 12:41:05 - INFO - __main__ - save last model!
05/27/2022 12:41:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 12:41:05 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 12:41:05 - INFO - __main__ - Printing 3 examples
05/27/2022 12:41:05 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 12:41:05 - INFO - __main__ - ['contradiction']
05/27/2022 12:41:05 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 12:41:05 - INFO - __main__ - ['entailment']
05/27/2022 12:41:05 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 12:41:05 - INFO - __main__ - ['contradiction']
05/27/2022 12:41:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:41:06 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:41:07 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 12:41:18 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:41:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:41:19 - INFO - __main__ - Starting training!
05/27/2022 12:41:31 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_13_0.2_8_predictions.txt
05/27/2022 12:41:31 - INFO - __main__ - Classification-F1 on test data: 0.2832
05/27/2022 12:41:31 - INFO - __main__ - prefix=anli_64_13, lr=0.2, bsz=8, dev_performance=0.3533049157450307, test_performance=0.28319289258448094
05/27/2022 12:41:31 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.5, bsz=8 ...
05/27/2022 12:41:32 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:41:32 - INFO - __main__ - Printing 3 examples
05/27/2022 12:41:32 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 12:41:32 - INFO - __main__ - ['entailment']
05/27/2022 12:41:32 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 12:41:32 - INFO - __main__ - ['entailment']
05/27/2022 12:41:32 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 12:41:32 - INFO - __main__ - ['entailment']
05/27/2022 12:41:32 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:41:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:41:33 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 12:41:33 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 12:41:33 - INFO - __main__ - Printing 3 examples
05/27/2022 12:41:33 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 12:41:33 - INFO - __main__ - ['entailment']
05/27/2022 12:41:33 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 12:41:33 - INFO - __main__ - ['entailment']
05/27/2022 12:41:33 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 12:41:33 - INFO - __main__ - ['entailment']
05/27/2022 12:41:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 12:41:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 12:41:33 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 12:41:48 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 12:41:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 12:41:49 - INFO - __main__ - Starting training!
05/27/2022 12:41:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.43 on epoch=0
05/27/2022 12:41:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.43 on epoch=1
05/27/2022 12:41:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=2
05/27/2022 12:42:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=3
05/27/2022 12:42:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=4
05/27/2022 12:42:08 - INFO - __main__ - Global step 50 Train loss 0.45 Classification-F1 0.18571428571428572 on epoch=4
05/27/2022 12:42:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18571428571428572 on epoch=4, global_step=50
05/27/2022 12:42:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=4
05/27/2022 12:42:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=5
05/27/2022 12:42:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=6
05/27/2022 12:42:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.49 on epoch=7
05/27/2022 12:42:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.46 on epoch=8
05/27/2022 12:42:27 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 12:42:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=9
05/27/2022 12:42:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=9
05/27/2022 12:42:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=10
05/27/2022 12:42:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=11
05/27/2022 12:42:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=12
05/27/2022 12:42:45 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 12:42:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=13
05/27/2022 12:42:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
05/27/2022 12:42:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=14
05/27/2022 12:42:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=15
05/27/2022 12:42:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/27/2022 12:43:03 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 12:43:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
05/27/2022 12:43:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=18
05/27/2022 12:43:11 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=19
05/27/2022 12:43:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
05/27/2022 12:43:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=20
05/27/2022 12:43:22 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.17595815389455882 on epoch=20
05/27/2022 12:43:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/27/2022 12:43:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=22
05/27/2022 12:43:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
05/27/2022 12:43:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=24
05/27/2022 12:43:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=24
05/27/2022 12:43:40 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 12:43:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
05/27/2022 12:43:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=26
05/27/2022 12:43:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=27
05/27/2022 12:43:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=28
05/27/2022 12:43:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=29
05/27/2022 12:43:58 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.2883561882829129 on epoch=29
05/27/2022 12:43:58 - INFO - __main__ - Saving model with best Classification-F1: 0.18571428571428572 -> 0.2883561882829129 on epoch=29, global_step=350
05/27/2022 12:44:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=29
05/27/2022 12:44:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=30
05/27/2022 12:44:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=31
05/27/2022 12:44:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
05/27/2022 12:44:11 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=33
05/27/2022 12:44:16 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.2871016533667136 on epoch=33
05/27/2022 12:44:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
05/27/2022 12:44:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=34
05/27/2022 12:44:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=35
05/27/2022 12:44:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=36
05/27/2022 12:44:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=37
05/27/2022 12:44:35 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.2588120653154776 on epoch=37
05/27/2022 12:44:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=38
05/27/2022 12:44:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=39
05/27/2022 12:44:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/27/2022 12:44:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=40
05/27/2022 12:44:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=41
05/27/2022 12:44:53 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.27176925706500216 on epoch=41
05/27/2022 12:44:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=42
05/27/2022 12:44:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
05/27/2022 12:45:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=44
05/27/2022 12:45:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=44
05/27/2022 12:45:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
05/27/2022 12:45:12 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.35310773020187397 on epoch=45
05/27/2022 12:45:12 - INFO - __main__ - Saving model with best Classification-F1: 0.2883561882829129 -> 0.35310773020187397 on epoch=45, global_step=550
05/27/2022 12:45:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=46
05/27/2022 12:45:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=47
05/27/2022 12:45:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=48
05/27/2022 12:45:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=49
05/27/2022 12:45:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.39 on epoch=49
05/27/2022 12:45:30 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.35551230964075 on epoch=49
05/27/2022 12:45:30 - INFO - __main__ - Saving model with best Classification-F1: 0.35310773020187397 -> 0.35551230964075 on epoch=49, global_step=600
05/27/2022 12:45:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=50
05/27/2022 12:45:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=51
05/27/2022 12:45:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=52
05/27/2022 12:45:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
05/27/2022 12:45:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=54
05/27/2022 12:45:47 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.3858527698942587 on epoch=54
05/27/2022 12:45:47 - INFO - __main__ - Saving model with best Classification-F1: 0.35551230964075 -> 0.3858527698942587 on epoch=54, global_step=650
05/27/2022 12:45:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=54
05/27/2022 12:45:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/27/2022 12:45:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=56
05/27/2022 12:45:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
05/27/2022 12:46:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=58
05/27/2022 12:46:05 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.3640454286286657 on epoch=58
05/27/2022 12:46:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=59
05/27/2022 12:46:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=59
05/27/2022 12:46:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=60
05/27/2022 12:46:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=61
05/27/2022 12:46:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=62
05/27/2022 12:46:22 - INFO - __main__ - Global step 750 Train loss 0.37 Classification-F1 0.38465094775774394 on epoch=62
05/27/2022 12:46:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=63
05/27/2022 12:46:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
05/27/2022 12:46:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=64
05/27/2022 12:46:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=65
05/27/2022 12:46:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=66
05/27/2022 12:46:39 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.2881371029519178 on epoch=66
05/27/2022 12:46:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
05/27/2022 12:46:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
05/27/2022 12:46:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=69
05/27/2022 12:46:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=69
05/27/2022 12:46:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=70
05/27/2022 12:46:57 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.42497895139686187 on epoch=70
05/27/2022 12:46:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3858527698942587 -> 0.42497895139686187 on epoch=70, global_step=850
05/27/2022 12:47:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=71
05/27/2022 12:47:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=72
05/27/2022 12:47:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=73
05/27/2022 12:47:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=74
05/27/2022 12:47:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=74
05/27/2022 12:47:15 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.40558361391694725 on epoch=74
05/27/2022 12:47:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=75
05/27/2022 12:47:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=76
05/27/2022 12:47:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=77
05/27/2022 12:47:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=78
05/27/2022 12:47:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=79
05/27/2022 12:47:34 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.46686976389946694 on epoch=79
05/27/2022 12:47:34 - INFO - __main__ - Saving model with best Classification-F1: 0.42497895139686187 -> 0.46686976389946694 on epoch=79, global_step=950
05/27/2022 12:47:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=79
05/27/2022 12:47:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=80
05/27/2022 12:47:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=81
05/27/2022 12:47:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=82
05/27/2022 12:47:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=83
05/27/2022 12:47:52 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.44566539141881617 on epoch=83
05/27/2022 12:47:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=84
05/27/2022 12:47:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=84
05/27/2022 12:48:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=85
05/27/2022 12:48:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=86
05/27/2022 12:48:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=87
05/27/2022 12:48:10 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.4556921716054843 on epoch=87
05/27/2022 12:48:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=88
05/27/2022 12:48:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=89
05/27/2022 12:48:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=89
05/27/2022 12:48:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=90
05/27/2022 12:48:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=91
05/27/2022 12:48:27 - INFO - __main__ - Global step 1100 Train loss 0.32 Classification-F1 0.4187166017030361 on epoch=91
05/27/2022 12:48:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=92
05/27/2022 12:48:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=93
05/27/2022 12:48:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=94
05/27/2022 12:48:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=94
05/27/2022 12:48:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=95
05/27/2022 12:48:45 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.4430301240902789 on epoch=95
05/27/2022 12:48:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=96
05/27/2022 12:48:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=97
05/27/2022 12:48:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=98
05/27/2022 12:48:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=99
05/27/2022 12:48:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=99
05/27/2022 12:49:03 - INFO - __main__ - Global step 1200 Train loss 0.29 Classification-F1 0.48307448251157786 on epoch=99
05/27/2022 12:49:03 - INFO - __main__ - Saving model with best Classification-F1: 0.46686976389946694 -> 0.48307448251157786 on epoch=99, global_step=1200
05/27/2022 12:49:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=100
05/27/2022 12:49:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=101
05/27/2022 12:49:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=102
05/27/2022 12:49:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=103
05/27/2022 12:49:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=104
05/27/2022 12:49:22 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.4544151131583955 on epoch=104
05/27/2022 12:49:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=104
05/27/2022 12:49:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=105
05/27/2022 12:49:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=106
05/27/2022 12:49:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=107
05/27/2022 12:49:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=108
05/27/2022 12:49:40 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.46319444444444446 on epoch=108
05/27/2022 12:49:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=109
05/27/2022 12:49:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=109
05/27/2022 12:49:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=110
05/27/2022 12:49:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=111
05/27/2022 12:49:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=112
05/27/2022 12:49:57 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.4141041272824219 on epoch=112
05/27/2022 12:50:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=113
05/27/2022 12:50:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=114
05/27/2022 12:50:05 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=114
05/27/2022 12:50:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=115
05/27/2022 12:50:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=116
05/27/2022 12:50:15 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.4185780378053002 on epoch=116
05/27/2022 12:50:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=117
05/27/2022 12:50:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=118
05/27/2022 12:50:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=119
05/27/2022 12:50:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=119
05/27/2022 12:50:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=120
05/27/2022 12:50:34 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.49436584617744034 on epoch=120
05/27/2022 12:50:34 - INFO - __main__ - Saving model with best Classification-F1: 0.48307448251157786 -> 0.49436584617744034 on epoch=120, global_step=1450
05/27/2022 12:50:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=121
05/27/2022 12:50:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=122
05/27/2022 12:50:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=123
05/27/2022 12:50:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=124
05/27/2022 12:50:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=124
05/27/2022 12:50:52 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.5247737761779784 on epoch=124
05/27/2022 12:50:52 - INFO - __main__ - Saving model with best Classification-F1: 0.49436584617744034 -> 0.5247737761779784 on epoch=124, global_step=1500
05/27/2022 12:50:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=125
05/27/2022 12:50:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=126
05/27/2022 12:51:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=127
05/27/2022 12:51:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=128
05/27/2022 12:51:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=129
05/27/2022 12:51:11 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.542718557355851 on epoch=129
05/27/2022 12:51:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5247737761779784 -> 0.542718557355851 on epoch=129, global_step=1550
05/27/2022 12:51:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=129
05/27/2022 12:51:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=130
05/27/2022 12:51:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=131
05/27/2022 12:51:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=132
05/27/2022 12:51:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=133
05/27/2022 12:51:30 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.5653283841775436 on epoch=133
05/27/2022 12:51:30 - INFO - __main__ - Saving model with best Classification-F1: 0.542718557355851 -> 0.5653283841775436 on epoch=133, global_step=1600
05/27/2022 12:51:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=134
05/27/2022 12:51:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=134
05/27/2022 12:51:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=135
05/27/2022 12:51:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=136
05/27/2022 12:51:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=137
05/27/2022 12:51:48 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.49318281339154196 on epoch=137
05/27/2022 12:51:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=138
05/27/2022 12:51:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=139
05/27/2022 12:51:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=139
05/27/2022 12:51:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=140
05/27/2022 12:52:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=141
05/27/2022 12:52:07 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.4963454646681405 on epoch=141
05/27/2022 12:52:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=142
05/27/2022 12:52:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=143
05/27/2022 12:52:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=144
05/27/2022 12:52:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=144
05/27/2022 12:52:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=145
05/27/2022 12:52:26 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.5342131935908219 on epoch=145
05/27/2022 12:52:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=146
05/27/2022 12:52:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=147
05/27/2022 12:52:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=148
05/27/2022 12:52:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=149
05/27/2022 12:52:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=149
05/27/2022 12:52:44 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.5199533908510066 on epoch=149
05/27/2022 12:52:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=150
05/27/2022 12:52:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=151
05/27/2022 12:52:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=152
05/27/2022 12:52:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=153
05/27/2022 12:52:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=154
05/27/2022 12:53:03 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.3960196292257361 on epoch=154
05/27/2022 12:53:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=154
05/27/2022 12:53:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=155
05/27/2022 12:53:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=156
05/27/2022 12:53:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=157
05/27/2022 12:53:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=158
05/27/2022 12:53:22 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.31243414562690086 on epoch=158
05/27/2022 12:53:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=159
05/27/2022 12:53:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=159
05/27/2022 12:53:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=160
05/27/2022 12:53:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=161
05/27/2022 12:53:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=162
05/27/2022 12:53:41 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.290460470811178 on epoch=162
05/27/2022 12:53:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=163
05/27/2022 12:53:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=164
05/27/2022 12:53:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=164
05/27/2022 12:53:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=165
05/27/2022 12:53:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=166
05/27/2022 12:54:00 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.36450760079312616 on epoch=166
05/27/2022 12:54:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=167
05/27/2022 12:54:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=168
05/27/2022 12:54:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=169
05/27/2022 12:54:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=169
05/27/2022 12:54:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=170
05/27/2022 12:54:18 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.2571737385093669 on epoch=170
05/27/2022 12:54:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=171
05/27/2022 12:54:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=172
05/27/2022 12:54:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=173
05/27/2022 12:54:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=174
05/27/2022 12:54:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=174
05/27/2022 12:54:37 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.19697110261392842 on epoch=174
05/27/2022 12:54:40 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=175
05/27/2022 12:54:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=176
05/27/2022 12:54:45 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=177
05/27/2022 12:54:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=178
05/27/2022 12:54:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=179
05/27/2022 12:54:55 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.27804607437673357 on epoch=179
05/27/2022 12:54:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=179
05/27/2022 12:55:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=180
05/27/2022 12:55:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=181
05/27/2022 12:55:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=182
05/27/2022 12:55:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=183
05/27/2022 12:55:14 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.1514730959718469 on epoch=183
05/27/2022 12:55:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=184
05/27/2022 12:55:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=184
05/27/2022 12:55:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=185
05/27/2022 12:55:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=186
05/27/2022 12:55:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=187
05/27/2022 12:55:33 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.1835780024494558 on epoch=187
05/27/2022 12:55:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=188
05/27/2022 12:55:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=189
05/27/2022 12:55:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=189
05/27/2022 12:55:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=190
05/27/2022 12:55:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=191
05/27/2022 12:55:52 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.14826547464968348 on epoch=191
05/27/2022 12:55:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=192
05/27/2022 12:55:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=193
05/27/2022 12:55:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=194
05/27/2022 12:56:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=194
05/27/2022 12:56:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=195
05/27/2022 12:56:10 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.17489339325629918 on epoch=195
05/27/2022 12:56:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=196
05/27/2022 12:56:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=197
05/27/2022 12:56:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.16 on epoch=198
05/27/2022 12:56:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=199
05/27/2022 12:56:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=199
05/27/2022 12:56:30 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.15003850579799946 on epoch=199
05/27/2022 12:56:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=200
05/27/2022 12:56:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=201
05/27/2022 12:56:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=202
05/27/2022 12:56:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=203
05/27/2022 12:56:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=204
05/27/2022 12:56:49 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.11876530948097679 on epoch=204
05/27/2022 12:56:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=204
05/27/2022 12:56:54 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=205
05/27/2022 12:56:57 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=206
05/27/2022 12:56:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=207
05/27/2022 12:57:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=208
05/27/2022 12:57:07 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.1239779828015122 on epoch=208
05/27/2022 12:57:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=209
05/27/2022 12:57:13 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=209
05/27/2022 12:57:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=210
05/27/2022 12:57:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=211
05/27/2022 12:57:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=212
05/27/2022 12:57:26 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.15407134560925 on epoch=212
05/27/2022 12:57:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=213
05/27/2022 12:57:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=214
05/27/2022 12:57:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=214
05/27/2022 12:57:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=215
05/27/2022 12:57:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=216
05/27/2022 12:57:44 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.13111696742453918 on epoch=216
05/27/2022 12:57:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=217
05/27/2022 12:57:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=218
05/27/2022 12:57:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=219
05/27/2022 12:57:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=219
05/27/2022 12:57:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=220
05/27/2022 12:58:03 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.09521328769520679 on epoch=220
05/27/2022 12:58:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=221
05/27/2022 12:58:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=222
05/27/2022 12:58:11 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=223
05/27/2022 12:58:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=224
05/27/2022 12:58:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=224
05/27/2022 12:58:22 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.12293563134684629 on epoch=224
05/27/2022 12:58:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=225
05/27/2022 12:58:27 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=226
05/27/2022 12:58:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=227
05/27/2022 12:58:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=228
05/27/2022 12:58:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=229
05/27/2022 12:58:40 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.06246809450687474 on epoch=229
05/27/2022 12:58:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=229
05/27/2022 12:58:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=230
05/27/2022 12:58:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=231
05/27/2022 12:58:51 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=232
05/27/2022 12:58:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=233
05/27/2022 12:58:59 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.10080292485657859 on epoch=233
05/27/2022 12:59:02 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=234
05/27/2022 12:59:04 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=234
05/27/2022 12:59:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=235
05/27/2022 12:59:09 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=236
05/27/2022 12:59:12 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=237
05/27/2022 12:59:17 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.0616556665498388 on epoch=237
05/27/2022 12:59:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=238
05/27/2022 12:59:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=239
05/27/2022 12:59:25 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=239
05/27/2022 12:59:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=240
05/27/2022 12:59:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=241
05/27/2022 12:59:35 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.07949340062111801 on epoch=241
05/27/2022 12:59:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=242
05/27/2022 12:59:41 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=243
05/27/2022 12:59:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=244
05/27/2022 12:59:46 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=244
05/27/2022 12:59:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=245
05/27/2022 12:59:54 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.06868852459016393 on epoch=245
05/27/2022 12:59:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=246
05/27/2022 12:59:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=247
05/27/2022 13:00:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=248
05/27/2022 13:00:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=249
05/27/2022 13:00:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=249
05/27/2022 13:00:08 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:00:08 - INFO - __main__ - Printing 3 examples
05/27/2022 13:00:08 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:00:08 - INFO - __main__ - ['entailment']
05/27/2022 13:00:08 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:00:08 - INFO - __main__ - ['entailment']
05/27/2022 13:00:08 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:00:08 - INFO - __main__ - ['entailment']
05/27/2022 13:00:08 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:00:09 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:00:09 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:00:09 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:00:09 - INFO - __main__ - Printing 3 examples
05/27/2022 13:00:09 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:00:09 - INFO - __main__ - ['entailment']
05/27/2022 13:00:09 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:00:09 - INFO - __main__ - ['entailment']
05/27/2022 13:00:09 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:00:09 - INFO - __main__ - ['entailment']
05/27/2022 13:00:09 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:00:09 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:00:09 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:00:12 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.09844384190564907 on epoch=249
05/27/2022 13:00:12 - INFO - __main__ - save last model!
05/27/2022 13:00:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 13:00:12 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 13:00:12 - INFO - __main__ - Printing 3 examples
05/27/2022 13:00:12 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 13:00:12 - INFO - __main__ - ['contradiction']
05/27/2022 13:00:12 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 13:00:12 - INFO - __main__ - ['entailment']
05/27/2022 13:00:12 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 13:00:12 - INFO - __main__ - ['contradiction']
05/27/2022 13:00:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:00:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:00:14 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 13:00:25 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:00:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:00:25 - INFO - __main__ - Starting training!
05/27/2022 13:00:43 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_21_0.5_8_predictions.txt
05/27/2022 13:00:43 - INFO - __main__ - Classification-F1 on test data: 0.0141
05/27/2022 13:00:43 - INFO - __main__ - prefix=anli_64_21, lr=0.5, bsz=8, dev_performance=0.5653283841775436, test_performance=0.014145131393293485
05/27/2022 13:00:43 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.4, bsz=8 ...
05/27/2022 13:00:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:00:44 - INFO - __main__ - Printing 3 examples
05/27/2022 13:00:44 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:00:44 - INFO - __main__ - ['entailment']
05/27/2022 13:00:44 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:00:44 - INFO - __main__ - ['entailment']
05/27/2022 13:00:44 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:00:44 - INFO - __main__ - ['entailment']
05/27/2022 13:00:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:00:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:00:45 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:00:45 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:00:45 - INFO - __main__ - Printing 3 examples
05/27/2022 13:00:45 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:00:45 - INFO - __main__ - ['entailment']
05/27/2022 13:00:45 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:00:45 - INFO - __main__ - ['entailment']
05/27/2022 13:00:45 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:00:45 - INFO - __main__ - ['entailment']
05/27/2022 13:00:45 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:00:45 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:00:45 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:01:00 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:01:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:01:01 - INFO - __main__ - Starting training!
05/27/2022 13:01:05 - INFO - __main__ - Step 10 Global step 10 Train loss 0.51 on epoch=0
05/27/2022 13:01:07 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
05/27/2022 13:01:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=2
05/27/2022 13:01:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=3
05/27/2022 13:01:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=4
05/27/2022 13:01:20 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 13:01:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 13:01:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=4
05/27/2022 13:01:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=5
05/27/2022 13:01:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=6
05/27/2022 13:01:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=7
05/27/2022 13:01:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=8
05/27/2022 13:01:39 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 13:01:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=9
05/27/2022 13:01:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=9
05/27/2022 13:01:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=10
05/27/2022 13:01:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=11
05/27/2022 13:01:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=12
05/27/2022 13:01:57 - INFO - __main__ - Global step 150 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 13:01:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=13
05/27/2022 13:02:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=14
05/27/2022 13:02:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/27/2022 13:02:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=15
05/27/2022 13:02:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=16
05/27/2022 13:02:14 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 13:02:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=17
05/27/2022 13:02:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=18
05/27/2022 13:02:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=19
05/27/2022 13:02:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=19
05/27/2022 13:02:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=20
05/27/2022 13:02:33 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.1754553408096715 on epoch=20
05/27/2022 13:02:33 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1754553408096715 on epoch=20, global_step=250
05/27/2022 13:02:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=21
05/27/2022 13:02:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
05/27/2022 13:02:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.43 on epoch=23
05/27/2022 13:02:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
05/27/2022 13:02:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
05/27/2022 13:02:51 - INFO - __main__ - Global step 300 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 13:02:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=25
05/27/2022 13:02:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/27/2022 13:02:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/27/2022 13:03:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/27/2022 13:03:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=29
05/27/2022 13:03:10 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.32918521209919155 on epoch=29
05/27/2022 13:03:10 - INFO - __main__ - Saving model with best Classification-F1: 0.1754553408096715 -> 0.32918521209919155 on epoch=29, global_step=350
05/27/2022 13:03:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=29
05/27/2022 13:03:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=30
05/27/2022 13:03:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=31
05/27/2022 13:03:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=32
05/27/2022 13:03:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=33
05/27/2022 13:03:28 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.1754553408096715 on epoch=33
05/27/2022 13:03:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=34
05/27/2022 13:03:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
05/27/2022 13:03:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
05/27/2022 13:03:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=36
05/27/2022 13:03:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=37
05/27/2022 13:03:47 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.1922508133688258 on epoch=37
05/27/2022 13:03:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/27/2022 13:03:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
05/27/2022 13:03:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=39
05/27/2022 13:03:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=40
05/27/2022 13:04:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=41
05/27/2022 13:04:05 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.17561074096507168 on epoch=41
05/27/2022 13:04:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=42
05/27/2022 13:04:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/27/2022 13:04:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=44
05/27/2022 13:04:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
05/27/2022 13:04:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=45
05/27/2022 13:04:24 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.24403933464184466 on epoch=45
05/27/2022 13:04:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=46
05/27/2022 13:04:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=47
05/27/2022 13:04:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=48
05/27/2022 13:04:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=49
05/27/2022 13:04:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=49
05/27/2022 13:04:42 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.2001929867783526 on epoch=49
05/27/2022 13:04:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
05/27/2022 13:04:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/27/2022 13:04:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=52
05/27/2022 13:04:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
05/27/2022 13:04:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=54
05/27/2022 13:05:00 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.4153928813503282 on epoch=54
05/27/2022 13:05:00 - INFO - __main__ - Saving model with best Classification-F1: 0.32918521209919155 -> 0.4153928813503282 on epoch=54, global_step=650
05/27/2022 13:05:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=54
05/27/2022 13:05:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=55
05/27/2022 13:05:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=56
05/27/2022 13:05:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=57
05/27/2022 13:05:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=58
05/27/2022 13:05:18 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.2927484687455028 on epoch=58
05/27/2022 13:05:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=59
05/27/2022 13:05:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=59
05/27/2022 13:05:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
05/27/2022 13:05:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=61
05/27/2022 13:05:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=62
05/27/2022 13:05:36 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3871043239131707 on epoch=62
05/27/2022 13:05:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=63
05/27/2022 13:05:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=64
05/27/2022 13:05:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=64
05/27/2022 13:05:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=65
05/27/2022 13:05:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
05/27/2022 13:05:55 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.40299542323122606 on epoch=66
05/27/2022 13:05:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=67
05/27/2022 13:06:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=68
05/27/2022 13:06:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=69
05/27/2022 13:06:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=69
05/27/2022 13:06:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=70
05/27/2022 13:06:13 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.40420208974870503 on epoch=70
05/27/2022 13:06:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=71
05/27/2022 13:06:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=72
05/27/2022 13:06:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=73
05/27/2022 13:06:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=74
05/27/2022 13:06:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=74
05/27/2022 13:06:31 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.35867926577604 on epoch=74
05/27/2022 13:06:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
05/27/2022 13:06:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=76
05/27/2022 13:06:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=77
05/27/2022 13:06:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=78
05/27/2022 13:06:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=79
05/27/2022 13:06:48 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3924410343847684 on epoch=79
05/27/2022 13:06:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=79
05/27/2022 13:06:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=80
05/27/2022 13:06:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=81
05/27/2022 13:06:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.31 on epoch=82
05/27/2022 13:07:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=83
05/27/2022 13:07:07 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.4494489118549269 on epoch=83
05/27/2022 13:07:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4153928813503282 -> 0.4494489118549269 on epoch=83, global_step=1000
05/27/2022 13:07:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=84
05/27/2022 13:07:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=84
05/27/2022 13:07:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=85
05/27/2022 13:07:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=86
05/27/2022 13:07:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=87
05/27/2022 13:07:24 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.4487300193815709 on epoch=87
05/27/2022 13:07:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=88
05/27/2022 13:07:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=89
05/27/2022 13:07:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=89
05/27/2022 13:07:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=90
05/27/2022 13:07:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=91
05/27/2022 13:07:42 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.46667139913235306 on epoch=91
05/27/2022 13:07:42 - INFO - __main__ - Saving model with best Classification-F1: 0.4494489118549269 -> 0.46667139913235306 on epoch=91, global_step=1100
05/27/2022 13:07:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=92
05/27/2022 13:07:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=93
05/27/2022 13:07:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=94
05/27/2022 13:07:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=94
05/27/2022 13:07:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=95
05/27/2022 13:08:00 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.4154671812206059 on epoch=95
05/27/2022 13:08:03 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=96
05/27/2022 13:08:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=97
05/27/2022 13:08:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=98
05/27/2022 13:08:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=99
05/27/2022 13:08:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=99
05/27/2022 13:08:19 - INFO - __main__ - Global step 1200 Train loss 0.30 Classification-F1 0.4689511964532991 on epoch=99
05/27/2022 13:08:19 - INFO - __main__ - Saving model with best Classification-F1: 0.46667139913235306 -> 0.4689511964532991 on epoch=99, global_step=1200
05/27/2022 13:08:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=100
05/27/2022 13:08:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=101
05/27/2022 13:08:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=102
05/27/2022 13:08:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=103
05/27/2022 13:08:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=104
05/27/2022 13:08:37 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.5133792760911405 on epoch=104
05/27/2022 13:08:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4689511964532991 -> 0.5133792760911405 on epoch=104, global_step=1250
05/27/2022 13:08:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=104
05/27/2022 13:08:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.32 on epoch=105
05/27/2022 13:08:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=106
05/27/2022 13:08:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=107
05/27/2022 13:08:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=108
05/27/2022 13:08:56 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.48744980969306956 on epoch=108
05/27/2022 13:08:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=109
05/27/2022 13:09:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=109
05/27/2022 13:09:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=110
05/27/2022 13:09:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=111
05/27/2022 13:09:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=112
05/27/2022 13:09:14 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.5079426274568598 on epoch=112
05/27/2022 13:09:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=113
05/27/2022 13:09:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=114
05/27/2022 13:09:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.28 on epoch=114
05/27/2022 13:09:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=115
05/27/2022 13:09:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=116
05/27/2022 13:09:33 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.49395317320208987 on epoch=116
05/27/2022 13:09:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.26 on epoch=117
05/27/2022 13:09:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=118
05/27/2022 13:09:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=119
05/27/2022 13:09:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=119
05/27/2022 13:09:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=120
05/27/2022 13:09:52 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.49669800521683644 on epoch=120
05/27/2022 13:09:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=121
05/27/2022 13:09:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=122
05/27/2022 13:10:00 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=123
05/27/2022 13:10:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=124
05/27/2022 13:10:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=124
05/27/2022 13:10:11 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.5535482084025775 on epoch=124
05/27/2022 13:10:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5133792760911405 -> 0.5535482084025775 on epoch=124, global_step=1500
05/27/2022 13:10:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=125
05/27/2022 13:10:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=126
05/27/2022 13:10:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=127
05/27/2022 13:10:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=128
05/27/2022 13:10:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=129
05/27/2022 13:10:29 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.564389696863992 on epoch=129
05/27/2022 13:10:29 - INFO - __main__ - Saving model with best Classification-F1: 0.5535482084025775 -> 0.564389696863992 on epoch=129, global_step=1550
05/27/2022 13:10:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=129
05/27/2022 13:10:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=130
05/27/2022 13:10:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=131
05/27/2022 13:10:40 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=132
05/27/2022 13:10:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=133
05/27/2022 13:10:48 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.4785748806367363 on epoch=133
05/27/2022 13:10:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=134
05/27/2022 13:10:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=134
05/27/2022 13:10:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=135
05/27/2022 13:10:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=136
05/27/2022 13:11:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=137
05/27/2022 13:11:07 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.5298494177766608 on epoch=137
05/27/2022 13:11:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=138
05/27/2022 13:11:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=139
05/27/2022 13:11:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=139
05/27/2022 13:11:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=140
05/27/2022 13:11:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=141
05/27/2022 13:11:26 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.5531725017161911 on epoch=141
05/27/2022 13:11:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=142
05/27/2022 13:11:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=143
05/27/2022 13:11:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=144
05/27/2022 13:11:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=144
05/27/2022 13:11:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=145
05/27/2022 13:11:45 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.5323122432796059 on epoch=145
05/27/2022 13:11:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=146
05/27/2022 13:11:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=147
05/27/2022 13:11:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=148
05/27/2022 13:11:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=149
05/27/2022 13:11:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=149
05/27/2022 13:12:05 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.5374299510943749 on epoch=149
05/27/2022 13:12:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=150
05/27/2022 13:12:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=151
05/27/2022 13:12:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=152
05/27/2022 13:12:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=153
05/27/2022 13:12:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=154
05/27/2022 13:12:24 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.5201778907521614 on epoch=154
05/27/2022 13:12:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=154
05/27/2022 13:12:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=155
05/27/2022 13:12:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=156
05/27/2022 13:12:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=157
05/27/2022 13:12:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=158
05/27/2022 13:12:42 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.4818813546924146 on epoch=158
05/27/2022 13:12:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=159
05/27/2022 13:12:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=159
05/27/2022 13:12:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=160
05/27/2022 13:12:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=161
05/27/2022 13:12:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=162
05/27/2022 13:13:01 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.4535995325816873 on epoch=162
05/27/2022 13:13:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=163
05/27/2022 13:13:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=164
05/27/2022 13:13:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=164
05/27/2022 13:13:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=165
05/27/2022 13:13:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=166
05/27/2022 13:13:20 - INFO - __main__ - Global step 2000 Train loss 0.21 Classification-F1 0.34016096412409774 on epoch=166
05/27/2022 13:13:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=167
05/27/2022 13:13:25 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=168
05/27/2022 13:13:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=169
05/27/2022 13:13:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=169
05/27/2022 13:13:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=170
05/27/2022 13:13:39 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.33786224929560127 on epoch=170
05/27/2022 13:13:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=171
05/27/2022 13:13:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=172
05/27/2022 13:13:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=173
05/27/2022 13:13:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=174
05/27/2022 13:13:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=174
05/27/2022 13:13:58 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.39181250422150504 on epoch=174
05/27/2022 13:14:01 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=175
05/27/2022 13:14:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=176
05/27/2022 13:14:06 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=177
05/27/2022 13:14:09 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=178
05/27/2022 13:14:11 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=179
05/27/2022 13:14:17 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.4014938521517469 on epoch=179
05/27/2022 13:14:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.15 on epoch=179
05/27/2022 13:14:22 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=180
05/27/2022 13:14:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=181
05/27/2022 13:14:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=182
05/27/2022 13:14:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=183
05/27/2022 13:14:36 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.3824074074074074 on epoch=183
05/27/2022 13:14:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=184
05/27/2022 13:14:41 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=184
05/27/2022 13:14:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.11 on epoch=185
05/27/2022 13:14:46 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=186
05/27/2022 13:14:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=187
05/27/2022 13:14:55 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.3349259121747055 on epoch=187
05/27/2022 13:14:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=188
05/27/2022 13:15:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=189
05/27/2022 13:15:03 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=189
05/27/2022 13:15:05 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=190
05/27/2022 13:15:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=191
05/27/2022 13:15:14 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.2644775687010264 on epoch=191
05/27/2022 13:15:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.15 on epoch=192
05/27/2022 13:15:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=193
05/27/2022 13:15:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=194
05/27/2022 13:15:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=194
05/27/2022 13:15:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=195
05/27/2022 13:15:32 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.19363465305701616 on epoch=195
05/27/2022 13:15:35 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=196
05/27/2022 13:15:38 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=197
05/27/2022 13:15:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=198
05/27/2022 13:15:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=199
05/27/2022 13:15:45 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=199
05/27/2022 13:15:51 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.23376675306499867 on epoch=199
05/27/2022 13:15:53 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=200
05/27/2022 13:15:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=201
05/27/2022 13:15:58 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=202
05/27/2022 13:16:01 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=203
05/27/2022 13:16:04 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=204
05/27/2022 13:16:09 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.14307327964724129 on epoch=204
05/27/2022 13:16:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=204
05/27/2022 13:16:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=205
05/27/2022 13:16:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=206
05/27/2022 13:16:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.23 on epoch=207
05/27/2022 13:16:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=208
05/27/2022 13:16:28 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.16009944593304026 on epoch=208
05/27/2022 13:16:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=209
05/27/2022 13:16:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=209
05/27/2022 13:16:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=210
05/27/2022 13:16:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=211
05/27/2022 13:16:41 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=212
05/27/2022 13:16:47 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.15696649029982365 on epoch=212
05/27/2022 13:16:49 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=213
05/27/2022 13:16:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=214
05/27/2022 13:16:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=214
05/27/2022 13:16:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=215
05/27/2022 13:17:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=216
05/27/2022 13:17:05 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.20497588353941948 on epoch=216
05/27/2022 13:17:08 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=217
05/27/2022 13:17:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=218
05/27/2022 13:17:13 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=219
05/27/2022 13:17:16 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=219
05/27/2022 13:17:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=220
05/27/2022 13:17:24 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.13662953357701585 on epoch=220
05/27/2022 13:17:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.15 on epoch=221
05/27/2022 13:17:29 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=222
05/27/2022 13:17:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=223
05/27/2022 13:17:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=224
05/27/2022 13:17:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=224
05/27/2022 13:17:43 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.11806555822293352 on epoch=224
05/27/2022 13:17:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=225
05/27/2022 13:17:48 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=226
05/27/2022 13:17:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=227
05/27/2022 13:17:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=228
05/27/2022 13:17:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=229
05/27/2022 13:18:01 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.2019299460778172 on epoch=229
05/27/2022 13:18:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=229
05/27/2022 13:18:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=230
05/27/2022 13:18:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=231
05/27/2022 13:18:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=232
05/27/2022 13:18:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=233
05/27/2022 13:18:20 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.1660028733533007 on epoch=233
05/27/2022 13:18:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=234
05/27/2022 13:18:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=234
05/27/2022 13:18:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.13 on epoch=235
05/27/2022 13:18:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=236
05/27/2022 13:18:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=237
05/27/2022 13:18:39 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.1368632273316556 on epoch=237
05/27/2022 13:18:41 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=238
05/27/2022 13:18:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=239
05/27/2022 13:18:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=239
05/27/2022 13:18:49 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=240
05/27/2022 13:18:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=241
05/27/2022 13:18:58 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.1514279438389944 on epoch=241
05/27/2022 13:19:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=242
05/27/2022 13:19:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=243
05/27/2022 13:19:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=244
05/27/2022 13:19:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=244
05/27/2022 13:19:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=245
05/27/2022 13:19:16 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.17283630945637876 on epoch=245
05/27/2022 13:19:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=246
05/27/2022 13:19:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=247
05/27/2022 13:19:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=248
05/27/2022 13:19:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.10 on epoch=249
05/27/2022 13:19:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=249
05/27/2022 13:19:31 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:19:31 - INFO - __main__ - Printing 3 examples
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:19:31 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:19:31 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:19:31 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:19:31 - INFO - __main__ - Printing 3 examples
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:19:31 - INFO - __main__ - ['entailment']
05/27/2022 13:19:31 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:19:31 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:19:32 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:19:35 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.16913411848204582 on epoch=249
05/27/2022 13:19:35 - INFO - __main__ - save last model!
05/27/2022 13:19:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 13:19:35 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 13:19:35 - INFO - __main__ - Printing 3 examples
05/27/2022 13:19:35 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 13:19:35 - INFO - __main__ - ['contradiction']
05/27/2022 13:19:35 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 13:19:35 - INFO - __main__ - ['entailment']
05/27/2022 13:19:35 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 13:19:35 - INFO - __main__ - ['contradiction']
05/27/2022 13:19:35 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:19:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:19:37 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 13:19:47 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:19:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:19:48 - INFO - __main__ - Starting training!
05/27/2022 13:20:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_21_0.4_8_predictions.txt
05/27/2022 13:20:08 - INFO - __main__ - Classification-F1 on test data: 0.0244
05/27/2022 13:20:09 - INFO - __main__ - prefix=anli_64_21, lr=0.4, bsz=8, dev_performance=0.564389696863992, test_performance=0.024411333879932915
05/27/2022 13:20:09 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.3, bsz=8 ...
05/27/2022 13:20:10 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:20:10 - INFO - __main__ - Printing 3 examples
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:20:10 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:20:10 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:20:10 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:20:10 - INFO - __main__ - Printing 3 examples
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:20:10 - INFO - __main__ - ['entailment']
05/27/2022 13:20:10 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:20:10 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:20:10 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:20:25 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:20:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:20:26 - INFO - __main__ - Starting training!
05/27/2022 13:20:30 - INFO - __main__ - Step 10 Global step 10 Train loss 0.53 on epoch=0
05/27/2022 13:20:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=1
05/27/2022 13:20:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=2
05/27/2022 13:20:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=3
05/27/2022 13:20:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=4
05/27/2022 13:20:46 - INFO - __main__ - Global step 50 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 13:20:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 13:20:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=4
05/27/2022 13:20:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=5
05/27/2022 13:20:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=6
05/27/2022 13:20:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.48 on epoch=7
05/27/2022 13:20:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=8
05/27/2022 13:21:04 - INFO - __main__ - Global step 100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 13:21:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=9
05/27/2022 13:21:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=9
05/27/2022 13:21:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
05/27/2022 13:21:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=11
05/27/2022 13:21:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=12
05/27/2022 13:21:21 - INFO - __main__ - Global step 150 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 13:21:24 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=13
05/27/2022 13:21:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.41 on epoch=14
05/27/2022 13:21:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=14
05/27/2022 13:21:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
05/27/2022 13:21:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/27/2022 13:21:40 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 13:21:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=17
05/27/2022 13:21:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=18
05/27/2022 13:21:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
05/27/2022 13:21:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/27/2022 13:21:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=20
05/27/2022 13:21:58 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 13:22:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/27/2022 13:22:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/27/2022 13:22:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
05/27/2022 13:22:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=24
05/27/2022 13:22:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
05/27/2022 13:22:17 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 13:22:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=25
05/27/2022 13:22:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
05/27/2022 13:22:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=27
05/27/2022 13:22:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=28
05/27/2022 13:22:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=29
05/27/2022 13:22:35 - INFO - __main__ - Global step 350 Train loss 0.43 Classification-F1 0.2769567021661869 on epoch=29
05/27/2022 13:22:35 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2769567021661869 on epoch=29, global_step=350
05/27/2022 13:22:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=29
05/27/2022 13:22:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=30
05/27/2022 13:22:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=31
05/27/2022 13:22:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=32
05/27/2022 13:22:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/27/2022 13:22:54 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.16535433070866143 on epoch=33
05/27/2022 13:22:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=34
05/27/2022 13:22:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=34
05/27/2022 13:23:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=35
05/27/2022 13:23:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=36
05/27/2022 13:23:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=37
05/27/2022 13:23:12 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.16535433070866143 on epoch=37
05/27/2022 13:23:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
05/27/2022 13:23:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.44 on epoch=39
05/27/2022 13:23:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/27/2022 13:23:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
05/27/2022 13:23:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=41
05/27/2022 13:23:31 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.1647058823529412 on epoch=41
05/27/2022 13:23:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=42
05/27/2022 13:23:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=43
05/27/2022 13:23:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
05/27/2022 13:23:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=44
05/27/2022 13:23:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
05/27/2022 13:23:49 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.23246197016688816 on epoch=45
05/27/2022 13:23:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/27/2022 13:23:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=47
05/27/2022 13:23:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
05/27/2022 13:24:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
05/27/2022 13:24:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=49
05/27/2022 13:24:08 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.20409256071906676 on epoch=49
05/27/2022 13:24:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=50
05/27/2022 13:24:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=51
05/27/2022 13:24:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=52
05/27/2022 13:24:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=53
05/27/2022 13:24:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=54
05/27/2022 13:24:26 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.3334284898658293 on epoch=54
05/27/2022 13:24:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2769567021661869 -> 0.3334284898658293 on epoch=54, global_step=650
05/27/2022 13:24:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=54
05/27/2022 13:24:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/27/2022 13:24:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=56
05/27/2022 13:24:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=57
05/27/2022 13:24:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
05/27/2022 13:24:45 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.35787651603732495 on epoch=58
05/27/2022 13:24:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3334284898658293 -> 0.35787651603732495 on epoch=58, global_step=700
05/27/2022 13:24:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=59
05/27/2022 13:24:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=59
05/27/2022 13:24:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
05/27/2022 13:24:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=61
05/27/2022 13:24:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=62
05/27/2022 13:25:03 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.28083229984842223 on epoch=62
05/27/2022 13:25:06 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
05/27/2022 13:25:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=64
05/27/2022 13:25:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=64
05/27/2022 13:25:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/27/2022 13:25:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
05/27/2022 13:25:22 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.196742441996719 on epoch=66
05/27/2022 13:25:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
05/27/2022 13:25:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=68
05/27/2022 13:25:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=69
05/27/2022 13:25:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
05/27/2022 13:25:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=70
05/27/2022 13:25:40 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.34642217101894524 on epoch=70
05/27/2022 13:25:43 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
05/27/2022 13:25:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=72
05/27/2022 13:25:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=73
05/27/2022 13:25:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=74
05/27/2022 13:25:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=74
05/27/2022 13:25:58 - INFO - __main__ - Global step 900 Train loss 0.38 Classification-F1 0.30997150997150996 on epoch=74
05/27/2022 13:26:01 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=75
05/27/2022 13:26:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=76
05/27/2022 13:26:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=77
05/27/2022 13:26:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=78
05/27/2022 13:26:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/27/2022 13:26:16 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.386078431372549 on epoch=79
05/27/2022 13:26:16 - INFO - __main__ - Saving model with best Classification-F1: 0.35787651603732495 -> 0.386078431372549 on epoch=79, global_step=950
05/27/2022 13:26:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=79
05/27/2022 13:26:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=80
05/27/2022 13:26:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=81
05/27/2022 13:26:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
05/27/2022 13:26:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=83
05/27/2022 13:26:34 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.3684179144802271 on epoch=83
05/27/2022 13:26:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=84
05/27/2022 13:26:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=84
05/27/2022 13:26:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=85
05/27/2022 13:26:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=86
05/27/2022 13:26:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.38 on epoch=87
05/27/2022 13:26:52 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.3707878258336974 on epoch=87
05/27/2022 13:26:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=88
05/27/2022 13:26:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=89
05/27/2022 13:27:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=89
05/27/2022 13:27:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=90
05/27/2022 13:27:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.28 on epoch=91
05/27/2022 13:27:10 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.3794196476382415 on epoch=91
05/27/2022 13:27:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=92
05/27/2022 13:27:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=93
05/27/2022 13:27:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=94
05/27/2022 13:27:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.37 on epoch=94
05/27/2022 13:27:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=95
05/27/2022 13:27:29 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.4501146601146601 on epoch=95
05/27/2022 13:27:29 - INFO - __main__ - Saving model with best Classification-F1: 0.386078431372549 -> 0.4501146601146601 on epoch=95, global_step=1150
05/27/2022 13:27:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.30 on epoch=96
05/27/2022 13:27:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=97
05/27/2022 13:27:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=98
05/27/2022 13:27:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=99
05/27/2022 13:27:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=99
05/27/2022 13:27:47 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.36103569632981397 on epoch=99
05/27/2022 13:27:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=100
05/27/2022 13:27:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=101
05/27/2022 13:27:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=102
05/27/2022 13:27:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=103
05/27/2022 13:28:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=104
05/27/2022 13:28:05 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.4304188385406659 on epoch=104
05/27/2022 13:28:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=104
05/27/2022 13:28:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=105
05/27/2022 13:28:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=106
05/27/2022 13:28:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.34 on epoch=107
05/27/2022 13:28:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=108
05/27/2022 13:28:23 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.45134750729542167 on epoch=108
05/27/2022 13:28:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4501146601146601 -> 0.45134750729542167 on epoch=108, global_step=1300
05/27/2022 13:28:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=109
05/27/2022 13:28:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=109
05/27/2022 13:28:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=110
05/27/2022 13:28:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=111
05/27/2022 13:28:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=112
05/27/2022 13:28:42 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.4813340670090486 on epoch=112
05/27/2022 13:28:42 - INFO - __main__ - Saving model with best Classification-F1: 0.45134750729542167 -> 0.4813340670090486 on epoch=112, global_step=1350
05/27/2022 13:28:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=113
05/27/2022 13:28:47 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=114
05/27/2022 13:28:50 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=114
05/27/2022 13:28:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=115
05/27/2022 13:28:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=116
05/27/2022 13:29:00 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.4391084093211753 on epoch=116
05/27/2022 13:29:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=117
05/27/2022 13:29:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.34 on epoch=118
05/27/2022 13:29:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=119
05/27/2022 13:29:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.28 on epoch=119
05/27/2022 13:29:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=120
05/27/2022 13:29:19 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.4571093194946407 on epoch=120
05/27/2022 13:29:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=121
05/27/2022 13:29:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=122
05/27/2022 13:29:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=123
05/27/2022 13:29:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=124
05/27/2022 13:29:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=124
05/27/2022 13:29:37 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.4287619067030832 on epoch=124
05/27/2022 13:29:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=125
05/27/2022 13:29:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=126
05/27/2022 13:29:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=127
05/27/2022 13:29:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.31 on epoch=128
05/27/2022 13:29:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=129
05/27/2022 13:29:56 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.4272319546760734 on epoch=129
05/27/2022 13:29:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=129
05/27/2022 13:30:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=130
05/27/2022 13:30:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=131
05/27/2022 13:30:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=132
05/27/2022 13:30:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=133
05/27/2022 13:30:14 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.3961692844559219 on epoch=133
05/27/2022 13:30:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=134
05/27/2022 13:30:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=134
05/27/2022 13:30:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.30 on epoch=135
05/27/2022 13:30:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=136
05/27/2022 13:30:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=137
05/27/2022 13:30:33 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.4018855218855219 on epoch=137
05/27/2022 13:30:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.31 on epoch=138
05/27/2022 13:30:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=139
05/27/2022 13:30:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=139
05/27/2022 13:30:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=140
05/27/2022 13:30:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=141
05/27/2022 13:30:51 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.34902719814632555 on epoch=141
05/27/2022 13:30:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.34 on epoch=142
05/27/2022 13:30:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=143
05/27/2022 13:30:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=144
05/27/2022 13:31:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=144
05/27/2022 13:31:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=145
05/27/2022 13:31:10 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.43369113126146375 on epoch=145
05/27/2022 13:31:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=146
05/27/2022 13:31:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=147
05/27/2022 13:31:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=148
05/27/2022 13:31:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=149
05/27/2022 13:31:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=149
05/27/2022 13:31:29 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.4582273717567835 on epoch=149
05/27/2022 13:31:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=150
05/27/2022 13:31:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=151
05/27/2022 13:31:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=152
05/27/2022 13:31:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=153
05/27/2022 13:31:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=154
05/27/2022 13:31:48 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.47962595167163197 on epoch=154
05/27/2022 13:31:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=154
05/27/2022 13:31:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=155
05/27/2022 13:31:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=156
05/27/2022 13:31:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=157
05/27/2022 13:32:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=158
05/27/2022 13:32:07 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.338879781420765 on epoch=158
05/27/2022 13:32:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=159
05/27/2022 13:32:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=159
05/27/2022 13:32:15 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=160
05/27/2022 13:32:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=161
05/27/2022 13:32:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=162
05/27/2022 13:32:26 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.48264768065591596 on epoch=162
05/27/2022 13:32:26 - INFO - __main__ - Saving model with best Classification-F1: 0.4813340670090486 -> 0.48264768065591596 on epoch=162, global_step=1950
05/27/2022 13:32:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=163
05/27/2022 13:32:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=164
05/27/2022 13:32:34 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.26 on epoch=164
05/27/2022 13:32:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=165
05/27/2022 13:32:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=166
05/27/2022 13:32:45 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.2909978789203542 on epoch=166
05/27/2022 13:32:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=167
05/27/2022 13:32:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.21 on epoch=168
05/27/2022 13:32:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.30 on epoch=169
05/27/2022 13:32:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=169
05/27/2022 13:32:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=170
05/27/2022 13:33:03 - INFO - __main__ - Global step 2050 Train loss 0.25 Classification-F1 0.36465201465201463 on epoch=170
05/27/2022 13:33:06 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=171
05/27/2022 13:33:09 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=172
05/27/2022 13:33:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=173
05/27/2022 13:33:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.27 on epoch=174
05/27/2022 13:33:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=174
05/27/2022 13:33:22 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.3463052284685549 on epoch=174
05/27/2022 13:33:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.26 on epoch=175
05/27/2022 13:33:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=176
05/27/2022 13:33:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=177
05/27/2022 13:33:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=178
05/27/2022 13:33:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=179
05/27/2022 13:33:41 - INFO - __main__ - Global step 2150 Train loss 0.25 Classification-F1 0.3806143545389022 on epoch=179
05/27/2022 13:33:44 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=179
05/27/2022 13:33:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=180
05/27/2022 13:33:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=181
05/27/2022 13:33:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=182
05/27/2022 13:33:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=183
05/27/2022 13:34:00 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.28228037712573795 on epoch=183
05/27/2022 13:34:03 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=184
05/27/2022 13:34:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=184
05/27/2022 13:34:08 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.25 on epoch=185
05/27/2022 13:34:11 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=186
05/27/2022 13:34:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=187
05/27/2022 13:34:19 - INFO - __main__ - Global step 2250 Train loss 0.24 Classification-F1 0.2793562465902891 on epoch=187
05/27/2022 13:34:22 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=188
05/27/2022 13:34:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.23 on epoch=189
05/27/2022 13:34:27 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=189
05/27/2022 13:34:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=190
05/27/2022 13:34:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=191
05/27/2022 13:34:38 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.21390350958622653 on epoch=191
05/27/2022 13:34:41 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=192
05/27/2022 13:34:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.22 on epoch=193
05/27/2022 13:34:46 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=194
05/27/2022 13:34:49 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=194
05/27/2022 13:34:51 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=195
05/27/2022 13:34:57 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.2912514516396616 on epoch=195
05/27/2022 13:35:00 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=196
05/27/2022 13:35:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.22 on epoch=197
05/27/2022 13:35:05 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=198
05/27/2022 13:35:08 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.18 on epoch=199
05/27/2022 13:35:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=199
05/27/2022 13:35:16 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.29968290416051613 on epoch=199
05/27/2022 13:35:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=200
05/27/2022 13:35:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=201
05/27/2022 13:35:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=202
05/27/2022 13:35:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=203
05/27/2022 13:35:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=204
05/27/2022 13:35:35 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.2606340768865686 on epoch=204
05/27/2022 13:35:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=204
05/27/2022 13:35:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=205
05/27/2022 13:35:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=206
05/27/2022 13:35:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=207
05/27/2022 13:35:49 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.21 on epoch=208
05/27/2022 13:35:54 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.24322169059011167 on epoch=208
05/27/2022 13:35:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.22 on epoch=209
05/27/2022 13:36:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.21 on epoch=209
05/27/2022 13:36:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=210
05/27/2022 13:36:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=211
05/27/2022 13:36:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=212
05/27/2022 13:36:14 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.28841001521886794 on epoch=212
05/27/2022 13:36:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=213
05/27/2022 13:36:19 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=214
05/27/2022 13:36:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=214
05/27/2022 13:36:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=215
05/27/2022 13:36:27 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=216
05/27/2022 13:36:33 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.27118371658882673 on epoch=216
05/27/2022 13:36:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.16 on epoch=217
05/27/2022 13:36:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=218
05/27/2022 13:36:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=219
05/27/2022 13:36:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=219
05/27/2022 13:36:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=220
05/27/2022 13:36:52 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.29740908329143617 on epoch=220
05/27/2022 13:36:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=221
05/27/2022 13:36:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=222
05/27/2022 13:37:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=223
05/27/2022 13:37:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=224
05/27/2022 13:37:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=224
05/27/2022 13:37:11 - INFO - __main__ - Global step 2700 Train loss 0.18 Classification-F1 0.21791705877774556 on epoch=224
05/27/2022 13:37:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=225
05/27/2022 13:37:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=226
05/27/2022 13:37:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=227
05/27/2022 13:37:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=228
05/27/2022 13:37:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=229
05/27/2022 13:37:30 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.27833992094861665 on epoch=229
05/27/2022 13:37:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=229
05/27/2022 13:37:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=230
05/27/2022 13:37:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=231
05/27/2022 13:37:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=232
05/27/2022 13:37:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=233
05/27/2022 13:37:49 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.279581174162455 on epoch=233
05/27/2022 13:37:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=234
05/27/2022 13:37:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=234
05/27/2022 13:37:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=235
05/27/2022 13:37:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=236
05/27/2022 13:38:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=237
05/27/2022 13:38:08 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.24967827721932015 on epoch=237
05/27/2022 13:38:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=238
05/27/2022 13:38:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=239
05/27/2022 13:38:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=239
05/27/2022 13:38:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.16 on epoch=240
05/27/2022 13:38:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=241
05/27/2022 13:38:27 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.23256736034641134 on epoch=241
05/27/2022 13:38:30 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=242
05/27/2022 13:38:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=243
05/27/2022 13:38:35 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=244
05/27/2022 13:38:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=244
05/27/2022 13:38:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=245
05/27/2022 13:38:46 - INFO - __main__ - Global step 2950 Train loss 0.13 Classification-F1 0.24247193814099566 on epoch=245
05/27/2022 13:38:49 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.16 on epoch=246
05/27/2022 13:38:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=247
05/27/2022 13:38:54 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=248
05/27/2022 13:38:57 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=249
05/27/2022 13:39:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=249
05/27/2022 13:39:01 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:39:01 - INFO - __main__ - Printing 3 examples
05/27/2022 13:39:01 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:39:01 - INFO - __main__ - ['entailment']
05/27/2022 13:39:01 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:39:01 - INFO - __main__ - ['entailment']
05/27/2022 13:39:01 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:39:01 - INFO - __main__ - ['entailment']
05/27/2022 13:39:01 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:39:01 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:39:02 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:39:02 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:39:02 - INFO - __main__ - Printing 3 examples
05/27/2022 13:39:02 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:39:02 - INFO - __main__ - ['entailment']
05/27/2022 13:39:02 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:39:02 - INFO - __main__ - ['entailment']
05/27/2022 13:39:02 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:39:02 - INFO - __main__ - ['entailment']
05/27/2022 13:39:02 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:39:02 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:39:02 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:39:06 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.23959002140181915 on epoch=249
05/27/2022 13:39:06 - INFO - __main__ - save last model!
05/27/2022 13:39:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 13:39:06 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 13:39:06 - INFO - __main__ - Printing 3 examples
05/27/2022 13:39:06 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 13:39:06 - INFO - __main__ - ['contradiction']
05/27/2022 13:39:06 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 13:39:06 - INFO - __main__ - ['entailment']
05/27/2022 13:39:06 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 13:39:06 - INFO - __main__ - ['contradiction']
05/27/2022 13:39:06 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:39:06 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:39:07 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 13:39:20 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:39:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:39:21 - INFO - __main__ - Starting training!
05/27/2022 13:39:40 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_21_0.3_8_predictions.txt
05/27/2022 13:39:40 - INFO - __main__ - Classification-F1 on test data: 0.0552
05/27/2022 13:39:41 - INFO - __main__ - prefix=anli_64_21, lr=0.3, bsz=8, dev_performance=0.48264768065591596, test_performance=0.05517734652499442
05/27/2022 13:39:41 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.2, bsz=8 ...
05/27/2022 13:39:42 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:39:42 - INFO - __main__ - Printing 3 examples
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:39:42 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:39:42 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:39:42 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:39:42 - INFO - __main__ - Printing 3 examples
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
05/27/2022 13:39:42 - INFO - __main__ - ['entailment']
05/27/2022 13:39:42 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:39:42 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:39:43 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:40:01 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:40:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:40:02 - INFO - __main__ - Starting training!
05/27/2022 13:40:06 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=0
05/27/2022 13:40:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=1
05/27/2022 13:40:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=2
05/27/2022 13:40:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=3
05/27/2022 13:40:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=4
05/27/2022 13:40:22 - INFO - __main__ - Global step 50 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 13:40:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 13:40:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.46 on epoch=4
05/27/2022 13:40:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=5
05/27/2022 13:40:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=6
05/27/2022 13:40:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=7
05/27/2022 13:40:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=8
05/27/2022 13:40:41 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 13:40:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
05/27/2022 13:40:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=9
05/27/2022 13:40:48 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=10
05/27/2022 13:40:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=11
05/27/2022 13:40:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=12
05/27/2022 13:40:59 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 13:41:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.45 on epoch=13
05/27/2022 13:41:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=14
05/27/2022 13:41:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=14
05/27/2022 13:41:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=15
05/27/2022 13:41:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
05/27/2022 13:41:17 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 13:41:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=17
05/27/2022 13:41:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
05/27/2022 13:41:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
05/27/2022 13:41:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/27/2022 13:41:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=20
05/27/2022 13:41:35 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 13:41:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/27/2022 13:41:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
05/27/2022 13:41:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.41 on epoch=23
05/27/2022 13:41:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
05/27/2022 13:41:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
05/27/2022 13:41:53 - INFO - __main__ - Global step 300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 13:41:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=25
05/27/2022 13:41:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=26
05/27/2022 13:42:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/27/2022 13:42:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=28
05/27/2022 13:42:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
05/27/2022 13:42:11 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.17595815389455882 on epoch=29
05/27/2022 13:42:11 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.17595815389455882 on epoch=29, global_step=350
05/27/2022 13:42:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/27/2022 13:42:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=30
05/27/2022 13:42:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/27/2022 13:42:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=32
05/27/2022 13:42:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=33
05/27/2022 13:42:29 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.1775766716943188 on epoch=33
05/27/2022 13:42:29 - INFO - __main__ - Saving model with best Classification-F1: 0.17595815389455882 -> 0.1775766716943188 on epoch=33, global_step=400
05/27/2022 13:42:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=34
05/27/2022 13:42:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=34
05/27/2022 13:42:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=35
05/27/2022 13:42:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=36
05/27/2022 13:42:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
05/27/2022 13:42:47 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.1647058823529412 on epoch=37
05/27/2022 13:42:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=38
05/27/2022 13:42:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
05/27/2022 13:42:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=39
05/27/2022 13:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
05/27/2022 13:43:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=41
05/27/2022 13:43:05 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.1647058823529412 on epoch=41
05/27/2022 13:43:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=42
05/27/2022 13:43:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=43
05/27/2022 13:43:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=44
05/27/2022 13:43:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=44
05/27/2022 13:43:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=45
05/27/2022 13:43:24 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.17595815389455882 on epoch=45
05/27/2022 13:43:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=46
05/27/2022 13:43:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=47
05/27/2022 13:43:32 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=48
05/27/2022 13:43:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
05/27/2022 13:43:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=49
05/27/2022 13:43:42 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.1647058823529412 on epoch=49
05/27/2022 13:43:45 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
05/27/2022 13:43:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
05/27/2022 13:43:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
05/27/2022 13:43:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=53
05/27/2022 13:43:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=54
05/27/2022 13:44:01 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.196742441996719 on epoch=54
05/27/2022 13:44:01 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.196742441996719 on epoch=54, global_step=650
05/27/2022 13:44:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
05/27/2022 13:44:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/27/2022 13:44:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
05/27/2022 13:44:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
05/27/2022 13:44:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/27/2022 13:44:19 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.23177054605626032 on epoch=58
05/27/2022 13:44:19 - INFO - __main__ - Saving model with best Classification-F1: 0.196742441996719 -> 0.23177054605626032 on epoch=58, global_step=700
05/27/2022 13:44:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
05/27/2022 13:44:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/27/2022 13:44:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=60
05/27/2022 13:44:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
05/27/2022 13:44:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=62
05/27/2022 13:44:37 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.23177054605626032 on epoch=62
05/27/2022 13:44:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
05/27/2022 13:44:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
05/27/2022 13:44:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=64
05/27/2022 13:44:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/27/2022 13:44:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
05/27/2022 13:44:56 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.17610891523935002 on epoch=66
05/27/2022 13:44:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
05/27/2022 13:45:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.40 on epoch=68
05/27/2022 13:45:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=69
05/27/2022 13:45:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=69
05/27/2022 13:45:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=70
05/27/2022 13:45:14 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.23396742646922894 on epoch=70
05/27/2022 13:45:14 - INFO - __main__ - Saving model with best Classification-F1: 0.23177054605626032 -> 0.23396742646922894 on epoch=70, global_step=850
05/27/2022 13:45:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
05/27/2022 13:45:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=72
05/27/2022 13:45:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=73
05/27/2022 13:45:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
05/27/2022 13:45:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
05/27/2022 13:45:32 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.27553968170597193 on epoch=74
05/27/2022 13:45:32 - INFO - __main__ - Saving model with best Classification-F1: 0.23396742646922894 -> 0.27553968170597193 on epoch=74, global_step=900
05/27/2022 13:45:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=75
05/27/2022 13:45:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=76
05/27/2022 13:45:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/27/2022 13:45:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.45 on epoch=78
05/27/2022 13:45:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=79
05/27/2022 13:45:50 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.2493989071038251 on epoch=79
05/27/2022 13:45:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=79
05/27/2022 13:45:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=80
05/27/2022 13:45:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=81
05/27/2022 13:46:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
05/27/2022 13:46:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=83
05/27/2022 13:46:09 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.27493529057192495 on epoch=83
05/27/2022 13:46:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.40 on epoch=84
05/27/2022 13:46:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=84
05/27/2022 13:46:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
05/27/2022 13:46:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=86
05/27/2022 13:46:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=87
05/27/2022 13:46:27 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.30440058104759465 on epoch=87
05/27/2022 13:46:27 - INFO - __main__ - Saving model with best Classification-F1: 0.27553968170597193 -> 0.30440058104759465 on epoch=87, global_step=1050
05/27/2022 13:46:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=88
05/27/2022 13:46:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
05/27/2022 13:46:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=89
05/27/2022 13:46:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=90
05/27/2022 13:46:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
05/27/2022 13:46:45 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.23554652213188795 on epoch=91
05/27/2022 13:46:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=92
05/27/2022 13:46:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=93
05/27/2022 13:46:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=94
05/27/2022 13:46:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
05/27/2022 13:46:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=95
05/27/2022 13:47:03 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.31080715974332995 on epoch=95
05/27/2022 13:47:03 - INFO - __main__ - Saving model with best Classification-F1: 0.30440058104759465 -> 0.31080715974332995 on epoch=95, global_step=1150
05/27/2022 13:47:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=96
05/27/2022 13:47:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=97
05/27/2022 13:47:10 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=98
05/27/2022 13:47:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=99
05/27/2022 13:47:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=99
05/27/2022 13:47:20 - INFO - __main__ - Global step 1200 Train loss 0.37 Classification-F1 0.22491039426523296 on epoch=99
05/27/2022 13:47:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=100
05/27/2022 13:47:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
05/27/2022 13:47:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.38 on epoch=102
05/27/2022 13:47:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/27/2022 13:47:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=104
05/27/2022 13:47:38 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.3721965651309757 on epoch=104
05/27/2022 13:47:38 - INFO - __main__ - Saving model with best Classification-F1: 0.31080715974332995 -> 0.3721965651309757 on epoch=104, global_step=1250
05/27/2022 13:47:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.39 on epoch=104
05/27/2022 13:47:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=105
05/27/2022 13:47:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=106
05/27/2022 13:47:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
05/27/2022 13:47:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=108
05/27/2022 13:47:56 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.3064694626345485 on epoch=108
05/27/2022 13:47:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=109
05/27/2022 13:48:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=109
05/27/2022 13:48:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=110
05/27/2022 13:48:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
05/27/2022 13:48:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=112
05/27/2022 13:48:13 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.35095350669818753 on epoch=112
05/27/2022 13:48:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
05/27/2022 13:48:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=114
05/27/2022 13:48:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=114
05/27/2022 13:48:24 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
05/27/2022 13:48:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
05/27/2022 13:48:31 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.29635346986381256 on epoch=116
05/27/2022 13:48:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=117
05/27/2022 13:48:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=118
05/27/2022 13:48:39 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=119
05/27/2022 13:48:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=119
05/27/2022 13:48:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=120
05/27/2022 13:48:48 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.33918128654970764 on epoch=120
05/27/2022 13:48:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=121
05/27/2022 13:48:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=122
05/27/2022 13:48:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=123
05/27/2022 13:48:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=124
05/27/2022 13:49:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=124
05/27/2022 13:49:06 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3153273148833309 on epoch=124
05/27/2022 13:49:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=125
05/27/2022 13:49:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=126
05/27/2022 13:49:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
05/27/2022 13:49:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=128
05/27/2022 13:49:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=129
05/27/2022 13:49:23 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.37337536640672536 on epoch=129
05/27/2022 13:49:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3721965651309757 -> 0.37337536640672536 on epoch=129, global_step=1550
05/27/2022 13:49:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.29 on epoch=129
05/27/2022 13:49:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=130
05/27/2022 13:49:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=131
05/27/2022 13:49:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=132
05/27/2022 13:49:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=133
05/27/2022 13:49:40 - INFO - __main__ - Global step 1600 Train loss 0.37 Classification-F1 0.3538406188163339 on epoch=133
05/27/2022 13:49:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.35 on epoch=134
05/27/2022 13:49:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.38 on epoch=134
05/27/2022 13:49:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=135
05/27/2022 13:49:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=136
05/27/2022 13:49:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=137
05/27/2022 13:49:58 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3451286337884276 on epoch=137
05/27/2022 13:50:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=138
05/27/2022 13:50:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=139
05/27/2022 13:50:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=139
05/27/2022 13:50:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
05/27/2022 13:50:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=141
05/27/2022 13:50:15 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3412441899836858 on epoch=141
05/27/2022 13:50:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=142
05/27/2022 13:50:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.38 on epoch=143
05/27/2022 13:50:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=144
05/27/2022 13:50:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=144
05/27/2022 13:50:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=145
05/27/2022 13:50:33 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3780255384028968 on epoch=145
05/27/2022 13:50:33 - INFO - __main__ - Saving model with best Classification-F1: 0.37337536640672536 -> 0.3780255384028968 on epoch=145, global_step=1750
05/27/2022 13:50:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=146
05/27/2022 13:50:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=147
05/27/2022 13:50:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.38 on epoch=148
05/27/2022 13:50:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=149
05/27/2022 13:50:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=149
05/27/2022 13:50:50 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.3638923013923014 on epoch=149
05/27/2022 13:50:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=150
05/27/2022 13:50:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=151
05/27/2022 13:50:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=152
05/27/2022 13:51:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=153
05/27/2022 13:51:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=154
05/27/2022 13:51:07 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.3891278072912275 on epoch=154
05/27/2022 13:51:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3780255384028968 -> 0.3891278072912275 on epoch=154, global_step=1850
05/27/2022 13:51:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=154
05/27/2022 13:51:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=155
05/27/2022 13:51:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=156
05/27/2022 13:51:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=157
05/27/2022 13:51:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=158
05/27/2022 13:51:24 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.37265388350218115 on epoch=158
05/27/2022 13:51:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=159
05/27/2022 13:51:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.31 on epoch=159
05/27/2022 13:51:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=160
05/27/2022 13:51:35 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=161
05/27/2022 13:51:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=162
05/27/2022 13:51:41 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3560796501972973 on epoch=162
05/27/2022 13:51:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=163
05/27/2022 13:51:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=164
05/27/2022 13:51:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=164
05/27/2022 13:51:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.30 on epoch=165
05/27/2022 13:51:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.29 on epoch=166
05/27/2022 13:51:59 - INFO - __main__ - Global step 2000 Train loss 0.31 Classification-F1 0.3946594816160034 on epoch=166
05/27/2022 13:51:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3891278072912275 -> 0.3946594816160034 on epoch=166, global_step=2000
05/27/2022 13:52:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.31 on epoch=167
05/27/2022 13:52:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=168
05/27/2022 13:52:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.34 on epoch=169
05/27/2022 13:52:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.35 on epoch=169
05/27/2022 13:52:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.38 on epoch=170
05/27/2022 13:52:16 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.37211989172773485 on epoch=170
05/27/2022 13:52:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=171
05/27/2022 13:52:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.34 on epoch=172
05/27/2022 13:52:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=173
05/27/2022 13:52:26 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.33 on epoch=174
05/27/2022 13:52:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.36 on epoch=174
05/27/2022 13:52:33 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.3705291446575851 on epoch=174
05/27/2022 13:52:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.33 on epoch=175
05/27/2022 13:52:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.29 on epoch=176
05/27/2022 13:52:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=177
05/27/2022 13:52:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=178
05/27/2022 13:52:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.34 on epoch=179
05/27/2022 13:52:50 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.3739521270194079 on epoch=179
05/27/2022 13:52:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=179
05/27/2022 13:52:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=180
05/27/2022 13:52:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.31 on epoch=181
05/27/2022 13:53:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.30 on epoch=182
05/27/2022 13:53:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.31 on epoch=183
05/27/2022 13:53:07 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.3530283475704943 on epoch=183
05/27/2022 13:53:10 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=184
05/27/2022 13:53:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=184
05/27/2022 13:53:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.32 on epoch=185
05/27/2022 13:53:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.30 on epoch=186
05/27/2022 13:53:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.26 on epoch=187
05/27/2022 13:53:25 - INFO - __main__ - Global step 2250 Train loss 0.32 Classification-F1 0.3499937717987045 on epoch=187
05/27/2022 13:53:27 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=188
05/27/2022 13:53:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=189
05/27/2022 13:53:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.28 on epoch=189
05/27/2022 13:53:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.32 on epoch=190
05/27/2022 13:53:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.31 on epoch=191
05/27/2022 13:53:42 - INFO - __main__ - Global step 2300 Train loss 0.30 Classification-F1 0.3501537689072993 on epoch=191
05/27/2022 13:53:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.31 on epoch=192
05/27/2022 13:53:47 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=193
05/27/2022 13:53:50 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.31 on epoch=194
05/27/2022 13:53:52 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.34 on epoch=194
05/27/2022 13:53:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.31 on epoch=195
05/27/2022 13:53:59 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.3799787958666463 on epoch=195
05/27/2022 13:54:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=196
05/27/2022 13:54:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=197
05/27/2022 13:54:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=198
05/27/2022 13:54:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.30 on epoch=199
05/27/2022 13:54:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=199
05/27/2022 13:54:16 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.3471483163890691 on epoch=199
05/27/2022 13:54:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=200
05/27/2022 13:54:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=201
05/27/2022 13:54:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.24 on epoch=202
05/27/2022 13:54:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.31 on epoch=203
05/27/2022 13:54:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=204
05/27/2022 13:54:33 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.40704899200044825 on epoch=204
05/27/2022 13:54:33 - INFO - __main__ - Saving model with best Classification-F1: 0.3946594816160034 -> 0.40704899200044825 on epoch=204, global_step=2450
05/27/2022 13:54:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.33 on epoch=204
05/27/2022 13:54:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.22 on epoch=205
05/27/2022 13:54:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=206
05/27/2022 13:54:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=207
05/27/2022 13:54:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.25 on epoch=208
05/27/2022 13:54:51 - INFO - __main__ - Global step 2500 Train loss 0.28 Classification-F1 0.3987045067554456 on epoch=208
05/27/2022 13:54:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=209
05/27/2022 13:54:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.29 on epoch=209
05/27/2022 13:54:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=210
05/27/2022 13:55:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.31 on epoch=211
05/27/2022 13:55:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=212
05/27/2022 13:55:08 - INFO - __main__ - Global step 2550 Train loss 0.30 Classification-F1 0.32918191199924945 on epoch=212
05/27/2022 13:55:11 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=213
05/27/2022 13:55:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.31 on epoch=214
05/27/2022 13:55:16 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=214
05/27/2022 13:55:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=215
05/27/2022 13:55:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.27 on epoch=216
05/27/2022 13:55:26 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.3692176121018626 on epoch=216
05/27/2022 13:55:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.26 on epoch=217
05/27/2022 13:55:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.25 on epoch=218
05/27/2022 13:55:33 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=219
05/27/2022 13:55:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.29 on epoch=219
05/27/2022 13:55:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.34 on epoch=220
05/27/2022 13:55:43 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.37595390062183237 on epoch=220
05/27/2022 13:55:46 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.32 on epoch=221
05/27/2022 13:55:48 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.23 on epoch=222
05/27/2022 13:55:51 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=223
05/27/2022 13:55:53 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=224
05/27/2022 13:55:56 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.25 on epoch=224
05/27/2022 13:56:01 - INFO - __main__ - Global step 2700 Train loss 0.28 Classification-F1 0.39142817166722765 on epoch=224
05/27/2022 13:56:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.27 on epoch=225
05/27/2022 13:56:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.26 on epoch=226
05/27/2022 13:56:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=227
05/27/2022 13:56:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.27 on epoch=228
05/27/2022 13:56:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=229
05/27/2022 13:56:19 - INFO - __main__ - Global step 2750 Train loss 0.27 Classification-F1 0.4207516339869281 on epoch=229
05/27/2022 13:56:19 - INFO - __main__ - Saving model with best Classification-F1: 0.40704899200044825 -> 0.4207516339869281 on epoch=229, global_step=2750
05/27/2022 13:56:21 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.25 on epoch=229
05/27/2022 13:56:24 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.27 on epoch=230
05/27/2022 13:56:26 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=231
05/27/2022 13:56:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.26 on epoch=232
05/27/2022 13:56:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.27 on epoch=233
05/27/2022 13:56:37 - INFO - __main__ - Global step 2800 Train loss 0.25 Classification-F1 0.37565405837608595 on epoch=233
05/27/2022 13:56:39 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.27 on epoch=234
05/27/2022 13:56:42 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=234
05/27/2022 13:56:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=235
05/27/2022 13:56:47 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.27 on epoch=236
05/27/2022 13:56:50 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.27 on epoch=237
05/27/2022 13:56:54 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.3649176461320674 on epoch=237
05/27/2022 13:56:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.26 on epoch=238
05/27/2022 13:57:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.27 on epoch=239
05/27/2022 13:57:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=239
05/27/2022 13:57:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=240
05/27/2022 13:57:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=241
05/27/2022 13:57:12 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.3756662297946702 on epoch=241
05/27/2022 13:57:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=242
05/27/2022 13:57:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=243
05/27/2022 13:57:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=244
05/27/2022 13:57:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=244
05/27/2022 13:57:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.29 on epoch=245
05/27/2022 13:57:31 - INFO - __main__ - Global step 2950 Train loss 0.24 Classification-F1 0.37185682445811913 on epoch=245
05/27/2022 13:57:33 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.27 on epoch=246
05/27/2022 13:57:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.25 on epoch=247
05/27/2022 13:57:38 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.22 on epoch=248
05/27/2022 13:57:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=249
05/27/2022 13:57:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.28 on epoch=249
05/27/2022 13:57:45 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:57:45 - INFO - __main__ - Printing 3 examples
05/27/2022 13:57:45 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 13:57:45 - INFO - __main__ - ['neutral']
05/27/2022 13:57:45 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 13:57:45 - INFO - __main__ - ['neutral']
05/27/2022 13:57:45 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 13:57:45 - INFO - __main__ - ['neutral']
05/27/2022 13:57:45 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:57:45 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:57:46 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:57:46 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:57:46 - INFO - __main__ - Printing 3 examples
05/27/2022 13:57:46 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 13:57:46 - INFO - __main__ - ['neutral']
05/27/2022 13:57:46 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 13:57:46 - INFO - __main__ - ['neutral']
05/27/2022 13:57:46 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 13:57:46 - INFO - __main__ - ['neutral']
05/27/2022 13:57:46 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:57:46 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:57:46 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:57:49 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.3683119912646417 on epoch=249
05/27/2022 13:57:49 - INFO - __main__ - save last model!
05/27/2022 13:57:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 13:57:49 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 13:57:49 - INFO - __main__ - Printing 3 examples
05/27/2022 13:57:49 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 13:57:49 - INFO - __main__ - ['contradiction']
05/27/2022 13:57:49 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 13:57:49 - INFO - __main__ - ['entailment']
05/27/2022 13:57:49 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 13:57:49 - INFO - __main__ - ['contradiction']
05/27/2022 13:57:49 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:57:49 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:57:50 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 13:58:02 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:58:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:58:02 - INFO - __main__ - Starting training!
05/27/2022 13:58:17 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_21_0.2_8_predictions.txt
05/27/2022 13:58:17 - INFO - __main__ - Classification-F1 on test data: 0.2580
05/27/2022 13:58:17 - INFO - __main__ - prefix=anli_64_21, lr=0.2, bsz=8, dev_performance=0.4207516339869281, test_performance=0.25804944376216543
05/27/2022 13:58:17 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.5, bsz=8 ...
05/27/2022 13:58:18 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:58:18 - INFO - __main__ - Printing 3 examples
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:58:18 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:58:18 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 13:58:18 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 13:58:18 - INFO - __main__ - Printing 3 examples
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 13:58:18 - INFO - __main__ - ['neutral']
05/27/2022 13:58:18 - INFO - __main__ - Tokenizing Input ...
05/27/2022 13:58:19 - INFO - __main__ - Tokenizing Output ...
05/27/2022 13:58:19 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 13:58:34 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 13:58:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 13:58:35 - INFO - __main__ - Starting training!
05/27/2022 13:58:39 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=0
05/27/2022 13:58:41 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=1
05/27/2022 13:58:44 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=2
05/27/2022 13:58:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=3
05/27/2022 13:58:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=4
05/27/2022 13:58:54 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 13:58:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 13:58:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.56 on epoch=4
05/27/2022 13:59:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=5
05/27/2022 13:59:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
05/27/2022 13:59:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.55 on epoch=7
05/27/2022 13:59:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=8
05/27/2022 13:59:13 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 13:59:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.51 on epoch=9
05/27/2022 13:59:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=9
05/27/2022 13:59:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=10
05/27/2022 13:59:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
05/27/2022 13:59:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=12
05/27/2022 13:59:31 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 13:59:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=13
05/27/2022 13:59:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=14
05/27/2022 13:59:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=14
05/27/2022 13:59:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.55 on epoch=15
05/27/2022 13:59:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.55 on epoch=16
05/27/2022 13:59:50 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.2256928668786915 on epoch=16
05/27/2022 13:59:50 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2256928668786915 on epoch=16, global_step=200
05/27/2022 13:59:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
05/27/2022 13:59:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
05/27/2022 13:59:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
05/27/2022 14:00:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/27/2022 14:00:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
05/27/2022 14:00:08 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.22849493210938995 on epoch=20
05/27/2022 14:00:08 - INFO - __main__ - Saving model with best Classification-F1: 0.2256928668786915 -> 0.22849493210938995 on epoch=20, global_step=250
05/27/2022 14:00:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/27/2022 14:00:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=22
05/27/2022 14:00:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=23
05/27/2022 14:00:18 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=24
05/27/2022 14:00:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.55 on epoch=24
05/27/2022 14:00:26 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 14:00:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=25
05/27/2022 14:00:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/27/2022 14:00:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/27/2022 14:00:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.43 on epoch=28
05/27/2022 14:00:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
05/27/2022 14:00:45 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 14:00:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
05/27/2022 14:00:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=30
05/27/2022 14:00:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=31
05/27/2022 14:00:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
05/27/2022 14:00:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
05/27/2022 14:01:03 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.22161706318389587 on epoch=33
05/27/2022 14:01:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.55 on epoch=34
05/27/2022 14:01:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=34
05/27/2022 14:01:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=35
05/27/2022 14:01:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=36
05/27/2022 14:01:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
05/27/2022 14:01:21 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.19849369752030363 on epoch=37
05/27/2022 14:01:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=38
05/27/2022 14:01:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=39
05/27/2022 14:01:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=39
05/27/2022 14:01:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=40
05/27/2022 14:01:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=41
05/27/2022 14:01:39 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.3249597423510467 on epoch=41
05/27/2022 14:01:39 - INFO - __main__ - Saving model with best Classification-F1: 0.22849493210938995 -> 0.3249597423510467 on epoch=41, global_step=500
05/27/2022 14:01:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
05/27/2022 14:01:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=43
05/27/2022 14:01:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
05/27/2022 14:01:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=44
05/27/2022 14:01:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
05/27/2022 14:01:57 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.30557211952560787 on epoch=45
05/27/2022 14:01:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=46
05/27/2022 14:02:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
05/27/2022 14:02:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=48
05/27/2022 14:02:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=49
05/27/2022 14:02:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=49
05/27/2022 14:02:15 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.3614841864645479 on epoch=49
05/27/2022 14:02:15 - INFO - __main__ - Saving model with best Classification-F1: 0.3249597423510467 -> 0.3614841864645479 on epoch=49, global_step=600
05/27/2022 14:02:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
05/27/2022 14:02:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
05/27/2022 14:02:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
05/27/2022 14:02:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=53
05/27/2022 14:02:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=54
05/27/2022 14:02:32 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.32585891012857304 on epoch=54
05/27/2022 14:02:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=54
05/27/2022 14:02:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
05/27/2022 14:02:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=56
05/27/2022 14:02:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=57
05/27/2022 14:02:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=58
05/27/2022 14:02:50 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3019420189762057 on epoch=58
05/27/2022 14:02:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=59
05/27/2022 14:02:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/27/2022 14:02:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=60
05/27/2022 14:03:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=61
05/27/2022 14:03:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=62
05/27/2022 14:03:07 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.30272311667660506 on epoch=62
05/27/2022 14:03:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=63
05/27/2022 14:03:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
05/27/2022 14:03:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
05/27/2022 14:03:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=65
05/27/2022 14:03:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=66
05/27/2022 14:03:25 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.4011148272017837 on epoch=66
05/27/2022 14:03:25 - INFO - __main__ - Saving model with best Classification-F1: 0.3614841864645479 -> 0.4011148272017837 on epoch=66, global_step=800
05/27/2022 14:03:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
05/27/2022 14:03:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=68
05/27/2022 14:03:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=69
05/27/2022 14:03:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=69
05/27/2022 14:03:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=70
05/27/2022 14:03:43 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3754623208506704 on epoch=70
05/27/2022 14:03:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=71
05/27/2022 14:03:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=72
05/27/2022 14:03:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=73
05/27/2022 14:03:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=74
05/27/2022 14:03:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=74
05/27/2022 14:04:01 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.36560903662133964 on epoch=74
05/27/2022 14:04:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=75
05/27/2022 14:04:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=76
05/27/2022 14:04:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=77
05/27/2022 14:04:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=78
05/27/2022 14:04:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=79
05/27/2022 14:04:19 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.39943255885284873 on epoch=79
05/27/2022 14:04:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=79
05/27/2022 14:04:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=80
05/27/2022 14:04:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=81
05/27/2022 14:04:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=82
05/27/2022 14:04:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=83
05/27/2022 14:04:38 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.40740740740740744 on epoch=83
05/27/2022 14:04:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4011148272017837 -> 0.40740740740740744 on epoch=83, global_step=1000
05/27/2022 14:04:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=84
05/27/2022 14:04:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=84
05/27/2022 14:04:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=85
05/27/2022 14:04:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=86
05/27/2022 14:04:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.28 on epoch=87
05/27/2022 14:04:56 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.37368038712549945 on epoch=87
05/27/2022 14:04:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=88
05/27/2022 14:05:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=89
05/27/2022 14:05:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=89
05/27/2022 14:05:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=90
05/27/2022 14:05:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=91
05/27/2022 14:05:15 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.44236719861536083 on epoch=91
05/27/2022 14:05:15 - INFO - __main__ - Saving model with best Classification-F1: 0.40740740740740744 -> 0.44236719861536083 on epoch=91, global_step=1100
05/27/2022 14:05:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=92
05/27/2022 14:05:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=93
05/27/2022 14:05:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=94
05/27/2022 14:05:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=94
05/27/2022 14:05:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=95
05/27/2022 14:05:33 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.4424914791105447 on epoch=95
05/27/2022 14:05:33 - INFO - __main__ - Saving model with best Classification-F1: 0.44236719861536083 -> 0.4424914791105447 on epoch=95, global_step=1150
05/27/2022 14:05:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=96
05/27/2022 14:05:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=97
05/27/2022 14:05:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=98
05/27/2022 14:05:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=99
05/27/2022 14:05:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=99
05/27/2022 14:05:52 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.4368607789617143 on epoch=99
05/27/2022 14:05:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=100
05/27/2022 14:05:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=101
05/27/2022 14:06:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=102
05/27/2022 14:06:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=103
05/27/2022 14:06:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=104
05/27/2022 14:06:10 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.4184439734904632 on epoch=104
05/27/2022 14:06:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=104
05/27/2022 14:06:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=105
05/27/2022 14:06:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=106
05/27/2022 14:06:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=107
05/27/2022 14:06:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=108
05/27/2022 14:06:29 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.44564097532257274 on epoch=108
05/27/2022 14:06:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4424914791105447 -> 0.44564097532257274 on epoch=108, global_step=1300
05/27/2022 14:06:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=109
05/27/2022 14:06:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=109
05/27/2022 14:06:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=110
05/27/2022 14:06:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=111
05/27/2022 14:06:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=112
05/27/2022 14:06:47 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.4286693698458404 on epoch=112
05/27/2022 14:06:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=113
05/27/2022 14:06:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=114
05/27/2022 14:06:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=114
05/27/2022 14:06:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=115
05/27/2022 14:07:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=116
05/27/2022 14:07:06 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.4910575261912924 on epoch=116
05/27/2022 14:07:06 - INFO - __main__ - Saving model with best Classification-F1: 0.44564097532257274 -> 0.4910575261912924 on epoch=116, global_step=1400
05/27/2022 14:07:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=117
05/27/2022 14:07:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=118
05/27/2022 14:07:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=119
05/27/2022 14:07:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=119
05/27/2022 14:07:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=120
05/27/2022 14:07:24 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.43454963293024007 on epoch=120
05/27/2022 14:07:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=121
05/27/2022 14:07:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=122
05/27/2022 14:07:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=123
05/27/2022 14:07:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=124
05/27/2022 14:07:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=124
05/27/2022 14:07:43 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.47089223286213716 on epoch=124
05/27/2022 14:07:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=125
05/27/2022 14:07:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=126
05/27/2022 14:07:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=127
05/27/2022 14:07:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=128
05/27/2022 14:07:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=129
05/27/2022 14:08:01 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.4791157169313364 on epoch=129
05/27/2022 14:08:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=129
05/27/2022 14:08:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=130
05/27/2022 14:08:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=131
05/27/2022 14:08:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=132
05/27/2022 14:08:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=133
05/27/2022 14:08:20 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.4328598484848485 on epoch=133
05/27/2022 14:08:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=134
05/27/2022 14:08:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=134
05/27/2022 14:08:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=135
05/27/2022 14:08:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=136
05/27/2022 14:08:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=137
05/27/2022 14:08:38 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.4192891818999101 on epoch=137
05/27/2022 14:08:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=138
05/27/2022 14:08:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=139
05/27/2022 14:08:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=139
05/27/2022 14:08:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=140
05/27/2022 14:08:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=141
05/27/2022 14:08:57 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.4434987978802412 on epoch=141
05/27/2022 14:08:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=142
05/27/2022 14:09:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=143
05/27/2022 14:09:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=144
05/27/2022 14:09:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=144
05/27/2022 14:09:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=145
05/27/2022 14:09:16 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.34431756878259306 on epoch=145
05/27/2022 14:09:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=146
05/27/2022 14:09:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=147
05/27/2022 14:09:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=148
05/27/2022 14:09:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=149
05/27/2022 14:09:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=149
05/27/2022 14:09:34 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.4184444062750487 on epoch=149
05/27/2022 14:09:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=150
05/27/2022 14:09:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=151
05/27/2022 14:09:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=152
05/27/2022 14:09:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=153
05/27/2022 14:09:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=154
05/27/2022 14:09:53 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.22999331256904496 on epoch=154
05/27/2022 14:09:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=154
05/27/2022 14:09:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=155
05/27/2022 14:10:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=156
05/27/2022 14:10:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=157
05/27/2022 14:10:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=158
05/27/2022 14:10:11 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.3318186071039738 on epoch=158
05/27/2022 14:10:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=159
05/27/2022 14:10:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=159
05/27/2022 14:10:19 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=160
05/27/2022 14:10:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=161
05/27/2022 14:10:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=162
05/27/2022 14:10:30 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.3317624601750467 on epoch=162
05/27/2022 14:10:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=163
05/27/2022 14:10:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=164
05/27/2022 14:10:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=164
05/27/2022 14:10:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=165
05/27/2022 14:10:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=166
05/27/2022 14:10:48 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.36377579784556535 on epoch=166
05/27/2022 14:10:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=167
05/27/2022 14:10:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=168
05/27/2022 14:10:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=169
05/27/2022 14:10:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=169
05/27/2022 14:11:01 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.15 on epoch=170
05/27/2022 14:11:07 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.48437498407898044 on epoch=170
05/27/2022 14:11:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=171
05/27/2022 14:11:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=172
05/27/2022 14:11:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=173
05/27/2022 14:11:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=174
05/27/2022 14:11:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=174
05/27/2022 14:11:26 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.25461945461945457 on epoch=174
05/27/2022 14:11:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=175
05/27/2022 14:11:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=176
05/27/2022 14:11:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=177
05/27/2022 14:11:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=178
05/27/2022 14:11:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=179
05/27/2022 14:11:45 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.25772145417306713 on epoch=179
05/27/2022 14:11:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=179
05/27/2022 14:11:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=180
05/27/2022 14:11:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=181
05/27/2022 14:11:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=182
05/27/2022 14:11:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=183
05/27/2022 14:12:03 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.27345974683174035 on epoch=183
05/27/2022 14:12:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=184
05/27/2022 14:12:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=184
05/27/2022 14:12:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=185
05/27/2022 14:12:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=186
05/27/2022 14:12:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=187
05/27/2022 14:12:22 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.17190929579304937 on epoch=187
05/27/2022 14:12:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=188
05/27/2022 14:12:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=189
05/27/2022 14:12:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=189
05/27/2022 14:12:33 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=190
05/27/2022 14:12:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=191
05/27/2022 14:12:41 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.24937908496732025 on epoch=191
05/27/2022 14:12:44 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=192
05/27/2022 14:12:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=193
05/27/2022 14:12:49 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=194
05/27/2022 14:12:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=194
05/27/2022 14:12:54 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=195
05/27/2022 14:13:00 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.25990705319118435 on epoch=195
05/27/2022 14:13:02 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=196
05/27/2022 14:13:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=197
05/27/2022 14:13:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=198
05/27/2022 14:13:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=199
05/27/2022 14:13:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=199
05/27/2022 14:13:18 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.10265399930546233 on epoch=199
05/27/2022 14:13:21 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=200
05/27/2022 14:13:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=201
05/27/2022 14:13:26 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=202
05/27/2022 14:13:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=203
05/27/2022 14:13:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=204
05/27/2022 14:13:37 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.13161544610931314 on epoch=204
05/27/2022 14:13:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=204
05/27/2022 14:13:42 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=205
05/27/2022 14:13:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=206
05/27/2022 14:13:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=207
05/27/2022 14:13:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=208
05/27/2022 14:13:56 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.13469214694569795 on epoch=208
05/27/2022 14:13:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=209
05/27/2022 14:14:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=209
05/27/2022 14:14:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=210
05/27/2022 14:14:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=211
05/27/2022 14:14:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=212
05/27/2022 14:14:15 - INFO - __main__ - Global step 2550 Train loss 0.08 Classification-F1 0.0957433650442769 on epoch=212
05/27/2022 14:14:17 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=213
05/27/2022 14:14:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=214
05/27/2022 14:14:22 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=214
05/27/2022 14:14:25 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=215
05/27/2022 14:14:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=216
05/27/2022 14:14:33 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.10925673042376854 on epoch=216
05/27/2022 14:14:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=217
05/27/2022 14:14:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=218
05/27/2022 14:14:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=219
05/27/2022 14:14:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=219
05/27/2022 14:14:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=220
05/27/2022 14:14:52 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.14775037219335826 on epoch=220
05/27/2022 14:14:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=221
05/27/2022 14:14:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=222
05/27/2022 14:15:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=223
05/27/2022 14:15:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=224
05/27/2022 14:15:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=224
05/27/2022 14:15:10 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.07633933314445585 on epoch=224
05/27/2022 14:15:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=225
05/27/2022 14:15:16 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=226
05/27/2022 14:15:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=227
05/27/2022 14:15:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=228
05/27/2022 14:15:24 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=229
05/27/2022 14:15:29 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.0692781954887218 on epoch=229
05/27/2022 14:15:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=229
05/27/2022 14:15:35 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.07 on epoch=230
05/27/2022 14:15:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=231
05/27/2022 14:15:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=232
05/27/2022 14:15:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=233
05/27/2022 14:15:48 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.08706573659070345 on epoch=233
05/27/2022 14:15:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=234
05/27/2022 14:15:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=234
05/27/2022 14:15:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=235
05/27/2022 14:15:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=236
05/27/2022 14:16:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=237
05/27/2022 14:16:07 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.0811226251087835 on epoch=237
05/27/2022 14:16:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=238
05/27/2022 14:16:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=239
05/27/2022 14:16:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=239
05/27/2022 14:16:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=240
05/27/2022 14:16:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=241
05/27/2022 14:16:25 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.10309250136836344 on epoch=241
05/27/2022 14:16:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=242
05/27/2022 14:16:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=243
05/27/2022 14:16:33 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.22 on epoch=244
05/27/2022 14:16:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=244
05/27/2022 14:16:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=245
05/27/2022 14:16:44 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.10204720185946717 on epoch=245
05/27/2022 14:16:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=246
05/27/2022 14:16:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=247
05/27/2022 14:16:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=248
05/27/2022 14:16:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=249
05/27/2022 14:16:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=249
05/27/2022 14:16:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:16:59 - INFO - __main__ - Printing 3 examples
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:16:59 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:16:59 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:16:59 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:16:59 - INFO - __main__ - Printing 3 examples
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:16:59 - INFO - __main__ - ['neutral']
05/27/2022 14:16:59 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:16:59 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:17:00 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:17:03 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.10988033463941543 on epoch=249
05/27/2022 14:17:03 - INFO - __main__ - save last model!
05/27/2022 14:17:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 14:17:03 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 14:17:03 - INFO - __main__ - Printing 3 examples
05/27/2022 14:17:03 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 14:17:03 - INFO - __main__ - ['contradiction']
05/27/2022 14:17:03 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 14:17:03 - INFO - __main__ - ['entailment']
05/27/2022 14:17:03 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 14:17:03 - INFO - __main__ - ['contradiction']
05/27/2022 14:17:03 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:17:04 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:17:05 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 14:17:15 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:17:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:17:16 - INFO - __main__ - Starting training!
05/27/2022 14:17:35 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_42_0.5_8_predictions.txt
05/27/2022 14:17:35 - INFO - __main__ - Classification-F1 on test data: 0.0297
05/27/2022 14:17:35 - INFO - __main__ - prefix=anli_64_42, lr=0.5, bsz=8, dev_performance=0.4910575261912924, test_performance=0.029673701868025374
05/27/2022 14:17:35 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.4, bsz=8 ...
05/27/2022 14:17:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:17:36 - INFO - __main__ - Printing 3 examples
05/27/2022 14:17:36 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:17:36 - INFO - __main__ - ['neutral']
05/27/2022 14:17:36 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:17:36 - INFO - __main__ - ['neutral']
05/27/2022 14:17:36 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:17:36 - INFO - __main__ - ['neutral']
05/27/2022 14:17:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:17:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:17:37 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:17:37 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:17:37 - INFO - __main__ - Printing 3 examples
05/27/2022 14:17:37 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:17:37 - INFO - __main__ - ['neutral']
05/27/2022 14:17:37 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:17:37 - INFO - __main__ - ['neutral']
05/27/2022 14:17:37 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:17:37 - INFO - __main__ - ['neutral']
05/27/2022 14:17:37 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:17:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:17:37 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:17:52 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:17:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:17:53 - INFO - __main__ - Starting training!
05/27/2022 14:17:57 - INFO - __main__ - Step 10 Global step 10 Train loss 0.60 on epoch=0
05/27/2022 14:17:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=1
05/27/2022 14:18:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=2
05/27/2022 14:18:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=3
05/27/2022 14:18:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=4
05/27/2022 14:18:13 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 14:18:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 14:18:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=4
05/27/2022 14:18:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.51 on epoch=5
05/27/2022 14:18:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=6
05/27/2022 14:18:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=7
05/27/2022 14:18:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=8
05/27/2022 14:18:31 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 14:18:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=9
05/27/2022 14:18:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=9
05/27/2022 14:18:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.51 on epoch=10
05/27/2022 14:18:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=11
05/27/2022 14:18:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=12
05/27/2022 14:18:50 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 14:18:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=13
05/27/2022 14:18:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
05/27/2022 14:18:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.55 on epoch=14
05/27/2022 14:19:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=15
05/27/2022 14:19:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=16
05/27/2022 14:19:08 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.21568734296007028 on epoch=16
05/27/2022 14:19:08 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21568734296007028 on epoch=16, global_step=200
05/27/2022 14:19:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
05/27/2022 14:19:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=18
05/27/2022 14:19:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=19
05/27/2022 14:19:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=19
05/27/2022 14:19:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=20
05/27/2022 14:19:27 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.1881810228266921 on epoch=20
05/27/2022 14:19:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/27/2022 14:19:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.54 on epoch=22
05/27/2022 14:19:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=23
05/27/2022 14:19:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=24
05/27/2022 14:19:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=24
05/27/2022 14:19:45 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.19961519961519958 on epoch=24
05/27/2022 14:19:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=25
05/27/2022 14:19:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
05/27/2022 14:19:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=27
05/27/2022 14:19:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/27/2022 14:19:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=29
05/27/2022 14:20:04 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 14:20:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=29
05/27/2022 14:20:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=30
05/27/2022 14:20:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
05/27/2022 14:20:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
05/27/2022 14:20:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=33
05/27/2022 14:20:22 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.24681361523466785 on epoch=33
05/27/2022 14:20:22 - INFO - __main__ - Saving model with best Classification-F1: 0.21568734296007028 -> 0.24681361523466785 on epoch=33, global_step=400
05/27/2022 14:20:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=34
05/27/2022 14:20:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=34
05/27/2022 14:20:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=35
05/27/2022 14:20:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
05/27/2022 14:20:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=37
05/27/2022 14:20:40 - INFO - __main__ - Global step 450 Train loss 0.50 Classification-F1 0.2085278555866791 on epoch=37
05/27/2022 14:20:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=38
05/27/2022 14:20:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=39
05/27/2022 14:20:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
05/27/2022 14:20:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=40
05/27/2022 14:20:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
05/27/2022 14:20:59 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.284704641350211 on epoch=41
05/27/2022 14:20:59 - INFO - __main__ - Saving model with best Classification-F1: 0.24681361523466785 -> 0.284704641350211 on epoch=41, global_step=500
05/27/2022 14:21:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=42
05/27/2022 14:21:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
05/27/2022 14:21:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=44
05/27/2022 14:21:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=44
05/27/2022 14:21:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=45
05/27/2022 14:21:17 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.3260386923991991 on epoch=45
05/27/2022 14:21:17 - INFO - __main__ - Saving model with best Classification-F1: 0.284704641350211 -> 0.3260386923991991 on epoch=45, global_step=550
05/27/2022 14:21:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
05/27/2022 14:21:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=47
05/27/2022 14:21:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=48
05/27/2022 14:21:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=49
05/27/2022 14:21:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=49
05/27/2022 14:21:36 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.3777873400225582 on epoch=49
05/27/2022 14:21:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3260386923991991 -> 0.3777873400225582 on epoch=49, global_step=600
05/27/2022 14:21:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
05/27/2022 14:21:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/27/2022 14:21:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
05/27/2022 14:21:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=53
05/27/2022 14:21:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
05/27/2022 14:21:54 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.310061877858488 on epoch=54
05/27/2022 14:21:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=54
05/27/2022 14:21:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/27/2022 14:22:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=56
05/27/2022 14:22:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=57
05/27/2022 14:22:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/27/2022 14:22:12 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.310061877858488 on epoch=58
05/27/2022 14:22:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=59
05/27/2022 14:22:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
05/27/2022 14:22:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=60
05/27/2022 14:22:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=61
05/27/2022 14:22:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=62
05/27/2022 14:22:30 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.2805555555555556 on epoch=62
05/27/2022 14:22:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=63
05/27/2022 14:22:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=64
05/27/2022 14:22:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.36 on epoch=64
05/27/2022 14:22:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=65
05/27/2022 14:22:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
05/27/2022 14:22:48 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.35095350669818753 on epoch=66
05/27/2022 14:22:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
05/27/2022 14:22:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
05/27/2022 14:22:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=69
05/27/2022 14:22:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=69
05/27/2022 14:23:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
05/27/2022 14:23:06 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.3650175221603793 on epoch=70
05/27/2022 14:23:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=71
05/27/2022 14:23:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=72
05/27/2022 14:23:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=73
05/27/2022 14:23:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=74
05/27/2022 14:23:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=74
05/27/2022 14:23:24 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.35747508305647835 on epoch=74
05/27/2022 14:23:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=75
05/27/2022 14:23:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.36 on epoch=76
05/27/2022 14:23:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=77
05/27/2022 14:23:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=78
05/27/2022 14:23:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=79
05/27/2022 14:23:41 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.35016835016835013 on epoch=79
05/27/2022 14:23:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=79
05/27/2022 14:23:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=80
05/27/2022 14:23:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=81
05/27/2022 14:23:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=82
05/27/2022 14:23:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=83
05/27/2022 14:23:59 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.36859157613874594 on epoch=83
05/27/2022 14:24:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/27/2022 14:24:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=84
05/27/2022 14:24:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=85
05/27/2022 14:24:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=86
05/27/2022 14:24:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=87
05/27/2022 14:24:17 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.3259273206552438 on epoch=87
05/27/2022 14:24:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=88
05/27/2022 14:24:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=89
05/27/2022 14:24:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=89
05/27/2022 14:24:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.34 on epoch=90
05/27/2022 14:24:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=91
05/27/2022 14:24:35 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.4153204451711914 on epoch=91
05/27/2022 14:24:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3777873400225582 -> 0.4153204451711914 on epoch=91, global_step=1100
05/27/2022 14:24:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=92
05/27/2022 14:24:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=93
05/27/2022 14:24:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=94
05/27/2022 14:24:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=94
05/27/2022 14:24:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=95
05/27/2022 14:24:52 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.37810263131364047 on epoch=95
05/27/2022 14:24:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=96
05/27/2022 14:24:58 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=97
05/27/2022 14:25:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=98
05/27/2022 14:25:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=99
05/27/2022 14:25:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=99
05/27/2022 14:25:10 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.4141841792503355 on epoch=99
05/27/2022 14:25:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=100
05/27/2022 14:25:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=101
05/27/2022 14:25:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=102
05/27/2022 14:25:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=103
05/27/2022 14:25:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=104
05/27/2022 14:25:29 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.4523823176229594 on epoch=104
05/27/2022 14:25:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4153204451711914 -> 0.4523823176229594 on epoch=104, global_step=1250
05/27/2022 14:25:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=104
05/27/2022 14:25:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=105
05/27/2022 14:25:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=106
05/27/2022 14:25:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=107
05/27/2022 14:25:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=108
05/27/2022 14:25:47 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.45105458148936406 on epoch=108
05/27/2022 14:25:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=109
05/27/2022 14:25:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=109
05/27/2022 14:25:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=110
05/27/2022 14:25:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=111
05/27/2022 14:26:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=112
05/27/2022 14:26:05 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.4180641448988372 on epoch=112
05/27/2022 14:26:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=113
05/27/2022 14:26:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=114
05/27/2022 14:26:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=114
05/27/2022 14:26:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=115
05/27/2022 14:26:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=116
05/27/2022 14:26:24 - INFO - __main__ - Global step 1400 Train loss 0.30 Classification-F1 0.47828282828282837 on epoch=116
05/27/2022 14:26:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4523823176229594 -> 0.47828282828282837 on epoch=116, global_step=1400
05/27/2022 14:26:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=117
05/27/2022 14:26:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=118
05/27/2022 14:26:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=119
05/27/2022 14:26:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=119
05/27/2022 14:26:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=120
05/27/2022 14:26:42 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.4833223388726701 on epoch=120
05/27/2022 14:26:42 - INFO - __main__ - Saving model with best Classification-F1: 0.47828282828282837 -> 0.4833223388726701 on epoch=120, global_step=1450
05/27/2022 14:26:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=121
05/27/2022 14:26:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.30 on epoch=122
05/27/2022 14:26:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=123
05/27/2022 14:26:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=124
05/27/2022 14:26:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=124
05/27/2022 14:27:01 - INFO - __main__ - Global step 1500 Train loss 0.27 Classification-F1 0.4795144112217283 on epoch=124
05/27/2022 14:27:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=125
05/27/2022 14:27:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=126
05/27/2022 14:27:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=127
05/27/2022 14:27:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.28 on epoch=128
05/27/2022 14:27:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=129
05/27/2022 14:27:19 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.44769477748030884 on epoch=129
05/27/2022 14:27:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=129
05/27/2022 14:27:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.29 on epoch=130
05/27/2022 14:27:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=131
05/27/2022 14:27:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=132
05/27/2022 14:27:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=133
05/27/2022 14:27:37 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.5075001558527273 on epoch=133
05/27/2022 14:27:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4833223388726701 -> 0.5075001558527273 on epoch=133, global_step=1600
05/27/2022 14:27:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=134
05/27/2022 14:27:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=134
05/27/2022 14:27:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=135
05/27/2022 14:27:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=136
05/27/2022 14:27:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=137
05/27/2022 14:27:56 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.44772763694982043 on epoch=137
05/27/2022 14:27:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=138
05/27/2022 14:28:01 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=139
05/27/2022 14:28:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=139
05/27/2022 14:28:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=140
05/27/2022 14:28:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=141
05/27/2022 14:28:14 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.46591361721470115 on epoch=141
05/27/2022 14:28:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=142
05/27/2022 14:28:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=143
05/27/2022 14:28:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=144
05/27/2022 14:28:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=144
05/27/2022 14:28:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=145
05/27/2022 14:28:32 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.4663233236867074 on epoch=145
05/27/2022 14:28:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=146
05/27/2022 14:28:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=147
05/27/2022 14:28:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=148
05/27/2022 14:28:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=149
05/27/2022 14:28:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=149
05/27/2022 14:28:51 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.39060035056967574 on epoch=149
05/27/2022 14:28:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=150
05/27/2022 14:28:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=151
05/27/2022 14:28:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=152
05/27/2022 14:29:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=153
05/27/2022 14:29:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=154
05/27/2022 14:29:10 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.37486161169670185 on epoch=154
05/27/2022 14:29:12 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=154
05/27/2022 14:29:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=155
05/27/2022 14:29:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=156
05/27/2022 14:29:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=157
05/27/2022 14:29:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=158
05/27/2022 14:29:28 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.2473956972389165 on epoch=158
05/27/2022 14:29:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=159
05/27/2022 14:29:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=159
05/27/2022 14:29:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=160
05/27/2022 14:29:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=161
05/27/2022 14:29:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=162
05/27/2022 14:29:48 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.5253694300751441 on epoch=162
05/27/2022 14:29:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5075001558527273 -> 0.5253694300751441 on epoch=162, global_step=1950
05/27/2022 14:29:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=163
05/27/2022 14:29:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=164
05/27/2022 14:29:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=164
05/27/2022 14:29:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=165
05/27/2022 14:30:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=166
05/27/2022 14:30:07 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.40186229743038765 on epoch=166
05/27/2022 14:30:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=167
05/27/2022 14:30:12 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=168
05/27/2022 14:30:14 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=169
05/27/2022 14:30:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=169
05/27/2022 14:30:20 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=170
05/27/2022 14:30:26 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.2681307712004097 on epoch=170
05/27/2022 14:30:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.24 on epoch=171
05/27/2022 14:30:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=172
05/27/2022 14:30:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=173
05/27/2022 14:30:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=174
05/27/2022 14:30:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=174
05/27/2022 14:30:45 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.40053258826917615 on epoch=174
05/27/2022 14:30:47 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=175
05/27/2022 14:30:50 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=176
05/27/2022 14:30:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=177
05/27/2022 14:30:55 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=178
05/27/2022 14:30:58 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=179
05/27/2022 14:31:04 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.49399882730496963 on epoch=179
05/27/2022 14:31:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.17 on epoch=179
05/27/2022 14:31:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=180
05/27/2022 14:31:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=181
05/27/2022 14:31:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=182
05/27/2022 14:31:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=183
05/27/2022 14:31:23 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.5381801843544435 on epoch=183
05/27/2022 14:31:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5253694300751441 -> 0.5381801843544435 on epoch=183, global_step=2200
05/27/2022 14:31:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=184
05/27/2022 14:31:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=184
05/27/2022 14:31:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=185
05/27/2022 14:31:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.19 on epoch=186
05/27/2022 14:31:36 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=187
05/27/2022 14:31:42 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.38347975216429775 on epoch=187
05/27/2022 14:31:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=188
05/27/2022 14:31:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=189
05/27/2022 14:31:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=189
05/27/2022 14:31:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=190
05/27/2022 14:31:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=191
05/27/2022 14:32:01 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.23029952828389127 on epoch=191
05/27/2022 14:32:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=192
05/27/2022 14:32:06 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=193
05/27/2022 14:32:08 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=194
05/27/2022 14:32:11 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=194
05/27/2022 14:32:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=195
05/27/2022 14:32:20 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.1928596818626144 on epoch=195
05/27/2022 14:32:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=196
05/27/2022 14:32:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.09 on epoch=197
05/27/2022 14:32:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=198
05/27/2022 14:32:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=199
05/27/2022 14:32:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=199
05/27/2022 14:32:39 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.26635555951400325 on epoch=199
05/27/2022 14:32:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=200
05/27/2022 14:32:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=201
05/27/2022 14:32:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=202
05/27/2022 14:32:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=203
05/27/2022 14:32:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=204
05/27/2022 14:32:58 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.2647755753018911 on epoch=204
05/27/2022 14:33:01 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=204
05/27/2022 14:33:04 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=205
05/27/2022 14:33:06 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=206
05/27/2022 14:33:09 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=207
05/27/2022 14:33:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=208
05/27/2022 14:33:18 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.2636846405228758 on epoch=208
05/27/2022 14:33:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=209
05/27/2022 14:33:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=209
05/27/2022 14:33:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=210
05/27/2022 14:33:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=211
05/27/2022 14:33:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=212
05/27/2022 14:33:37 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.39930090592722733 on epoch=212
05/27/2022 14:33:39 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=213
05/27/2022 14:33:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=214
05/27/2022 14:33:45 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=214
05/27/2022 14:33:47 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=215
05/27/2022 14:33:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=216
05/27/2022 14:33:56 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.21613714752400884 on epoch=216
05/27/2022 14:33:58 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=217
05/27/2022 14:34:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=218
05/27/2022 14:34:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=219
05/27/2022 14:34:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=219
05/27/2022 14:34:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=220
05/27/2022 14:34:15 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.2595329787884345 on epoch=220
05/27/2022 14:34:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=221
05/27/2022 14:34:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=222
05/27/2022 14:34:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=223
05/27/2022 14:34:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=224
05/27/2022 14:34:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=224
05/27/2022 14:34:34 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.32219848306804827 on epoch=224
05/27/2022 14:34:37 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=225
05/27/2022 14:34:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=226
05/27/2022 14:34:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=227
05/27/2022 14:34:45 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=228
05/27/2022 14:34:47 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=229
05/27/2022 14:34:53 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.40220682595749724 on epoch=229
05/27/2022 14:34:56 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=229
05/27/2022 14:34:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=230
05/27/2022 14:35:01 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=231
05/27/2022 14:35:04 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=232
05/27/2022 14:35:06 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=233
05/27/2022 14:35:12 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.197867683036749 on epoch=233
05/27/2022 14:35:15 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=234
05/27/2022 14:35:17 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=234
05/27/2022 14:35:20 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=235
05/27/2022 14:35:22 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=236
05/27/2022 14:35:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=237
05/27/2022 14:35:31 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.20743791128252576 on epoch=237
05/27/2022 14:35:33 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.15 on epoch=238
05/27/2022 14:35:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=239
05/27/2022 14:35:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=239
05/27/2022 14:35:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.06 on epoch=240
05/27/2022 14:35:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=241
05/27/2022 14:35:50 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.2692883872292247 on epoch=241
05/27/2022 14:35:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=242
05/27/2022 14:35:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=243
05/27/2022 14:35:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=244
05/27/2022 14:36:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=244
05/27/2022 14:36:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=245
05/27/2022 14:36:10 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.26074109910587895 on epoch=245
05/27/2022 14:36:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=246
05/27/2022 14:36:15 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=247
05/27/2022 14:36:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=248
05/27/2022 14:36:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=249
05/27/2022 14:36:23 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=249
05/27/2022 14:36:25 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:36:25 - INFO - __main__ - Printing 3 examples
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:36:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:36:25 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:36:25 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:36:25 - INFO - __main__ - Printing 3 examples
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:36:25 - INFO - __main__ - ['neutral']
05/27/2022 14:36:25 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:36:25 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:36:25 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:36:29 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.167899362059946 on epoch=249
05/27/2022 14:36:29 - INFO - __main__ - save last model!
05/27/2022 14:36:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 14:36:29 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 14:36:29 - INFO - __main__ - Printing 3 examples
05/27/2022 14:36:29 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 14:36:29 - INFO - __main__ - ['contradiction']
05/27/2022 14:36:29 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 14:36:29 - INFO - __main__ - ['entailment']
05/27/2022 14:36:29 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 14:36:29 - INFO - __main__ - ['contradiction']
05/27/2022 14:36:29 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:36:30 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:36:31 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 14:36:43 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:36:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:36:44 - INFO - __main__ - Starting training!
05/27/2022 14:37:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_42_0.4_8_predictions.txt
05/27/2022 14:37:03 - INFO - __main__ - Classification-F1 on test data: 0.0476
05/27/2022 14:37:04 - INFO - __main__ - prefix=anli_64_42, lr=0.4, bsz=8, dev_performance=0.5381801843544435, test_performance=0.04761172855123876
05/27/2022 14:37:04 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.3, bsz=8 ...
05/27/2022 14:37:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:37:05 - INFO - __main__ - Printing 3 examples
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:37:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:37:05 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:37:05 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:37:05 - INFO - __main__ - Printing 3 examples
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:37:05 - INFO - __main__ - ['neutral']
05/27/2022 14:37:05 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:37:05 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:37:05 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:37:24 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:37:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:37:25 - INFO - __main__ - Starting training!
05/27/2022 14:37:28 - INFO - __main__ - Step 10 Global step 10 Train loss 0.60 on epoch=0
05/27/2022 14:37:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=1
05/27/2022 14:37:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=2
05/27/2022 14:37:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
05/27/2022 14:37:39 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=4
05/27/2022 14:37:44 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 14:37:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 14:37:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=4
05/27/2022 14:37:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=5
05/27/2022 14:37:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.58 on epoch=6
05/27/2022 14:37:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.57 on epoch=7
05/27/2022 14:37:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=8
05/27/2022 14:38:02 - INFO - __main__ - Global step 100 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 14:38:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
05/27/2022 14:38:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=9
05/27/2022 14:38:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.55 on epoch=10
05/27/2022 14:38:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.52 on epoch=11
05/27/2022 14:38:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
05/27/2022 14:38:20 - INFO - __main__ - Global step 150 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 14:38:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=13
05/27/2022 14:38:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
05/27/2022 14:38:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
05/27/2022 14:38:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=15
05/27/2022 14:38:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=16
05/27/2022 14:38:39 - INFO - __main__ - Global step 200 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 14:38:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.50 on epoch=17
05/27/2022 14:38:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=18
05/27/2022 14:38:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=19
05/27/2022 14:38:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
05/27/2022 14:38:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
05/27/2022 14:38:57 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.1881810228266921 on epoch=20
05/27/2022 14:38:57 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1881810228266921 on epoch=20, global_step=250
05/27/2022 14:39:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
05/27/2022 14:39:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.53 on epoch=22
05/27/2022 14:39:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
05/27/2022 14:39:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
05/27/2022 14:39:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
05/27/2022 14:39:16 - INFO - __main__ - Global step 300 Train loss 0.51 Classification-F1 0.1881810228266921 on epoch=24
05/27/2022 14:39:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=25
05/27/2022 14:39:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=26
05/27/2022 14:39:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=27
05/27/2022 14:39:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=28
05/27/2022 14:39:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.59 on epoch=29
05/27/2022 14:39:35 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.2085278555866791 on epoch=29
05/27/2022 14:39:35 - INFO - __main__ - Saving model with best Classification-F1: 0.1881810228266921 -> 0.2085278555866791 on epoch=29, global_step=350
05/27/2022 14:39:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
05/27/2022 14:39:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
05/27/2022 14:39:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=31
05/27/2022 14:39:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
05/27/2022 14:39:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
05/27/2022 14:39:53 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.17823541288108216 on epoch=33
05/27/2022 14:39:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=34
05/27/2022 14:39:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
05/27/2022 14:40:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=35
05/27/2022 14:40:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=36
05/27/2022 14:40:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=37
05/27/2022 14:40:12 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.1775766716943188 on epoch=37
05/27/2022 14:40:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=38
05/27/2022 14:40:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=39
05/27/2022 14:40:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=39
05/27/2022 14:40:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
05/27/2022 14:40:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=41
05/27/2022 14:40:30 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.2276901031355469 on epoch=41
05/27/2022 14:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.2276901031355469 on epoch=41, global_step=500
05/27/2022 14:40:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=42
05/27/2022 14:40:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=43
05/27/2022 14:40:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=44
05/27/2022 14:40:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=44
05/27/2022 14:40:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=45
05/27/2022 14:40:49 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.2457989018911874 on epoch=45
05/27/2022 14:40:49 - INFO - __main__ - Saving model with best Classification-F1: 0.2276901031355469 -> 0.2457989018911874 on epoch=45, global_step=550
05/27/2022 14:40:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
05/27/2022 14:40:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=47
05/27/2022 14:40:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/27/2022 14:40:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=49
05/27/2022 14:41:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=49
05/27/2022 14:41:07 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.30456023227107565 on epoch=49
05/27/2022 14:41:07 - INFO - __main__ - Saving model with best Classification-F1: 0.2457989018911874 -> 0.30456023227107565 on epoch=49, global_step=600
05/27/2022 14:41:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=50
05/27/2022 14:41:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=51
05/27/2022 14:41:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=52
05/27/2022 14:41:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=53
05/27/2022 14:41:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=54
05/27/2022 14:41:26 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.27611782157236703 on epoch=54
05/27/2022 14:41:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=54
05/27/2022 14:41:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=55
05/27/2022 14:41:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=56
05/27/2022 14:41:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
05/27/2022 14:41:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/27/2022 14:41:44 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.2545228154984253 on epoch=58
05/27/2022 14:41:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=59
05/27/2022 14:41:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.48 on epoch=59
05/27/2022 14:41:52 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
05/27/2022 14:41:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
05/27/2022 14:41:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
05/27/2022 14:42:03 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.23685782556750298 on epoch=62
05/27/2022 14:42:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=63
05/27/2022 14:42:08 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=64
05/27/2022 14:42:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=64
05/27/2022 14:42:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=65
05/27/2022 14:42:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=66
05/27/2022 14:42:21 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.32585891012857304 on epoch=66
05/27/2022 14:42:21 - INFO - __main__ - Saving model with best Classification-F1: 0.30456023227107565 -> 0.32585891012857304 on epoch=66, global_step=800
05/27/2022 14:42:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=67
05/27/2022 14:42:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=68
05/27/2022 14:42:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=69
05/27/2022 14:42:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=69
05/27/2022 14:42:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=70
05/27/2022 14:42:39 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.2846894855503177 on epoch=70
05/27/2022 14:42:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=71
05/27/2022 14:42:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=72
05/27/2022 14:42:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=73
05/27/2022 14:42:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=74
05/27/2022 14:42:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=74
05/27/2022 14:42:57 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3330195192368041 on epoch=74
05/27/2022 14:42:57 - INFO - __main__ - Saving model with best Classification-F1: 0.32585891012857304 -> 0.3330195192368041 on epoch=74, global_step=900
05/27/2022 14:43:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=75
05/27/2022 14:43:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
05/27/2022 14:43:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
05/27/2022 14:43:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=78
05/27/2022 14:43:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
05/27/2022 14:43:15 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.319226750261233 on epoch=79
05/27/2022 14:43:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=79
05/27/2022 14:43:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=80
05/27/2022 14:43:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.36 on epoch=81
05/27/2022 14:43:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=82
05/27/2022 14:43:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=83
05/27/2022 14:43:33 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.3285186429291233 on epoch=83
05/27/2022 14:43:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=84
05/27/2022 14:43:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=84
05/27/2022 14:43:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=85
05/27/2022 14:43:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.38 on epoch=86
05/27/2022 14:43:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
05/27/2022 14:43:51 - INFO - __main__ - Global step 1050 Train loss 0.40 Classification-F1 0.3063829787234043 on epoch=87
05/27/2022 14:43:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=88
05/27/2022 14:43:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
05/27/2022 14:43:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=89
05/27/2022 14:44:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=90
05/27/2022 14:44:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=91
05/27/2022 14:44:08 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3348588863463005 on epoch=91
05/27/2022 14:44:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3330195192368041 -> 0.3348588863463005 on epoch=91, global_step=1100
05/27/2022 14:44:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=92
05/27/2022 14:44:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=93
05/27/2022 14:44:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=94
05/27/2022 14:44:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=94
05/27/2022 14:44:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=95
05/27/2022 14:44:26 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.35995682676740426 on epoch=95
05/27/2022 14:44:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3348588863463005 -> 0.35995682676740426 on epoch=95, global_step=1150
05/27/2022 14:44:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=96
05/27/2022 14:44:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=97
05/27/2022 14:44:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
05/27/2022 14:44:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=99
05/27/2022 14:44:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=99
05/27/2022 14:44:44 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.3511935031513415 on epoch=99
05/27/2022 14:44:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=100
05/27/2022 14:44:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.39 on epoch=101
05/27/2022 14:44:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=102
05/27/2022 14:44:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=103
05/27/2022 14:44:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=104
05/27/2022 14:45:02 - INFO - __main__ - Global step 1250 Train loss 0.36 Classification-F1 0.35538504404483784 on epoch=104
05/27/2022 14:45:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=104
05/27/2022 14:45:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=105
05/27/2022 14:45:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=106
05/27/2022 14:45:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=107
05/27/2022 14:45:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.36 on epoch=108
05/27/2022 14:45:19 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.3699001405858226 on epoch=108
05/27/2022 14:45:19 - INFO - __main__ - Saving model with best Classification-F1: 0.35995682676740426 -> 0.3699001405858226 on epoch=108, global_step=1300
05/27/2022 14:45:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=109
05/27/2022 14:45:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=109
05/27/2022 14:45:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=110
05/27/2022 14:45:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=111
05/27/2022 14:45:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=112
05/27/2022 14:45:37 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.33918128654970764 on epoch=112
05/27/2022 14:45:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=113
05/27/2022 14:45:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=114
05/27/2022 14:45:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=114
05/27/2022 14:45:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=115
05/27/2022 14:45:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=116
05/27/2022 14:45:56 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.42094127065820325 on epoch=116
05/27/2022 14:45:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3699001405858226 -> 0.42094127065820325 on epoch=116, global_step=1400
05/27/2022 14:45:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=117
05/27/2022 14:46:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.29 on epoch=118
05/27/2022 14:46:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=119
05/27/2022 14:46:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=119
05/27/2022 14:46:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=120
05/27/2022 14:46:14 - INFO - __main__ - Global step 1450 Train loss 0.32 Classification-F1 0.4000185102627675 on epoch=120
05/27/2022 14:46:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=121
05/27/2022 14:46:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=122
05/27/2022 14:46:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=123
05/27/2022 14:46:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=124
05/27/2022 14:46:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=124
05/27/2022 14:46:32 - INFO - __main__ - Global step 1500 Train loss 0.34 Classification-F1 0.42539283122257965 on epoch=124
05/27/2022 14:46:32 - INFO - __main__ - Saving model with best Classification-F1: 0.42094127065820325 -> 0.42539283122257965 on epoch=124, global_step=1500
05/27/2022 14:46:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=125
05/27/2022 14:46:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=126
05/27/2022 14:46:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=127
05/27/2022 14:46:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=128
05/27/2022 14:46:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=129
05/27/2022 14:46:50 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.41894148963933403 on epoch=129
05/27/2022 14:46:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=129
05/27/2022 14:46:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=130
05/27/2022 14:46:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=131
05/27/2022 14:47:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=132
05/27/2022 14:47:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=133
05/27/2022 14:47:09 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.40524363207832437 on epoch=133
05/27/2022 14:47:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=134
05/27/2022 14:47:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=134
05/27/2022 14:47:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=135
05/27/2022 14:47:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=136
05/27/2022 14:47:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=137
05/27/2022 14:47:27 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.38819776714513554 on epoch=137
05/27/2022 14:47:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=138
05/27/2022 14:47:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=139
05/27/2022 14:47:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=139
05/27/2022 14:47:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=140
05/27/2022 14:47:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=141
05/27/2022 14:47:45 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.4040761772947155 on epoch=141
05/27/2022 14:47:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=142
05/27/2022 14:47:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=143
05/27/2022 14:47:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=144
05/27/2022 14:47:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.28 on epoch=144
05/27/2022 14:47:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.29 on epoch=145
05/27/2022 14:48:03 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.4397600586722521 on epoch=145
05/27/2022 14:48:03 - INFO - __main__ - Saving model with best Classification-F1: 0.42539283122257965 -> 0.4397600586722521 on epoch=145, global_step=1750
05/27/2022 14:48:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=146
05/27/2022 14:48:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=147
05/27/2022 14:48:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=148
05/27/2022 14:48:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=149
05/27/2022 14:48:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=149
05/27/2022 14:48:22 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.44526353276353275 on epoch=149
05/27/2022 14:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4397600586722521 -> 0.44526353276353275 on epoch=149, global_step=1800
05/27/2022 14:48:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=150
05/27/2022 14:48:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.27 on epoch=151
05/27/2022 14:48:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=152
05/27/2022 14:48:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=153
05/27/2022 14:48:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=154
05/27/2022 14:48:40 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.44466537334565376 on epoch=154
05/27/2022 14:48:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=154
05/27/2022 14:48:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=155
05/27/2022 14:48:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=156
05/27/2022 14:48:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=157
05/27/2022 14:48:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.27 on epoch=158
05/27/2022 14:48:58 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.41038447971781306 on epoch=158
05/27/2022 14:49:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=159
05/27/2022 14:49:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=159
05/27/2022 14:49:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=160
05/27/2022 14:49:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=161
05/27/2022 14:49:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=162
05/27/2022 14:49:17 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.4616497527972938 on epoch=162
05/27/2022 14:49:17 - INFO - __main__ - Saving model with best Classification-F1: 0.44526353276353275 -> 0.4616497527972938 on epoch=162, global_step=1950
05/27/2022 14:49:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=163
05/27/2022 14:49:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=164
05/27/2022 14:49:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=164
05/27/2022 14:49:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.29 on epoch=165
05/27/2022 14:49:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=166
05/27/2022 14:49:35 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.4318699975745816 on epoch=166
05/27/2022 14:49:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.26 on epoch=167
05/27/2022 14:49:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=168
05/27/2022 14:49:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=169
05/27/2022 14:49:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=169
05/27/2022 14:49:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=170
05/27/2022 14:49:53 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.4509196372612972 on epoch=170
05/27/2022 14:49:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=171
05/27/2022 14:49:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=172
05/27/2022 14:50:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.26 on epoch=173
05/27/2022 14:50:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=174
05/27/2022 14:50:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=174
05/27/2022 14:50:12 - INFO - __main__ - Global step 2100 Train loss 0.28 Classification-F1 0.45882080204163134 on epoch=174
05/27/2022 14:50:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=175
05/27/2022 14:50:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=176
05/27/2022 14:50:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=177
05/27/2022 14:50:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.26 on epoch=178
05/27/2022 14:50:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.27 on epoch=179
05/27/2022 14:50:30 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.45432968289448405 on epoch=179
05/27/2022 14:50:33 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=179
05/27/2022 14:50:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=180
05/27/2022 14:50:38 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=181
05/27/2022 14:50:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=182
05/27/2022 14:50:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.24 on epoch=183
05/27/2022 14:50:49 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.4450116440987227 on epoch=183
05/27/2022 14:50:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=184
05/27/2022 14:50:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.25 on epoch=184
05/27/2022 14:50:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.23 on epoch=185
05/27/2022 14:50:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.22 on epoch=186
05/27/2022 14:51:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=187
05/27/2022 14:51:07 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.46769776645589767 on epoch=187
05/27/2022 14:51:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4616497527972938 -> 0.46769776645589767 on epoch=187, global_step=2250
05/27/2022 14:51:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=188
05/27/2022 14:51:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.25 on epoch=189
05/27/2022 14:51:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=189
05/27/2022 14:51:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=190
05/27/2022 14:51:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=191
05/27/2022 14:51:26 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.3442581300813008 on epoch=191
05/27/2022 14:51:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=192
05/27/2022 14:51:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.23 on epoch=193
05/27/2022 14:51:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=194
05/27/2022 14:51:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=194
05/27/2022 14:51:39 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.22 on epoch=195
05/27/2022 14:51:45 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.45407087328523127 on epoch=195
05/27/2022 14:51:47 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.17 on epoch=196
05/27/2022 14:51:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=197
05/27/2022 14:51:53 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=198
05/27/2022 14:51:55 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=199
05/27/2022 14:51:58 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=199
05/27/2022 14:52:04 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.37934799936234653 on epoch=199
05/27/2022 14:52:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.20 on epoch=200
05/27/2022 14:52:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=201
05/27/2022 14:52:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=202
05/27/2022 14:52:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=203
05/27/2022 14:52:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.18 on epoch=204
05/27/2022 14:52:22 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.4615503875968992 on epoch=204
05/27/2022 14:52:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=204
05/27/2022 14:52:28 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=205
05/27/2022 14:52:30 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=206
05/27/2022 14:52:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=207
05/27/2022 14:52:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.22 on epoch=208
05/27/2022 14:52:41 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.3382977158616788 on epoch=208
05/27/2022 14:52:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.20 on epoch=209
05/27/2022 14:52:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.25 on epoch=209
05/27/2022 14:52:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.18 on epoch=210
05/27/2022 14:52:52 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.27 on epoch=211
05/27/2022 14:52:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=212
05/27/2022 14:52:59 - INFO - __main__ - Global step 2550 Train loss 0.22 Classification-F1 0.4619601629871206 on epoch=212
05/27/2022 14:53:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.22 on epoch=213
05/27/2022 14:53:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.22 on epoch=214
05/27/2022 14:53:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=214
05/27/2022 14:53:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=215
05/27/2022 14:53:13 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.21 on epoch=216
05/27/2022 14:53:18 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3423913043478261 on epoch=216
05/27/2022 14:53:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=217
05/27/2022 14:53:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=218
05/27/2022 14:53:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=219
05/27/2022 14:53:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=219
05/27/2022 14:53:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=220
05/27/2022 14:53:37 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.35027706734867864 on epoch=220
05/27/2022 14:53:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=221
05/27/2022 14:53:42 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=222
05/27/2022 14:53:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=223
05/27/2022 14:53:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=224
05/27/2022 14:53:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=224
05/27/2022 14:53:56 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.2507655514648522 on epoch=224
05/27/2022 14:53:58 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=225
05/27/2022 14:54:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.22 on epoch=226
05/27/2022 14:54:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=227
05/27/2022 14:54:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=228
05/27/2022 14:54:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=229
05/27/2022 14:54:15 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.30148817604695965 on epoch=229
05/27/2022 14:54:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=229
05/27/2022 14:54:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=230
05/27/2022 14:54:23 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=231
05/27/2022 14:54:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=232
05/27/2022 14:54:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.21 on epoch=233
05/27/2022 14:54:33 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.3518392123502219 on epoch=233
05/27/2022 14:54:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=234
05/27/2022 14:54:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=234
05/27/2022 14:54:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=235
05/27/2022 14:54:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.15 on epoch=236
05/27/2022 14:54:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=237
05/27/2022 14:54:52 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.34628078817733987 on epoch=237
05/27/2022 14:54:54 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=238
05/27/2022 14:54:57 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=239
05/27/2022 14:55:00 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=239
05/27/2022 14:55:02 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=240
05/27/2022 14:55:05 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=241
05/27/2022 14:55:10 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.3723219991255507 on epoch=241
05/27/2022 14:55:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=242
05/27/2022 14:55:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=243
05/27/2022 14:55:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=244
05/27/2022 14:55:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=244
05/27/2022 14:55:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=245
05/27/2022 14:55:29 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.3506410256410256 on epoch=245
05/27/2022 14:55:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=246
05/27/2022 14:55:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=247
05/27/2022 14:55:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=248
05/27/2022 14:55:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=249
05/27/2022 14:55:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=249
05/27/2022 14:55:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:55:44 - INFO - __main__ - Printing 3 examples
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:55:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:55:44 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:55:44 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:55:44 - INFO - __main__ - Printing 3 examples
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:55:44 - INFO - __main__ - ['neutral']
05/27/2022 14:55:44 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:55:44 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:55:44 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:55:48 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.2963485783615551 on epoch=249
05/27/2022 14:55:48 - INFO - __main__ - save last model!
05/27/2022 14:55:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 14:55:48 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 14:55:48 - INFO - __main__ - Printing 3 examples
05/27/2022 14:55:48 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 14:55:48 - INFO - __main__ - ['contradiction']
05/27/2022 14:55:48 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 14:55:48 - INFO - __main__ - ['entailment']
05/27/2022 14:55:48 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 14:55:48 - INFO - __main__ - ['contradiction']
05/27/2022 14:55:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:55:49 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:55:50 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 14:56:00 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:56:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:56:01 - INFO - __main__ - Starting training!
05/27/2022 14:56:19 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_42_0.3_8_predictions.txt
05/27/2022 14:56:19 - INFO - __main__ - Classification-F1 on test data: 0.0849
05/27/2022 14:56:20 - INFO - __main__ - prefix=anli_64_42, lr=0.3, bsz=8, dev_performance=0.46769776645589767, test_performance=0.08494662552532382
05/27/2022 14:56:20 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.2, bsz=8 ...
05/27/2022 14:56:21 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:56:21 - INFO - __main__ - Printing 3 examples
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:56:21 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:56:21 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 14:56:21 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 14:56:21 - INFO - __main__ - Printing 3 examples
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
05/27/2022 14:56:21 - INFO - __main__ - ['neutral']
05/27/2022 14:56:21 - INFO - __main__ - Tokenizing Input ...
05/27/2022 14:56:21 - INFO - __main__ - Tokenizing Output ...
05/27/2022 14:56:21 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 14:56:37 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 14:56:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 14:56:38 - INFO - __main__ - Starting training!
05/27/2022 14:56:42 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=0
05/27/2022 14:56:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=1
05/27/2022 14:56:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.53 on epoch=2
05/27/2022 14:56:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=3
05/27/2022 14:56:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=4
05/27/2022 14:56:58 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 14:56:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 14:57:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=4
05/27/2022 14:57:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.63 on epoch=5
05/27/2022 14:57:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=6
05/27/2022 14:57:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
05/27/2022 14:57:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=8
05/27/2022 14:57:16 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 14:57:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=9
05/27/2022 14:57:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
05/27/2022 14:57:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=10
05/27/2022 14:57:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=11
05/27/2022 14:57:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
05/27/2022 14:57:34 - INFO - __main__ - Global step 150 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 14:57:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=13
05/27/2022 14:57:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=14
05/27/2022 14:57:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=14
05/27/2022 14:57:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=15
05/27/2022 14:57:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=16
05/27/2022 14:57:52 - INFO - __main__ - Global step 200 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 14:57:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=17
05/27/2022 14:57:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=18
05/27/2022 14:58:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=19
05/27/2022 14:58:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
05/27/2022 14:58:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=20
05/27/2022 14:58:10 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 14:58:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=21
05/27/2022 14:58:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.52 on epoch=22
05/27/2022 14:58:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=23
05/27/2022 14:58:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=24
05/27/2022 14:58:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=24
05/27/2022 14:58:29 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 14:58:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/27/2022 14:58:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=26
05/27/2022 14:58:37 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=27
05/27/2022 14:58:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=28
05/27/2022 14:58:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=29
05/27/2022 14:58:47 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 14:58:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/27/2022 14:58:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=30
05/27/2022 14:58:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=31
05/27/2022 14:58:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=32
05/27/2022 14:59:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=33
05/27/2022 14:59:06 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 14:59:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=34
05/27/2022 14:59:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=34
05/27/2022 14:59:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=35
05/27/2022 14:59:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=36
05/27/2022 14:59:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=37
05/27/2022 14:59:24 - INFO - __main__ - Global step 450 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=37
05/27/2022 14:59:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=38
05/27/2022 14:59:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=39
05/27/2022 14:59:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.49 on epoch=39
05/27/2022 14:59:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=40
05/27/2022 14:59:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=41
05/27/2022 14:59:43 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=41
05/27/2022 14:59:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=42
05/27/2022 14:59:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=43
05/27/2022 14:59:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=44
05/27/2022 14:59:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=44
05/27/2022 14:59:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=45
05/27/2022 15:00:01 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=45
05/27/2022 15:00:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=46
05/27/2022 15:00:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=47
05/27/2022 15:00:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/27/2022 15:00:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=49
05/27/2022 15:00:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=49
05/27/2022 15:00:20 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.16732026143790854 on epoch=49
05/27/2022 15:00:20 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16732026143790854 on epoch=49, global_step=600
05/27/2022 15:00:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=50
05/27/2022 15:00:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=51
05/27/2022 15:00:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=52
05/27/2022 15:00:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=53
05/27/2022 15:00:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=54
05/27/2022 15:00:39 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.2085278555866791 on epoch=54
05/27/2022 15:00:39 - INFO - __main__ - Saving model with best Classification-F1: 0.16732026143790854 -> 0.2085278555866791 on epoch=54, global_step=650
05/27/2022 15:00:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.49 on epoch=54
05/27/2022 15:00:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=55
05/27/2022 15:00:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
05/27/2022 15:00:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=57
05/27/2022 15:00:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
05/27/2022 15:00:57 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.17823541288108216 on epoch=58
05/27/2022 15:01:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=59
05/27/2022 15:01:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=59
05/27/2022 15:01:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=60
05/27/2022 15:01:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=61
05/27/2022 15:01:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=62
05/27/2022 15:01:16 - INFO - __main__ - Global step 750 Train loss 0.51 Classification-F1 0.18884497145366708 on epoch=62
05/27/2022 15:01:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
05/27/2022 15:01:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=64
05/27/2022 15:01:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
05/27/2022 15:01:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
05/27/2022 15:01:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=66
05/27/2022 15:01:34 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.2005174129353234 on epoch=66
05/27/2022 15:01:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=67
05/27/2022 15:01:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=68
05/27/2022 15:01:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=69
05/27/2022 15:01:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=69
05/27/2022 15:01:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=70
05/27/2022 15:01:53 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.21828571428571428 on epoch=70
05/27/2022 15:01:53 - INFO - __main__ - Saving model with best Classification-F1: 0.2085278555866791 -> 0.21828571428571428 on epoch=70, global_step=850
05/27/2022 15:01:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.58 on epoch=71
05/27/2022 15:01:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
05/27/2022 15:02:01 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
05/27/2022 15:02:03 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=74
05/27/2022 15:02:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=74
05/27/2022 15:02:11 - INFO - __main__ - Global step 900 Train loss 0.49 Classification-F1 0.27611782157236703 on epoch=74
05/27/2022 15:02:12 - INFO - __main__ - Saving model with best Classification-F1: 0.21828571428571428 -> 0.27611782157236703 on epoch=74, global_step=900
05/27/2022 15:02:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=75
05/27/2022 15:02:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=76
05/27/2022 15:02:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=77
05/27/2022 15:02:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
05/27/2022 15:02:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
05/27/2022 15:02:30 - INFO - __main__ - Global step 950 Train loss 0.47 Classification-F1 0.26303854875283444 on epoch=79
05/27/2022 15:02:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
05/27/2022 15:02:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
05/27/2022 15:02:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=81
05/27/2022 15:02:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=82
05/27/2022 15:02:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
05/27/2022 15:02:49 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.2552307409450267 on epoch=83
05/27/2022 15:02:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.47 on epoch=84
05/27/2022 15:02:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
05/27/2022 15:02:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=85
05/27/2022 15:02:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=86
05/27/2022 15:03:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=87
05/27/2022 15:03:07 - INFO - __main__ - Global step 1050 Train loss 0.45 Classification-F1 0.2545228154984253 on epoch=87
05/27/2022 15:03:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=88
05/27/2022 15:03:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=89
05/27/2022 15:03:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
05/27/2022 15:03:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=90
05/27/2022 15:03:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
05/27/2022 15:03:26 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.284704641350211 on epoch=91
05/27/2022 15:03:26 - INFO - __main__ - Saving model with best Classification-F1: 0.27611782157236703 -> 0.284704641350211 on epoch=91, global_step=1100
05/27/2022 15:03:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
05/27/2022 15:03:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=93
05/27/2022 15:03:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=94
05/27/2022 15:03:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.50 on epoch=94
05/27/2022 15:03:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
05/27/2022 15:03:45 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.282662565154234 on epoch=95
05/27/2022 15:03:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=96
05/27/2022 15:03:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=97
05/27/2022 15:03:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=98
05/27/2022 15:03:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.50 on epoch=99
05/27/2022 15:03:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=99
05/27/2022 15:04:03 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.27786423211259814 on epoch=99
05/27/2022 15:04:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=100
05/27/2022 15:04:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=101
05/27/2022 15:04:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=102
05/27/2022 15:04:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=103
05/27/2022 15:04:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=104
05/27/2022 15:04:21 - INFO - __main__ - Global step 1250 Train loss 0.40 Classification-F1 0.2985398414685023 on epoch=104
05/27/2022 15:04:22 - INFO - __main__ - Saving model with best Classification-F1: 0.284704641350211 -> 0.2985398414685023 on epoch=104, global_step=1250
05/27/2022 15:04:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
05/27/2022 15:04:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=105
05/27/2022 15:04:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
05/27/2022 15:04:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=107
05/27/2022 15:04:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=108
05/27/2022 15:04:40 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.29209726443769 on epoch=108
05/27/2022 15:04:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=109
05/27/2022 15:04:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=109
05/27/2022 15:04:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
05/27/2022 15:04:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=111
05/27/2022 15:04:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=112
05/27/2022 15:04:59 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.2985398414685023 on epoch=112
05/27/2022 15:05:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=113
05/27/2022 15:05:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=114
05/27/2022 15:05:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
05/27/2022 15:05:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
05/27/2022 15:05:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
05/27/2022 15:05:17 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.33005760759064723 on epoch=116
05/27/2022 15:05:17 - INFO - __main__ - Saving model with best Classification-F1: 0.2985398414685023 -> 0.33005760759064723 on epoch=116, global_step=1400
05/27/2022 15:05:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
05/27/2022 15:05:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=118
05/27/2022 15:05:25 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.50 on epoch=119
05/27/2022 15:05:28 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=119
05/27/2022 15:05:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=120
05/27/2022 15:05:36 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.30952380952380953 on epoch=120
05/27/2022 15:05:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
05/27/2022 15:05:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
05/27/2022 15:05:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.37 on epoch=123
05/27/2022 15:05:46 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=124
05/27/2022 15:05:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
05/27/2022 15:05:54 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.32358674463937626 on epoch=124
05/27/2022 15:05:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=125
05/27/2022 15:05:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=126
05/27/2022 15:06:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=127
05/27/2022 15:06:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=128
05/27/2022 15:06:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=129
05/27/2022 15:06:12 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.3339175469344576 on epoch=129
05/27/2022 15:06:12 - INFO - __main__ - Saving model with best Classification-F1: 0.33005760759064723 -> 0.3339175469344576 on epoch=129, global_step=1550
05/27/2022 15:06:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=129
05/27/2022 15:06:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=130
05/27/2022 15:06:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=131
05/27/2022 15:06:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=132
05/27/2022 15:06:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.39 on epoch=133
05/27/2022 15:06:31 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.334697243632442 on epoch=133
05/27/2022 15:06:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3339175469344576 -> 0.334697243632442 on epoch=133, global_step=1600
05/27/2022 15:06:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=134
05/27/2022 15:06:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
05/27/2022 15:06:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.39 on epoch=135
05/27/2022 15:06:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=136
05/27/2022 15:06:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.40 on epoch=137
05/27/2022 15:06:49 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3228206372311176 on epoch=137
05/27/2022 15:06:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=138
05/27/2022 15:06:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
05/27/2022 15:06:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=139
05/27/2022 15:07:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.35 on epoch=140
05/27/2022 15:07:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=141
05/27/2022 15:07:07 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.36756263929222666 on epoch=141
05/27/2022 15:07:07 - INFO - __main__ - Saving model with best Classification-F1: 0.334697243632442 -> 0.36756263929222666 on epoch=141, global_step=1700
05/27/2022 15:07:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.33 on epoch=142
05/27/2022 15:07:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=143
05/27/2022 15:07:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=144
05/27/2022 15:07:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=144
05/27/2022 15:07:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.44 on epoch=145
05/27/2022 15:07:26 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.3440625568285142 on epoch=145
05/27/2022 15:07:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
05/27/2022 15:07:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=147
05/27/2022 15:07:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=148
05/27/2022 15:07:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=149
05/27/2022 15:07:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=149
05/27/2022 15:07:45 - INFO - __main__ - Global step 1800 Train loss 0.36 Classification-F1 0.3910065515187611 on epoch=149
05/27/2022 15:07:45 - INFO - __main__ - Saving model with best Classification-F1: 0.36756263929222666 -> 0.3910065515187611 on epoch=149, global_step=1800
05/27/2022 15:07:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=150
05/27/2022 15:07:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
05/27/2022 15:07:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=152
05/27/2022 15:07:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=153
05/27/2022 15:07:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=154
05/27/2022 15:08:03 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.38498189441585673 on epoch=154
05/27/2022 15:08:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=154
05/27/2022 15:08:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=155
05/27/2022 15:08:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=156
05/27/2022 15:08:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=157
05/27/2022 15:08:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=158
05/27/2022 15:08:22 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.3577780080439655 on epoch=158
05/27/2022 15:08:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=159
05/27/2022 15:08:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=159
05/27/2022 15:08:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=160
05/27/2022 15:08:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=161
05/27/2022 15:08:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=162
05/27/2022 15:08:40 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.36441689136111766 on epoch=162
05/27/2022 15:08:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=163
05/27/2022 15:08:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=164
05/27/2022 15:08:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=164
05/27/2022 15:08:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.34 on epoch=165
05/27/2022 15:08:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=166
05/27/2022 15:08:59 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.40810345126731545 on epoch=166
05/27/2022 15:08:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3910065515187611 -> 0.40810345126731545 on epoch=166, global_step=2000
05/27/2022 15:09:02 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.30 on epoch=167
05/27/2022 15:09:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.32 on epoch=168
05/27/2022 15:09:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=169
05/27/2022 15:09:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=169
05/27/2022 15:09:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=170
05/27/2022 15:09:18 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.401012295000299 on epoch=170
05/27/2022 15:09:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=171
05/27/2022 15:09:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.29 on epoch=172
05/27/2022 15:09:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.33 on epoch=173
05/27/2022 15:09:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.30 on epoch=174
05/27/2022 15:09:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.33 on epoch=174
05/27/2022 15:09:36 - INFO - __main__ - Global step 2100 Train loss 0.32 Classification-F1 0.40025065175628144 on epoch=174
05/27/2022 15:09:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.30 on epoch=175
05/27/2022 15:09:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
05/27/2022 15:09:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.30 on epoch=177
05/27/2022 15:09:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
05/27/2022 15:09:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.28 on epoch=179
05/27/2022 15:09:55 - INFO - __main__ - Global step 2150 Train loss 0.33 Classification-F1 0.3871106103365857 on epoch=179
05/27/2022 15:09:58 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.32 on epoch=179
05/27/2022 15:10:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.32 on epoch=180
05/27/2022 15:10:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=181
05/27/2022 15:10:06 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=182
05/27/2022 15:10:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.34 on epoch=183
05/27/2022 15:10:14 - INFO - __main__ - Global step 2200 Train loss 0.34 Classification-F1 0.3963077693294534 on epoch=183
05/27/2022 15:10:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.31 on epoch=184
05/27/2022 15:10:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=184
05/27/2022 15:10:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=185
05/27/2022 15:10:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.34 on epoch=186
05/27/2022 15:10:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.33 on epoch=187
05/27/2022 15:10:32 - INFO - __main__ - Global step 2250 Train loss 0.32 Classification-F1 0.38152901510425813 on epoch=187
05/27/2022 15:10:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.33 on epoch=188
05/27/2022 15:10:38 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=189
05/27/2022 15:10:41 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.27 on epoch=189
05/27/2022 15:10:43 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.32 on epoch=190
05/27/2022 15:10:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.33 on epoch=191
05/27/2022 15:10:51 - INFO - __main__ - Global step 2300 Train loss 0.31 Classification-F1 0.42746507535239925 on epoch=191
05/27/2022 15:10:51 - INFO - __main__ - Saving model with best Classification-F1: 0.40810345126731545 -> 0.42746507535239925 on epoch=191, global_step=2300
05/27/2022 15:10:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.32 on epoch=192
05/27/2022 15:10:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=193
05/27/2022 15:11:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.33 on epoch=194
05/27/2022 15:11:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.34 on epoch=194
05/27/2022 15:11:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.28 on epoch=195
05/27/2022 15:11:10 - INFO - __main__ - Global step 2350 Train loss 0.31 Classification-F1 0.4230908610060777 on epoch=195
05/27/2022 15:11:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.29 on epoch=196
05/27/2022 15:11:15 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.28 on epoch=197
05/27/2022 15:11:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.33 on epoch=198
05/27/2022 15:11:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=199
05/27/2022 15:11:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.31 on epoch=199
05/27/2022 15:11:29 - INFO - __main__ - Global step 2400 Train loss 0.31 Classification-F1 0.4233833588209895 on epoch=199
05/27/2022 15:11:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.27 on epoch=200
05/27/2022 15:11:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.35 on epoch=201
05/27/2022 15:11:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.29 on epoch=202
05/27/2022 15:11:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.29 on epoch=203
05/27/2022 15:11:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.32 on epoch=204
05/27/2022 15:11:48 - INFO - __main__ - Global step 2450 Train loss 0.31 Classification-F1 0.41591366380098777 on epoch=204
05/27/2022 15:11:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=204
05/27/2022 15:11:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=205
05/27/2022 15:11:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.31 on epoch=206
05/27/2022 15:11:59 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.30 on epoch=207
05/27/2022 15:12:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.30 on epoch=208
05/27/2022 15:12:07 - INFO - __main__ - Global step 2500 Train loss 0.29 Classification-F1 0.4258023438337252 on epoch=208
05/27/2022 15:12:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.25 on epoch=209
05/27/2022 15:12:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.27 on epoch=209
05/27/2022 15:12:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.27 on epoch=210
05/27/2022 15:12:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.28 on epoch=211
05/27/2022 15:12:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.26 on epoch=212
05/27/2022 15:12:26 - INFO - __main__ - Global step 2550 Train loss 0.26 Classification-F1 0.41317699609156894 on epoch=212
05/27/2022 15:12:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.26 on epoch=213
05/27/2022 15:12:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.33 on epoch=214
05/27/2022 15:12:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.23 on epoch=214
05/27/2022 15:12:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=215
05/27/2022 15:12:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.31 on epoch=216
05/27/2022 15:12:45 - INFO - __main__ - Global step 2600 Train loss 0.27 Classification-F1 0.42909858734138373 on epoch=216
05/27/2022 15:12:45 - INFO - __main__ - Saving model with best Classification-F1: 0.42746507535239925 -> 0.42909858734138373 on epoch=216, global_step=2600
05/27/2022 15:12:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.23 on epoch=217
05/27/2022 15:12:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=218
05/27/2022 15:12:53 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=219
05/27/2022 15:12:56 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.23 on epoch=219
05/27/2022 15:12:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.21 on epoch=220
05/27/2022 15:13:04 - INFO - __main__ - Global step 2650 Train loss 0.25 Classification-F1 0.417561853351327 on epoch=220
05/27/2022 15:13:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.33 on epoch=221
05/27/2022 15:13:09 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.28 on epoch=222
05/27/2022 15:13:12 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.27 on epoch=223
05/27/2022 15:13:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.32 on epoch=224
05/27/2022 15:13:17 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.28 on epoch=224
05/27/2022 15:13:23 - INFO - __main__ - Global step 2700 Train loss 0.30 Classification-F1 0.4360401002506266 on epoch=224
05/27/2022 15:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.42909858734138373 -> 0.4360401002506266 on epoch=224, global_step=2700
05/27/2022 15:13:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.26 on epoch=225
05/27/2022 15:13:28 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.30 on epoch=226
05/27/2022 15:13:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.28 on epoch=227
05/27/2022 15:13:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=228
05/27/2022 15:13:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=229
05/27/2022 15:13:42 - INFO - __main__ - Global step 2750 Train loss 0.29 Classification-F1 0.43221707268766085 on epoch=229
05/27/2022 15:13:45 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.23 on epoch=229
05/27/2022 15:13:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=230
05/27/2022 15:13:50 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.30 on epoch=231
05/27/2022 15:13:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.29 on epoch=232
05/27/2022 15:13:56 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.26 on epoch=233
05/27/2022 15:14:01 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.39765096045472603 on epoch=233
05/27/2022 15:14:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.21 on epoch=234
05/27/2022 15:14:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.24 on epoch=234
05/27/2022 15:14:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.24 on epoch=235
05/27/2022 15:14:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.33 on epoch=236
05/27/2022 15:14:14 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.28 on epoch=237
05/27/2022 15:14:19 - INFO - __main__ - Global step 2850 Train loss 0.26 Classification-F1 0.4116909285303068 on epoch=237
05/27/2022 15:14:22 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.22 on epoch=238
05/27/2022 15:14:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.24 on epoch=239
05/27/2022 15:14:28 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=239
05/27/2022 15:14:30 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=240
05/27/2022 15:14:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.29 on epoch=241
05/27/2022 15:14:38 - INFO - __main__ - Global step 2900 Train loss 0.24 Classification-F1 0.4215229433340819 on epoch=241
05/27/2022 15:14:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.32 on epoch=242
05/27/2022 15:14:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.23 on epoch=243
05/27/2022 15:14:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.25 on epoch=244
05/27/2022 15:14:49 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=244
05/27/2022 15:14:52 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.25 on epoch=245
05/27/2022 15:14:57 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.3304892101403729 on epoch=245
05/27/2022 15:15:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.27 on epoch=246
05/27/2022 15:15:03 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.20 on epoch=247
05/27/2022 15:15:06 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=248
05/27/2022 15:15:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.33 on epoch=249
05/27/2022 15:15:11 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.24 on epoch=249
05/27/2022 15:15:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:15:13 - INFO - __main__ - Printing 3 examples
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:15:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:15:13 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:15:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:15:13 - INFO - __main__ - Printing 3 examples
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:15:13 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:13 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:15:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:15:14 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:15:17 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.4197191697191698 on epoch=249
05/27/2022 15:15:17 - INFO - __main__ - save last model!
05/27/2022 15:15:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 15:15:17 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 15:15:17 - INFO - __main__ - Printing 3 examples
05/27/2022 15:15:17 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 15:15:17 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:17 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 15:15:17 - INFO - __main__ - ['entailment']
05/27/2022 15:15:17 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 15:15:17 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:17 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:15:17 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:15:18 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 15:15:29 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:15:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:15:30 - INFO - __main__ - Starting training!
05/27/2022 15:15:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_42_0.2_8_predictions.txt
05/27/2022 15:15:46 - INFO - __main__ - Classification-F1 on test data: 0.3255
05/27/2022 15:15:47 - INFO - __main__ - prefix=anli_64_42, lr=0.2, bsz=8, dev_performance=0.4360401002506266, test_performance=0.3254588463194637
05/27/2022 15:15:47 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.5, bsz=8 ...
05/27/2022 15:15:48 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:15:48 - INFO - __main__ - Printing 3 examples
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:15:48 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:15:48 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:15:48 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:15:48 - INFO - __main__ - Printing 3 examples
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:15:48 - INFO - __main__ - ['contradiction']
05/27/2022 15:15:48 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:15:48 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:15:48 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:16:04 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:16:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:16:05 - INFO - __main__ - Starting training!
05/27/2022 15:16:09 - INFO - __main__ - Step 10 Global step 10 Train loss 0.56 on epoch=0
05/27/2022 15:16:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=1
05/27/2022 15:16:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=2
05/27/2022 15:16:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.51 on epoch=3
05/27/2022 15:16:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.50 on epoch=4
05/27/2022 15:16:25 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 15:16:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 15:16:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=4
05/27/2022 15:16:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=5
05/27/2022 15:16:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=6
05/27/2022 15:16:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=7
05/27/2022 15:16:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=8
05/27/2022 15:16:44 - INFO - __main__ - Global step 100 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 15:16:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=9
05/27/2022 15:16:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
05/27/2022 15:16:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=10
05/27/2022 15:16:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=11
05/27/2022 15:16:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=12
05/27/2022 15:17:02 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 15:17:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=13
05/27/2022 15:17:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=14
05/27/2022 15:17:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=14
05/27/2022 15:17:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.51 on epoch=15
05/27/2022 15:17:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/27/2022 15:17:21 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
05/27/2022 15:17:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=17
05/27/2022 15:17:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=18
05/27/2022 15:17:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
05/27/2022 15:17:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=19
05/27/2022 15:17:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=20
05/27/2022 15:17:39 - INFO - __main__ - Global step 250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 15:17:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/27/2022 15:17:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=22
05/27/2022 15:17:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=23
05/27/2022 15:17:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.42 on epoch=24
05/27/2022 15:17:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=24
05/27/2022 15:17:58 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.2386374491781125 on epoch=24
05/27/2022 15:17:58 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2386374491781125 on epoch=24, global_step=300
05/27/2022 15:18:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=25
05/27/2022 15:18:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
05/27/2022 15:18:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=27
05/27/2022 15:18:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
05/27/2022 15:18:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=29
05/27/2022 15:18:17 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 15:18:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/27/2022 15:18:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=30
05/27/2022 15:18:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
05/27/2022 15:18:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=32
05/27/2022 15:18:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
05/27/2022 15:18:36 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.24989731633296527 on epoch=33
05/27/2022 15:18:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2386374491781125 -> 0.24989731633296527 on epoch=33, global_step=400
05/27/2022 15:18:39 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=34
05/27/2022 15:18:42 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=34
05/27/2022 15:18:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=35
05/27/2022 15:18:47 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=36
05/27/2022 15:18:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
05/27/2022 15:18:56 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.29892821535393815 on epoch=37
05/27/2022 15:18:56 - INFO - __main__ - Saving model with best Classification-F1: 0.24989731633296527 -> 0.29892821535393815 on epoch=37, global_step=450
05/27/2022 15:18:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=38
05/27/2022 15:19:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=39
05/27/2022 15:19:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/27/2022 15:19:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=40
05/27/2022 15:19:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=41
05/27/2022 15:19:15 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.2949341438703141 on epoch=41
05/27/2022 15:19:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
05/27/2022 15:19:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=43
05/27/2022 15:19:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=44
05/27/2022 15:19:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=44
05/27/2022 15:19:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
05/27/2022 15:19:34 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.3364828724502855 on epoch=45
05/27/2022 15:19:34 - INFO - __main__ - Saving model with best Classification-F1: 0.29892821535393815 -> 0.3364828724502855 on epoch=45, global_step=550
05/27/2022 15:19:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
05/27/2022 15:19:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=47
05/27/2022 15:19:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=48
05/27/2022 15:19:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=49
05/27/2022 15:19:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=49
05/27/2022 15:19:53 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3883037122895135 on epoch=49
05/27/2022 15:19:53 - INFO - __main__ - Saving model with best Classification-F1: 0.3364828724502855 -> 0.3883037122895135 on epoch=49, global_step=600
05/27/2022 15:19:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
05/27/2022 15:19:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/27/2022 15:20:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=52
05/27/2022 15:20:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=53
05/27/2022 15:20:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=54
05/27/2022 15:20:12 - INFO - __main__ - Global step 650 Train loss 0.41 Classification-F1 0.3284732614715047 on epoch=54
05/27/2022 15:20:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=54
05/27/2022 15:20:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.50 on epoch=55
05/27/2022 15:20:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=56
05/27/2022 15:20:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=57
05/27/2022 15:20:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=58
05/27/2022 15:20:31 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.3670727205610927 on epoch=58
05/27/2022 15:20:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=59
05/27/2022 15:20:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=59
05/27/2022 15:20:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=60
05/27/2022 15:20:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=61
05/27/2022 15:20:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=62
05/27/2022 15:20:50 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.37575757575757573 on epoch=62
05/27/2022 15:20:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=63
05/27/2022 15:20:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=64
05/27/2022 15:20:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=64
05/27/2022 15:21:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=65
05/27/2022 15:21:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=66
05/27/2022 15:21:09 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.42293850856826154 on epoch=66
05/27/2022 15:21:09 - INFO - __main__ - Saving model with best Classification-F1: 0.3883037122895135 -> 0.42293850856826154 on epoch=66, global_step=800
05/27/2022 15:21:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.32 on epoch=67
05/27/2022 15:21:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=68
05/27/2022 15:21:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=69
05/27/2022 15:21:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=69
05/27/2022 15:21:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=70
05/27/2022 15:21:27 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.38153823672027043 on epoch=70
05/27/2022 15:21:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=71
05/27/2022 15:21:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=72
05/27/2022 15:21:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=73
05/27/2022 15:21:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=74
05/27/2022 15:21:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=74
05/27/2022 15:21:46 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.40444835114480276 on epoch=74
05/27/2022 15:21:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=75
05/27/2022 15:21:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=76
05/27/2022 15:21:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=77
05/27/2022 15:21:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=78
05/27/2022 15:21:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=79
05/27/2022 15:22:05 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.3470948012232416 on epoch=79
05/27/2022 15:22:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=79
05/27/2022 15:22:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=80
05/27/2022 15:22:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=81
05/27/2022 15:22:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=82
05/27/2022 15:22:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=83
05/27/2022 15:22:23 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.4334810961483781 on epoch=83
05/27/2022 15:22:23 - INFO - __main__ - Saving model with best Classification-F1: 0.42293850856826154 -> 0.4334810961483781 on epoch=83, global_step=1000
05/27/2022 15:22:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=84
05/27/2022 15:22:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=84
05/27/2022 15:22:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=85
05/27/2022 15:22:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=86
05/27/2022 15:22:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=87
05/27/2022 15:22:41 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.40338827838827845 on epoch=87
05/27/2022 15:22:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=88
05/27/2022 15:22:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=89
05/27/2022 15:22:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=89
05/27/2022 15:22:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=90
05/27/2022 15:22:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=91
05/27/2022 15:23:00 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.4558593390407806 on epoch=91
05/27/2022 15:23:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4334810961483781 -> 0.4558593390407806 on epoch=91, global_step=1100
05/27/2022 15:23:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=92
05/27/2022 15:23:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=93
05/27/2022 15:23:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=94
05/27/2022 15:23:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=94
05/27/2022 15:23:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=95
05/27/2022 15:23:19 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.3889360745983598 on epoch=95
05/27/2022 15:23:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=96
05/27/2022 15:23:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=97
05/27/2022 15:23:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=98
05/27/2022 15:23:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=99
05/27/2022 15:23:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=99
05/27/2022 15:23:38 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.4136529665691914 on epoch=99
05/27/2022 15:23:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=100
05/27/2022 15:23:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=101
05/27/2022 15:23:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=102
05/27/2022 15:23:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=103
05/27/2022 15:23:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=104
05/27/2022 15:23:56 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.44312606688610473 on epoch=104
05/27/2022 15:23:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=104
05/27/2022 15:24:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=105
05/27/2022 15:24:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=106
05/27/2022 15:24:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=107
05/27/2022 15:24:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=108
05/27/2022 15:24:15 - INFO - __main__ - Global step 1300 Train loss 0.27 Classification-F1 0.42080690627202255 on epoch=108
05/27/2022 15:24:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=109
05/27/2022 15:24:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=109
05/27/2022 15:24:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=110
05/27/2022 15:24:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=111
05/27/2022 15:24:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=112
05/27/2022 15:24:34 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.4373486381853757 on epoch=112
05/27/2022 15:24:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=113
05/27/2022 15:24:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=114
05/27/2022 15:24:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=114
05/27/2022 15:24:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=115
05/27/2022 15:24:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.27 on epoch=116
05/27/2022 15:24:53 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.41473137925431436 on epoch=116
05/27/2022 15:24:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=117
05/27/2022 15:24:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=118
05/27/2022 15:25:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=119
05/27/2022 15:25:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=119
05/27/2022 15:25:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=120
05/27/2022 15:25:11 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.4009671920119682 on epoch=120
05/27/2022 15:25:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=121
05/27/2022 15:25:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=122
05/27/2022 15:25:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=123
05/27/2022 15:25:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=124
05/27/2022 15:25:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=124
05/27/2022 15:25:30 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.4062801561696301 on epoch=124
05/27/2022 15:25:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=125
05/27/2022 15:25:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=126
05/27/2022 15:25:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=127
05/27/2022 15:25:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=128
05/27/2022 15:25:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=129
05/27/2022 15:25:49 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.4589225589225589 on epoch=129
05/27/2022 15:25:49 - INFO - __main__ - Saving model with best Classification-F1: 0.4558593390407806 -> 0.4589225589225589 on epoch=129, global_step=1550
05/27/2022 15:25:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=129
05/27/2022 15:25:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=130
05/27/2022 15:25:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=131
05/27/2022 15:25:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=132
05/27/2022 15:26:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=133
05/27/2022 15:26:07 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.42591978908919526 on epoch=133
05/27/2022 15:26:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=134
05/27/2022 15:26:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=134
05/27/2022 15:26:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=135
05/27/2022 15:26:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=136
05/27/2022 15:26:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=137
05/27/2022 15:26:26 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.42117413411323845 on epoch=137
05/27/2022 15:26:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=138
05/27/2022 15:26:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=139
05/27/2022 15:26:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=139
05/27/2022 15:26:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=140
05/27/2022 15:26:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=141
05/27/2022 15:26:45 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.44135103015755356 on epoch=141
05/27/2022 15:26:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=142
05/27/2022 15:26:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=143
05/27/2022 15:26:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=144
05/27/2022 15:26:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=144
05/27/2022 15:26:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=145
05/27/2022 15:27:04 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.4292302472302265 on epoch=145
05/27/2022 15:27:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=146
05/27/2022 15:27:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=147
05/27/2022 15:27:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=148
05/27/2022 15:27:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.19 on epoch=149
05/27/2022 15:27:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=149
05/27/2022 15:27:23 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.4201059597436316 on epoch=149
05/27/2022 15:27:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=150
05/27/2022 15:27:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=151
05/27/2022 15:27:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=152
05/27/2022 15:27:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=153
05/27/2022 15:27:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=154
05/27/2022 15:27:41 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.27388063844552396 on epoch=154
05/27/2022 15:27:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=154
05/27/2022 15:27:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=155
05/27/2022 15:27:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=156
05/27/2022 15:27:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=157
05/27/2022 15:27:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=158
05/27/2022 15:28:00 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.340215943476813 on epoch=158
05/27/2022 15:28:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=159
05/27/2022 15:28:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=159
05/27/2022 15:28:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=160
05/27/2022 15:28:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=161
05/27/2022 15:28:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=162
05/27/2022 15:28:19 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.2700845987752646 on epoch=162
05/27/2022 15:28:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=163
05/27/2022 15:28:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=164
05/27/2022 15:28:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=164
05/27/2022 15:28:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=165
05/27/2022 15:28:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=166
05/27/2022 15:28:38 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.32809220916899245 on epoch=166
05/27/2022 15:28:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=167
05/27/2022 15:28:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=168
05/27/2022 15:28:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=169
05/27/2022 15:28:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=169
05/27/2022 15:28:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=170
05/27/2022 15:28:57 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.2710201100999887 on epoch=170
05/27/2022 15:29:00 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.17 on epoch=171
05/27/2022 15:29:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=172
05/27/2022 15:29:05 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=173
05/27/2022 15:29:08 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=174
05/27/2022 15:29:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=174
05/27/2022 15:29:16 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.4172151245321977 on epoch=174
05/27/2022 15:29:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=175
05/27/2022 15:29:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=176
05/27/2022 15:29:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=177
05/27/2022 15:29:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=178
05/27/2022 15:29:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=179
05/27/2022 15:29:34 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.3304073974805682 on epoch=179
05/27/2022 15:29:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=179
05/27/2022 15:29:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=180
05/27/2022 15:29:42 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=181
05/27/2022 15:29:45 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=182
05/27/2022 15:29:48 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=183
05/27/2022 15:29:53 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.21510946434009223 on epoch=183
05/27/2022 15:29:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=184
05/27/2022 15:29:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=184
05/27/2022 15:30:02 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=185
05/27/2022 15:30:04 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=186
05/27/2022 15:30:07 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=187
05/27/2022 15:30:13 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.21498462403865318 on epoch=187
05/27/2022 15:30:15 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=188
05/27/2022 15:30:18 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=189
05/27/2022 15:30:21 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=189
05/27/2022 15:30:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=190
05/27/2022 15:30:26 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=191
05/27/2022 15:30:32 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.19068960917700412 on epoch=191
05/27/2022 15:30:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=192
05/27/2022 15:30:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=193
05/27/2022 15:30:40 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=194
05/27/2022 15:30:43 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=194
05/27/2022 15:30:45 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=195
05/27/2022 15:30:51 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.13162417853872285 on epoch=195
05/27/2022 15:30:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=196
05/27/2022 15:30:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=197
05/27/2022 15:30:59 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=198
05/27/2022 15:31:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=199
05/27/2022 15:31:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=199
05/27/2022 15:31:10 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.13654471889766007 on epoch=199
05/27/2022 15:31:13 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.13 on epoch=200
05/27/2022 15:31:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=201
05/27/2022 15:31:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=202
05/27/2022 15:31:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=203
05/27/2022 15:31:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=204
05/27/2022 15:31:29 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.14377507554895472 on epoch=204
05/27/2022 15:31:32 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=204
05/27/2022 15:31:34 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=205
05/27/2022 15:31:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=206
05/27/2022 15:31:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=207
05/27/2022 15:31:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=208
05/27/2022 15:31:48 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.09721090890343868 on epoch=208
05/27/2022 15:31:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=209
05/27/2022 15:31:54 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=209
05/27/2022 15:31:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=210
05/27/2022 15:31:59 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=211
05/27/2022 15:32:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=212
05/27/2022 15:32:07 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.09519630299377135 on epoch=212
05/27/2022 15:32:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=213
05/27/2022 15:32:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=214
05/27/2022 15:32:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=214
05/27/2022 15:32:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=215
05/27/2022 15:32:21 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=216
05/27/2022 15:32:26 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.1319420600673984 on epoch=216
05/27/2022 15:32:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=217
05/27/2022 15:32:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=218
05/27/2022 15:32:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=219
05/27/2022 15:32:37 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.16 on epoch=219
05/27/2022 15:32:40 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.12 on epoch=220
05/27/2022 15:32:45 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.19711280980749096 on epoch=220
05/27/2022 15:32:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=221
05/27/2022 15:32:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=222
05/27/2022 15:32:54 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=223
05/27/2022 15:32:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=224
05/27/2022 15:32:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.09 on epoch=224
05/27/2022 15:33:05 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.1009770766864865 on epoch=224
05/27/2022 15:33:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=225
05/27/2022 15:33:10 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=226
05/27/2022 15:33:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=227
05/27/2022 15:33:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=228
05/27/2022 15:33:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=229
05/27/2022 15:33:24 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.07713269012458061 on epoch=229
05/27/2022 15:33:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=229
05/27/2022 15:33:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=230
05/27/2022 15:33:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=231
05/27/2022 15:33:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=232
05/27/2022 15:33:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=233
05/27/2022 15:33:43 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.1009006294361859 on epoch=233
05/27/2022 15:33:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.10 on epoch=234
05/27/2022 15:33:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=234
05/27/2022 15:33:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=235
05/27/2022 15:33:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=236
05/27/2022 15:33:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=237
05/27/2022 15:34:02 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.06344624766140704 on epoch=237
05/27/2022 15:34:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=238
05/27/2022 15:34:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=239
05/27/2022 15:34:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=239
05/27/2022 15:34:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=240
05/27/2022 15:34:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=241
05/27/2022 15:34:22 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.07060248734105508 on epoch=241
05/27/2022 15:34:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=242
05/27/2022 15:34:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=243
05/27/2022 15:34:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=244
05/27/2022 15:34:33 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=244
05/27/2022 15:34:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=245
05/27/2022 15:34:41 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.08529020849376799 on epoch=245
05/27/2022 15:34:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=246
05/27/2022 15:34:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=247
05/27/2022 15:34:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=248
05/27/2022 15:34:52 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=249
05/27/2022 15:34:55 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=249
05/27/2022 15:34:56 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:34:56 - INFO - __main__ - Printing 3 examples
05/27/2022 15:34:56 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:34:56 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:56 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:34:56 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:56 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:34:56 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:56 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:34:56 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:34:57 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:34:57 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:34:57 - INFO - __main__ - Printing 3 examples
05/27/2022 15:34:57 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:34:57 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:57 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:34:57 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:57 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:34:57 - INFO - __main__ - ['contradiction']
05/27/2022 15:34:57 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:34:57 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:34:57 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:35:00 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.08107146979052454 on epoch=249
05/27/2022 15:35:00 - INFO - __main__ - save last model!
05/27/2022 15:35:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 15:35:00 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 15:35:00 - INFO - __main__ - Printing 3 examples
05/27/2022 15:35:00 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 15:35:00 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:00 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 15:35:00 - INFO - __main__ - ['entailment']
05/27/2022 15:35:00 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 15:35:00 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:00 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:35:01 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:35:02 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 15:35:13 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:35:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:35:13 - INFO - __main__ - Starting training!
05/27/2022 15:35:32 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_87_0.5_8_predictions.txt
05/27/2022 15:35:32 - INFO - __main__ - Classification-F1 on test data: 0.0198
05/27/2022 15:35:32 - INFO - __main__ - prefix=anli_64_87, lr=0.5, bsz=8, dev_performance=0.4589225589225589, test_performance=0.019779426390139843
05/27/2022 15:35:32 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.4, bsz=8 ...
05/27/2022 15:35:33 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:35:33 - INFO - __main__ - Printing 3 examples
05/27/2022 15:35:33 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:35:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:33 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:35:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:33 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:35:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:35:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:35:34 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:35:34 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:35:34 - INFO - __main__ - Printing 3 examples
05/27/2022 15:35:34 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:35:34 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:34 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:35:34 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:34 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:35:34 - INFO - __main__ - ['contradiction']
05/27/2022 15:35:34 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:35:34 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:35:34 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:35:52 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:35:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:35:53 - INFO - __main__ - Starting training!
05/27/2022 15:35:57 - INFO - __main__ - Step 10 Global step 10 Train loss 0.48 on epoch=0
05/27/2022 15:35:59 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=1
05/27/2022 15:36:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=2
05/27/2022 15:36:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=3
05/27/2022 15:36:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=4
05/27/2022 15:36:13 - INFO - __main__ - Global step 50 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 15:36:13 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 15:36:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=4
05/27/2022 15:36:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=5
05/27/2022 15:36:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.51 on epoch=6
05/27/2022 15:36:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=7
05/27/2022 15:36:26 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=8
05/27/2022 15:36:31 - INFO - __main__ - Global step 100 Train loss 0.51 Classification-F1 0.1775766716943188 on epoch=8
05/27/2022 15:36:31 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1775766716943188 on epoch=8, global_step=100
05/27/2022 15:36:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=9
05/27/2022 15:36:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=9
05/27/2022 15:36:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=10
05/27/2022 15:36:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=11
05/27/2022 15:36:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=12
05/27/2022 15:36:49 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.1775766716943188 on epoch=12
05/27/2022 15:36:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=13
05/27/2022 15:36:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=14
05/27/2022 15:36:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=14
05/27/2022 15:37:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
05/27/2022 15:37:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=16
05/27/2022 15:37:08 - INFO - __main__ - Global step 200 Train loss 0.45 Classification-F1 0.3099808781160403 on epoch=16
05/27/2022 15:37:08 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.3099808781160403 on epoch=16, global_step=200
05/27/2022 15:37:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=17
05/27/2022 15:37:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.44 on epoch=18
05/27/2022 15:37:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=19
05/27/2022 15:37:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=19
05/27/2022 15:37:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.44 on epoch=20
05/27/2022 15:37:26 - INFO - __main__ - Global step 250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 15:37:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=21
05/27/2022 15:37:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
05/27/2022 15:37:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
05/27/2022 15:37:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
05/27/2022 15:37:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
05/27/2022 15:37:45 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.18849182313749244 on epoch=24
05/27/2022 15:37:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.54 on epoch=25
05/27/2022 15:37:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
05/27/2022 15:37:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=27
05/27/2022 15:37:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
05/27/2022 15:37:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
05/27/2022 15:38:04 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.17647058823529413 on epoch=29
05/27/2022 15:38:06 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=29
05/27/2022 15:38:09 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=30
05/27/2022 15:38:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=31
05/27/2022 15:38:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=32
05/27/2022 15:38:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=33
05/27/2022 15:38:22 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.2615130662207645 on epoch=33
05/27/2022 15:38:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=34
05/27/2022 15:38:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=34
05/27/2022 15:38:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
05/27/2022 15:38:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=36
05/27/2022 15:38:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
05/27/2022 15:38:41 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.3580952380952381 on epoch=37
05/27/2022 15:38:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3099808781160403 -> 0.3580952380952381 on epoch=37, global_step=450
05/27/2022 15:38:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=38
05/27/2022 15:38:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=39
05/27/2022 15:38:49 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=39
05/27/2022 15:38:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
05/27/2022 15:38:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
05/27/2022 15:39:00 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.30272849880693015 on epoch=41
05/27/2022 15:39:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=42
05/27/2022 15:39:05 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=43
05/27/2022 15:39:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=44
05/27/2022 15:39:11 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
05/27/2022 15:39:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=45
05/27/2022 15:39:19 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.2952544668437777 on epoch=45
05/27/2022 15:39:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=46
05/27/2022 15:39:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=47
05/27/2022 15:39:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=48
05/27/2022 15:39:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=49
05/27/2022 15:39:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/27/2022 15:39:38 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.35856307435254803 on epoch=49
05/27/2022 15:39:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3580952380952381 -> 0.35856307435254803 on epoch=49, global_step=600
05/27/2022 15:39:41 - INFO - __main__ - Step 610 Global step 610 Train loss 0.36 on epoch=50
05/27/2022 15:39:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=51
05/27/2022 15:39:46 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=52
05/27/2022 15:39:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=53
05/27/2022 15:39:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=54
05/27/2022 15:39:56 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.34363081617086194 on epoch=54
05/27/2022 15:39:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=54
05/27/2022 15:40:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=55
05/27/2022 15:40:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
05/27/2022 15:40:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=57
05/27/2022 15:40:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=58
05/27/2022 15:40:14 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.3382557815547506 on epoch=58
05/27/2022 15:40:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=59
05/27/2022 15:40:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=59
05/27/2022 15:40:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=60
05/27/2022 15:40:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=61
05/27/2022 15:40:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=62
05/27/2022 15:40:33 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.35515151515151516 on epoch=62
05/27/2022 15:40:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=63
05/27/2022 15:40:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=64
05/27/2022 15:40:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=64
05/27/2022 15:40:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=65
05/27/2022 15:40:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
05/27/2022 15:40:51 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.38329642877420084 on epoch=66
05/27/2022 15:40:51 - INFO - __main__ - Saving model with best Classification-F1: 0.35856307435254803 -> 0.38329642877420084 on epoch=66, global_step=800
05/27/2022 15:40:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=67
05/27/2022 15:40:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.37 on epoch=68
05/27/2022 15:40:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=69
05/27/2022 15:41:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=69
05/27/2022 15:41:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=70
05/27/2022 15:41:09 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.37157942462668014 on epoch=70
05/27/2022 15:41:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=71
05/27/2022 15:41:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=72
05/27/2022 15:41:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
05/27/2022 15:41:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=74
05/27/2022 15:41:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=74
05/27/2022 15:41:27 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.37780995422169567 on epoch=74
05/27/2022 15:41:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=75
05/27/2022 15:41:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=76
05/27/2022 15:41:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.31 on epoch=77
05/27/2022 15:41:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=78
05/27/2022 15:41:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.31 on epoch=79
05/27/2022 15:41:45 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.34821428571428575 on epoch=79
05/27/2022 15:41:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=79
05/27/2022 15:41:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=80
05/27/2022 15:41:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=81
05/27/2022 15:41:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=82
05/27/2022 15:41:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=83
05/27/2022 15:42:03 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.37203406260719135 on epoch=83
05/27/2022 15:42:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=84
05/27/2022 15:42:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=84
05/27/2022 15:42:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=85
05/27/2022 15:42:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=86
05/27/2022 15:42:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=87
05/27/2022 15:42:21 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.3684179144802271 on epoch=87
05/27/2022 15:42:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=88
05/27/2022 15:42:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=89
05/27/2022 15:42:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=89
05/27/2022 15:42:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=90
05/27/2022 15:42:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=91
05/27/2022 15:42:38 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.3955555555555555 on epoch=91
05/27/2022 15:42:38 - INFO - __main__ - Saving model with best Classification-F1: 0.38329642877420084 -> 0.3955555555555555 on epoch=91, global_step=1100
05/27/2022 15:42:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=92
05/27/2022 15:42:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=93
05/27/2022 15:42:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=94
05/27/2022 15:42:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=94
05/27/2022 15:42:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.29 on epoch=95
05/27/2022 15:42:56 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.39643903538426867 on epoch=95
05/27/2022 15:42:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3955555555555555 -> 0.39643903538426867 on epoch=95, global_step=1150
05/27/2022 15:42:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=96
05/27/2022 15:43:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=97
05/27/2022 15:43:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=98
05/27/2022 15:43:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=99
05/27/2022 15:43:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=99
05/27/2022 15:43:14 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.3997070092404778 on epoch=99
05/27/2022 15:43:14 - INFO - __main__ - Saving model with best Classification-F1: 0.39643903538426867 -> 0.3997070092404778 on epoch=99, global_step=1200
05/27/2022 15:43:17 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=100
05/27/2022 15:43:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=101
05/27/2022 15:43:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=102
05/27/2022 15:43:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=103
05/27/2022 15:43:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=104
05/27/2022 15:43:32 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.383020241735838 on epoch=104
05/27/2022 15:43:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=104
05/27/2022 15:43:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=105
05/27/2022 15:43:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=106
05/27/2022 15:43:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=107
05/27/2022 15:43:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=108
05/27/2022 15:43:50 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.37071977638015374 on epoch=108
05/27/2022 15:43:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=109
05/27/2022 15:43:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=109
05/27/2022 15:43:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=110
05/27/2022 15:44:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=111
05/27/2022 15:44:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=112
05/27/2022 15:44:08 - INFO - __main__ - Global step 1350 Train loss 0.30 Classification-F1 0.40342047380882323 on epoch=112
05/27/2022 15:44:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3997070092404778 -> 0.40342047380882323 on epoch=112, global_step=1350
05/27/2022 15:44:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=113
05/27/2022 15:44:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=114
05/27/2022 15:44:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=114
05/27/2022 15:44:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=115
05/27/2022 15:44:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=116
05/27/2022 15:44:26 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.3930809522752719 on epoch=116
05/27/2022 15:44:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=117
05/27/2022 15:44:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=118
05/27/2022 15:44:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=119
05/27/2022 15:44:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=119
05/27/2022 15:44:40 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=120
05/27/2022 15:44:45 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.3772716816195077 on epoch=120
05/27/2022 15:44:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=121
05/27/2022 15:44:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=122
05/27/2022 15:44:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=123
05/27/2022 15:44:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=124
05/27/2022 15:44:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=124
05/27/2022 15:45:03 - INFO - __main__ - Global step 1500 Train loss 0.24 Classification-F1 0.4028123509957935 on epoch=124
05/27/2022 15:45:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=125
05/27/2022 15:45:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=126
05/27/2022 15:45:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=127
05/27/2022 15:45:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=128
05/27/2022 15:45:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=129
05/27/2022 15:45:22 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.29386812396521134 on epoch=129
05/27/2022 15:45:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=129
05/27/2022 15:45:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=130
05/27/2022 15:45:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=131
05/27/2022 15:45:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=132
05/27/2022 15:45:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=133
05/27/2022 15:45:41 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.4023911971673166 on epoch=133
05/27/2022 15:45:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=134
05/27/2022 15:45:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=134
05/27/2022 15:45:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=135
05/27/2022 15:45:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.27 on epoch=136
05/27/2022 15:45:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=137
05/27/2022 15:46:00 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.4026436781609195 on epoch=137
05/27/2022 15:46:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=138
05/27/2022 15:46:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=139
05/27/2022 15:46:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=139
05/27/2022 15:46:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=140
05/27/2022 15:46:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=141
05/27/2022 15:46:18 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.40835748792270526 on epoch=141
05/27/2022 15:46:18 - INFO - __main__ - Saving model with best Classification-F1: 0.40342047380882323 -> 0.40835748792270526 on epoch=141, global_step=1700
05/27/2022 15:46:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=142
05/27/2022 15:46:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=143
05/27/2022 15:46:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=144
05/27/2022 15:46:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=144
05/27/2022 15:46:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=145
05/27/2022 15:46:37 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3985048531982878 on epoch=145
05/27/2022 15:46:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=146
05/27/2022 15:46:43 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=147
05/27/2022 15:46:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=148
05/27/2022 15:46:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=149
05/27/2022 15:46:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=149
05/27/2022 15:46:56 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3973255900008565 on epoch=149
05/27/2022 15:46:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=150
05/27/2022 15:47:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=151
05/27/2022 15:47:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=152
05/27/2022 15:47:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=153
05/27/2022 15:47:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=154
05/27/2022 15:47:15 - INFO - __main__ - Global step 1850 Train loss 0.22 Classification-F1 0.4152634345494283 on epoch=154
05/27/2022 15:47:15 - INFO - __main__ - Saving model with best Classification-F1: 0.40835748792270526 -> 0.4152634345494283 on epoch=154, global_step=1850
05/27/2022 15:47:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=154
05/27/2022 15:47:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=155
05/27/2022 15:47:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=156
05/27/2022 15:47:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=157
05/27/2022 15:47:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=158
05/27/2022 15:47:34 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.4212761490303863 on epoch=158
05/27/2022 15:47:34 - INFO - __main__ - Saving model with best Classification-F1: 0.4152634345494283 -> 0.4212761490303863 on epoch=158, global_step=1900
05/27/2022 15:47:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=159
05/27/2022 15:47:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=159
05/27/2022 15:47:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=160
05/27/2022 15:47:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=161
05/27/2022 15:47:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=162
05/27/2022 15:47:53 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.25611682049869133 on epoch=162
05/27/2022 15:47:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=163
05/27/2022 15:47:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=164
05/27/2022 15:48:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=164
05/27/2022 15:48:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=165
05/27/2022 15:48:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=166
05/27/2022 15:48:12 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.42219064491523334 on epoch=166
05/27/2022 15:48:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4212761490303863 -> 0.42219064491523334 on epoch=166, global_step=2000
05/27/2022 15:48:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=167
05/27/2022 15:48:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=168
05/27/2022 15:48:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=169
05/27/2022 15:48:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=169
05/27/2022 15:48:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=170
05/27/2022 15:48:32 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.4459229045151683 on epoch=170
05/27/2022 15:48:32 - INFO - __main__ - Saving model with best Classification-F1: 0.42219064491523334 -> 0.4459229045151683 on epoch=170, global_step=2050
05/27/2022 15:48:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=171
05/27/2022 15:48:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=172
05/27/2022 15:48:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=173
05/27/2022 15:48:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=174
05/27/2022 15:48:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=174
05/27/2022 15:48:51 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.4198896116696235 on epoch=174
05/27/2022 15:48:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=175
05/27/2022 15:48:56 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=176
05/27/2022 15:48:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=177
05/27/2022 15:49:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=178
05/27/2022 15:49:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=179
05/27/2022 15:49:10 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.4035605971089842 on epoch=179
05/27/2022 15:49:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=179
05/27/2022 15:49:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=180
05/27/2022 15:49:18 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=181
05/27/2022 15:49:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=182
05/27/2022 15:49:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=183
05/27/2022 15:49:29 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.43111651362044245 on epoch=183
05/27/2022 15:49:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=184
05/27/2022 15:49:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=184
05/27/2022 15:49:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=185
05/27/2022 15:49:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=186
05/27/2022 15:49:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=187
05/27/2022 15:49:48 - INFO - __main__ - Global step 2250 Train loss 0.16 Classification-F1 0.44004439221830527 on epoch=187
05/27/2022 15:49:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=188
05/27/2022 15:49:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=189
05/27/2022 15:49:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=189
05/27/2022 15:49:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=190
05/27/2022 15:50:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=191
05/27/2022 15:50:08 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.3253904494382023 on epoch=191
05/27/2022 15:50:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=192
05/27/2022 15:50:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=193
05/27/2022 15:50:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=194
05/27/2022 15:50:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=194
05/27/2022 15:50:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=195
05/27/2022 15:50:27 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.4196291616256665 on epoch=195
05/27/2022 15:50:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=196
05/27/2022 15:50:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=197
05/27/2022 15:50:35 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=198
05/27/2022 15:50:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=199
05/27/2022 15:50:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=199
05/27/2022 15:50:46 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.4218783834327447 on epoch=199
05/27/2022 15:50:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=200
05/27/2022 15:50:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=201
05/27/2022 15:50:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=202
05/27/2022 15:50:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=203
05/27/2022 15:51:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=204
05/27/2022 15:51:05 - INFO - __main__ - Global step 2450 Train loss 0.14 Classification-F1 0.1579843339136459 on epoch=204
05/27/2022 15:51:08 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=204
05/27/2022 15:51:11 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=205
05/27/2022 15:51:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=206
05/27/2022 15:51:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=207
05/27/2022 15:51:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=208
05/27/2022 15:51:25 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.26053006554573954 on epoch=208
05/27/2022 15:51:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=209
05/27/2022 15:51:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=209
05/27/2022 15:51:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=210
05/27/2022 15:51:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=211
05/27/2022 15:51:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=212
05/27/2022 15:51:44 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.12381173619422184 on epoch=212
05/27/2022 15:51:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=213
05/27/2022 15:51:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=214
05/27/2022 15:51:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=214
05/27/2022 15:51:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.12 on epoch=215
05/27/2022 15:51:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=216
05/27/2022 15:52:03 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.29900329527522507 on epoch=216
05/27/2022 15:52:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=217
05/27/2022 15:52:08 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=218
05/27/2022 15:52:11 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=219
05/27/2022 15:52:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=219
05/27/2022 15:52:16 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=220
05/27/2022 15:52:22 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.13254181641928903 on epoch=220
05/27/2022 15:52:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.13 on epoch=221
05/27/2022 15:52:27 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=222
05/27/2022 15:52:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=223
05/27/2022 15:52:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=224
05/27/2022 15:52:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=224
05/27/2022 15:52:41 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.09916986554853442 on epoch=224
05/27/2022 15:52:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=225
05/27/2022 15:52:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=226
05/27/2022 15:52:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=227
05/27/2022 15:52:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=228
05/27/2022 15:52:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=229
05/27/2022 15:53:00 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.11538227543299572 on epoch=229
05/27/2022 15:53:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=229
05/27/2022 15:53:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=230
05/27/2022 15:53:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=231
05/27/2022 15:53:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=232
05/27/2022 15:53:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.16 on epoch=233
05/27/2022 15:53:20 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.11020458889277297 on epoch=233
05/27/2022 15:53:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=234
05/27/2022 15:53:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=234
05/27/2022 15:53:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=235
05/27/2022 15:53:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.14 on epoch=236
05/27/2022 15:53:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=237
05/27/2022 15:53:39 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.10122935179632556 on epoch=237
05/27/2022 15:53:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=238
05/27/2022 15:53:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=239
05/27/2022 15:53:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.11 on epoch=239
05/27/2022 15:53:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=240
05/27/2022 15:53:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=241
05/27/2022 15:53:58 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.11123526090278006 on epoch=241
05/27/2022 15:54:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=242
05/27/2022 15:54:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=243
05/27/2022 15:54:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=244
05/27/2022 15:54:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=244
05/27/2022 15:54:12 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=245
05/27/2022 15:54:17 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.12363095238095238 on epoch=245
05/27/2022 15:54:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=246
05/27/2022 15:54:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=247
05/27/2022 15:54:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=248
05/27/2022 15:54:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=249
05/27/2022 15:54:31 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=249
05/27/2022 15:54:32 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:54:32 - INFO - __main__ - Printing 3 examples
05/27/2022 15:54:32 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:54:32 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:32 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:54:32 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:32 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:54:32 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:32 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:54:32 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:54:33 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:54:33 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:54:33 - INFO - __main__ - Printing 3 examples
05/27/2022 15:54:33 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:54:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:33 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:54:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:33 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:54:33 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:33 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:54:33 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:54:33 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:54:36 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.16511000721358776 on epoch=249
05/27/2022 15:54:36 - INFO - __main__ - save last model!
05/27/2022 15:54:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 15:54:36 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 15:54:36 - INFO - __main__ - Printing 3 examples
05/27/2022 15:54:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 15:54:36 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 15:54:36 - INFO - __main__ - ['entailment']
05/27/2022 15:54:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 15:54:36 - INFO - __main__ - ['contradiction']
05/27/2022 15:54:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:54:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:54:38 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 15:54:49 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:54:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:54:49 - INFO - __main__ - Starting training!
05/27/2022 15:55:11 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_87_0.4_8_predictions.txt
05/27/2022 15:55:11 - INFO - __main__ - Classification-F1 on test data: 0.0315
05/27/2022 15:55:11 - INFO - __main__ - prefix=anli_64_87, lr=0.4, bsz=8, dev_performance=0.4459229045151683, test_performance=0.031466698103583796
05/27/2022 15:55:11 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.3, bsz=8 ...
05/27/2022 15:55:12 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:55:12 - INFO - __main__ - Printing 3 examples
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:55:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:55:12 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 15:55:12 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 15:55:12 - INFO - __main__ - Printing 3 examples
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 15:55:12 - INFO - __main__ - ['contradiction']
05/27/2022 15:55:12 - INFO - __main__ - Tokenizing Input ...
05/27/2022 15:55:12 - INFO - __main__ - Tokenizing Output ...
05/27/2022 15:55:13 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 15:55:28 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 15:55:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 15:55:29 - INFO - __main__ - Starting training!
05/27/2022 15:55:33 - INFO - __main__ - Step 10 Global step 10 Train loss 0.52 on epoch=0
05/27/2022 15:55:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=1
05/27/2022 15:55:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=2
05/27/2022 15:55:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=3
05/27/2022 15:55:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=4
05/27/2022 15:55:49 - INFO - __main__ - Global step 50 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 15:55:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 15:55:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=4
05/27/2022 15:55:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=5
05/27/2022 15:55:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=6
05/27/2022 15:55:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=7
05/27/2022 15:56:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=8
05/27/2022 15:56:07 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=8
05/27/2022 15:56:07 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1775766716943188 on epoch=8, global_step=100
05/27/2022 15:56:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=9
05/27/2022 15:56:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=9
05/27/2022 15:56:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
05/27/2022 15:56:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=11
05/27/2022 15:56:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=12
05/27/2022 15:56:25 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=12
05/27/2022 15:56:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.43 on epoch=13
05/27/2022 15:56:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
05/27/2022 15:56:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=14
05/27/2022 15:56:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
05/27/2022 15:56:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
05/27/2022 15:56:44 - INFO - __main__ - Global step 200 Train loss 0.46 Classification-F1 0.1775766716943188 on epoch=16
05/27/2022 15:56:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
05/27/2022 15:56:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=18
05/27/2022 15:56:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=19
05/27/2022 15:56:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
05/27/2022 15:56:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=20
05/27/2022 15:57:03 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 15:57:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
05/27/2022 15:57:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.47 on epoch=22
05/27/2022 15:57:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
05/27/2022 15:57:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=24
05/27/2022 15:57:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
05/27/2022 15:57:21 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.18687332567929582 on epoch=24
05/27/2022 15:57:21 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.18687332567929582 on epoch=24, global_step=300
05/27/2022 15:57:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=25
05/27/2022 15:57:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=26
05/27/2022 15:57:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=27
05/27/2022 15:57:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
05/27/2022 15:57:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=29
05/27/2022 15:57:40 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.18687332567929582 on epoch=29
05/27/2022 15:57:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
05/27/2022 15:57:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
05/27/2022 15:57:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
05/27/2022 15:57:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=32
05/27/2022 15:57:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
05/27/2022 15:57:58 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.21904088606103447 on epoch=33
05/27/2022 15:57:58 - INFO - __main__ - Saving model with best Classification-F1: 0.18687332567929582 -> 0.21904088606103447 on epoch=33, global_step=400
05/27/2022 15:58:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=34
05/27/2022 15:58:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=34
05/27/2022 15:58:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
05/27/2022 15:58:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=36
05/27/2022 15:58:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=37
05/27/2022 15:58:17 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.29584929757343553 on epoch=37
05/27/2022 15:58:17 - INFO - __main__ - Saving model with best Classification-F1: 0.21904088606103447 -> 0.29584929757343553 on epoch=37, global_step=450
05/27/2022 15:58:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=38
05/27/2022 15:58:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=39
05/27/2022 15:58:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
05/27/2022 15:58:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.39 on epoch=40
05/27/2022 15:58:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=41
05/27/2022 15:58:36 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.2933675562275119 on epoch=41
05/27/2022 15:58:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=42
05/27/2022 15:58:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=43
05/27/2022 15:58:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=44
05/27/2022 15:58:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=44
05/27/2022 15:58:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=45
05/27/2022 15:58:55 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.2386374491781125 on epoch=45
05/27/2022 15:58:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/27/2022 15:59:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=47
05/27/2022 15:59:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
05/27/2022 15:59:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=49
05/27/2022 15:59:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
05/27/2022 15:59:13 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.36830651028489575 on epoch=49
05/27/2022 15:59:13 - INFO - __main__ - Saving model with best Classification-F1: 0.29584929757343553 -> 0.36830651028489575 on epoch=49, global_step=600
05/27/2022 15:59:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
05/27/2022 15:59:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
05/27/2022 15:59:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=52
05/27/2022 15:59:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=53
05/27/2022 15:59:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
05/27/2022 15:59:32 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.3073083778966132 on epoch=54
05/27/2022 15:59:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=54
05/27/2022 15:59:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/27/2022 15:59:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=56
05/27/2022 15:59:42 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=57
05/27/2022 15:59:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=58
05/27/2022 15:59:50 - INFO - __main__ - Global step 700 Train loss 0.41 Classification-F1 0.3412698412698412 on epoch=58
05/27/2022 15:59:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
05/27/2022 15:59:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
05/27/2022 15:59:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=60
05/27/2022 16:00:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=61
05/27/2022 16:00:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=62
05/27/2022 16:00:08 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3493456434632905 on epoch=62
05/27/2022 16:00:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
05/27/2022 16:00:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=64
05/27/2022 16:00:16 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=64
05/27/2022 16:00:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=65
05/27/2022 16:00:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
05/27/2022 16:00:26 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.35426556354265565 on epoch=66
05/27/2022 16:00:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=67
05/27/2022 16:00:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=68
05/27/2022 16:00:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=69
05/27/2022 16:00:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=69
05/27/2022 16:00:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=70
05/27/2022 16:00:43 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.34848484848484845 on epoch=70
05/27/2022 16:00:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=71
05/27/2022 16:00:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=72
05/27/2022 16:00:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=73
05/27/2022 16:00:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=74
05/27/2022 16:00:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=74
05/27/2022 16:01:01 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3823260073260073 on epoch=74
05/27/2022 16:01:01 - INFO - __main__ - Saving model with best Classification-F1: 0.36830651028489575 -> 0.3823260073260073 on epoch=74, global_step=900
05/27/2022 16:01:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.47 on epoch=75
05/27/2022 16:01:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=76
05/27/2022 16:01:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=77
05/27/2022 16:01:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=78
05/27/2022 16:01:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=79
05/27/2022 16:01:19 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.34848484848484845 on epoch=79
05/27/2022 16:01:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=79
05/27/2022 16:01:24 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=80
05/27/2022 16:01:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=81
05/27/2022 16:01:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=82
05/27/2022 16:01:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=83
05/27/2022 16:01:37 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3655615706978062 on epoch=83
05/27/2022 16:01:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=84
05/27/2022 16:01:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.39 on epoch=84
05/27/2022 16:01:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.31 on epoch=85
05/27/2022 16:01:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=86
05/27/2022 16:01:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=87
05/27/2022 16:01:55 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3710826210826211 on epoch=87
05/27/2022 16:01:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=88
05/27/2022 16:02:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=89
05/27/2022 16:02:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=89
05/27/2022 16:02:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=90
05/27/2022 16:02:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=91
05/27/2022 16:02:12 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.3710826210826211 on epoch=91
05/27/2022 16:02:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=92
05/27/2022 16:02:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=93
05/27/2022 16:02:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=94
05/27/2022 16:02:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=94
05/27/2022 16:02:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=95
05/27/2022 16:02:30 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.3655615706978062 on epoch=95
05/27/2022 16:02:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=96
05/27/2022 16:02:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=97
05/27/2022 16:02:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=98
05/27/2022 16:02:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=99
05/27/2022 16:02:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=99
05/27/2022 16:02:48 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.35426556354265565 on epoch=99
05/27/2022 16:02:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=100
05/27/2022 16:02:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=101
05/27/2022 16:02:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=102
05/27/2022 16:02:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=103
05/27/2022 16:03:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=104
05/27/2022 16:03:06 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.34848484848484845 on epoch=104
05/27/2022 16:03:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=104
05/27/2022 16:03:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=105
05/27/2022 16:03:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=106
05/27/2022 16:03:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=107
05/27/2022 16:03:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=108
05/27/2022 16:03:23 - INFO - __main__ - Global step 1300 Train loss 0.32 Classification-F1 0.411201262114041 on epoch=108
05/27/2022 16:03:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3823260073260073 -> 0.411201262114041 on epoch=108, global_step=1300
05/27/2022 16:03:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=109
05/27/2022 16:03:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=109
05/27/2022 16:03:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.38 on epoch=110
05/27/2022 16:03:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=111
05/27/2022 16:03:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=112
05/27/2022 16:03:41 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.38730158730158726 on epoch=112
05/27/2022 16:03:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=113
05/27/2022 16:03:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=114
05/27/2022 16:03:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=114
05/27/2022 16:03:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=115
05/27/2022 16:03:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=116
05/27/2022 16:03:59 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.4113404488004762 on epoch=116
05/27/2022 16:03:59 - INFO - __main__ - Saving model with best Classification-F1: 0.411201262114041 -> 0.4113404488004762 on epoch=116, global_step=1400
05/27/2022 16:04:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=117
05/27/2022 16:04:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=118
05/27/2022 16:04:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=119
05/27/2022 16:04:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.31 on epoch=119
05/27/2022 16:04:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=120
05/27/2022 16:04:17 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.3773920783266578 on epoch=120
05/27/2022 16:04:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=121
05/27/2022 16:04:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=122
05/27/2022 16:04:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=123
05/27/2022 16:04:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=124
05/27/2022 16:04:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=124
05/27/2022 16:04:35 - INFO - __main__ - Global step 1500 Train loss 0.31 Classification-F1 0.3984386252988794 on epoch=124
05/27/2022 16:04:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=125
05/27/2022 16:04:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.32 on epoch=126
05/27/2022 16:04:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=127
05/27/2022 16:04:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=128
05/27/2022 16:04:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=129
05/27/2022 16:04:53 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.40332343229494355 on epoch=129
05/27/2022 16:04:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=129
05/27/2022 16:04:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=130
05/27/2022 16:05:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=131
05/27/2022 16:05:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=132
05/27/2022 16:05:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=133
05/27/2022 16:05:10 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.4045864735519908 on epoch=133
05/27/2022 16:05:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=134
05/27/2022 16:05:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=134
05/27/2022 16:05:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=135
05/27/2022 16:05:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=136
05/27/2022 16:05:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=137
05/27/2022 16:05:28 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.40472777356157064 on epoch=137
05/27/2022 16:05:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=138
05/27/2022 16:05:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=139
05/27/2022 16:05:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=139
05/27/2022 16:05:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=140
05/27/2022 16:05:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.28 on epoch=141
05/27/2022 16:05:46 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.3916256157635469 on epoch=141
05/27/2022 16:05:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=142
05/27/2022 16:05:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=143
05/27/2022 16:05:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=144
05/27/2022 16:05:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=144
05/27/2022 16:05:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=145
05/27/2022 16:06:04 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.3914835164835164 on epoch=145
05/27/2022 16:06:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.29 on epoch=146
05/27/2022 16:06:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=147
05/27/2022 16:06:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=148
05/27/2022 16:06:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=149
05/27/2022 16:06:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=149
05/27/2022 16:06:22 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.3812409812409812 on epoch=149
05/27/2022 16:06:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=150
05/27/2022 16:06:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=151
05/27/2022 16:06:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=152
05/27/2022 16:06:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=153
05/27/2022 16:06:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=154
05/27/2022 16:06:39 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.3768926158863266 on epoch=154
05/27/2022 16:06:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=154
05/27/2022 16:06:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.30 on epoch=155
05/27/2022 16:06:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=156
05/27/2022 16:06:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=157
05/27/2022 16:06:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=158
05/27/2022 16:06:57 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.3883415435139573 on epoch=158
05/27/2022 16:07:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=159
05/27/2022 16:07:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.27 on epoch=159
05/27/2022 16:07:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=160
05/27/2022 16:07:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=161
05/27/2022 16:07:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=162
05/27/2022 16:07:15 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.4051161186575891 on epoch=162
05/27/2022 16:07:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.28 on epoch=163
05/27/2022 16:07:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=164
05/27/2022 16:07:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=164
05/27/2022 16:07:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=165
05/27/2022 16:07:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=166
05/27/2022 16:07:33 - INFO - __main__ - Global step 2000 Train loss 0.26 Classification-F1 0.3969668675551028 on epoch=166
05/27/2022 16:07:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=167
05/27/2022 16:07:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.30 on epoch=168
05/27/2022 16:07:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=169
05/27/2022 16:07:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=169
05/27/2022 16:07:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=170
05/27/2022 16:07:51 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.406824173942818 on epoch=170
05/27/2022 16:07:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=171
05/27/2022 16:07:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=172
05/27/2022 16:07:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.25 on epoch=173
05/27/2022 16:08:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.21 on epoch=174
05/27/2022 16:08:04 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.30 on epoch=174
05/27/2022 16:08:08 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.38633676975945014 on epoch=174
05/27/2022 16:08:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=175
05/27/2022 16:08:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=176
05/27/2022 16:08:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=177
05/27/2022 16:08:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=178
05/27/2022 16:08:22 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=179
05/27/2022 16:08:26 - INFO - __main__ - Global step 2150 Train loss 0.24 Classification-F1 0.39414967137739415 on epoch=179
05/27/2022 16:08:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=179
05/27/2022 16:08:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.20 on epoch=180
05/27/2022 16:08:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.25 on epoch=181
05/27/2022 16:08:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=182
05/27/2022 16:08:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=183
05/27/2022 16:08:44 - INFO - __main__ - Global step 2200 Train loss 0.24 Classification-F1 0.4025248925462752 on epoch=183
05/27/2022 16:08:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=184
05/27/2022 16:08:49 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=184
05/27/2022 16:08:52 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.26 on epoch=185
05/27/2022 16:08:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.24 on epoch=186
05/27/2022 16:08:57 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=187
05/27/2022 16:09:02 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.41872372308223005 on epoch=187
05/27/2022 16:09:02 - INFO - __main__ - Saving model with best Classification-F1: 0.4113404488004762 -> 0.41872372308223005 on epoch=187, global_step=2250
05/27/2022 16:09:05 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=188
05/27/2022 16:09:08 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=189
05/27/2022 16:09:10 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=189
05/27/2022 16:09:13 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=190
05/27/2022 16:09:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=191
05/27/2022 16:09:21 - INFO - __main__ - Global step 2300 Train loss 0.22 Classification-F1 0.4100891861761427 on epoch=191
05/27/2022 16:09:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.24 on epoch=192
05/27/2022 16:09:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.31 on epoch=193
05/27/2022 16:09:28 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=194
05/27/2022 16:09:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=194
05/27/2022 16:09:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.24 on epoch=195
05/27/2022 16:09:39 - INFO - __main__ - Global step 2350 Train loss 0.24 Classification-F1 0.39024955127091654 on epoch=195
05/27/2022 16:09:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.25 on epoch=196
05/27/2022 16:09:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.20 on epoch=197
05/27/2022 16:09:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=198
05/27/2022 16:09:49 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.29 on epoch=199
05/27/2022 16:09:52 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.23 on epoch=199
05/27/2022 16:09:57 - INFO - __main__ - Global step 2400 Train loss 0.24 Classification-F1 0.4210112911143839 on epoch=199
05/27/2022 16:09:57 - INFO - __main__ - Saving model with best Classification-F1: 0.41872372308223005 -> 0.4210112911143839 on epoch=199, global_step=2400
05/27/2022 16:09:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=200
05/27/2022 16:10:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.23 on epoch=201
05/27/2022 16:10:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=202
05/27/2022 16:10:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.24 on epoch=203
05/27/2022 16:10:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.22 on epoch=204
05/27/2022 16:10:15 - INFO - __main__ - Global step 2450 Train loss 0.22 Classification-F1 0.31089659224441835 on epoch=204
05/27/2022 16:10:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=204
05/27/2022 16:10:20 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=205
05/27/2022 16:10:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=206
05/27/2022 16:10:25 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.21 on epoch=207
05/27/2022 16:10:28 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=208
05/27/2022 16:10:33 - INFO - __main__ - Global step 2500 Train loss 0.21 Classification-F1 0.41242691369092094 on epoch=208
05/27/2022 16:10:36 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=209
05/27/2022 16:10:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.23 on epoch=209
05/27/2022 16:10:41 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=210
05/27/2022 16:10:44 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.22 on epoch=211
05/27/2022 16:10:47 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=212
05/27/2022 16:10:52 - INFO - __main__ - Global step 2550 Train loss 0.20 Classification-F1 0.2621094369547977 on epoch=212
05/27/2022 16:10:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=213
05/27/2022 16:10:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=214
05/27/2022 16:11:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.22 on epoch=214
05/27/2022 16:11:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=215
05/27/2022 16:11:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.19 on epoch=216
05/27/2022 16:11:11 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3235119047619047 on epoch=216
05/27/2022 16:11:13 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=217
05/27/2022 16:11:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=218
05/27/2022 16:11:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=219
05/27/2022 16:11:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=219
05/27/2022 16:11:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=220
05/27/2022 16:11:29 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.3086067158647804 on epoch=220
05/27/2022 16:11:32 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.20 on epoch=221
05/27/2022 16:11:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.19 on epoch=222
05/27/2022 16:11:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=223
05/27/2022 16:11:40 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=224
05/27/2022 16:11:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=224
05/27/2022 16:11:48 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.3100444654104534 on epoch=224
05/27/2022 16:11:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.16 on epoch=225
05/27/2022 16:11:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.19 on epoch=226
05/27/2022 16:11:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=227
05/27/2022 16:11:58 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.16 on epoch=228
05/27/2022 16:12:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=229
05/27/2022 16:12:07 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.2523308270676692 on epoch=229
05/27/2022 16:12:09 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=229
05/27/2022 16:12:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=230
05/27/2022 16:12:14 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=231
05/27/2022 16:12:17 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=232
05/27/2022 16:12:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=233
05/27/2022 16:12:25 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.32311875992738825 on epoch=233
05/27/2022 16:12:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=234
05/27/2022 16:12:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.19 on epoch=234
05/27/2022 16:12:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=235
05/27/2022 16:12:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=236
05/27/2022 16:12:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=237
05/27/2022 16:12:44 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.23048665725616724 on epoch=237
05/27/2022 16:12:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=238
05/27/2022 16:12:49 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=239
05/27/2022 16:12:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=239
05/27/2022 16:12:54 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.19 on epoch=240
05/27/2022 16:12:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.19 on epoch=241
05/27/2022 16:13:03 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.22831492147420765 on epoch=241
05/27/2022 16:13:05 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.20 on epoch=242
05/27/2022 16:13:08 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=243
05/27/2022 16:13:11 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.16 on epoch=244
05/27/2022 16:13:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=244
05/27/2022 16:13:16 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=245
05/27/2022 16:13:21 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.26535251935349435 on epoch=245
05/27/2022 16:13:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.19 on epoch=246
05/27/2022 16:13:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=247
05/27/2022 16:13:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.15 on epoch=248
05/27/2022 16:13:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.17 on epoch=249
05/27/2022 16:13:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.22 on epoch=249
05/27/2022 16:13:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 16:13:36 - INFO - __main__ - Printing 3 examples
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:13:36 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:13:36 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 16:13:36 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 16:13:36 - INFO - __main__ - Printing 3 examples
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 16:13:36 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:36 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:13:37 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:13:37 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 16:13:40 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.19060954101604508 on epoch=249
05/27/2022 16:13:40 - INFO - __main__ - save last model!
05/27/2022 16:13:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 16:13:40 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 16:13:40 - INFO - __main__ - Printing 3 examples
05/27/2022 16:13:40 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 16:13:40 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:40 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 16:13:40 - INFO - __main__ - ['entailment']
05/27/2022 16:13:40 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 16:13:40 - INFO - __main__ - ['contradiction']
05/27/2022 16:13:40 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:13:41 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:13:42 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 16:13:52 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 16:13:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 16:13:53 - INFO - __main__ - Starting training!
05/27/2022 16:14:11 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_87_0.3_8_predictions.txt
05/27/2022 16:14:11 - INFO - __main__ - Classification-F1 on test data: 0.0876
05/27/2022 16:14:12 - INFO - __main__ - prefix=anli_64_87, lr=0.3, bsz=8, dev_performance=0.4210112911143839, test_performance=0.08756432150535888
05/27/2022 16:14:12 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.2, bsz=8 ...
05/27/2022 16:14:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 16:14:13 - INFO - __main__ - Printing 3 examples
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:14:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:14:13 - INFO - __main__ - Loaded 192 examples from train data
05/27/2022 16:14:13 - INFO - __main__ - Start tokenizing ... 192 instances
05/27/2022 16:14:13 - INFO - __main__ - Printing 3 examples
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
05/27/2022 16:14:13 - INFO - __main__ - ['contradiction']
05/27/2022 16:14:13 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:14:13 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:14:13 - INFO - __main__ - Loaded 192 examples from dev data
05/27/2022 16:14:29 - INFO - __main__ - load prompt embedding from ckpt
05/27/2022 16:14:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/27/2022 16:14:30 - INFO - __main__ - Starting training!
05/27/2022 16:14:33 - INFO - __main__ - Step 10 Global step 10 Train loss 0.51 on epoch=0
05/27/2022 16:14:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=1
05/27/2022 16:14:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=2
05/27/2022 16:14:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=3
05/27/2022 16:14:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=4
05/27/2022 16:14:49 - INFO - __main__ - Global step 50 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=4
05/27/2022 16:14:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
05/27/2022 16:14:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=4
05/27/2022 16:14:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=5
05/27/2022 16:14:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=6
05/27/2022 16:15:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
05/27/2022 16:15:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=8
05/27/2022 16:15:08 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=8
05/27/2022 16:15:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
05/27/2022 16:15:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
05/27/2022 16:15:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=10
05/27/2022 16:15:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=11
05/27/2022 16:15:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=12
05/27/2022 16:15:26 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
05/27/2022 16:15:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=13
05/27/2022 16:15:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=14
05/27/2022 16:15:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=14
05/27/2022 16:15:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
05/27/2022 16:15:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
05/27/2022 16:15:44 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=16
05/27/2022 16:15:44 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1775766716943188 on epoch=16, global_step=200
05/27/2022 16:15:47 - INFO - __main__ - Step 210 Global step 210 Train loss 0.49 on epoch=17
05/27/2022 16:15:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=18
05/27/2022 16:15:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
05/27/2022 16:15:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
05/27/2022 16:15:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=20
05/27/2022 16:16:02 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=20
05/27/2022 16:16:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.45 on epoch=21
05/27/2022 16:16:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
05/27/2022 16:16:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=23
05/27/2022 16:16:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=24
05/27/2022 16:16:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
05/27/2022 16:16:20 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
05/27/2022 16:16:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=25
05/27/2022 16:16:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
05/27/2022 16:16:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
05/27/2022 16:16:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
05/27/2022 16:16:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
05/27/2022 16:16:38 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
05/27/2022 16:16:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=29
05/27/2022 16:16:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=30
05/27/2022 16:16:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.42 on epoch=31
05/27/2022 16:16:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=32
05/27/2022 16:16:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
05/27/2022 16:16:56 - INFO - __main__ - Global step 400 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=33
05/27/2022 16:16:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
05/27/2022 16:17:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=34
05/27/2022 16:17:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=35
05/27/2022 16:17:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
05/27/2022 16:17:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
05/27/2022 16:17:14 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1775766716943188 on epoch=37
05/27/2022 16:17:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
05/27/2022 16:17:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=39
05/27/2022 16:17:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=39
05/27/2022 16:17:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=40
05/27/2022 16:17:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=41
05/27/2022 16:17:33 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.27400868712344123 on epoch=41
05/27/2022 16:17:33 - INFO - __main__ - Saving model with best Classification-F1: 0.1775766716943188 -> 0.27400868712344123 on epoch=41, global_step=500
05/27/2022 16:17:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=42
05/27/2022 16:17:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=43
05/27/2022 16:17:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=44
05/27/2022 16:17:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=44
05/27/2022 16:17:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
05/27/2022 16:17:51 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.1775766716943188 on epoch=45
05/27/2022 16:17:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
05/27/2022 16:17:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=47
05/27/2022 16:17:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
05/27/2022 16:18:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
05/27/2022 16:18:04 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=49
05/27/2022 16:18:10 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=49
05/27/2022 16:18:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
05/27/2022 16:18:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=51
05/27/2022 16:18:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
05/27/2022 16:18:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=53
05/27/2022 16:18:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
05/27/2022 16:18:29 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.24781076623181886 on epoch=54
05/27/2022 16:18:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
05/27/2022 16:18:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
05/27/2022 16:18:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=56
05/27/2022 16:18:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=57
05/27/2022 16:18:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=58
05/27/2022 16:18:47 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.25486338797814206 on epoch=58
05/27/2022 16:18:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
05/27/2022 16:18:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
05/27/2022 16:18:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=60
05/27/2022 16:18:58 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
05/27/2022 16:19:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
05/27/2022 16:19:06 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.24693546005021416 on epoch=62
05/27/2022 16:19:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
05/27/2022 16:19:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=64
05/27/2022 16:19:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
05/27/2022 16:19:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
05/27/2022 16:19:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=66
05/27/2022 16:19:25 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.33637544603949815 on epoch=66
05/27/2022 16:19:25 - INFO - __main__ - Saving model with best Classification-F1: 0.27400868712344123 -> 0.33637544603949815 on epoch=66, global_step=800
05/27/2022 16:19:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=67
05/27/2022 16:19:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=68
05/27/2022 16:19:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=69
05/27/2022 16:19:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=69
05/27/2022 16:19:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=70
05/27/2022 16:19:43 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.26362019723015573 on epoch=70
05/27/2022 16:19:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
05/27/2022 16:19:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=72
05/27/2022 16:19:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=73
05/27/2022 16:19:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=74
05/27/2022 16:19:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=74
05/27/2022 16:20:02 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.3580074920281106 on epoch=74
05/27/2022 16:20:02 - INFO - __main__ - Saving model with best Classification-F1: 0.33637544603949815 -> 0.3580074920281106 on epoch=74, global_step=900
05/27/2022 16:20:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=75
05/27/2022 16:20:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=76
05/27/2022 16:20:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=77
05/27/2022 16:20:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=78
05/27/2022 16:20:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
05/27/2022 16:20:20 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.3213371480198765 on epoch=79
05/27/2022 16:20:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
05/27/2022 16:20:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=80
05/27/2022 16:20:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
05/27/2022 16:20:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=82
05/27/2022 16:20:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=83
05/27/2022 16:20:38 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.33816425120772947 on epoch=83
05/27/2022 16:20:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
05/27/2022 16:20:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=84
05/27/2022 16:20:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=85
05/27/2022 16:20:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
05/27/2022 16:20:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=87
05/27/2022 16:20:57 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.35369532428355965 on epoch=87
05/27/2022 16:20:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=88
05/27/2022 16:21:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
05/27/2022 16:21:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=89
05/27/2022 16:21:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
05/27/2022 16:21:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=91
05/27/2022 16:21:15 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.3467547793249941 on epoch=91
05/27/2022 16:21:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=92
05/27/2022 16:21:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=93
05/27/2022 16:21:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=94
05/27/2022 16:21:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
05/27/2022 16:21:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=95
05/27/2022 16:21:33 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.35201793721973096 on epoch=95
05/27/2022 16:21:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=96
05/27/2022 16:21:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=97
05/27/2022 16:21:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=98
05/27/2022 16:21:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=99
05/27/2022 16:21:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=99
05/27/2022 16:21:52 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.36456081815552066 on epoch=99
05/27/2022 16:21:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3580074920281106 -> 0.36456081815552066 on epoch=99, global_step=1200
05/27/2022 16:21:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
05/27/2022 16:21:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=101
05/27/2022 16:21:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=102
05/27/2022 16:22:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
05/27/2022 16:22:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=104
05/27/2022 16:22:10 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.3468487089445314 on epoch=104
05/27/2022 16:22:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=104
05/27/2022 16:22:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=105
05/27/2022 16:22:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=106
05/27/2022 16:22:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=107
05/27/2022 16:22:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=108
05/27/2022 16:22:28 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.3464484893056321 on epoch=108
05/27/2022 16:22:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=109
05/27/2022 16:22:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=109
05/27/2022 16:22:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=110
05/27/2022 16:22:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=111
05/27/2022 16:22:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=112
05/27/2022 16:22:46 - INFO - __main__ - Global step 1350 Train loss 0.36 Classification-F1 0.35510340024828174 on epoch=112
05/27/2022 16:22:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=113
05/27/2022 16:22:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=114
05/27/2022 16:22:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=114
05/27/2022 16:22:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=115
05/27/2022 16:22:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=116
05/27/2022 16:23:04 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.37818401743635394 on epoch=116
05/27/2022 16:23:04 - INFO - __main__ - Saving model with best Classification-F1: 0.36456081815552066 -> 0.37818401743635394 on epoch=116, global_step=1400
05/27/2022 16:23:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=117
05/27/2022 16:23:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=118
05/27/2022 16:23:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=119
05/27/2022 16:23:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=119
05/27/2022 16:23:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=120
05/27/2022 16:23:21 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.36345506345506345 on epoch=120
05/27/2022 16:23:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=121
05/27/2022 16:23:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=122
05/27/2022 16:23:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=123
05/27/2022 16:23:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.34 on epoch=124
05/27/2022 16:23:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=124
05/27/2022 16:23:39 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3710826210826211 on epoch=124
05/27/2022 16:23:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.36 on epoch=125
05/27/2022 16:23:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=126
05/27/2022 16:23:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=127
05/27/2022 16:23:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=128
05/27/2022 16:23:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=129
05/27/2022 16:23:57 - INFO - __main__ - Global step 1550 Train loss 0.32 Classification-F1 0.3710826210826211 on epoch=129
05/27/2022 16:23:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=129
05/27/2022 16:24:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=130
05/27/2022 16:24:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=131
05/27/2022 16:24:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=132
05/27/2022 16:24:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=133
05/27/2022 16:24:15 - INFO - __main__ - Global step 1600 Train loss 0.34 Classification-F1 0.3765227021040974 on epoch=133
05/27/2022 16:24:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=134
05/27/2022 16:24:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=134
05/27/2022 16:24:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.31 on epoch=135
05/27/2022 16:24:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.36 on epoch=136
05/27/2022 16:24:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=137
05/27/2022 16:24:33 - INFO - __main__ - Global step 1650 Train loss 0.34 Classification-F1 0.3710826210826211 on epoch=137
05/27/2022 16:24:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=138
05/27/2022 16:24:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=139
05/27/2022 16:24:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=139
05/27/2022 16:24:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=140
05/27/2022 16:24:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.31 on epoch=141
05/27/2022 16:24:50 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.38001638001638005 on epoch=141
05/27/2022 16:24:50 - INFO - __main__ - Saving model with best Classification-F1: 0.37818401743635394 -> 0.38001638001638005 on epoch=141, global_step=1700
05/27/2022 16:24:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=142
05/27/2022 16:24:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.34 on epoch=143
05/27/2022 16:24:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=144
05/27/2022 16:25:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=144
05/27/2022 16:25:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=145
05/27/2022 16:25:08 - INFO - __main__ - Global step 1750 Train loss 0.32 Classification-F1 0.3765227021040974 on epoch=145
05/27/2022 16:25:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=146
05/27/2022 16:25:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=147
05/27/2022 16:25:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=148
05/27/2022 16:25:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=149
05/27/2022 16:25:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=149
05/27/2022 16:25:26 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.36307529536236993 on epoch=149
05/27/2022 16:25:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=150
05/27/2022 16:25:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=151
05/27/2022 16:25:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.29 on epoch=152
05/27/2022 16:25:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.36 on epoch=153
05/27/2022 16:25:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=154
05/27/2022 16:25:44 - INFO - __main__ - Global step 1850 Train loss 0.32 Classification-F1 0.368449534172846 on epoch=154
05/27/2022 16:25:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=154
05/27/2022 16:25:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=155
05/27/2022 16:25:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=156
05/27/2022 16:25:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.30 on epoch=157
05/27/2022 16:25:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=158
05/27/2022 16:26:01 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.368449534172846 on epoch=158
05/27/2022 16:26:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=159
05/27/2022 16:26:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=159
05/27/2022 16:26:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.29 on epoch=160
05/27/2022 16:26:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=161
05/27/2022 16:26:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=162
05/27/2022 16:26:19 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.3724798683359784 on epoch=162
05/27/2022 16:26:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=163
05/27/2022 16:26:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.27 on epoch=164
05/27/2022 16:26:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.33 on epoch=164
05/27/2022 16:26:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=165
05/27/2022 16:26:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.28 on epoch=166
05/27/2022 16:26:37 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.4093937094190066 on epoch=166
05/27/2022 16:26:37 - INFO - __main__ - Saving model with best Classification-F1: 0.38001638001638005 -> 0.4093937094190066 on epoch=166, global_step=2000
05/27/2022 16:26:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.27 on epoch=167
05/27/2022 16:26:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.28 on epoch=168
05/27/2022 16:26:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.29 on epoch=169
05/27/2022 16:26:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.32 on epoch=169
05/27/2022 16:26:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.27 on epoch=170
05/27/2022 16:26:55 - INFO - __main__ - Global step 2050 Train loss 0.29 Classification-F1 0.3873854616447921 on epoch=170
05/27/2022 16:26:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.28 on epoch=171
05/27/2022 16:27:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.27 on epoch=172
05/27/2022 16:27:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.29 on epoch=173
05/27/2022 16:27:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=174
05/27/2022 16:27:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.32 on epoch=174
05/27/2022 16:27:13 - INFO - __main__ - Global step 2100 Train loss 0.30 Classification-F1 0.3670727205610927 on epoch=174
05/27/2022 16:27:16 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.31 on epoch=175
05/27/2022 16:27:19 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.27 on epoch=176
05/27/2022 16:27:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.32 on epoch=177
05/27/2022 16:27:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.28 on epoch=178
05/27/2022 16:27:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=179
05/27/2022 16:27:31 - INFO - __main__ - Global step 2150 Train loss 0.30 Classification-F1 0.37777192674507054 on epoch=179
05/27/2022 16:27:34 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=179
05/27/2022 16:27:36 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=180
05/27/2022 16:27:39 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.27 on epoch=181
05/27/2022 16:27:42 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.27 on epoch=182
05/27/2022 16:27:44 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.28 on epoch=183
05/27/2022 16:27:49 - INFO - __main__ - Global step 2200 Train loss 0.27 Classification-F1 0.3873854616447921 on epoch=183
05/27/2022 16:27:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=184
05/27/2022 16:27:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.32 on epoch=184
05/27/2022 16:27:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.29 on epoch=185
05/27/2022 16:28:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.30 on epoch=186
05/27/2022 16:28:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=187
05/27/2022 16:28:07 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.37780995422169567 on epoch=187
05/27/2022 16:28:10 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.26 on epoch=188
05/27/2022 16:28:12 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=189
05/27/2022 16:28:15 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.29 on epoch=189
05/27/2022 16:28:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.30 on epoch=190
05/27/2022 16:28:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.28 on epoch=191
05/27/2022 16:28:25 - INFO - __main__ - Global step 2300 Train loss 0.28 Classification-F1 0.38394476625124335 on epoch=191
05/27/2022 16:28:28 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.27 on epoch=192
05/27/2022 16:28:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.29 on epoch=193
05/27/2022 16:28:33 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.28 on epoch=194
05/27/2022 16:28:36 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.27 on epoch=194
05/27/2022 16:28:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.26 on epoch=195
05/27/2022 16:28:43 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.40240548359055484 on epoch=195
05/27/2022 16:28:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.27 on epoch=196
05/27/2022 16:28:49 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=197
05/27/2022 16:28:51 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.27 on epoch=198
05/27/2022 16:28:54 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.32 on epoch=199
05/27/2022 16:28:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=199
05/27/2022 16:29:02 - INFO - __main__ - Global step 2400 Train loss 0.28 Classification-F1 0.38500797448165863 on epoch=199
05/27/2022 16:29:04 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=200
05/27/2022 16:29:07 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.29 on epoch=201
05/27/2022 16:29:10 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=202
05/27/2022 16:29:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=203
05/27/2022 16:29:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.30 on epoch=204
05/27/2022 16:29:20 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.3821325564879632 on epoch=204
05/27/2022 16:29:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.28 on epoch=204
05/27/2022 16:29:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.27 on epoch=205
05/27/2022 16:29:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=206
05/27/2022 16:29:31 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=207
05/27/2022 16:29:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.30 on epoch=208
05/27/2022 16:29:39 - INFO - __main__ - Global step 2500 Train loss 0.28 Classification-F1 0.39988093719436996 on epoch=208
05/27/2022 16:29:41 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.27 on epoch=209
05/27/2022 16:29:44 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=209
05/27/2022 16:29:47 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=210
05/27/2022 16:29:49 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=211
05/27/2022 16:29:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=212
05/27/2022 16:29:57 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.3825572801182557 on epoch=212
05/27/2022 16:30:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.29 on epoch=213
05/27/2022 16:30:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.30 on epoch=214
05/27/2022 16:30:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=214
05/27/2022 16:30:08 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.29 on epoch=215
05/27/2022 16:30:11 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.24 on epoch=216
05/27/2022 16:30:16 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.37777192674507054 on epoch=216
05/27/2022 16:30:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=217
05/27/2022 16:30:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.27 on epoch=218
05/27/2022 16:30:24 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.25 on epoch=219
05/27/2022 16:30:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=219
05/27/2022 16:30:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=220
05/27/2022 16:30:34 - INFO - __main__ - Global step 2650 Train loss 0.24 Classification-F1 0.3942208462332301 on epoch=220
05/27/2022 16:30:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=221
05/27/2022 16:30:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.26 on epoch=222
05/27/2022 16:30:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.32 on epoch=223
05/27/2022 16:30:45 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.25 on epoch=224
05/27/2022 16:30:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.28 on epoch=224
05/27/2022 16:30:53 - INFO - __main__ - Global step 2700 Train loss 0.27 Classification-F1 0.3882984901277584 on epoch=224
05/27/2022 16:30:55 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=225
05/27/2022 16:30:58 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.31 on epoch=226
05/27/2022 16:31:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=227
05/27/2022 16:31:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.25 on epoch=228
05/27/2022 16:31:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=229
05/27/2022 16:31:11 - INFO - __main__ - Global step 2750 Train loss 0.26 Classification-F1 0.3807348850827112 on epoch=229
05/27/2022 16:31:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.30 on epoch=229
05/27/2022 16:31:16 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.29 on epoch=230
05/27/2022 16:31:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.21 on epoch=231
05/27/2022 16:31:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=232
05/27/2022 16:31:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.29 on epoch=233
05/27/2022 16:31:30 - INFO - __main__ - Global step 2800 Train loss 0.26 Classification-F1 0.41996696114343174 on epoch=233
05/27/2022 16:31:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4093937094190066 -> 0.41996696114343174 on epoch=233, global_step=2800
05/27/2022 16:31:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.25 on epoch=234
05/27/2022 16:31:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.26 on epoch=234
05/27/2022 16:31:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.22 on epoch=235
05/27/2022 16:31:41 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.23 on epoch=236
05/27/2022 16:31:43 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.24 on epoch=237
05/27/2022 16:31:49 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.4074753593362715 on epoch=237
05/27/2022 16:31:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=238
05/27/2022 16:31:54 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.20 on epoch=239
05/27/2022 16:31:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.25 on epoch=239
05/27/2022 16:31:59 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.23 on epoch=240
05/27/2022 16:32:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.23 on epoch=241
05/27/2022 16:32:08 - INFO - __main__ - Global step 2900 Train loss 0.22 Classification-F1 0.4055546166413159 on epoch=241
05/27/2022 16:32:10 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.22 on epoch=242
05/27/2022 16:32:13 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.20 on epoch=243
05/27/2022 16:32:15 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.21 on epoch=244
05/27/2022 16:32:18 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.22 on epoch=244
05/27/2022 16:32:21 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=245
05/27/2022 16:32:26 - INFO - __main__ - Global step 2950 Train loss 0.21 Classification-F1 0.3964382500967867 on epoch=245
05/27/2022 16:32:29 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=246
05/27/2022 16:32:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.23 on epoch=247
05/27/2022 16:32:34 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.29 on epoch=248
05/27/2022 16:32:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=249
05/27/2022 16:32:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.26 on epoch=249
05/27/2022 16:32:45 - INFO - __main__ - Global step 3000 Train loss 0.25 Classification-F1 0.4095173119563363 on epoch=249
05/27/2022 16:32:45 - INFO - __main__ - save last model!
05/27/2022 16:32:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/27/2022 16:32:45 - INFO - __main__ - Start tokenizing ... 1000 instances
05/27/2022 16:32:45 - INFO - __main__ - Printing 3 examples
05/27/2022 16:32:45 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
05/27/2022 16:32:45 - INFO - __main__ - ['contradiction']
05/27/2022 16:32:45 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
05/27/2022 16:32:45 - INFO - __main__ - ['entailment']
05/27/2022 16:32:45 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
05/27/2022 16:32:45 - INFO - __main__ - ['contradiction']
05/27/2022 16:32:45 - INFO - __main__ - Tokenizing Input ...
05/27/2022 16:32:46 - INFO - __main__ - Tokenizing Output ...
05/27/2022 16:32:47 - INFO - __main__ - Loaded 1000 examples from test data
05/27/2022 16:33:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-anli/anli_64_87_0.2_8_predictions.txt
05/27/2022 16:33:16 - INFO - __main__ - Classification-F1 on test data: 0.2810
05/27/2022 16:33:16 - INFO - __main__ - prefix=anli_64_87, lr=0.2, bsz=8, dev_performance=0.41996696114343174, test_performance=0.28102164962209913
