05/25/2022 18:23:02 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/25/2022 18:23:02 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo
05/25/2022 18:23:02 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-multitask-cls2cls-5e-1-4-20-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-multitask-cls2cls-5e-1-4-20/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
05/25/2022 18:23:02 - INFO - __main__ - models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo
05/25/2022 18:23:03 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/25/2022 18:23:03 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/25/2022 18:23:03 - INFO - __main__ - args.device: cuda:1
05/25/2022 18:23:03 - INFO - __main__ - Using 2 gpus
05/25/2022 18:23:03 - INFO - __main__ - args.device: cuda:0
05/25/2022 18:23:03 - INFO - __main__ - Using 2 gpus
05/25/2022 18:23:03 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/25/2022 18:23:03 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/25/2022 18:23:08 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.5, bsz=8 ...
05/25/2022 18:23:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:23:09 - INFO - __main__ - Printing 3 examples
05/25/2022 18:23:09 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:23:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:23:09 - INFO - __main__ - Printing 3 examples
05/25/2022 18:23:09 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:23:09 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:23:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:23:09 - INFO - __main__ - Printing 3 examples
05/25/2022 18:23:09 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:23:09 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:23:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:23:09 - INFO - __main__ - Printing 3 examples
05/25/2022 18:23:09 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:23:09 - INFO - __main__ - ['others']
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:23:09 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:23:10 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:23:10 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:23:27 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:23:28 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:23:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:23:28 - INFO - __main__ - Starting training!
05/25/2022 18:23:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:23:34 - INFO - __main__ - Starting training!
05/25/2022 18:23:37 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=0
05/25/2022 18:23:40 - INFO - __main__ - Step 20 Global step 20 Train loss 2.61 on epoch=1
05/25/2022 18:23:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.22 on epoch=1
05/25/2022 18:23:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.88 on epoch=2
05/25/2022 18:23:47 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=3
05/25/2022 18:23:51 - INFO - __main__ - Global step 50 Train loss 2.41 Classification-F1 0.33881778177080185 on epoch=3
05/25/2022 18:23:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.33881778177080185 on epoch=3, global_step=50
05/25/2022 18:23:53 - INFO - __main__ - Step 60 Global step 60 Train loss 1.31 on epoch=3
05/25/2022 18:23:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=4
05/25/2022 18:23:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.80 on epoch=4
05/25/2022 18:24:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=5
05/25/2022 18:24:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.73 on epoch=6
05/25/2022 18:24:06 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.585980647032419 on epoch=6
05/25/2022 18:24:06 - INFO - __main__ - Saving model with best Classification-F1: 0.33881778177080185 -> 0.585980647032419 on epoch=6, global_step=100
05/25/2022 18:24:09 - INFO - __main__ - Step 110 Global step 110 Train loss 0.67 on epoch=6
05/25/2022 18:24:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.74 on epoch=7
05/25/2022 18:24:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.66 on epoch=8
05/25/2022 18:24:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.70 on epoch=8
05/25/2022 18:24:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.73 on epoch=9
05/25/2022 18:24:22 - INFO - __main__ - Global step 150 Train loss 0.70 Classification-F1 0.6086190445072162 on epoch=9
05/25/2022 18:24:22 - INFO - __main__ - Saving model with best Classification-F1: 0.585980647032419 -> 0.6086190445072162 on epoch=9, global_step=150
05/25/2022 18:24:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=9
05/25/2022 18:24:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.58 on epoch=10
05/25/2022 18:24:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=11
05/25/2022 18:24:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=11
05/25/2022 18:24:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=12
05/25/2022 18:24:38 - INFO - __main__ - Global step 200 Train loss 0.63 Classification-F1 0.6126797152711787 on epoch=12
05/25/2022 18:24:38 - INFO - __main__ - Saving model with best Classification-F1: 0.6086190445072162 -> 0.6126797152711787 on epoch=12, global_step=200
05/25/2022 18:24:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=13
05/25/2022 18:24:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.57 on epoch=13
05/25/2022 18:24:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=14
05/25/2022 18:24:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=14
05/25/2022 18:24:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=15
05/25/2022 18:24:54 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.6353561355265323 on epoch=15
05/25/2022 18:24:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6126797152711787 -> 0.6353561355265323 on epoch=15, global_step=250
05/25/2022 18:24:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=16
05/25/2022 18:24:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=16
05/25/2022 18:25:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=17
05/25/2022 18:25:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=18
05/25/2022 18:25:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=18
05/25/2022 18:25:10 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.6843089705549026 on epoch=18
05/25/2022 18:25:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6353561355265323 -> 0.6843089705549026 on epoch=18, global_step=300
05/25/2022 18:25:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.56 on epoch=19
05/25/2022 18:25:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.42 on epoch=19
05/25/2022 18:25:18 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=20
05/25/2022 18:25:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
05/25/2022 18:25:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
05/25/2022 18:25:26 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.6619807059956715 on epoch=21
05/25/2022 18:25:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=22
05/25/2022 18:25:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=23
05/25/2022 18:25:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=23
05/25/2022 18:25:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/25/2022 18:25:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=24
05/25/2022 18:25:42 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.730680572148667 on epoch=24
05/25/2022 18:25:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6843089705549026 -> 0.730680572148667 on epoch=24, global_step=400
05/25/2022 18:25:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=25
05/25/2022 18:25:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
05/25/2022 18:25:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=26
05/25/2022 18:25:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=27
05/25/2022 18:25:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=28
05/25/2022 18:25:58 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.7096617823330705 on epoch=28
05/25/2022 18:26:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=28
05/25/2022 18:26:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=29
05/25/2022 18:26:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
05/25/2022 18:26:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=30
05/25/2022 18:26:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=31
05/25/2022 18:26:14 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.740431044911409 on epoch=31
05/25/2022 18:26:14 - INFO - __main__ - Saving model with best Classification-F1: 0.730680572148667 -> 0.740431044911409 on epoch=31, global_step=500
05/25/2022 18:26:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=31
05/25/2022 18:26:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=32
05/25/2022 18:26:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
05/25/2022 18:26:24 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/25/2022 18:26:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
05/25/2022 18:26:30 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.7408175970238904 on epoch=34
05/25/2022 18:26:30 - INFO - __main__ - Saving model with best Classification-F1: 0.740431044911409 -> 0.7408175970238904 on epoch=34, global_step=550
05/25/2022 18:26:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=34
05/25/2022 18:26:35 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
05/25/2022 18:26:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.30 on epoch=36
05/25/2022 18:26:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/25/2022 18:26:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
05/25/2022 18:26:46 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.7390467505271436 on epoch=37
05/25/2022 18:26:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=38
05/25/2022 18:26:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=38
05/25/2022 18:26:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=39
05/25/2022 18:26:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=39
05/25/2022 18:26:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=40
05/25/2022 18:27:03 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.7294384325952223 on epoch=40
05/25/2022 18:27:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/25/2022 18:27:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
05/25/2022 18:27:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=42
05/25/2022 18:27:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=43
05/25/2022 18:27:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=43
05/25/2022 18:27:19 - INFO - __main__ - Global step 700 Train loss 0.30 Classification-F1 0.721857037808954 on epoch=43
05/25/2022 18:27:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=44
05/25/2022 18:27:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=44
05/25/2022 18:27:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/25/2022 18:27:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=46
05/25/2022 18:27:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/25/2022 18:27:35 - INFO - __main__ - Global step 750 Train loss 0.29 Classification-F1 0.7202235916241697 on epoch=46
05/25/2022 18:27:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=47
05/25/2022 18:27:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=48
05/25/2022 18:27:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=48
05/25/2022 18:27:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=49
05/25/2022 18:27:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/25/2022 18:27:51 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.7602482715273413 on epoch=49
05/25/2022 18:27:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7408175970238904 -> 0.7602482715273413 on epoch=49, global_step=800
05/25/2022 18:27:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=50
05/25/2022 18:27:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=51
05/25/2022 18:27:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=51
05/25/2022 18:28:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=52
05/25/2022 18:28:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/25/2022 18:28:07 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.7621385611843627 on epoch=53
05/25/2022 18:28:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7602482715273413 -> 0.7621385611843627 on epoch=53, global_step=850
05/25/2022 18:28:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=53
05/25/2022 18:28:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=54
05/25/2022 18:28:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/25/2022 18:28:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/25/2022 18:28:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=56
05/25/2022 18:28:23 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.7504145230273352 on epoch=56
05/25/2022 18:28:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=56
05/25/2022 18:28:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=57
05/25/2022 18:28:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=58
05/25/2022 18:28:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=58
05/25/2022 18:28:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=59
05/25/2022 18:28:39 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.7851307233745827 on epoch=59
05/25/2022 18:28:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7621385611843627 -> 0.7851307233745827 on epoch=59, global_step=950
05/25/2022 18:28:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=59
05/25/2022 18:28:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=60
05/25/2022 18:28:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=61
05/25/2022 18:28:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=61
05/25/2022 18:28:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=62
05/25/2022 18:28:56 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7781585038986355 on epoch=62
05/25/2022 18:28:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=63
05/25/2022 18:29:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=63
05/25/2022 18:29:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/25/2022 18:29:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=64
05/25/2022 18:29:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=65
05/25/2022 18:29:12 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.7834386245648106 on epoch=65
05/25/2022 18:29:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=66
05/25/2022 18:29:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=66
05/25/2022 18:29:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/25/2022 18:29:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/25/2022 18:29:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=68
05/25/2022 18:29:28 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.7518688801583538 on epoch=68
05/25/2022 18:29:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/25/2022 18:29:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/25/2022 18:29:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=70
05/25/2022 18:29:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
05/25/2022 18:29:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=71
05/25/2022 18:29:44 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.7792967201334744 on epoch=71
05/25/2022 18:29:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=72
05/25/2022 18:29:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=73
05/25/2022 18:29:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/25/2022 18:29:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
05/25/2022 18:29:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=74
05/25/2022 18:30:00 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.7893638888199622 on epoch=74
05/25/2022 18:30:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7851307233745827 -> 0.7893638888199622 on epoch=74, global_step=1200
05/25/2022 18:30:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=75
05/25/2022 18:30:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=76
05/25/2022 18:30:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.16 on epoch=76
05/25/2022 18:30:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/25/2022 18:30:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=78
05/25/2022 18:30:16 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.7917844728376314 on epoch=78
05/25/2022 18:30:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7893638888199622 -> 0.7917844728376314 on epoch=78, global_step=1250
05/25/2022 18:30:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
05/25/2022 18:30:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=79
05/25/2022 18:30:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=79
05/25/2022 18:30:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/25/2022 18:30:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=81
05/25/2022 18:30:32 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.7866584564860426 on epoch=81
05/25/2022 18:30:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=81
05/25/2022 18:30:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=82
05/25/2022 18:30:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=83
05/25/2022 18:30:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=83
05/25/2022 18:30:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=84
05/25/2022 18:30:48 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.8011454938263955 on epoch=84
05/25/2022 18:30:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7917844728376314 -> 0.8011454938263955 on epoch=84, global_step=1350
05/25/2022 18:30:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=84
05/25/2022 18:30:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=85
05/25/2022 18:30:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=86
05/25/2022 18:30:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=86
05/25/2022 18:31:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=87
05/25/2022 18:31:04 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7983730419830684 on epoch=87
05/25/2022 18:31:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=88
05/25/2022 18:31:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=88
05/25/2022 18:31:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
05/25/2022 18:31:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=89
05/25/2022 18:31:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=90
05/25/2022 18:31:21 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.7953732311756733 on epoch=90
05/25/2022 18:31:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=91
05/25/2022 18:31:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/25/2022 18:31:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=92
05/25/2022 18:31:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=93
05/25/2022 18:31:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=93
05/25/2022 18:31:37 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7765033239323127 on epoch=93
05/25/2022 18:31:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=94
05/25/2022 18:31:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/25/2022 18:31:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=95
05/25/2022 18:31:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=96
05/25/2022 18:31:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=96
05/25/2022 18:31:53 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.7733575702865564 on epoch=96
05/25/2022 18:31:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
05/25/2022 18:31:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=98
05/25/2022 18:32:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=98
05/25/2022 18:32:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.10 on epoch=99
05/25/2022 18:32:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/25/2022 18:32:09 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.7925958437429026 on epoch=99
05/25/2022 18:32:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=100
05/25/2022 18:32:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=101
05/25/2022 18:32:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=101
05/25/2022 18:32:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=102
05/25/2022 18:32:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=103
05/25/2022 18:32:25 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.8032327238482133 on epoch=103
05/25/2022 18:32:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8011454938263955 -> 0.8032327238482133 on epoch=103, global_step=1650
05/25/2022 18:32:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=103
05/25/2022 18:32:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=104
05/25/2022 18:32:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=104
05/25/2022 18:32:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=105
05/25/2022 18:32:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=106
05/25/2022 18:32:41 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.8089019926461738 on epoch=106
05/25/2022 18:32:41 - INFO - __main__ - Saving model with best Classification-F1: 0.8032327238482133 -> 0.8089019926461738 on epoch=106, global_step=1700
05/25/2022 18:32:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=106
05/25/2022 18:32:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=107
05/25/2022 18:32:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.10 on epoch=108
05/25/2022 18:32:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=108
05/25/2022 18:32:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=109
05/25/2022 18:32:57 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7889584278155707 on epoch=109
05/25/2022 18:33:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=109
05/25/2022 18:33:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=110
05/25/2022 18:33:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=111
05/25/2022 18:33:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=111
05/25/2022 18:33:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=112
05/25/2022 18:33:13 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.7972828186140961 on epoch=112
05/25/2022 18:33:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.05 on epoch=113
05/25/2022 18:33:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=113
05/25/2022 18:33:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
05/25/2022 18:33:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=114
05/25/2022 18:33:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=115
05/25/2022 18:33:29 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.7905741457160153 on epoch=115
05/25/2022 18:33:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=116
05/25/2022 18:33:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
05/25/2022 18:33:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=117
05/25/2022 18:33:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
05/25/2022 18:33:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=118
05/25/2022 18:33:45 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.7910899006958969 on epoch=118
05/25/2022 18:33:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=119
05/25/2022 18:33:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=119
05/25/2022 18:33:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=120
05/25/2022 18:33:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=121
05/25/2022 18:33:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=121
05/25/2022 18:34:02 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.7889043509681407 on epoch=121
05/25/2022 18:34:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/25/2022 18:34:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=123
05/25/2022 18:34:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=123
05/25/2022 18:34:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/25/2022 18:34:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=124
05/25/2022 18:34:18 - INFO - __main__ - Global step 2000 Train loss 0.07 Classification-F1 0.7966174555403382 on epoch=124
05/25/2022 18:34:20 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=125
05/25/2022 18:34:23 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=126
05/25/2022 18:34:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=126
05/25/2022 18:34:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=127
05/25/2022 18:34:30 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=128
05/25/2022 18:34:34 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.8010958347431616 on epoch=128
05/25/2022 18:34:36 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=128
05/25/2022 18:34:39 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
05/25/2022 18:34:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=129
05/25/2022 18:34:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=130
05/25/2022 18:34:46 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/25/2022 18:34:50 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.8031007719946957 on epoch=131
05/25/2022 18:34:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=131
05/25/2022 18:34:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/25/2022 18:34:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/25/2022 18:35:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=133
05/25/2022 18:35:03 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.10 on epoch=134
05/25/2022 18:35:07 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.8040538756923504 on epoch=134
05/25/2022 18:35:09 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/25/2022 18:35:12 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=135
05/25/2022 18:35:14 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/25/2022 18:35:17 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=136
05/25/2022 18:35:19 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=137
05/25/2022 18:35:23 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.8089440872477429 on epoch=137
05/25/2022 18:35:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8089019926461738 -> 0.8089440872477429 on epoch=137, global_step=2200
05/25/2022 18:35:26 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=138
05/25/2022 18:35:28 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=138
05/25/2022 18:35:30 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/25/2022 18:35:33 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=139
05/25/2022 18:35:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/25/2022 18:35:39 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.8091533977897615 on epoch=140
05/25/2022 18:35:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8089440872477429 -> 0.8091533977897615 on epoch=140, global_step=2250
05/25/2022 18:35:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=141
05/25/2022 18:35:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=141
05/25/2022 18:35:47 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/25/2022 18:35:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=143
05/25/2022 18:35:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/25/2022 18:35:55 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.8010381877048319 on epoch=143
05/25/2022 18:35:58 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=144
05/25/2022 18:36:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/25/2022 18:36:03 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
05/25/2022 18:36:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=146
05/25/2022 18:36:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=146
05/25/2022 18:36:11 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7970577560765209 on epoch=146
05/25/2022 18:36:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=147
05/25/2022 18:36:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=148
05/25/2022 18:36:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=148
05/25/2022 18:36:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/25/2022 18:36:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=149
05/25/2022 18:36:28 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.7715508112064279 on epoch=149
05/25/2022 18:36:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/25/2022 18:36:33 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
05/25/2022 18:36:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
05/25/2022 18:36:38 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=152
05/25/2022 18:36:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=153
05/25/2022 18:36:44 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.7917899929527836 on epoch=153
05/25/2022 18:36:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/25/2022 18:36:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=154
05/25/2022 18:36:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=154
05/25/2022 18:36:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=155
05/25/2022 18:36:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=156
05/25/2022 18:37:00 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7936773752563226 on epoch=156
05/25/2022 18:37:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=156
05/25/2022 18:37:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 18:37:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=158
05/25/2022 18:37:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=158
05/25/2022 18:37:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/25/2022 18:37:17 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.800011220532959 on epoch=159
05/25/2022 18:37:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.01 on epoch=159
05/25/2022 18:37:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/25/2022 18:37:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=161
05/25/2022 18:37:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/25/2022 18:37:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/25/2022 18:37:33 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.7984624417397184 on epoch=162
05/25/2022 18:37:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/25/2022 18:37:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=163
05/25/2022 18:37:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/25/2022 18:37:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=164
05/25/2022 18:37:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
05/25/2022 18:37:49 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7873117956047465 on epoch=165
05/25/2022 18:37:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
05/25/2022 18:37:54 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
05/25/2022 18:37:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=167
05/25/2022 18:37:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/25/2022 18:38:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
05/25/2022 18:38:06 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.7838290338145911 on epoch=168
05/25/2022 18:38:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
05/25/2022 18:38:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=169
05/25/2022 18:38:13 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=170
05/25/2022 18:38:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/25/2022 18:38:18 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/25/2022 18:38:22 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.7932399411272866 on epoch=171
05/25/2022 18:38:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/25/2022 18:38:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/25/2022 18:38:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
05/25/2022 18:38:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=174
05/25/2022 18:38:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=174
05/25/2022 18:38:38 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.8050985248273383 on epoch=174
05/25/2022 18:38:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
05/25/2022 18:38:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/25/2022 18:38:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=176
05/25/2022 18:38:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=177
05/25/2022 18:38:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
05/25/2022 18:38:55 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7991875295560138 on epoch=178
05/25/2022 18:38:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=178
05/25/2022 18:39:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/25/2022 18:39:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/25/2022 18:39:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/25/2022 18:39:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/25/2022 18:39:11 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.8292743163302071 on epoch=181
05/25/2022 18:39:11 - INFO - __main__ - Saving model with best Classification-F1: 0.8091533977897615 -> 0.8292743163302071 on epoch=181, global_step=2900
05/25/2022 18:39:14 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/25/2022 18:39:16 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
05/25/2022 18:39:19 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=183
05/25/2022 18:39:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=183
05/25/2022 18:39:24 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/25/2022 18:39:27 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.8113748223010668 on epoch=184
05/25/2022 18:39:30 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=184
05/25/2022 18:39:32 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/25/2022 18:39:35 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
05/25/2022 18:39:37 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/25/2022 18:39:40 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=187
05/25/2022 18:39:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:39:41 - INFO - __main__ - Printing 3 examples
05/25/2022 18:39:41 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:39:41 - INFO - __main__ - ['others']
05/25/2022 18:39:41 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:39:41 - INFO - __main__ - ['others']
05/25/2022 18:39:41 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:39:41 - INFO - __main__ - ['others']
05/25/2022 18:39:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:39:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:39:42 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:39:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:39:42 - INFO - __main__ - Printing 3 examples
05/25/2022 18:39:42 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:39:42 - INFO - __main__ - ['others']
05/25/2022 18:39:42 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:39:42 - INFO - __main__ - ['others']
05/25/2022 18:39:42 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:39:42 - INFO - __main__ - ['others']
05/25/2022 18:39:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:39:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:39:42 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:39:44 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7962531964449658 on epoch=187
05/25/2022 18:39:44 - INFO - __main__ - save last model!
05/25/2022 18:39:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 18:39:44 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 18:39:44 - INFO - __main__ - Printing 3 examples
05/25/2022 18:39:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 18:39:44 - INFO - __main__ - ['others']
05/25/2022 18:39:44 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 18:39:44 - INFO - __main__ - ['others']
05/25/2022 18:39:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 18:39:44 - INFO - __main__ - ['others']
05/25/2022 18:39:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:39:46 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:39:51 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 18:40:01 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:40:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:40:02 - INFO - __main__ - Starting training!
05/25/2022 18:41:12 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_100_0.5_8_predictions.txt
05/25/2022 18:41:12 - INFO - __main__ - Classification-F1 on test data: 0.4947
05/25/2022 18:41:12 - INFO - __main__ - prefix=emo_64_100, lr=0.5, bsz=8, dev_performance=0.8292743163302071, test_performance=0.4946811850832039
05/25/2022 18:41:12 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.4, bsz=8 ...
05/25/2022 18:41:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:41:13 - INFO - __main__ - Printing 3 examples
05/25/2022 18:41:13 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:41:13 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:41:13 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:41:13 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:41:13 - INFO - __main__ - Printing 3 examples
05/25/2022 18:41:13 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:41:13 - INFO - __main__ - ['others']
05/25/2022 18:41:13 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:41:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:41:14 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:41:30 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:41:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:41:30 - INFO - __main__ - Starting training!
05/25/2022 18:41:34 - INFO - __main__ - Step 10 Global step 10 Train loss 3.79 on epoch=0
05/25/2022 18:41:36 - INFO - __main__ - Step 20 Global step 20 Train loss 2.93 on epoch=1
05/25/2022 18:41:39 - INFO - __main__ - Step 30 Global step 30 Train loss 2.53 on epoch=1
05/25/2022 18:41:41 - INFO - __main__ - Step 40 Global step 40 Train loss 2.03 on epoch=2
05/25/2022 18:41:44 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=3
05/25/2022 18:41:48 - INFO - __main__ - Global step 50 Train loss 2.60 Classification-F1 0.18368602994036914 on epoch=3
05/25/2022 18:41:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18368602994036914 on epoch=3, global_step=50
05/25/2022 18:41:50 - INFO - __main__ - Step 60 Global step 60 Train loss 1.53 on epoch=3
05/25/2022 18:41:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.21 on epoch=4
05/25/2022 18:41:55 - INFO - __main__ - Step 80 Global step 80 Train loss 1.09 on epoch=4
05/25/2022 18:41:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.95 on epoch=5
05/25/2022 18:42:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=6
05/25/2022 18:42:04 - INFO - __main__ - Global step 100 Train loss 1.12 Classification-F1 0.4991909399513319 on epoch=6
05/25/2022 18:42:04 - INFO - __main__ - Saving model with best Classification-F1: 0.18368602994036914 -> 0.4991909399513319 on epoch=6, global_step=100
05/25/2022 18:42:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.92 on epoch=6
05/25/2022 18:42:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=7
05/25/2022 18:42:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.73 on epoch=8
05/25/2022 18:42:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.72 on epoch=8
05/25/2022 18:42:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=9
05/25/2022 18:42:20 - INFO - __main__ - Global step 150 Train loss 0.80 Classification-F1 0.6212830487141902 on epoch=9
05/25/2022 18:42:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4991909399513319 -> 0.6212830487141902 on epoch=9, global_step=150
05/25/2022 18:42:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.67 on epoch=9
05/25/2022 18:42:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=10
05/25/2022 18:42:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.64 on epoch=11
05/25/2022 18:42:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=11
05/25/2022 18:42:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.67 on epoch=12
05/25/2022 18:42:36 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.636907966665926 on epoch=12
05/25/2022 18:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6212830487141902 -> 0.636907966665926 on epoch=12, global_step=200
05/25/2022 18:42:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=13
05/25/2022 18:42:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=13
05/25/2022 18:42:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=14
05/25/2022 18:42:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.55 on epoch=14
05/25/2022 18:42:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.55 on epoch=15
05/25/2022 18:42:52 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.6367859364758424 on epoch=15
05/25/2022 18:42:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.56 on epoch=16
05/25/2022 18:42:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=16
05/25/2022 18:42:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.58 on epoch=17
05/25/2022 18:43:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=18
05/25/2022 18:43:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=18
05/25/2022 18:43:08 - INFO - __main__ - Global step 300 Train loss 0.55 Classification-F1 0.6773547786876464 on epoch=18
05/25/2022 18:43:08 - INFO - __main__ - Saving model with best Classification-F1: 0.636907966665926 -> 0.6773547786876464 on epoch=18, global_step=300
05/25/2022 18:43:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=19
05/25/2022 18:43:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=19
05/25/2022 18:43:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.63 on epoch=20
05/25/2022 18:43:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=21
05/25/2022 18:43:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=21
05/25/2022 18:43:24 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.6838060446590326 on epoch=21
05/25/2022 18:43:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6773547786876464 -> 0.6838060446590326 on epoch=21, global_step=350
05/25/2022 18:43:26 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=22
05/25/2022 18:43:29 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=23
05/25/2022 18:43:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=23
05/25/2022 18:43:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=24
05/25/2022 18:43:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=24
05/25/2022 18:43:40 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.6984400691145687 on epoch=24
05/25/2022 18:43:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6838060446590326 -> 0.6984400691145687 on epoch=24, global_step=400
05/25/2022 18:43:42 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/25/2022 18:43:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=26
05/25/2022 18:43:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=26
05/25/2022 18:43:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=27
05/25/2022 18:43:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=28
05/25/2022 18:43:56 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.7286853313449058 on epoch=28
05/25/2022 18:43:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6984400691145687 -> 0.7286853313449058 on epoch=28, global_step=450
05/25/2022 18:43:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=28
05/25/2022 18:44:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=29
05/25/2022 18:44:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/25/2022 18:44:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=30
05/25/2022 18:44:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=31
05/25/2022 18:44:12 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.7481210509606293 on epoch=31
05/25/2022 18:44:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7286853313449058 -> 0.7481210509606293 on epoch=31, global_step=500
05/25/2022 18:44:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=31
05/25/2022 18:44:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=32
05/25/2022 18:44:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/25/2022 18:44:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=33
05/25/2022 18:44:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=34
05/25/2022 18:44:28 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.7555153131147951 on epoch=34
05/25/2022 18:44:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7481210509606293 -> 0.7555153131147951 on epoch=34, global_step=550
05/25/2022 18:44:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/25/2022 18:44:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
05/25/2022 18:44:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.29 on epoch=36
05/25/2022 18:44:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=36
05/25/2022 18:44:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=37
05/25/2022 18:44:44 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.7501237350793736 on epoch=37
05/25/2022 18:44:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/25/2022 18:44:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=38
05/25/2022 18:44:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/25/2022 18:44:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=39
05/25/2022 18:44:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=40
05/25/2022 18:45:00 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.7524626385987466 on epoch=40
05/25/2022 18:45:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=41
05/25/2022 18:45:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
05/25/2022 18:45:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
05/25/2022 18:45:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
05/25/2022 18:45:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=43
05/25/2022 18:45:16 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.7354144775495446 on epoch=43
05/25/2022 18:45:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=44
05/25/2022 18:45:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=44
05/25/2022 18:45:24 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=45
05/25/2022 18:45:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=46
05/25/2022 18:45:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=46
05/25/2022 18:45:32 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.7486637634278835 on epoch=46
05/25/2022 18:45:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=47
05/25/2022 18:45:37 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
05/25/2022 18:45:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
05/25/2022 18:45:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=49
05/25/2022 18:45:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=49
05/25/2022 18:45:48 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.7650141214781198 on epoch=49
05/25/2022 18:45:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7555153131147951 -> 0.7650141214781198 on epoch=49, global_step=800
05/25/2022 18:45:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/25/2022 18:45:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
05/25/2022 18:45:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=51
05/25/2022 18:45:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.39 on epoch=52
05/25/2022 18:46:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
05/25/2022 18:46:04 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.7537814763445831 on epoch=53
05/25/2022 18:46:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/25/2022 18:46:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=54
05/25/2022 18:46:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/25/2022 18:46:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=55
05/25/2022 18:46:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=56
05/25/2022 18:46:21 - INFO - __main__ - Global step 900 Train loss 0.27 Classification-F1 0.7493862708964065 on epoch=56
05/25/2022 18:46:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=56
05/25/2022 18:46:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
05/25/2022 18:46:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/25/2022 18:46:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=58
05/25/2022 18:46:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=59
05/25/2022 18:46:37 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7568486376897592 on epoch=59
05/25/2022 18:46:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=59
05/25/2022 18:46:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
05/25/2022 18:46:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=61
05/25/2022 18:46:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=61
05/25/2022 18:46:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=62
05/25/2022 18:46:53 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.7547492827838732 on epoch=62
05/25/2022 18:46:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=63
05/25/2022 18:46:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=63
05/25/2022 18:47:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/25/2022 18:47:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=64
05/25/2022 18:47:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=65
05/25/2022 18:47:10 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.7565178066954961 on epoch=65
05/25/2022 18:47:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
05/25/2022 18:47:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=66
05/25/2022 18:47:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=67
05/25/2022 18:47:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/25/2022 18:47:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=68
05/25/2022 18:47:26 - INFO - __main__ - Global step 1100 Train loss 0.24 Classification-F1 0.7649828450416687 on epoch=68
05/25/2022 18:47:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=69
05/25/2022 18:47:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=69
05/25/2022 18:47:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=70
05/25/2022 18:47:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=71
05/25/2022 18:47:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=71
05/25/2022 18:47:42 - INFO - __main__ - Global step 1150 Train loss 0.24 Classification-F1 0.749073149073149 on epoch=71
05/25/2022 18:47:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/25/2022 18:47:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=73
05/25/2022 18:47:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=73
05/25/2022 18:47:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=74
05/25/2022 18:47:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/25/2022 18:47:59 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.7687964530054898 on epoch=74
05/25/2022 18:47:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7650141214781198 -> 0.7687964530054898 on epoch=74, global_step=1200
05/25/2022 18:48:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.15 on epoch=75
05/25/2022 18:48:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=76
05/25/2022 18:48:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
05/25/2022 18:48:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
05/25/2022 18:48:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=78
05/25/2022 18:48:15 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.7621459982848368 on epoch=78
05/25/2022 18:48:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=78
05/25/2022 18:48:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=79
05/25/2022 18:48:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/25/2022 18:48:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=80
05/25/2022 18:48:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
05/25/2022 18:48:31 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.7640233186732637 on epoch=81
05/25/2022 18:48:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=81
05/25/2022 18:48:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/25/2022 18:48:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
05/25/2022 18:48:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
05/25/2022 18:48:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=84
05/25/2022 18:48:47 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.7627047145674926 on epoch=84
05/25/2022 18:48:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/25/2022 18:48:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=85
05/25/2022 18:48:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=86
05/25/2022 18:48:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=86
05/25/2022 18:49:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=87
05/25/2022 18:49:04 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.7833871511780965 on epoch=87
05/25/2022 18:49:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7687964530054898 -> 0.7833871511780965 on epoch=87, global_step=1400
05/25/2022 18:49:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
05/25/2022 18:49:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=88
05/25/2022 18:49:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/25/2022 18:49:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
05/25/2022 18:49:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
05/25/2022 18:49:20 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.7742150310394929 on epoch=90
05/25/2022 18:49:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
05/25/2022 18:49:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/25/2022 18:49:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=92
05/25/2022 18:49:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=93
05/25/2022 18:49:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=93
05/25/2022 18:49:36 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.7676336586708813 on epoch=93
05/25/2022 18:49:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=94
05/25/2022 18:49:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=94
05/25/2022 18:49:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=95
05/25/2022 18:49:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=96
05/25/2022 18:49:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/25/2022 18:49:52 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.7666353246206036 on epoch=96
05/25/2022 18:49:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
05/25/2022 18:49:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=98
05/25/2022 18:50:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.19 on epoch=98
05/25/2022 18:50:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=99
05/25/2022 18:50:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=99
05/25/2022 18:50:08 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7722139220037418 on epoch=99
05/25/2022 18:50:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=100
05/25/2022 18:50:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/25/2022 18:50:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=101
05/25/2022 18:50:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=102
05/25/2022 18:50:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.22 on epoch=103
05/25/2022 18:50:25 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.7650814358001264 on epoch=103
05/25/2022 18:50:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=103
05/25/2022 18:50:30 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=104
05/25/2022 18:50:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/25/2022 18:50:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
05/25/2022 18:50:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/25/2022 18:50:41 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.7593878400675338 on epoch=106
05/25/2022 18:50:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=106
05/25/2022 18:50:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=107
05/25/2022 18:50:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=108
05/25/2022 18:50:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=108
05/25/2022 18:50:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=109
05/25/2022 18:50:57 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.7615099863591718 on epoch=109
05/25/2022 18:51:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
05/25/2022 18:51:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=110
05/25/2022 18:51:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=111
05/25/2022 18:51:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
05/25/2022 18:51:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=112
05/25/2022 18:51:13 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.7780240831964971 on epoch=112
05/25/2022 18:51:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=113
05/25/2022 18:51:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=113
05/25/2022 18:51:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/25/2022 18:51:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/25/2022 18:51:26 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=115
05/25/2022 18:51:30 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.7650966171519984 on epoch=115
05/25/2022 18:51:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=116
05/25/2022 18:51:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=116
05/25/2022 18:51:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=117
05/25/2022 18:51:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=118
05/25/2022 18:51:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
05/25/2022 18:51:46 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.7761437044867278 on epoch=118
05/25/2022 18:51:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
05/25/2022 18:51:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
05/25/2022 18:51:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/25/2022 18:51:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=121
05/25/2022 18:51:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=121
05/25/2022 18:52:02 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7795447065231942 on epoch=121
05/25/2022 18:52:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
05/25/2022 18:52:08 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
05/25/2022 18:52:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=123
05/25/2022 18:52:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=124
05/25/2022 18:52:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=124
05/25/2022 18:52:19 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.7735551559062378 on epoch=124
05/25/2022 18:52:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=125
05/25/2022 18:52:24 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=126
05/25/2022 18:52:26 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=126
05/25/2022 18:52:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=127
05/25/2022 18:52:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=128
05/25/2022 18:52:35 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.7762031653746772 on epoch=128
05/25/2022 18:52:38 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
05/25/2022 18:52:40 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=129
05/25/2022 18:52:43 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
05/25/2022 18:52:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=130
05/25/2022 18:52:48 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=131
05/25/2022 18:52:52 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7599370011389874 on epoch=131
05/25/2022 18:52:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=131
05/25/2022 18:52:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=132
05/25/2022 18:52:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/25/2022 18:53:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=133
05/25/2022 18:53:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
05/25/2022 18:53:08 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7586205999808111 on epoch=134
05/25/2022 18:53:11 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/25/2022 18:53:13 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
05/25/2022 18:53:16 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=136
05/25/2022 18:53:18 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/25/2022 18:53:21 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/25/2022 18:53:25 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7672465872901564 on epoch=137
05/25/2022 18:53:27 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/25/2022 18:53:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=138
05/25/2022 18:53:32 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
05/25/2022 18:53:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
05/25/2022 18:53:37 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
05/25/2022 18:53:41 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.7659427489164496 on epoch=140
05/25/2022 18:53:44 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=141
05/25/2022 18:53:46 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/25/2022 18:53:49 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
05/25/2022 18:53:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/25/2022 18:53:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=143
05/25/2022 18:53:57 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.7771525538815073 on epoch=143
05/25/2022 18:54:00 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
05/25/2022 18:54:02 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/25/2022 18:54:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=145
05/25/2022 18:54:07 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
05/25/2022 18:54:10 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
05/25/2022 18:54:14 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7628092889720797 on epoch=146
05/25/2022 18:54:16 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
05/25/2022 18:54:19 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/25/2022 18:54:21 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=148
05/25/2022 18:54:24 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=149
05/25/2022 18:54:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
05/25/2022 18:54:30 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.7662650949152152 on epoch=149
05/25/2022 18:54:33 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=150
05/25/2022 18:54:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/25/2022 18:54:38 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/25/2022 18:54:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/25/2022 18:54:43 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=153
05/25/2022 18:54:46 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.7768417574256706 on epoch=153
05/25/2022 18:54:49 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=153
05/25/2022 18:54:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
05/25/2022 18:54:54 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/25/2022 18:54:56 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/25/2022 18:54:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=156
05/25/2022 18:55:02 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.7762228845182982 on epoch=156
05/25/2022 18:55:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=156
05/25/2022 18:55:07 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
05/25/2022 18:55:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
05/25/2022 18:55:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=158
05/25/2022 18:55:15 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.19 on epoch=159
05/25/2022 18:55:19 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.7804243460986646 on epoch=159
05/25/2022 18:55:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=159
05/25/2022 18:55:24 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
05/25/2022 18:55:26 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/25/2022 18:55:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
05/25/2022 18:55:31 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
05/25/2022 18:55:35 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.7670604420494653 on epoch=162
05/25/2022 18:55:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=163
05/25/2022 18:55:40 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/25/2022 18:55:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
05/25/2022 18:55:45 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
05/25/2022 18:55:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
05/25/2022 18:55:51 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7766836274648774 on epoch=165
05/25/2022 18:55:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
05/25/2022 18:55:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/25/2022 18:55:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
05/25/2022 18:56:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/25/2022 18:56:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
05/25/2022 18:56:08 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.7745158279536439 on epoch=168
05/25/2022 18:56:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/25/2022 18:56:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/25/2022 18:56:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
05/25/2022 18:56:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/25/2022 18:56:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=171
05/25/2022 18:56:24 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7574056554189004 on epoch=171
05/25/2022 18:56:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=172
05/25/2022 18:56:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/25/2022 18:56:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/25/2022 18:56:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=174
05/25/2022 18:56:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/25/2022 18:56:40 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.766724658519051 on epoch=174
05/25/2022 18:56:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
05/25/2022 18:56:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/25/2022 18:56:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/25/2022 18:56:51 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=177
05/25/2022 18:56:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.11 on epoch=178
05/25/2022 18:56:57 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7848148072286003 on epoch=178
05/25/2022 18:56:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7833871511780965 -> 0.7848148072286003 on epoch=178, global_step=2850
05/25/2022 18:56:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/25/2022 18:57:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
05/25/2022 18:57:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
05/25/2022 18:57:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
05/25/2022 18:57:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/25/2022 18:57:13 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7587131054112186 on epoch=181
05/25/2022 18:57:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=181
05/25/2022 18:57:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
05/25/2022 18:57:21 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/25/2022 18:57:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 18:57:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=184
05/25/2022 18:57:30 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7617541475745417 on epoch=184
05/25/2022 18:57:32 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/25/2022 18:57:35 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
05/25/2022 18:57:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=186
05/25/2022 18:57:40 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/25/2022 18:57:42 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
05/25/2022 18:57:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:57:44 - INFO - __main__ - Printing 3 examples
05/25/2022 18:57:44 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:57:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:57:44 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:57:44 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:57:44 - INFO - __main__ - Printing 3 examples
05/25/2022 18:57:44 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:57:44 - INFO - __main__ - ['others']
05/25/2022 18:57:44 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:57:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:57:44 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:57:46 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7893521165261153 on epoch=187
05/25/2022 18:57:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7848148072286003 -> 0.7893521165261153 on epoch=187, global_step=3000
05/25/2022 18:57:46 - INFO - __main__ - save last model!
05/25/2022 18:57:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 18:57:46 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 18:57:46 - INFO - __main__ - Printing 3 examples
05/25/2022 18:57:46 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 18:57:46 - INFO - __main__ - ['others']
05/25/2022 18:57:46 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 18:57:46 - INFO - __main__ - ['others']
05/25/2022 18:57:46 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 18:57:46 - INFO - __main__ - ['others']
05/25/2022 18:57:46 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:57:48 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:57:54 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 18:58:00 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:58:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:58:01 - INFO - __main__ - Starting training!
05/25/2022 18:59:13 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_100_0.4_8_predictions.txt
05/25/2022 18:59:13 - INFO - __main__ - Classification-F1 on test data: 0.4260
05/25/2022 18:59:13 - INFO - __main__ - prefix=emo_64_100, lr=0.4, bsz=8, dev_performance=0.7893521165261153, test_performance=0.42596975105405044
05/25/2022 18:59:13 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.3, bsz=8 ...
05/25/2022 18:59:14 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:59:14 - INFO - __main__ - Printing 3 examples
05/25/2022 18:59:14 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 18:59:14 - INFO - __main__ - ['others']
05/25/2022 18:59:14 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 18:59:14 - INFO - __main__ - ['others']
05/25/2022 18:59:14 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 18:59:14 - INFO - __main__ - ['others']
05/25/2022 18:59:14 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:59:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:59:15 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 18:59:15 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 18:59:15 - INFO - __main__ - Printing 3 examples
05/25/2022 18:59:15 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 18:59:15 - INFO - __main__ - ['others']
05/25/2022 18:59:15 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 18:59:15 - INFO - __main__ - ['others']
05/25/2022 18:59:15 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 18:59:15 - INFO - __main__ - ['others']
05/25/2022 18:59:15 - INFO - __main__ - Tokenizing Input ...
05/25/2022 18:59:15 - INFO - __main__ - Tokenizing Output ...
05/25/2022 18:59:15 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 18:59:34 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 18:59:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 18:59:34 - INFO - __main__ - Starting training!
05/25/2022 18:59:37 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=0
05/25/2022 18:59:40 - INFO - __main__ - Step 20 Global step 20 Train loss 2.95 on epoch=1
05/25/2022 18:59:42 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=1
05/25/2022 18:59:45 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=2
05/25/2022 18:59:47 - INFO - __main__ - Step 50 Global step 50 Train loss 2.05 on epoch=3
05/25/2022 18:59:51 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.044983416252072965 on epoch=3
05/25/2022 18:59:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.044983416252072965 on epoch=3, global_step=50
05/25/2022 18:59:54 - INFO - __main__ - Step 60 Global step 60 Train loss 1.89 on epoch=3
05/25/2022 18:59:56 - INFO - __main__ - Step 70 Global step 70 Train loss 1.60 on epoch=4
05/25/2022 18:59:59 - INFO - __main__ - Step 80 Global step 80 Train loss 1.40 on epoch=4
05/25/2022 19:00:01 - INFO - __main__ - Step 90 Global step 90 Train loss 1.24 on epoch=5
05/25/2022 19:00:04 - INFO - __main__ - Step 100 Global step 100 Train loss 1.08 on epoch=6
05/25/2022 19:00:07 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.32322723162462963 on epoch=6
05/25/2022 19:00:07 - INFO - __main__ - Saving model with best Classification-F1: 0.044983416252072965 -> 0.32322723162462963 on epoch=6, global_step=100
05/25/2022 19:00:10 - INFO - __main__ - Step 110 Global step 110 Train loss 1.05 on epoch=6
05/25/2022 19:00:12 - INFO - __main__ - Step 120 Global step 120 Train loss 1.00 on epoch=7
05/25/2022 19:00:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=8
05/25/2022 19:00:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=8
05/25/2022 19:00:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.81 on epoch=9
05/25/2022 19:00:23 - INFO - __main__ - Global step 150 Train loss 0.92 Classification-F1 0.5630305624681787 on epoch=9
05/25/2022 19:00:23 - INFO - __main__ - Saving model with best Classification-F1: 0.32322723162462963 -> 0.5630305624681787 on epoch=9, global_step=150
05/25/2022 19:00:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.77 on epoch=9
05/25/2022 19:00:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=10
05/25/2022 19:00:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=11
05/25/2022 19:00:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.75 on epoch=11
05/25/2022 19:00:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.76 on epoch=12
05/25/2022 19:00:39 - INFO - __main__ - Global step 200 Train loss 0.76 Classification-F1 0.5850581963672736 on epoch=12
05/25/2022 19:00:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5630305624681787 -> 0.5850581963672736 on epoch=12, global_step=200
05/25/2022 19:00:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=13
05/25/2022 19:00:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.69 on epoch=13
05/25/2022 19:00:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=14
05/25/2022 19:00:49 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=14
05/25/2022 19:00:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=15
05/25/2022 19:00:55 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.57446188402247 on epoch=15
05/25/2022 19:00:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=16
05/25/2022 19:01:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=16
05/25/2022 19:01:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=17
05/25/2022 19:01:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=18
05/25/2022 19:01:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=18
05/25/2022 19:01:11 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.5899219951675949 on epoch=18
05/25/2022 19:01:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5850581963672736 -> 0.5899219951675949 on epoch=18, global_step=300
05/25/2022 19:01:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=19
05/25/2022 19:01:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=19
05/25/2022 19:01:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=20
05/25/2022 19:01:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=21
05/25/2022 19:01:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.56 on epoch=21
05/25/2022 19:01:28 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.6101199632587065 on epoch=21
05/25/2022 19:01:28 - INFO - __main__ - Saving model with best Classification-F1: 0.5899219951675949 -> 0.6101199632587065 on epoch=21, global_step=350
05/25/2022 19:01:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=22
05/25/2022 19:01:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=23
05/25/2022 19:01:35 - INFO - __main__ - Step 380 Global step 380 Train loss 0.59 on epoch=23
05/25/2022 19:01:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.64 on epoch=24
05/25/2022 19:01:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=24
05/25/2022 19:01:44 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.6418964416510113 on epoch=24
05/25/2022 19:01:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6101199632587065 -> 0.6418964416510113 on epoch=24, global_step=400
05/25/2022 19:01:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=25
05/25/2022 19:01:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=26
05/25/2022 19:01:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=26
05/25/2022 19:01:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=27
05/25/2022 19:01:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=28
05/25/2022 19:02:00 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.6738008930916732 on epoch=28
05/25/2022 19:02:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6418964416510113 -> 0.6738008930916732 on epoch=28, global_step=450
05/25/2022 19:02:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=28
05/25/2022 19:02:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=29
05/25/2022 19:02:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=29
05/25/2022 19:02:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=30
05/25/2022 19:02:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
05/25/2022 19:02:16 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.6964930217619909 on epoch=31
05/25/2022 19:02:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6738008930916732 -> 0.6964930217619909 on epoch=31, global_step=500
05/25/2022 19:02:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=31
05/25/2022 19:02:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=32
05/25/2022 19:02:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=33
05/25/2022 19:02:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=33
05/25/2022 19:02:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=34
05/25/2022 19:02:32 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.7270743819998841 on epoch=34
05/25/2022 19:02:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6964930217619909 -> 0.7270743819998841 on epoch=34, global_step=550
05/25/2022 19:02:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=34
05/25/2022 19:02:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=35
05/25/2022 19:02:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=36
05/25/2022 19:02:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=36
05/25/2022 19:02:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=37
05/25/2022 19:02:48 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.724020059228837 on epoch=37
05/25/2022 19:02:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/25/2022 19:02:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=38
05/25/2022 19:02:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=39
05/25/2022 19:02:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=39
05/25/2022 19:03:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=40
05/25/2022 19:03:04 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.7181683665872544 on epoch=40
05/25/2022 19:03:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.32 on epoch=41
05/25/2022 19:03:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=41
05/25/2022 19:03:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=42
05/25/2022 19:03:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=43
05/25/2022 19:03:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/25/2022 19:03:20 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.7243105382443739 on epoch=43
05/25/2022 19:03:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=44
05/25/2022 19:03:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=44
05/25/2022 19:03:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
05/25/2022 19:03:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=46
05/25/2022 19:03:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=46
05/25/2022 19:03:36 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.7472872467332257 on epoch=46
05/25/2022 19:03:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7270743819998841 -> 0.7472872467332257 on epoch=46, global_step=750
05/25/2022 19:03:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=47
05/25/2022 19:03:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=48
05/25/2022 19:03:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=48
05/25/2022 19:03:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=49
05/25/2022 19:03:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
05/25/2022 19:03:52 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.759324703502504 on epoch=49
05/25/2022 19:03:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7472872467332257 -> 0.759324703502504 on epoch=49, global_step=800
05/25/2022 19:03:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/25/2022 19:03:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=51
05/25/2022 19:04:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
05/25/2022 19:04:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
05/25/2022 19:04:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=53
05/25/2022 19:04:09 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.7548349541158745 on epoch=53
05/25/2022 19:04:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
05/25/2022 19:04:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=54
05/25/2022 19:04:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=54
05/25/2022 19:04:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=55
05/25/2022 19:04:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=56
05/25/2022 19:04:25 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7537071694248163 on epoch=56
05/25/2022 19:04:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/25/2022 19:04:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=57
05/25/2022 19:04:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.34 on epoch=58
05/25/2022 19:04:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=58
05/25/2022 19:04:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
05/25/2022 19:04:41 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.7707690509583918 on epoch=59
05/25/2022 19:04:41 - INFO - __main__ - Saving model with best Classification-F1: 0.759324703502504 -> 0.7707690509583918 on epoch=59, global_step=950
05/25/2022 19:04:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/25/2022 19:04:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=60
05/25/2022 19:04:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=61
05/25/2022 19:04:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/25/2022 19:04:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=62
05/25/2022 19:04:57 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.7786837276198978 on epoch=62
05/25/2022 19:04:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7707690509583918 -> 0.7786837276198978 on epoch=62, global_step=1000
05/25/2022 19:05:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/25/2022 19:05:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
05/25/2022 19:05:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/25/2022 19:05:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
05/25/2022 19:05:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=65
05/25/2022 19:05:13 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.7791473093302361 on epoch=65
05/25/2022 19:05:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7786837276198978 -> 0.7791473093302361 on epoch=65, global_step=1050
05/25/2022 19:05:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=66
05/25/2022 19:05:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/25/2022 19:05:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=67
05/25/2022 19:05:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=68
05/25/2022 19:05:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=68
05/25/2022 19:05:29 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.7743854045683314 on epoch=68
05/25/2022 19:05:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=69
05/25/2022 19:05:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=69
05/25/2022 19:05:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/25/2022 19:05:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
05/25/2022 19:05:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=71
05/25/2022 19:05:45 - INFO - __main__ - Global step 1150 Train loss 0.27 Classification-F1 0.7667029554540262 on epoch=71
05/25/2022 19:05:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=72
05/25/2022 19:05:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=73
05/25/2022 19:05:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=73
05/25/2022 19:05:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=74
05/25/2022 19:05:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=74
05/25/2022 19:06:02 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7617883556169732 on epoch=74
05/25/2022 19:06:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=75
05/25/2022 19:06:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
05/25/2022 19:06:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
05/25/2022 19:06:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=77
05/25/2022 19:06:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=78
05/25/2022 19:06:18 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.7599459376686871 on epoch=78
05/25/2022 19:06:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
05/25/2022 19:06:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=79
05/25/2022 19:06:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
05/25/2022 19:06:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=80
05/25/2022 19:06:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=81
05/25/2022 19:06:34 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.7610061238893053 on epoch=81
05/25/2022 19:06:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=81
05/25/2022 19:06:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=82
05/25/2022 19:06:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=83
05/25/2022 19:06:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=83
05/25/2022 19:06:46 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/25/2022 19:06:50 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7720934677713803 on epoch=84
05/25/2022 19:06:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=84
05/25/2022 19:06:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/25/2022 19:06:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=86
05/25/2022 19:07:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/25/2022 19:07:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=87
05/25/2022 19:07:06 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.7800265964467674 on epoch=87
05/25/2022 19:07:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7791473093302361 -> 0.7800265964467674 on epoch=87, global_step=1400
05/25/2022 19:07:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/25/2022 19:07:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=88
05/25/2022 19:07:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/25/2022 19:07:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=89
05/25/2022 19:07:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=90
05/25/2022 19:07:22 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.7675976253592034 on epoch=90
05/25/2022 19:07:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=91
05/25/2022 19:07:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=91
05/25/2022 19:07:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=92
05/25/2022 19:07:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
05/25/2022 19:07:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=93
05/25/2022 19:07:38 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.7753653805775576 on epoch=93
05/25/2022 19:07:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/25/2022 19:07:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/25/2022 19:07:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
05/25/2022 19:07:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
05/25/2022 19:07:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
05/25/2022 19:07:55 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.7620492546333602 on epoch=96
05/25/2022 19:07:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=97
05/25/2022 19:08:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
05/25/2022 19:08:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
05/25/2022 19:08:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
05/25/2022 19:08:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=99
05/25/2022 19:08:11 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.7720298198482161 on epoch=99
05/25/2022 19:08:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=100
05/25/2022 19:08:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/25/2022 19:08:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=101
05/25/2022 19:08:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=102
05/25/2022 19:08:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
05/25/2022 19:08:27 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.7667263327690754 on epoch=103
05/25/2022 19:08:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=103
05/25/2022 19:08:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=104
05/25/2022 19:08:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/25/2022 19:08:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/25/2022 19:08:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
05/25/2022 19:08:43 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7720298198482161 on epoch=106
05/25/2022 19:08:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=106
05/25/2022 19:08:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
05/25/2022 19:08:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=108
05/25/2022 19:08:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=108
05/25/2022 19:08:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=109
05/25/2022 19:08:59 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.7756931376109433 on epoch=109
05/25/2022 19:09:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
05/25/2022 19:09:04 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=110
05/25/2022 19:09:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
05/25/2022 19:09:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=111
05/25/2022 19:09:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
05/25/2022 19:09:16 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.7831141350294161 on epoch=112
05/25/2022 19:09:16 - INFO - __main__ - Saving model with best Classification-F1: 0.7800265964467674 -> 0.7831141350294161 on epoch=112, global_step=1800
05/25/2022 19:09:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=113
05/25/2022 19:09:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=113
05/25/2022 19:09:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=114
05/25/2022 19:09:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/25/2022 19:09:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/25/2022 19:09:32 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.7869392299462018 on epoch=115
05/25/2022 19:09:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7831141350294161 -> 0.7869392299462018 on epoch=115, global_step=1850
05/25/2022 19:09:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/25/2022 19:09:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=116
05/25/2022 19:09:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/25/2022 19:09:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
05/25/2022 19:09:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
05/25/2022 19:09:48 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.7747040691998507 on epoch=118
05/25/2022 19:09:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
05/25/2022 19:09:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=119
05/25/2022 19:09:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=120
05/25/2022 19:09:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/25/2022 19:10:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
05/25/2022 19:10:05 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.7868872729337847 on epoch=121
05/25/2022 19:10:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.17 on epoch=122
05/25/2022 19:10:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/25/2022 19:10:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/25/2022 19:10:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=124
05/25/2022 19:10:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=124
05/25/2022 19:10:21 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.7904839990300072 on epoch=124
05/25/2022 19:10:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7869392299462018 -> 0.7904839990300072 on epoch=124, global_step=2000
05/25/2022 19:10:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/25/2022 19:10:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=126
05/25/2022 19:10:28 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=126
05/25/2022 19:10:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/25/2022 19:10:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=128
05/25/2022 19:10:37 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.7912378519340393 on epoch=128
05/25/2022 19:10:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7904839990300072 -> 0.7912378519340393 on epoch=128, global_step=2050
05/25/2022 19:10:40 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
05/25/2022 19:10:42 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=129
05/25/2022 19:10:45 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=129
05/25/2022 19:10:47 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=130
05/25/2022 19:10:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
05/25/2022 19:10:53 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7821916459151086 on epoch=131
05/25/2022 19:10:56 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.29 on epoch=131
05/25/2022 19:10:58 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=132
05/25/2022 19:11:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=133
05/25/2022 19:11:03 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
05/25/2022 19:11:06 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=134
05/25/2022 19:11:10 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7715866951918355 on epoch=134
05/25/2022 19:11:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=134
05/25/2022 19:11:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/25/2022 19:11:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=136
05/25/2022 19:11:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=136
05/25/2022 19:11:22 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
05/25/2022 19:11:26 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.7876518403287913 on epoch=137
05/25/2022 19:11:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
05/25/2022 19:11:31 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=138
05/25/2022 19:11:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/25/2022 19:11:36 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
05/25/2022 19:11:38 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=140
05/25/2022 19:11:42 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.7837901746703925 on epoch=140
05/25/2022 19:11:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
05/25/2022 19:11:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=141
05/25/2022 19:11:50 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=142
05/25/2022 19:11:52 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=143
05/25/2022 19:11:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=143
05/25/2022 19:11:58 - INFO - __main__ - Global step 2300 Train loss 0.12 Classification-F1 0.7994908815088689 on epoch=143
05/25/2022 19:11:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7912378519340393 -> 0.7994908815088689 on epoch=143, global_step=2300
05/25/2022 19:12:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=144
05/25/2022 19:12:04 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/25/2022 19:12:06 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
05/25/2022 19:12:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=146
05/25/2022 19:12:11 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=146
05/25/2022 19:12:15 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.7823317390222131 on epoch=146
05/25/2022 19:12:17 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
05/25/2022 19:12:20 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=148
05/25/2022 19:12:22 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=148
05/25/2022 19:12:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/25/2022 19:12:27 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
05/25/2022 19:12:31 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.8045677163757741 on epoch=149
05/25/2022 19:12:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7994908815088689 -> 0.8045677163757741 on epoch=149, global_step=2400
05/25/2022 19:12:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/25/2022 19:12:36 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/25/2022 19:12:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/25/2022 19:12:41 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=152
05/25/2022 19:12:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=153
05/25/2022 19:12:48 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.7859746565958006 on epoch=153
05/25/2022 19:12:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=153
05/25/2022 19:12:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
05/25/2022 19:12:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/25/2022 19:12:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
05/25/2022 19:13:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/25/2022 19:13:04 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.8067299207491919 on epoch=156
05/25/2022 19:13:04 - INFO - __main__ - Saving model with best Classification-F1: 0.8045677163757741 -> 0.8067299207491919 on epoch=156, global_step=2500
05/25/2022 19:13:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/25/2022 19:13:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/25/2022 19:13:12 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/25/2022 19:13:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/25/2022 19:13:17 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.28 on epoch=159
05/25/2022 19:13:20 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.8084156376582744 on epoch=159
05/25/2022 19:13:20 - INFO - __main__ - Saving model with best Classification-F1: 0.8067299207491919 -> 0.8084156376582744 on epoch=159, global_step=2550
05/25/2022 19:13:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/25/2022 19:13:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/25/2022 19:13:28 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/25/2022 19:13:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=161
05/25/2022 19:13:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.14 on epoch=162
05/25/2022 19:13:37 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.7998363625348357 on epoch=162
05/25/2022 19:13:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=163
05/25/2022 19:13:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
05/25/2022 19:13:44 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
05/25/2022 19:13:47 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/25/2022 19:13:49 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
05/25/2022 19:13:53 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.81099670629704 on epoch=165
05/25/2022 19:13:53 - INFO - __main__ - Saving model with best Classification-F1: 0.8084156376582744 -> 0.81099670629704 on epoch=165, global_step=2650
05/25/2022 19:13:55 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=166
05/25/2022 19:13:58 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=166
05/25/2022 19:14:01 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
05/25/2022 19:14:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=168
05/25/2022 19:14:06 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/25/2022 19:14:09 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.8045427370530078 on epoch=168
05/25/2022 19:14:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=169
05/25/2022 19:14:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
05/25/2022 19:14:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/25/2022 19:14:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
05/25/2022 19:14:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=171
05/25/2022 19:14:26 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.8038450878203786 on epoch=171
05/25/2022 19:14:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.10 on epoch=172
05/25/2022 19:14:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/25/2022 19:14:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/25/2022 19:14:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/25/2022 19:14:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/25/2022 19:14:42 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.8087593862714063 on epoch=174
05/25/2022 19:14:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=175
05/25/2022 19:14:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/25/2022 19:14:50 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
05/25/2022 19:14:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=177
05/25/2022 19:14:55 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/25/2022 19:14:59 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.8155421608424944 on epoch=178
05/25/2022 19:14:59 - INFO - __main__ - Saving model with best Classification-F1: 0.81099670629704 -> 0.8155421608424944 on epoch=178, global_step=2850
05/25/2022 19:15:01 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=178
05/25/2022 19:15:04 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
05/25/2022 19:15:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/25/2022 19:15:09 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
05/25/2022 19:15:11 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
05/25/2022 19:15:15 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.8063280648485139 on epoch=181
05/25/2022 19:15:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
05/25/2022 19:15:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/25/2022 19:15:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/25/2022 19:15:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 19:15:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/25/2022 19:15:32 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.7954561159885749 on epoch=184
05/25/2022 19:15:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
05/25/2022 19:15:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=185
05/25/2022 19:15:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/25/2022 19:15:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=186
05/25/2022 19:15:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
05/25/2022 19:15:46 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:15:46 - INFO - __main__ - Printing 3 examples
05/25/2022 19:15:46 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:15:46 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:15:46 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:15:46 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:15:46 - INFO - __main__ - Printing 3 examples
05/25/2022 19:15:46 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 19:15:46 - INFO - __main__ - ['others']
05/25/2022 19:15:46 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:15:46 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:15:46 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:15:48 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.7944519799322401 on epoch=187
05/25/2022 19:15:48 - INFO - __main__ - save last model!
05/25/2022 19:15:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 19:15:48 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 19:15:48 - INFO - __main__ - Printing 3 examples
05/25/2022 19:15:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 19:15:48 - INFO - __main__ - ['others']
05/25/2022 19:15:48 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 19:15:48 - INFO - __main__ - ['others']
05/25/2022 19:15:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 19:15:48 - INFO - __main__ - ['others']
05/25/2022 19:15:48 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:15:50 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:15:56 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 19:16:05 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:16:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:16:06 - INFO - __main__ - Starting training!
05/25/2022 19:17:14 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_100_0.3_8_predictions.txt
05/25/2022 19:17:14 - INFO - __main__ - Classification-F1 on test data: 0.5007
05/25/2022 19:17:14 - INFO - __main__ - prefix=emo_64_100, lr=0.3, bsz=8, dev_performance=0.8155421608424944, test_performance=0.5007167920088269
05/25/2022 19:17:14 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.2, bsz=8 ...
05/25/2022 19:17:15 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:17:15 - INFO - __main__ - Printing 3 examples
05/25/2022 19:17:15 - INFO - __main__ -  [emo] how cause yes am listening
05/25/2022 19:17:15 - INFO - __main__ - ['others']
05/25/2022 19:17:15 - INFO - __main__ -  [emo] ok that way i like living wwrong
05/25/2022 19:17:15 - INFO - __main__ - ['others']
05/25/2022 19:17:15 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
05/25/2022 19:17:15 - INFO - __main__ - ['others']
05/25/2022 19:17:15 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:17:15 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:17:16 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:17:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:17:16 - INFO - __main__ - Printing 3 examples
05/25/2022 19:17:16 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
05/25/2022 19:17:16 - INFO - __main__ - ['others']
05/25/2022 19:17:16 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
05/25/2022 19:17:16 - INFO - __main__ - ['others']
05/25/2022 19:17:16 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
05/25/2022 19:17:16 - INFO - __main__ - ['others']
05/25/2022 19:17:16 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:17:16 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:17:16 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:17:32 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:17:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:17:33 - INFO - __main__ - Starting training!
05/25/2022 19:17:36 - INFO - __main__ - Step 10 Global step 10 Train loss 4.47 on epoch=0
05/25/2022 19:17:39 - INFO - __main__ - Step 20 Global step 20 Train loss 3.45 on epoch=1
05/25/2022 19:17:41 - INFO - __main__ - Step 30 Global step 30 Train loss 3.10 on epoch=1
05/25/2022 19:17:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.39 on epoch=2
05/25/2022 19:17:46 - INFO - __main__ - Step 50 Global step 50 Train loss 2.24 on epoch=3
05/25/2022 19:17:50 - INFO - __main__ - Global step 50 Train loss 3.13 Classification-F1 0.03131431707651731 on epoch=3
05/25/2022 19:17:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.03131431707651731 on epoch=3, global_step=50
05/25/2022 19:17:52 - INFO - __main__ - Step 60 Global step 60 Train loss 2.37 on epoch=3
05/25/2022 19:17:54 - INFO - __main__ - Step 70 Global step 70 Train loss 1.96 on epoch=4
05/25/2022 19:17:57 - INFO - __main__ - Step 80 Global step 80 Train loss 1.78 on epoch=4
05/25/2022 19:17:59 - INFO - __main__ - Step 90 Global step 90 Train loss 1.73 on epoch=5
05/25/2022 19:18:02 - INFO - __main__ - Step 100 Global step 100 Train loss 1.53 on epoch=6
05/25/2022 19:18:05 - INFO - __main__ - Global step 100 Train loss 1.87 Classification-F1 0.15888681891444958 on epoch=6
05/25/2022 19:18:05 - INFO - __main__ - Saving model with best Classification-F1: 0.03131431707651731 -> 0.15888681891444958 on epoch=6, global_step=100
05/25/2022 19:18:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.45 on epoch=6
05/25/2022 19:18:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.25 on epoch=7
05/25/2022 19:18:12 - INFO - __main__ - Step 130 Global step 130 Train loss 1.12 on epoch=8
05/25/2022 19:18:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.01 on epoch=8
05/25/2022 19:18:17 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=9
05/25/2022 19:18:21 - INFO - __main__ - Global step 150 Train loss 1.17 Classification-F1 0.3634168567959546 on epoch=9
05/25/2022 19:18:21 - INFO - __main__ - Saving model with best Classification-F1: 0.15888681891444958 -> 0.3634168567959546 on epoch=9, global_step=150
05/25/2022 19:18:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=9
05/25/2022 19:18:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=10
05/25/2022 19:18:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.95 on epoch=11
05/25/2022 19:18:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=11
05/25/2022 19:18:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=12
05/25/2022 19:18:37 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.5270506904653247 on epoch=12
05/25/2022 19:18:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3634168567959546 -> 0.5270506904653247 on epoch=12, global_step=200
05/25/2022 19:18:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=13
05/25/2022 19:18:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=13
05/25/2022 19:18:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=14
05/25/2022 19:18:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.68 on epoch=14
05/25/2022 19:18:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=15
05/25/2022 19:18:53 - INFO - __main__ - Global step 250 Train loss 0.73 Classification-F1 0.5443540874446647 on epoch=15
05/25/2022 19:18:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5270506904653247 -> 0.5443540874446647 on epoch=15, global_step=250
05/25/2022 19:18:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=16
05/25/2022 19:18:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.79 on epoch=16
05/25/2022 19:19:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=17
05/25/2022 19:19:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=18
05/25/2022 19:19:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.74 on epoch=18
05/25/2022 19:19:09 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.5728164708379033 on epoch=18
05/25/2022 19:19:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5443540874446647 -> 0.5728164708379033 on epoch=18, global_step=300
05/25/2022 19:19:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=19
05/25/2022 19:19:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=19
05/25/2022 19:19:16 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=20
05/25/2022 19:19:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=21
05/25/2022 19:19:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=21
05/25/2022 19:19:25 - INFO - __main__ - Global step 350 Train loss 0.68 Classification-F1 0.5770535161862687 on epoch=21
05/25/2022 19:19:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5728164708379033 -> 0.5770535161862687 on epoch=21, global_step=350
05/25/2022 19:19:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=22
05/25/2022 19:19:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=23
05/25/2022 19:19:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=23
05/25/2022 19:19:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=24
05/25/2022 19:19:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.69 on epoch=24
05/25/2022 19:19:41 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.6505382916210942 on epoch=24
05/25/2022 19:19:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5770535161862687 -> 0.6505382916210942 on epoch=24, global_step=400
05/25/2022 19:19:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=25
05/25/2022 19:19:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=26
05/25/2022 19:19:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.67 on epoch=26
05/25/2022 19:19:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=27
05/25/2022 19:19:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=28
05/25/2022 19:19:57 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.6390333235353398 on epoch=28
05/25/2022 19:19:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=28
05/25/2022 19:20:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=29
05/25/2022 19:20:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=29
05/25/2022 19:20:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=30
05/25/2022 19:20:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=31
05/25/2022 19:20:13 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.6524545689117323 on epoch=31
05/25/2022 19:20:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6505382916210942 -> 0.6524545689117323 on epoch=31, global_step=500
05/25/2022 19:20:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=31
05/25/2022 19:20:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=32
05/25/2022 19:20:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=33
05/25/2022 19:20:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=33
05/25/2022 19:20:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=34
05/25/2022 19:20:29 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6693939729855926 on epoch=34
05/25/2022 19:20:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6524545689117323 -> 0.6693939729855926 on epoch=34, global_step=550
05/25/2022 19:20:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.51 on epoch=34
05/25/2022 19:20:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=35
05/25/2022 19:20:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=36
05/25/2022 19:20:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=36
05/25/2022 19:20:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=37
05/25/2022 19:20:45 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.64515141638023 on epoch=37
05/25/2022 19:20:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=38
05/25/2022 19:20:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=38
05/25/2022 19:20:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=39
05/25/2022 19:20:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=39
05/25/2022 19:20:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=40
05/25/2022 19:21:01 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.5330350621281609 on epoch=40
05/25/2022 19:21:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=41
05/25/2022 19:21:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=41
05/25/2022 19:21:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=42
05/25/2022 19:21:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=43
05/25/2022 19:21:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.61 on epoch=43
05/25/2022 19:21:17 - INFO - __main__ - Global step 700 Train loss 0.50 Classification-F1 0.6746883822122676 on epoch=43
05/25/2022 19:21:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6693939729855926 -> 0.6746883822122676 on epoch=43, global_step=700
05/25/2022 19:21:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=44
05/25/2022 19:21:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=44
05/25/2022 19:21:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.40 on epoch=45
05/25/2022 19:21:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
05/25/2022 19:21:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=46
05/25/2022 19:21:33 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.6950691874376655 on epoch=46
05/25/2022 19:21:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6746883822122676 -> 0.6950691874376655 on epoch=46, global_step=750
05/25/2022 19:21:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
05/25/2022 19:21:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=48
05/25/2022 19:21:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=48
05/25/2022 19:21:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=49
05/25/2022 19:21:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=49
05/25/2022 19:21:49 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.7077028593422036 on epoch=49
05/25/2022 19:21:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6950691874376655 -> 0.7077028593422036 on epoch=49, global_step=800
05/25/2022 19:21:52 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
05/25/2022 19:21:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
05/25/2022 19:21:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=51
05/25/2022 19:21:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=52
05/25/2022 19:22:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
05/25/2022 19:22:06 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.7259383831761008 on epoch=53
05/25/2022 19:22:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7077028593422036 -> 0.7259383831761008 on epoch=53, global_step=850
05/25/2022 19:22:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=53
05/25/2022 19:22:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=54
05/25/2022 19:22:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=54
05/25/2022 19:22:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=55
05/25/2022 19:22:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/25/2022 19:22:22 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.7412020326747721 on epoch=56
05/25/2022 19:22:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7259383831761008 -> 0.7412020326747721 on epoch=56, global_step=900
05/25/2022 19:22:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
05/25/2022 19:22:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=57
05/25/2022 19:22:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
05/25/2022 19:22:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=58
05/25/2022 19:22:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
05/25/2022 19:22:38 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7462472847596087 on epoch=59
05/25/2022 19:22:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7412020326747721 -> 0.7462472847596087 on epoch=59, global_step=950
05/25/2022 19:22:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=59
05/25/2022 19:22:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=60
05/25/2022 19:22:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/25/2022 19:22:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=61
05/25/2022 19:22:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=62
05/25/2022 19:22:54 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.7231791656056361 on epoch=62
05/25/2022 19:22:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=63
05/25/2022 19:22:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/25/2022 19:23:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
05/25/2022 19:23:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
05/25/2022 19:23:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=65
05/25/2022 19:23:10 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.7350826777781769 on epoch=65
05/25/2022 19:23:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
05/25/2022 19:23:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/25/2022 19:23:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/25/2022 19:23:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/25/2022 19:23:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=68
05/25/2022 19:23:26 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.7462656272615429 on epoch=68
05/25/2022 19:23:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7462472847596087 -> 0.7462656272615429 on epoch=68, global_step=1100
05/25/2022 19:23:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/25/2022 19:23:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=69
05/25/2022 19:23:34 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/25/2022 19:23:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=71
05/25/2022 19:23:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=71
05/25/2022 19:23:42 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.7496912482313974 on epoch=71
05/25/2022 19:23:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7462656272615429 -> 0.7496912482313974 on epoch=71, global_step=1150
05/25/2022 19:23:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=72
05/25/2022 19:23:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=73
05/25/2022 19:23:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=73
05/25/2022 19:23:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=74
05/25/2022 19:23:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=74
05/25/2022 19:23:58 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.7412155655517196 on epoch=74
05/25/2022 19:24:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
05/25/2022 19:24:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.33 on epoch=76
05/25/2022 19:24:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=76
05/25/2022 19:24:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=77
05/25/2022 19:24:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=78
05/25/2022 19:24:14 - INFO - __main__ - Global step 1250 Train loss 0.37 Classification-F1 0.7427796776605379 on epoch=78
05/25/2022 19:24:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.48 on epoch=78
05/25/2022 19:24:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=79
05/25/2022 19:24:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
05/25/2022 19:24:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=80
05/25/2022 19:24:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
05/25/2022 19:24:30 - INFO - __main__ - Global step 1300 Train loss 0.38 Classification-F1 0.7369468675654243 on epoch=81
05/25/2022 19:24:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
05/25/2022 19:24:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=82
05/25/2022 19:24:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=83
05/25/2022 19:24:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
05/25/2022 19:24:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=84
05/25/2022 19:24:46 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7586035387905892 on epoch=84
05/25/2022 19:24:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7496912482313974 -> 0.7586035387905892 on epoch=84, global_step=1350
05/25/2022 19:24:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=84
05/25/2022 19:24:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=85
05/25/2022 19:24:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=86
05/25/2022 19:24:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=86
05/25/2022 19:24:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=87
05/25/2022 19:25:02 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7632303539265981 on epoch=87
05/25/2022 19:25:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7586035387905892 -> 0.7632303539265981 on epoch=87, global_step=1400
05/25/2022 19:25:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/25/2022 19:25:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=88
05/25/2022 19:25:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.33 on epoch=89
05/25/2022 19:25:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=89
05/25/2022 19:25:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=90
05/25/2022 19:25:18 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.7624758630642038 on epoch=90
05/25/2022 19:25:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=91
05/25/2022 19:25:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=91
05/25/2022 19:25:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
05/25/2022 19:25:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=93
05/25/2022 19:25:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
05/25/2022 19:25:35 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.7556634858667877 on epoch=93
05/25/2022 19:25:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.30 on epoch=94
05/25/2022 19:25:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=94
05/25/2022 19:25:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=95
05/25/2022 19:25:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=96
05/25/2022 19:25:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=96
05/25/2022 19:25:51 - INFO - __main__ - Global step 1550 Train loss 0.30 Classification-F1 0.7473012984162292 on epoch=96
05/25/2022 19:25:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=97
05/25/2022 19:25:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/25/2022 19:25:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.28 on epoch=98
05/25/2022 19:26:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=99
05/25/2022 19:26:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/25/2022 19:26:07 - INFO - __main__ - Global step 1600 Train loss 0.28 Classification-F1 0.7482580917380603 on epoch=99
05/25/2022 19:26:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=100
05/25/2022 19:26:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=101
05/25/2022 19:26:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=101
05/25/2022 19:26:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
05/25/2022 19:26:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=103
05/25/2022 19:26:23 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7550494850718065 on epoch=103
05/25/2022 19:26:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
05/25/2022 19:26:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
05/25/2022 19:26:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=104
05/25/2022 19:26:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=105
05/25/2022 19:26:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=106
05/25/2022 19:26:39 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.7362218770125954 on epoch=106
05/25/2022 19:26:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=106
05/25/2022 19:26:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
05/25/2022 19:26:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/25/2022 19:26:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.30 on epoch=108
05/25/2022 19:26:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=109
05/25/2022 19:26:55 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7468640251147406 on epoch=109
05/25/2022 19:26:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=109
05/25/2022 19:27:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=110
05/25/2022 19:27:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=111
05/25/2022 19:27:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=111
05/25/2022 19:27:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/25/2022 19:27:11 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7558218825587978 on epoch=112
05/25/2022 19:27:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=113
05/25/2022 19:27:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/25/2022 19:27:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=114
05/25/2022 19:27:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
05/25/2022 19:27:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=115
05/25/2022 19:27:27 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7550063799706861 on epoch=115
05/25/2022 19:27:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
05/25/2022 19:27:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=116
05/25/2022 19:27:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
05/25/2022 19:27:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=118
05/25/2022 19:27:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/25/2022 19:27:43 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.7351812836010363 on epoch=118
05/25/2022 19:27:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
05/25/2022 19:27:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.28 on epoch=119
05/25/2022 19:27:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.31 on epoch=120
05/25/2022 19:27:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.28 on epoch=121
05/25/2022 19:27:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=121
05/25/2022 19:27:59 - INFO - __main__ - Global step 1950 Train loss 0.26 Classification-F1 0.7637949346992648 on epoch=121
05/25/2022 19:27:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7632303539265981 -> 0.7637949346992648 on epoch=121, global_step=1950
05/25/2022 19:28:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
05/25/2022 19:28:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=123
05/25/2022 19:28:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=123
05/25/2022 19:28:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/25/2022 19:28:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=124
05/25/2022 19:28:15 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.7589635838730172 on epoch=124
05/25/2022 19:28:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
05/25/2022 19:28:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=126
05/25/2022 19:28:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/25/2022 19:28:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.27 on epoch=127
05/25/2022 19:28:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.22 on epoch=128
05/25/2022 19:28:31 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7589531525396906 on epoch=128
05/25/2022 19:28:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.31 on epoch=128
05/25/2022 19:28:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
05/25/2022 19:28:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=129
05/25/2022 19:28:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
05/25/2022 19:28:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
05/25/2022 19:28:48 - INFO - __main__ - Global step 2100 Train loss 0.21 Classification-F1 0.7625174025170625 on epoch=131
05/25/2022 19:28:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=131
05/25/2022 19:28:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.22 on epoch=132
05/25/2022 19:28:55 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/25/2022 19:28:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.27 on epoch=133
05/25/2022 19:29:00 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=134
05/25/2022 19:29:04 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.761933039505284 on epoch=134
05/25/2022 19:29:06 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.25 on epoch=134
05/25/2022 19:29:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=135
05/25/2022 19:29:11 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=136
05/25/2022 19:29:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
05/25/2022 19:29:16 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
05/25/2022 19:29:20 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.7719471769692742 on epoch=137
05/25/2022 19:29:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7637949346992648 -> 0.7719471769692742 on epoch=137, global_step=2200
05/25/2022 19:29:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=138
05/25/2022 19:29:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=138
05/25/2022 19:29:27 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.21 on epoch=139
05/25/2022 19:29:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.26 on epoch=139
05/25/2022 19:29:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.19 on epoch=140
05/25/2022 19:29:36 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.7757051939069761 on epoch=140
05/25/2022 19:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7719471769692742 -> 0.7757051939069761 on epoch=140, global_step=2250
05/25/2022 19:29:38 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=141
05/25/2022 19:29:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=141
05/25/2022 19:29:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=142
05/25/2022 19:29:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=143
05/25/2022 19:29:48 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.16 on epoch=143
05/25/2022 19:29:52 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7675598287535176 on epoch=143
05/25/2022 19:29:54 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/25/2022 19:29:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/25/2022 19:29:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/25/2022 19:30:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/25/2022 19:30:04 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/25/2022 19:30:08 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.7674062814228255 on epoch=146
05/25/2022 19:30:10 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
05/25/2022 19:30:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=148
05/25/2022 19:30:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/25/2022 19:30:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/25/2022 19:30:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/25/2022 19:30:24 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7719471769692742 on epoch=149
05/25/2022 19:30:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
05/25/2022 19:30:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/25/2022 19:30:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=151
05/25/2022 19:30:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=152
05/25/2022 19:30:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/25/2022 19:30:42 - INFO - __main__ - Global step 2450 Train loss 0.17 Classification-F1 0.7679939518906024 on epoch=153
05/25/2022 19:30:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=153
05/25/2022 19:30:47 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.17 on epoch=154
05/25/2022 19:30:49 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.17 on epoch=154
05/25/2022 19:30:52 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
05/25/2022 19:30:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
05/25/2022 19:30:58 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7679939518906024 on epoch=156
05/25/2022 19:31:00 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.28 on epoch=156
05/25/2022 19:31:03 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/25/2022 19:31:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=158
05/25/2022 19:31:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/25/2022 19:31:10 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/25/2022 19:31:13 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7715514478174251 on epoch=159
05/25/2022 19:31:16 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/25/2022 19:31:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
05/25/2022 19:31:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
05/25/2022 19:31:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.25 on epoch=161
05/25/2022 19:31:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=162
05/25/2022 19:31:29 - INFO - __main__ - Global step 2600 Train loss 0.19 Classification-F1 0.7650907027921827 on epoch=162
05/25/2022 19:31:32 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/25/2022 19:31:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/25/2022 19:31:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/25/2022 19:31:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.22 on epoch=164
05/25/2022 19:31:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.16 on epoch=165
05/25/2022 19:31:46 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.7799484783129642 on epoch=165
05/25/2022 19:31:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7757051939069761 -> 0.7799484783129642 on epoch=165, global_step=2650
05/25/2022 19:31:48 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=166
05/25/2022 19:31:51 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=166
05/25/2022 19:31:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=167
05/25/2022 19:31:55 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
05/25/2022 19:31:58 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.18 on epoch=168
05/25/2022 19:32:02 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.7800817814860737 on epoch=168
05/25/2022 19:32:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7799484783129642 -> 0.7800817814860737 on epoch=168, global_step=2700
05/25/2022 19:32:04 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
05/25/2022 19:32:07 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=169
05/25/2022 19:32:09 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/25/2022 19:32:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=171
05/25/2022 19:32:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
05/25/2022 19:32:18 - INFO - __main__ - Global step 2750 Train loss 0.17 Classification-F1 0.7839208295626878 on epoch=171
05/25/2022 19:32:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7800817814860737 -> 0.7839208295626878 on epoch=171, global_step=2750
05/25/2022 19:32:20 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
05/25/2022 19:32:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.23 on epoch=173
05/25/2022 19:32:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
05/25/2022 19:32:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/25/2022 19:32:30 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=174
05/25/2022 19:32:34 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7714877023288238 on epoch=174
05/25/2022 19:32:36 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/25/2022 19:32:39 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.21 on epoch=176
05/25/2022 19:32:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=176
05/25/2022 19:32:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=177
05/25/2022 19:32:46 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.16 on epoch=178
05/25/2022 19:32:50 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.7829337307521269 on epoch=178
05/25/2022 19:32:52 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=178
05/25/2022 19:32:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
05/25/2022 19:32:57 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
05/25/2022 19:33:00 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
05/25/2022 19:33:02 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
05/25/2022 19:33:06 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.7751134033586576 on epoch=181
05/25/2022 19:33:08 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.25 on epoch=181
05/25/2022 19:33:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/25/2022 19:33:13 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=183
05/25/2022 19:33:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
05/25/2022 19:33:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
05/25/2022 19:33:22 - INFO - __main__ - Global step 2950 Train loss 0.16 Classification-F1 0.7799880958440589 on epoch=184
05/25/2022 19:33:24 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.14 on epoch=184
05/25/2022 19:33:27 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
05/25/2022 19:33:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=186
05/25/2022 19:33:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
05/25/2022 19:33:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.12 on epoch=187
05/25/2022 19:33:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:33:35 - INFO - __main__ - Printing 3 examples
05/25/2022 19:33:35 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 19:33:35 - INFO - __main__ - ['others']
05/25/2022 19:33:35 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 19:33:35 - INFO - __main__ - ['others']
05/25/2022 19:33:35 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 19:33:35 - INFO - __main__ - ['others']
05/25/2022 19:33:35 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:33:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:33:36 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:33:36 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:33:36 - INFO - __main__ - Printing 3 examples
05/25/2022 19:33:36 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 19:33:36 - INFO - __main__ - ['others']
05/25/2022 19:33:36 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 19:33:36 - INFO - __main__ - ['others']
05/25/2022 19:33:36 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 19:33:36 - INFO - __main__ - ['others']
05/25/2022 19:33:36 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:33:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:33:36 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:33:38 - INFO - __main__ - Global step 3000 Train loss 0.14 Classification-F1 0.7782235767976803 on epoch=187
05/25/2022 19:33:38 - INFO - __main__ - save last model!
05/25/2022 19:33:38 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 19:33:38 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 19:33:38 - INFO - __main__ - Printing 3 examples
05/25/2022 19:33:38 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 19:33:38 - INFO - __main__ - ['others']
05/25/2022 19:33:38 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 19:33:38 - INFO - __main__ - ['others']
05/25/2022 19:33:38 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 19:33:38 - INFO - __main__ - ['others']
05/25/2022 19:33:38 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:33:40 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:33:45 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 19:33:55 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:33:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:33:56 - INFO - __main__ - Starting training!
05/25/2022 19:35:03 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_100_0.2_8_predictions.txt
05/25/2022 19:35:03 - INFO - __main__ - Classification-F1 on test data: 0.4971
05/25/2022 19:35:03 - INFO - __main__ - prefix=emo_64_100, lr=0.2, bsz=8, dev_performance=0.7839208295626878, test_performance=0.49713880510810643
05/25/2022 19:35:03 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.5, bsz=8 ...
05/25/2022 19:35:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:35:04 - INFO - __main__ - Printing 3 examples
05/25/2022 19:35:04 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:35:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:35:04 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:35:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:35:04 - INFO - __main__ - Printing 3 examples
05/25/2022 19:35:04 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 19:35:04 - INFO - __main__ - ['others']
05/25/2022 19:35:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:35:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:35:05 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:35:20 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:35:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:35:21 - INFO - __main__ - Starting training!
05/25/2022 19:35:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.04 on epoch=0
05/25/2022 19:35:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.82 on epoch=1
05/25/2022 19:35:30 - INFO - __main__ - Step 30 Global step 30 Train loss 2.21 on epoch=1
05/25/2022 19:35:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.93 on epoch=2
05/25/2022 19:35:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.28 on epoch=3
05/25/2022 19:35:38 - INFO - __main__ - Global step 50 Train loss 2.46 Classification-F1 0.19523903647480495 on epoch=3
05/25/2022 19:35:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19523903647480495 on epoch=3, global_step=50
05/25/2022 19:35:41 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=3
05/25/2022 19:35:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=4
05/25/2022 19:35:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.94 on epoch=4
05/25/2022 19:35:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.91 on epoch=5
05/25/2022 19:35:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=6
05/25/2022 19:35:54 - INFO - __main__ - Global step 100 Train loss 0.96 Classification-F1 0.5232478911246035 on epoch=6
05/25/2022 19:35:54 - INFO - __main__ - Saving model with best Classification-F1: 0.19523903647480495 -> 0.5232478911246035 on epoch=6, global_step=100
05/25/2022 19:35:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.80 on epoch=6
05/25/2022 19:35:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=7
05/25/2022 19:36:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.62 on epoch=8
05/25/2022 19:36:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=8
05/25/2022 19:36:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.81 on epoch=9
05/25/2022 19:36:10 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.6163814425642605 on epoch=9
05/25/2022 19:36:10 - INFO - __main__ - Saving model with best Classification-F1: 0.5232478911246035 -> 0.6163814425642605 on epoch=9, global_step=150
05/25/2022 19:36:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.68 on epoch=9
05/25/2022 19:36:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=10
05/25/2022 19:36:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.65 on epoch=11
05/25/2022 19:36:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.61 on epoch=11
05/25/2022 19:36:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=12
05/25/2022 19:36:26 - INFO - __main__ - Global step 200 Train loss 0.65 Classification-F1 0.6947459834997829 on epoch=12
05/25/2022 19:36:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6163814425642605 -> 0.6947459834997829 on epoch=12, global_step=200
05/25/2022 19:36:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.54 on epoch=13
05/25/2022 19:36:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=13
05/25/2022 19:36:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.62 on epoch=14
05/25/2022 19:36:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=14
05/25/2022 19:36:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.57 on epoch=15
05/25/2022 19:36:43 - INFO - __main__ - Global step 250 Train loss 0.60 Classification-F1 0.6265583212293254 on epoch=15
05/25/2022 19:36:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.50 on epoch=16
05/25/2022 19:36:48 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=16
05/25/2022 19:36:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=17
05/25/2022 19:36:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=18
05/25/2022 19:36:55 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=18
05/25/2022 19:36:58 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.6667755566691235 on epoch=18
05/25/2022 19:37:01 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=19
05/25/2022 19:37:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=19
05/25/2022 19:37:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=20
05/25/2022 19:37:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=21
05/25/2022 19:37:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=21
05/25/2022 19:37:14 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.6880918644918614 on epoch=21
05/25/2022 19:37:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=22
05/25/2022 19:37:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=23
05/25/2022 19:37:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=23
05/25/2022 19:37:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=24
05/25/2022 19:37:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
05/25/2022 19:37:30 - INFO - __main__ - Global step 400 Train loss 0.43 Classification-F1 0.7115954508241242 on epoch=24
05/25/2022 19:37:30 - INFO - __main__ - Saving model with best Classification-F1: 0.6947459834997829 -> 0.7115954508241242 on epoch=24, global_step=400
05/25/2022 19:37:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.38 on epoch=25
05/25/2022 19:37:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=26
05/25/2022 19:37:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=26
05/25/2022 19:37:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
05/25/2022 19:37:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=28
05/25/2022 19:37:46 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.7262273350438607 on epoch=28
05/25/2022 19:37:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7115954508241242 -> 0.7262273350438607 on epoch=28, global_step=450
05/25/2022 19:37:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=28
05/25/2022 19:37:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=29
05/25/2022 19:37:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=29
05/25/2022 19:37:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
05/25/2022 19:37:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
05/25/2022 19:38:03 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.7173536954150663 on epoch=31
05/25/2022 19:38:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.35 on epoch=31
05/25/2022 19:38:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=32
05/25/2022 19:38:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
05/25/2022 19:38:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=33
05/25/2022 19:38:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/25/2022 19:38:19 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.7373719914554759 on epoch=34
05/25/2022 19:38:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7262273350438607 -> 0.7373719914554759 on epoch=34, global_step=550
05/25/2022 19:38:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/25/2022 19:38:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
05/25/2022 19:38:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
05/25/2022 19:38:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.34 on epoch=36
05/25/2022 19:38:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/25/2022 19:38:35 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.7533452259171683 on epoch=37
05/25/2022 19:38:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7373719914554759 -> 0.7533452259171683 on epoch=37, global_step=600
05/25/2022 19:38:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=38
05/25/2022 19:38:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=38
05/25/2022 19:38:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
05/25/2022 19:38:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=39
05/25/2022 19:38:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=40
05/25/2022 19:38:51 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.7558369020417246 on epoch=40
05/25/2022 19:38:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7533452259171683 -> 0.7558369020417246 on epoch=40, global_step=650
05/25/2022 19:38:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=41
05/25/2022 19:38:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.32 on epoch=41
05/25/2022 19:38:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
05/25/2022 19:39:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=43
05/25/2022 19:39:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
05/25/2022 19:39:07 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.7686429205187255 on epoch=43
05/25/2022 19:39:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7558369020417246 -> 0.7686429205187255 on epoch=43, global_step=700
05/25/2022 19:39:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=44
05/25/2022 19:39:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=44
05/25/2022 19:39:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/25/2022 19:39:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/25/2022 19:39:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=46
05/25/2022 19:39:22 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.760284183492502 on epoch=46
05/25/2022 19:39:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
05/25/2022 19:39:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=48
05/25/2022 19:39:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.29 on epoch=48
05/25/2022 19:39:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/25/2022 19:39:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=49
05/25/2022 19:39:38 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.7640594253543908 on epoch=49
05/25/2022 19:39:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=50
05/25/2022 19:39:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=51
05/25/2022 19:39:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/25/2022 19:39:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=52
05/25/2022 19:39:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
05/25/2022 19:39:54 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.7594196302140228 on epoch=53
05/25/2022 19:39:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/25/2022 19:39:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=54
05/25/2022 19:40:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=54
05/25/2022 19:40:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=55
05/25/2022 19:40:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/25/2022 19:40:10 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.7690882618633392 on epoch=56
05/25/2022 19:40:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7686429205187255 -> 0.7690882618633392 on epoch=56, global_step=900
05/25/2022 19:40:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=56
05/25/2022 19:40:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=57
05/25/2022 19:40:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=58
05/25/2022 19:40:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=58
05/25/2022 19:40:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=59
05/25/2022 19:40:26 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.7855347630534197 on epoch=59
05/25/2022 19:40:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7690882618633392 -> 0.7855347630534197 on epoch=59, global_step=950
05/25/2022 19:40:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=59
05/25/2022 19:40:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/25/2022 19:40:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
05/25/2022 19:40:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=61
05/25/2022 19:40:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=62
05/25/2022 19:40:42 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7969829421555041 on epoch=62
05/25/2022 19:40:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7855347630534197 -> 0.7969829421555041 on epoch=62, global_step=1000
05/25/2022 19:40:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=63
05/25/2022 19:40:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/25/2022 19:40:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
05/25/2022 19:40:52 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
05/25/2022 19:40:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=65
05/25/2022 19:40:58 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7841980457282182 on epoch=65
05/25/2022 19:41:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/25/2022 19:41:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/25/2022 19:41:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
05/25/2022 19:41:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=68
05/25/2022 19:41:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/25/2022 19:41:13 - INFO - __main__ - Global step 1100 Train loss 0.22 Classification-F1 0.7836066956390515 on epoch=68
05/25/2022 19:41:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=69
05/25/2022 19:41:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=69
05/25/2022 19:41:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=70
05/25/2022 19:41:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=71
05/25/2022 19:41:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=71
05/25/2022 19:41:29 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.777885144848329 on epoch=71
05/25/2022 19:41:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=72
05/25/2022 19:41:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=73
05/25/2022 19:41:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/25/2022 19:41:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=74
05/25/2022 19:41:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=74
05/25/2022 19:41:45 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.7767802573008179 on epoch=74
05/25/2022 19:41:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=75
05/25/2022 19:41:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/25/2022 19:41:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=76
05/25/2022 19:41:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
05/25/2022 19:41:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
05/25/2022 19:42:01 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.7676843264529831 on epoch=78
05/25/2022 19:42:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=78
05/25/2022 19:42:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=79
05/25/2022 19:42:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=79
05/25/2022 19:42:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/25/2022 19:42:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=81
05/25/2022 19:42:17 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.7730342366878016 on epoch=81
05/25/2022 19:42:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.11 on epoch=81
05/25/2022 19:42:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=82
05/25/2022 19:42:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=83
05/25/2022 19:42:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=83
05/25/2022 19:42:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=84
05/25/2022 19:42:33 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.7795274343769188 on epoch=84
05/25/2022 19:42:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.10 on epoch=84
05/25/2022 19:42:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=85
05/25/2022 19:42:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
05/25/2022 19:42:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=86
05/25/2022 19:42:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/25/2022 19:42:48 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7747278983217472 on epoch=87
05/25/2022 19:42:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=88
05/25/2022 19:42:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=88
05/25/2022 19:42:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
05/25/2022 19:42:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=89
05/25/2022 19:43:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=90
05/25/2022 19:43:04 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.7846191020364731 on epoch=90
05/25/2022 19:43:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=91
05/25/2022 19:43:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=91
05/25/2022 19:43:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=92
05/25/2022 19:43:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=93
05/25/2022 19:43:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=93
05/25/2022 19:43:20 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7820080794648615 on epoch=93
05/25/2022 19:43:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
05/25/2022 19:43:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=94
05/25/2022 19:43:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=95
05/25/2022 19:43:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
05/25/2022 19:43:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=96
05/25/2022 19:43:36 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.7893926855954417 on epoch=96
05/25/2022 19:43:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=97
05/25/2022 19:43:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=98
05/25/2022 19:43:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=98
05/25/2022 19:43:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=99
05/25/2022 19:43:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/25/2022 19:43:52 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.785503018108652 on epoch=99
05/25/2022 19:43:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=100
05/25/2022 19:43:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=101
05/25/2022 19:43:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=101
05/25/2022 19:44:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=102
05/25/2022 19:44:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=103
05/25/2022 19:44:08 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.7660786909669302 on epoch=103
05/25/2022 19:44:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=103
05/25/2022 19:44:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=104
05/25/2022 19:44:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=104
05/25/2022 19:44:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=105
05/25/2022 19:44:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=106
05/25/2022 19:44:24 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.7707685721469644 on epoch=106
05/25/2022 19:44:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=106
05/25/2022 19:44:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=107
05/25/2022 19:44:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/25/2022 19:44:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=108
05/25/2022 19:44:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=109
05/25/2022 19:44:39 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.7825520833333334 on epoch=109
05/25/2022 19:44:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=109
05/25/2022 19:44:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.06 on epoch=110
05/25/2022 19:44:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/25/2022 19:44:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=111
05/25/2022 19:44:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=112
05/25/2022 19:44:55 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7754937213644293 on epoch=112
05/25/2022 19:44:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
05/25/2022 19:45:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=113
05/25/2022 19:45:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/25/2022 19:45:05 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=114
05/25/2022 19:45:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=115
05/25/2022 19:45:11 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.7695776445776445 on epoch=115
05/25/2022 19:45:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=116
05/25/2022 19:45:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
05/25/2022 19:45:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=117
05/25/2022 19:45:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=118
05/25/2022 19:45:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=118
05/25/2022 19:45:27 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7624291303268499 on epoch=118
05/25/2022 19:45:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=119
05/25/2022 19:45:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=119
05/25/2022 19:45:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=120
05/25/2022 19:45:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=121
05/25/2022 19:45:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/25/2022 19:45:43 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7796402736256556 on epoch=121
05/25/2022 19:45:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=122
05/25/2022 19:45:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=123
05/25/2022 19:45:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=123
05/25/2022 19:45:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/25/2022 19:45:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=124
05/25/2022 19:45:59 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.7814807988930726 on epoch=124
05/25/2022 19:46:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=125
05/25/2022 19:46:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=126
05/25/2022 19:46:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.07 on epoch=126
05/25/2022 19:46:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=127
05/25/2022 19:46:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.07 on epoch=128
05/25/2022 19:46:14 - INFO - __main__ - Global step 2050 Train loss 0.09 Classification-F1 0.7775203133362525 on epoch=128
05/25/2022 19:46:17 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=128
05/25/2022 19:46:19 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=129
05/25/2022 19:46:22 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=129
05/25/2022 19:46:24 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.05 on epoch=130
05/25/2022 19:46:27 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=131
05/25/2022 19:46:30 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7692287785898626 on epoch=131
05/25/2022 19:46:33 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=131
05/25/2022 19:46:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=132
05/25/2022 19:46:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=133
05/25/2022 19:46:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=133
05/25/2022 19:46:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=134
05/25/2022 19:46:46 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.7663072423458945 on epoch=134
05/25/2022 19:46:49 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
05/25/2022 19:46:52 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=135
05/25/2022 19:46:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=136
05/25/2022 19:46:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=136
05/25/2022 19:46:59 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=137
05/25/2022 19:47:02 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.7904739986761528 on epoch=137
05/25/2022 19:47:05 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
05/25/2022 19:47:07 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
05/25/2022 19:47:10 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=139
05/25/2022 19:47:12 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=139
05/25/2022 19:47:15 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/25/2022 19:47:18 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7980824949328886 on epoch=140
05/25/2022 19:47:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7969829421555041 -> 0.7980824949328886 on epoch=140, global_step=2250
05/25/2022 19:47:21 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=141
05/25/2022 19:47:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/25/2022 19:47:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=142
05/25/2022 19:47:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
05/25/2022 19:47:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=143
05/25/2022 19:47:34 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7754475261294254 on epoch=143
05/25/2022 19:47:37 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=144
05/25/2022 19:47:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
05/25/2022 19:47:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=145
05/25/2022 19:47:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/25/2022 19:47:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/25/2022 19:47:50 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.8090579407396413 on epoch=146
05/25/2022 19:47:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7980824949328886 -> 0.8090579407396413 on epoch=146, global_step=2350
05/25/2022 19:47:53 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
05/25/2022 19:47:55 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/25/2022 19:47:58 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=148
05/25/2022 19:48:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/25/2022 19:48:03 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=149
05/25/2022 19:48:06 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.8145379774418714 on epoch=149
05/25/2022 19:48:06 - INFO - __main__ - Saving model with best Classification-F1: 0.8090579407396413 -> 0.8145379774418714 on epoch=149, global_step=2400
05/25/2022 19:48:09 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.15 on epoch=150
05/25/2022 19:48:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=151
05/25/2022 19:48:14 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=151
05/25/2022 19:48:16 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
05/25/2022 19:48:19 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=153
05/25/2022 19:48:22 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.8197074255290293 on epoch=153
05/25/2022 19:48:22 - INFO - __main__ - Saving model with best Classification-F1: 0.8145379774418714 -> 0.8197074255290293 on epoch=153, global_step=2450
05/25/2022 19:48:24 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/25/2022 19:48:27 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/25/2022 19:48:29 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
05/25/2022 19:48:32 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=155
05/25/2022 19:48:34 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/25/2022 19:48:38 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7950937781409658 on epoch=156
05/25/2022 19:48:40 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/25/2022 19:48:43 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/25/2022 19:48:46 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/25/2022 19:48:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
05/25/2022 19:48:51 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=159
05/25/2022 19:48:54 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.8009196847753186 on epoch=159
05/25/2022 19:48:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/25/2022 19:48:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=160
05/25/2022 19:49:02 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=161
05/25/2022 19:49:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
05/25/2022 19:49:07 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=162
05/25/2022 19:49:10 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.8230173862648508 on epoch=162
05/25/2022 19:49:10 - INFO - __main__ - Saving model with best Classification-F1: 0.8197074255290293 -> 0.8230173862648508 on epoch=162, global_step=2600
05/25/2022 19:49:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
05/25/2022 19:49:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/25/2022 19:49:17 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/25/2022 19:49:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=164
05/25/2022 19:49:22 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=165
05/25/2022 19:49:26 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.8126068376068376 on epoch=165
05/25/2022 19:49:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=166
05/25/2022 19:49:31 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
05/25/2022 19:49:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/25/2022 19:49:36 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=168
05/25/2022 19:49:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=168
05/25/2022 19:49:42 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.8204397412216046 on epoch=168
05/25/2022 19:49:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=169
05/25/2022 19:49:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=169
05/25/2022 19:49:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.10 on epoch=170
05/25/2022 19:49:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/25/2022 19:49:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/25/2022 19:49:58 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7966467232584173 on epoch=171
05/25/2022 19:50:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
05/25/2022 19:50:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=173
05/25/2022 19:50:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/25/2022 19:50:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=174
05/25/2022 19:50:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=174
05/25/2022 19:50:13 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7777813284623805 on epoch=174
05/25/2022 19:50:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/25/2022 19:50:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/25/2022 19:50:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/25/2022 19:50:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
05/25/2022 19:50:25 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/25/2022 19:50:29 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.810953548085901 on epoch=178
05/25/2022 19:50:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/25/2022 19:50:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
05/25/2022 19:50:36 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=179
05/25/2022 19:50:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
05/25/2022 19:50:41 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/25/2022 19:50:44 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.8154644995813093 on epoch=181
05/25/2022 19:50:47 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=181
05/25/2022 19:50:49 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=182
05/25/2022 19:50:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/25/2022 19:50:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=183
05/25/2022 19:50:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/25/2022 19:51:00 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.8153292272243885 on epoch=184
05/25/2022 19:51:03 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
05/25/2022 19:51:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/25/2022 19:51:07 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/25/2022 19:51:10 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/25/2022 19:51:12 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
05/25/2022 19:51:14 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:51:14 - INFO - __main__ - Printing 3 examples
05/25/2022 19:51:14 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:51:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:51:14 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:51:14 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:51:14 - INFO - __main__ - Printing 3 examples
05/25/2022 19:51:14 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 19:51:14 - INFO - __main__ - ['others']
05/25/2022 19:51:14 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:51:14 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:51:14 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:51:16 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7891241552212777 on epoch=187
05/25/2022 19:51:16 - INFO - __main__ - save last model!
05/25/2022 19:51:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 19:51:16 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 19:51:16 - INFO - __main__ - Printing 3 examples
05/25/2022 19:51:16 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 19:51:16 - INFO - __main__ - ['others']
05/25/2022 19:51:16 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 19:51:16 - INFO - __main__ - ['others']
05/25/2022 19:51:16 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 19:51:16 - INFO - __main__ - ['others']
05/25/2022 19:51:16 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:51:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:51:23 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 19:51:33 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:51:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:51:34 - INFO - __main__ - Starting training!
05/25/2022 19:52:39 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_13_0.5_8_predictions.txt
05/25/2022 19:52:39 - INFO - __main__ - Classification-F1 on test data: 0.2278
05/25/2022 19:52:40 - INFO - __main__ - prefix=emo_64_13, lr=0.5, bsz=8, dev_performance=0.8230173862648508, test_performance=0.2278479126043761
05/25/2022 19:52:40 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.4, bsz=8 ...
05/25/2022 19:52:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:52:41 - INFO - __main__ - Printing 3 examples
05/25/2022 19:52:41 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:52:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:52:41 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 19:52:41 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 19:52:41 - INFO - __main__ - Printing 3 examples
05/25/2022 19:52:41 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 19:52:41 - INFO - __main__ - ['others']
05/25/2022 19:52:41 - INFO - __main__ - Tokenizing Input ...
05/25/2022 19:52:41 - INFO - __main__ - Tokenizing Output ...
05/25/2022 19:52:41 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 19:53:00 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 19:53:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 19:53:01 - INFO - __main__ - Starting training!
05/25/2022 19:53:04 - INFO - __main__ - Step 10 Global step 10 Train loss 4.18 on epoch=0
05/25/2022 19:53:06 - INFO - __main__ - Step 20 Global step 20 Train loss 3.16 on epoch=1
05/25/2022 19:53:09 - INFO - __main__ - Step 30 Global step 30 Train loss 2.59 on epoch=1
05/25/2022 19:53:11 - INFO - __main__ - Step 40 Global step 40 Train loss 2.12 on epoch=2
05/25/2022 19:53:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.73 on epoch=3
05/25/2022 19:53:18 - INFO - __main__ - Global step 50 Train loss 2.76 Classification-F1 0.17450665276752234 on epoch=3
05/25/2022 19:53:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17450665276752234 on epoch=3, global_step=50
05/25/2022 19:53:20 - INFO - __main__ - Step 60 Global step 60 Train loss 1.46 on epoch=3
05/25/2022 19:53:23 - INFO - __main__ - Step 70 Global step 70 Train loss 1.30 on epoch=4
05/25/2022 19:53:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
05/25/2022 19:53:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=5
05/25/2022 19:53:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=6
05/25/2022 19:53:34 - INFO - __main__ - Global step 100 Train loss 1.11 Classification-F1 0.497460565572976 on epoch=6
05/25/2022 19:53:34 - INFO - __main__ - Saving model with best Classification-F1: 0.17450665276752234 -> 0.497460565572976 on epoch=6, global_step=100
05/25/2022 19:53:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=6
05/25/2022 19:53:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.94 on epoch=7
05/25/2022 19:53:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.63 on epoch=8
05/25/2022 19:53:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=8
05/25/2022 19:53:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.67 on epoch=9
05/25/2022 19:53:50 - INFO - __main__ - Global step 150 Train loss 0.79 Classification-F1 0.5235065686193506 on epoch=9
05/25/2022 19:53:50 - INFO - __main__ - Saving model with best Classification-F1: 0.497460565572976 -> 0.5235065686193506 on epoch=9, global_step=150
05/25/2022 19:53:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=9
05/25/2022 19:53:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=10
05/25/2022 19:53:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.58 on epoch=11
05/25/2022 19:54:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=11
05/25/2022 19:54:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=12
05/25/2022 19:54:06 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.6305003221946056 on epoch=12
05/25/2022 19:54:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5235065686193506 -> 0.6305003221946056 on epoch=12, global_step=200
05/25/2022 19:54:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=13
05/25/2022 19:54:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.63 on epoch=13
05/25/2022 19:54:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=14
05/25/2022 19:54:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=14
05/25/2022 19:54:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=15
05/25/2022 19:54:21 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.6135152812696417 on epoch=15
05/25/2022 19:54:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.70 on epoch=16
05/25/2022 19:54:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=16
05/25/2022 19:54:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=17
05/25/2022 19:54:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=18
05/25/2022 19:54:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=18
05/25/2022 19:54:37 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.6698020018836104 on epoch=18
05/25/2022 19:54:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6305003221946056 -> 0.6698020018836104 on epoch=18, global_step=300
05/25/2022 19:54:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=19
05/25/2022 19:54:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=19
05/25/2022 19:54:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=20
05/25/2022 19:54:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=21
05/25/2022 19:54:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
05/25/2022 19:54:53 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.6971864879046119 on epoch=21
05/25/2022 19:54:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6698020018836104 -> 0.6971864879046119 on epoch=21, global_step=350
05/25/2022 19:54:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=22
05/25/2022 19:54:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=23
05/25/2022 19:55:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=23
05/25/2022 19:55:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.58 on epoch=24
05/25/2022 19:55:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.45 on epoch=24
05/25/2022 19:55:09 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.711582084968891 on epoch=24
05/25/2022 19:55:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6971864879046119 -> 0.711582084968891 on epoch=24, global_step=400
05/25/2022 19:55:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/25/2022 19:55:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=26
05/25/2022 19:55:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=26
05/25/2022 19:55:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=27
05/25/2022 19:55:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=28
05/25/2022 19:55:25 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.7174864035335597 on epoch=28
05/25/2022 19:55:25 - INFO - __main__ - Saving model with best Classification-F1: 0.711582084968891 -> 0.7174864035335597 on epoch=28, global_step=450
05/25/2022 19:55:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
05/25/2022 19:55:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=29
05/25/2022 19:55:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=29
05/25/2022 19:55:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
05/25/2022 19:55:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=31
05/25/2022 19:55:41 - INFO - __main__ - Global step 500 Train loss 0.43 Classification-F1 0.7176074984444831 on epoch=31
05/25/2022 19:55:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7174864035335597 -> 0.7176074984444831 on epoch=31, global_step=500
05/25/2022 19:55:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
05/25/2022 19:55:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=32
05/25/2022 19:55:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=33
05/25/2022 19:55:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/25/2022 19:55:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
05/25/2022 19:55:57 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.733977954669444 on epoch=34
05/25/2022 19:55:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7176074984444831 -> 0.733977954669444 on epoch=34, global_step=550
05/25/2022 19:56:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=34
05/25/2022 19:56:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
05/25/2022 19:56:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=36
05/25/2022 19:56:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=36
05/25/2022 19:56:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=37
05/25/2022 19:56:13 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.7502149590634816 on epoch=37
05/25/2022 19:56:13 - INFO - __main__ - Saving model with best Classification-F1: 0.733977954669444 -> 0.7502149590634816 on epoch=37, global_step=600
05/25/2022 19:56:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=38
05/25/2022 19:56:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/25/2022 19:56:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=39
05/25/2022 19:56:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.35 on epoch=39
05/25/2022 19:56:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
05/25/2022 19:56:29 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.7462916417536596 on epoch=40
05/25/2022 19:56:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
05/25/2022 19:56:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=41
05/25/2022 19:56:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
05/25/2022 19:56:39 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=43
05/25/2022 19:56:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=43
05/25/2022 19:56:45 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.746774887782082 on epoch=43
05/25/2022 19:56:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=44
05/25/2022 19:56:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=44
05/25/2022 19:56:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.33 on epoch=45
05/25/2022 19:56:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=46
05/25/2022 19:56:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/25/2022 19:57:01 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.7476925001535989 on epoch=46
05/25/2022 19:57:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
05/25/2022 19:57:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/25/2022 19:57:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/25/2022 19:57:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/25/2022 19:57:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=49
05/25/2022 19:57:17 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.7594749098927563 on epoch=49
05/25/2022 19:57:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7502149590634816 -> 0.7594749098927563 on epoch=49, global_step=800
05/25/2022 19:57:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/25/2022 19:57:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=51
05/25/2022 19:57:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=51
05/25/2022 19:57:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=52
05/25/2022 19:57:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=53
05/25/2022 19:57:33 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.7603255440211961 on epoch=53
05/25/2022 19:57:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7594749098927563 -> 0.7603255440211961 on epoch=53, global_step=850
05/25/2022 19:57:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.32 on epoch=53
05/25/2022 19:57:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=54
05/25/2022 19:57:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=54
05/25/2022 19:57:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=55
05/25/2022 19:57:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=56
05/25/2022 19:57:49 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7878217608669604 on epoch=56
05/25/2022 19:57:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7603255440211961 -> 0.7878217608669604 on epoch=56, global_step=900
05/25/2022 19:57:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=56
05/25/2022 19:57:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=57
05/25/2022 19:57:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=58
05/25/2022 19:57:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
05/25/2022 19:58:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=59
05/25/2022 19:58:05 - INFO - __main__ - Global step 950 Train loss 0.27 Classification-F1 0.7715553933664663 on epoch=59
05/25/2022 19:58:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=59
05/25/2022 19:58:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
05/25/2022 19:58:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/25/2022 19:58:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=61
05/25/2022 19:58:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=62
05/25/2022 19:58:21 - INFO - __main__ - Global step 1000 Train loss 0.28 Classification-F1 0.7838044659307198 on epoch=62
05/25/2022 19:58:24 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/25/2022 19:58:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
05/25/2022 19:58:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=64
05/25/2022 19:58:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=64
05/25/2022 19:58:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=65
05/25/2022 19:58:37 - INFO - __main__ - Global step 1050 Train loss 0.23 Classification-F1 0.7779452988367679 on epoch=65
05/25/2022 19:58:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=66
05/25/2022 19:58:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=66
05/25/2022 19:58:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=67
05/25/2022 19:58:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=68
05/25/2022 19:58:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=68
05/25/2022 19:58:53 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.7699440217108431 on epoch=68
05/25/2022 19:58:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=69
05/25/2022 19:58:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/25/2022 19:59:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=70
05/25/2022 19:59:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=71
05/25/2022 19:59:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
05/25/2022 19:59:09 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.7798695511917978 on epoch=71
05/25/2022 19:59:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=72
05/25/2022 19:59:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=73
05/25/2022 19:59:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=73
05/25/2022 19:59:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=74
05/25/2022 19:59:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=74
05/25/2022 19:59:25 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.7786993591720639 on epoch=74
05/25/2022 19:59:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=75
05/25/2022 19:59:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
05/25/2022 19:59:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/25/2022 19:59:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=77
05/25/2022 19:59:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=78
05/25/2022 19:59:41 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.7754776729660585 on epoch=78
05/25/2022 19:59:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=78
05/25/2022 19:59:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=79
05/25/2022 19:59:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=79
05/25/2022 19:59:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=80
05/25/2022 19:59:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
05/25/2022 19:59:57 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.7923044962056126 on epoch=81
05/25/2022 19:59:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7878217608669604 -> 0.7923044962056126 on epoch=81, global_step=1300
05/25/2022 19:59:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=81
05/25/2022 20:00:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/25/2022 20:00:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
05/25/2022 20:00:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=83
05/25/2022 20:00:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=84
05/25/2022 20:00:13 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.7865319589646005 on epoch=84
05/25/2022 20:00:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/25/2022 20:00:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=85
05/25/2022 20:00:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
05/25/2022 20:00:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=86
05/25/2022 20:00:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=87
05/25/2022 20:00:29 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.7812560147601876 on epoch=87
05/25/2022 20:00:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/25/2022 20:00:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/25/2022 20:00:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
05/25/2022 20:00:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=89
05/25/2022 20:00:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=90
05/25/2022 20:00:45 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.785435303241469 on epoch=90
05/25/2022 20:00:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=91
05/25/2022 20:00:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=91
05/25/2022 20:00:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/25/2022 20:00:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=93
05/25/2022 20:00:57 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/25/2022 20:01:00 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.7931381722142592 on epoch=93
05/25/2022 20:01:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7923044962056126 -> 0.7931381722142592 on epoch=93, global_step=1500
05/25/2022 20:01:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
05/25/2022 20:01:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/25/2022 20:01:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=95
05/25/2022 20:01:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
05/25/2022 20:01:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
05/25/2022 20:01:16 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.7914376862616477 on epoch=96
05/25/2022 20:01:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=97
05/25/2022 20:01:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=98
05/25/2022 20:01:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=98
05/25/2022 20:01:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
05/25/2022 20:01:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
05/25/2022 20:01:32 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7783287642589969 on epoch=99
05/25/2022 20:01:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=100
05/25/2022 20:01:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/25/2022 20:01:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/25/2022 20:01:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
05/25/2022 20:01:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=103
05/25/2022 20:01:47 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.7785027360027531 on epoch=103
05/25/2022 20:01:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
05/25/2022 20:01:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=104
05/25/2022 20:01:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=104
05/25/2022 20:01:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=105
05/25/2022 20:02:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=106
05/25/2022 20:02:03 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.7719623296895692 on epoch=106
05/25/2022 20:02:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=106
05/25/2022 20:02:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=107
05/25/2022 20:02:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=108
05/25/2022 20:02:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=108
05/25/2022 20:02:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
05/25/2022 20:02:19 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.7891162766683686 on epoch=109
05/25/2022 20:02:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=109
05/25/2022 20:02:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=110
05/25/2022 20:02:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/25/2022 20:02:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=111
05/25/2022 20:02:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=112
05/25/2022 20:02:35 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.7852481145529397 on epoch=112
05/25/2022 20:02:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=113
05/25/2022 20:02:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/25/2022 20:02:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/25/2022 20:02:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=114
05/25/2022 20:02:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
05/25/2022 20:02:51 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.7847501791491968 on epoch=115
05/25/2022 20:02:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/25/2022 20:02:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=116
05/25/2022 20:02:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=117
05/25/2022 20:03:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=118
05/25/2022 20:03:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=118
05/25/2022 20:03:06 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.7914463937269639 on epoch=118
05/25/2022 20:03:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/25/2022 20:03:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=119
05/25/2022 20:03:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=120
05/25/2022 20:03:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=121
05/25/2022 20:03:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=121
05/25/2022 20:03:22 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.8037236986571464 on epoch=121
05/25/2022 20:03:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7931381722142592 -> 0.8037236986571464 on epoch=121, global_step=1950
05/25/2022 20:03:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/25/2022 20:03:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/25/2022 20:03:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=123
05/25/2022 20:03:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/25/2022 20:03:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/25/2022 20:03:38 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.7793044315722054 on epoch=124
05/25/2022 20:03:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/25/2022 20:03:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=126
05/25/2022 20:03:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=126
05/25/2022 20:03:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=127
05/25/2022 20:03:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
05/25/2022 20:03:53 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.798813653538063 on epoch=128
05/25/2022 20:03:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
05/25/2022 20:03:58 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=129
05/25/2022 20:04:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=129
05/25/2022 20:04:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=130
05/25/2022 20:04:05 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/25/2022 20:04:09 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.7736330645738312 on epoch=131
05/25/2022 20:04:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=131
05/25/2022 20:04:14 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=132
05/25/2022 20:04:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=133
05/25/2022 20:04:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
05/25/2022 20:04:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=134
05/25/2022 20:04:25 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.7867439214822591 on epoch=134
05/25/2022 20:04:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=134
05/25/2022 20:04:30 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=135
05/25/2022 20:04:32 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
05/25/2022 20:04:34 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=136
05/25/2022 20:04:37 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/25/2022 20:04:41 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.7802186960148304 on epoch=137
05/25/2022 20:04:43 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
05/25/2022 20:04:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=138
05/25/2022 20:04:48 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=139
05/25/2022 20:04:51 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
05/25/2022 20:04:53 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=140
05/25/2022 20:04:57 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.783623746098391 on epoch=140
05/25/2022 20:04:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=141
05/25/2022 20:05:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
05/25/2022 20:05:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.05 on epoch=142
05/25/2022 20:05:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.03 on epoch=143
05/25/2022 20:05:09 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.11 on epoch=143
05/25/2022 20:05:12 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.7880068688617433 on epoch=143
05/25/2022 20:05:15 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=144
05/25/2022 20:05:17 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/25/2022 20:05:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=145
05/25/2022 20:05:22 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
05/25/2022 20:05:25 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=146
05/25/2022 20:05:28 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.7813711362793677 on epoch=146
05/25/2022 20:05:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
05/25/2022 20:05:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/25/2022 20:05:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=148
05/25/2022 20:05:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=149
05/25/2022 20:05:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
05/25/2022 20:05:44 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.7936367686367686 on epoch=149
05/25/2022 20:05:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
05/25/2022 20:05:49 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=151
05/25/2022 20:05:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=151
05/25/2022 20:05:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=152
05/25/2022 20:05:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
05/25/2022 20:06:00 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.7787774058107516 on epoch=153
05/25/2022 20:06:02 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/25/2022 20:06:05 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
05/25/2022 20:06:07 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
05/25/2022 20:06:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/25/2022 20:06:12 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.15 on epoch=156
05/25/2022 20:06:16 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.7952835295884642 on epoch=156
05/25/2022 20:06:18 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=156
05/25/2022 20:06:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 20:06:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/25/2022 20:06:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
05/25/2022 20:06:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
05/25/2022 20:06:32 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.7966842111116673 on epoch=159
05/25/2022 20:06:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
05/25/2022 20:06:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=160
05/25/2022 20:06:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/25/2022 20:06:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=161
05/25/2022 20:06:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/25/2022 20:06:47 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.8051864729414624 on epoch=162
05/25/2022 20:06:47 - INFO - __main__ - Saving model with best Classification-F1: 0.8037236986571464 -> 0.8051864729414624 on epoch=162, global_step=2600
05/25/2022 20:06:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
05/25/2022 20:06:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
05/25/2022 20:06:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/25/2022 20:06:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=164
05/25/2022 20:06:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=165
05/25/2022 20:07:03 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.7927723350306415 on epoch=165
05/25/2022 20:07:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=166
05/25/2022 20:07:08 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
05/25/2022 20:07:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.11 on epoch=167
05/25/2022 20:07:13 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/25/2022 20:07:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/25/2022 20:07:19 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.8049544817927171 on epoch=168
05/25/2022 20:07:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=169
05/25/2022 20:07:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/25/2022 20:07:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/25/2022 20:07:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
05/25/2022 20:07:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/25/2022 20:07:34 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7843477393514 on epoch=171
05/25/2022 20:07:37 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/25/2022 20:07:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=173
05/25/2022 20:07:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/25/2022 20:07:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=174
05/25/2022 20:07:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=174
05/25/2022 20:07:50 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.7965734876232657 on epoch=174
05/25/2022 20:07:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
05/25/2022 20:07:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/25/2022 20:07:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
05/25/2022 20:08:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/25/2022 20:08:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/25/2022 20:08:05 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.7978027320781131 on epoch=178
05/25/2022 20:08:08 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
05/25/2022 20:08:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
05/25/2022 20:08:13 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
05/25/2022 20:08:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
05/25/2022 20:08:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
05/25/2022 20:08:21 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.8062602690212077 on epoch=181
05/25/2022 20:08:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8051864729414624 -> 0.8062602690212077 on epoch=181, global_step=2900
05/25/2022 20:08:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
05/25/2022 20:08:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
05/25/2022 20:08:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/25/2022 20:08:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
05/25/2022 20:08:33 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=184
05/25/2022 20:08:37 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7888400187661272 on epoch=184
05/25/2022 20:08:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/25/2022 20:08:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=185
05/25/2022 20:08:44 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/25/2022 20:08:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/25/2022 20:08:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
05/25/2022 20:08:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:08:50 - INFO - __main__ - Printing 3 examples
05/25/2022 20:08:50 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 20:08:50 - INFO - __main__ - ['others']
05/25/2022 20:08:50 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 20:08:50 - INFO - __main__ - ['others']
05/25/2022 20:08:50 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 20:08:50 - INFO - __main__ - ['others']
05/25/2022 20:08:50 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:08:50 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:08:51 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:08:51 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:08:51 - INFO - __main__ - Printing 3 examples
05/25/2022 20:08:51 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 20:08:51 - INFO - __main__ - ['others']
05/25/2022 20:08:51 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 20:08:51 - INFO - __main__ - ['others']
05/25/2022 20:08:51 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 20:08:51 - INFO - __main__ - ['others']
05/25/2022 20:08:51 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:08:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:08:51 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:08:52 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.794070364110382 on epoch=187
05/25/2022 20:08:52 - INFO - __main__ - save last model!
05/25/2022 20:08:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 20:08:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 20:08:52 - INFO - __main__ - Printing 3 examples
05/25/2022 20:08:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 20:08:52 - INFO - __main__ - ['others']
05/25/2022 20:08:53 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 20:08:53 - INFO - __main__ - ['others']
05/25/2022 20:08:53 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 20:08:53 - INFO - __main__ - ['others']
05/25/2022 20:08:53 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:08:55 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:09:00 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 20:09:07 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:09:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:09:08 - INFO - __main__ - Starting training!
05/25/2022 20:10:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_13_0.4_8_predictions.txt
05/25/2022 20:10:16 - INFO - __main__ - Classification-F1 on test data: 0.3866
05/25/2022 20:10:17 - INFO - __main__ - prefix=emo_64_13, lr=0.4, bsz=8, dev_performance=0.8062602690212077, test_performance=0.3865656858651304
05/25/2022 20:10:17 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.3, bsz=8 ...
05/25/2022 20:10:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:10:18 - INFO - __main__ - Printing 3 examples
05/25/2022 20:10:18 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:10:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:10:18 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:10:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:10:18 - INFO - __main__ - Printing 3 examples
05/25/2022 20:10:18 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 20:10:18 - INFO - __main__ - ['others']
05/25/2022 20:10:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:10:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:10:19 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:10:36 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:10:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:10:37 - INFO - __main__ - Starting training!
05/25/2022 20:10:40 - INFO - __main__ - Step 10 Global step 10 Train loss 4.23 on epoch=0
05/25/2022 20:10:42 - INFO - __main__ - Step 20 Global step 20 Train loss 2.94 on epoch=1
05/25/2022 20:10:45 - INFO - __main__ - Step 30 Global step 30 Train loss 2.79 on epoch=1
05/25/2022 20:10:47 - INFO - __main__ - Step 40 Global step 40 Train loss 2.36 on epoch=2
05/25/2022 20:10:50 - INFO - __main__ - Step 50 Global step 50 Train loss 2.10 on epoch=3
05/25/2022 20:10:54 - INFO - __main__ - Global step 50 Train loss 2.88 Classification-F1 0.05885292243987896 on epoch=3
05/25/2022 20:10:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.05885292243987896 on epoch=3, global_step=50
05/25/2022 20:10:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.00 on epoch=3
05/25/2022 20:10:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.58 on epoch=4
05/25/2022 20:11:01 - INFO - __main__ - Step 80 Global step 80 Train loss 1.36 on epoch=4
05/25/2022 20:11:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.21 on epoch=5
05/25/2022 20:11:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.06 on epoch=6
05/25/2022 20:11:09 - INFO - __main__ - Global step 100 Train loss 1.44 Classification-F1 0.38977713094947286 on epoch=6
05/25/2022 20:11:10 - INFO - __main__ - Saving model with best Classification-F1: 0.05885292243987896 -> 0.38977713094947286 on epoch=6, global_step=100
05/25/2022 20:11:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=6
05/25/2022 20:11:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.99 on epoch=7
05/25/2022 20:11:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=8
05/25/2022 20:11:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.77 on epoch=8
05/25/2022 20:11:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/25/2022 20:11:25 - INFO - __main__ - Global step 150 Train loss 0.89 Classification-F1 0.5503157972995978 on epoch=9
05/25/2022 20:11:25 - INFO - __main__ - Saving model with best Classification-F1: 0.38977713094947286 -> 0.5503157972995978 on epoch=9, global_step=150
05/25/2022 20:11:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.93 on epoch=9
05/25/2022 20:11:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=10
05/25/2022 20:11:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=11
05/25/2022 20:11:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=11
05/25/2022 20:11:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=12
05/25/2022 20:11:41 - INFO - __main__ - Global step 200 Train loss 0.79 Classification-F1 0.6342699780140889 on epoch=12
05/25/2022 20:11:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5503157972995978 -> 0.6342699780140889 on epoch=12, global_step=200
05/25/2022 20:11:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.65 on epoch=13
05/25/2022 20:11:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.73 on epoch=13
05/25/2022 20:11:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=14
05/25/2022 20:11:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=14
05/25/2022 20:11:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=15
05/25/2022 20:11:57 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.5837618008967581 on epoch=15
05/25/2022 20:12:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=16
05/25/2022 20:12:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=16
05/25/2022 20:12:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.72 on epoch=17
05/25/2022 20:12:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=18
05/25/2022 20:12:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.69 on epoch=18
05/25/2022 20:12:13 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.5977878216388683 on epoch=18
05/25/2022 20:12:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=19
05/25/2022 20:12:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=19
05/25/2022 20:12:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.63 on epoch=20
05/25/2022 20:12:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=21
05/25/2022 20:12:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=21
05/25/2022 20:12:29 - INFO - __main__ - Global step 350 Train loss 0.59 Classification-F1 0.6102907149815977 on epoch=21
05/25/2022 20:12:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=22
05/25/2022 20:12:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=23
05/25/2022 20:12:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=23
05/25/2022 20:12:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=24
05/25/2022 20:12:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.48 on epoch=24
05/25/2022 20:12:44 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.6769482886039488 on epoch=24
05/25/2022 20:12:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6342699780140889 -> 0.6769482886039488 on epoch=24, global_step=400
05/25/2022 20:12:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.51 on epoch=25
05/25/2022 20:12:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=26
05/25/2022 20:12:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=26
05/25/2022 20:12:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.65 on epoch=27
05/25/2022 20:12:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=28
05/25/2022 20:13:00 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.6697027143026407 on epoch=28
05/25/2022 20:13:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.61 on epoch=28
05/25/2022 20:13:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=29
05/25/2022 20:13:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=29
05/25/2022 20:13:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=30
05/25/2022 20:13:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=31
05/25/2022 20:13:16 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.7175832208726945 on epoch=31
05/25/2022 20:13:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6769482886039488 -> 0.7175832208726945 on epoch=31, global_step=500
05/25/2022 20:13:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=31
05/25/2022 20:13:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=32
05/25/2022 20:13:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=33
05/25/2022 20:13:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
05/25/2022 20:13:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.59 on epoch=34
05/25/2022 20:13:32 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.7106381349802402 on epoch=34
05/25/2022 20:13:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=34
05/25/2022 20:13:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=35
05/25/2022 20:13:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=36
05/25/2022 20:13:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=36
05/25/2022 20:13:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=37
05/25/2022 20:13:47 - INFO - __main__ - Global step 600 Train loss 0.42 Classification-F1 0.712154094542317 on epoch=37
05/25/2022 20:13:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=38
05/25/2022 20:13:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=38
05/25/2022 20:13:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=39
05/25/2022 20:13:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
05/25/2022 20:14:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=40
05/25/2022 20:14:03 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.7190832577266287 on epoch=40
05/25/2022 20:14:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7175832208726945 -> 0.7190832577266287 on epoch=40, global_step=650
05/25/2022 20:14:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=41
05/25/2022 20:14:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=41
05/25/2022 20:14:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.57 on epoch=42
05/25/2022 20:14:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
05/25/2022 20:14:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/25/2022 20:14:19 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.7213219340004533 on epoch=43
05/25/2022 20:14:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7190832577266287 -> 0.7213219340004533 on epoch=43, global_step=700
05/25/2022 20:14:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=44
05/25/2022 20:14:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
05/25/2022 20:14:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.42 on epoch=45
05/25/2022 20:14:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=46
05/25/2022 20:14:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/25/2022 20:14:35 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.7173450001781934 on epoch=46
05/25/2022 20:14:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=47
05/25/2022 20:14:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=48
05/25/2022 20:14:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=48
05/25/2022 20:14:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=49
05/25/2022 20:14:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
05/25/2022 20:14:51 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.7454939819750095 on epoch=49
05/25/2022 20:14:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7213219340004533 -> 0.7454939819750095 on epoch=49, global_step=800
05/25/2022 20:14:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
05/25/2022 20:14:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=51
05/25/2022 20:14:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
05/25/2022 20:15:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=52
05/25/2022 20:15:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=53
05/25/2022 20:15:06 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.7406589183883395 on epoch=53
05/25/2022 20:15:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=53
05/25/2022 20:15:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
05/25/2022 20:15:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=54
05/25/2022 20:15:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=55
05/25/2022 20:15:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=56
05/25/2022 20:15:22 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.7420750558287831 on epoch=56
05/25/2022 20:15:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=56
05/25/2022 20:15:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
05/25/2022 20:15:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
05/25/2022 20:15:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=58
05/25/2022 20:15:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
05/25/2022 20:15:38 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.7429178775586742 on epoch=59
05/25/2022 20:15:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=59
05/25/2022 20:15:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
05/25/2022 20:15:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=61
05/25/2022 20:15:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=61
05/25/2022 20:15:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=62
05/25/2022 20:15:54 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.754177907438777 on epoch=62
05/25/2022 20:15:54 - INFO - __main__ - Saving model with best Classification-F1: 0.7454939819750095 -> 0.754177907438777 on epoch=62, global_step=1000
05/25/2022 20:15:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
05/25/2022 20:15:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=63
05/25/2022 20:16:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=64
05/25/2022 20:16:04 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=64
05/25/2022 20:16:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
05/25/2022 20:16:10 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.7582242922134098 on epoch=65
05/25/2022 20:16:10 - INFO - __main__ - Saving model with best Classification-F1: 0.754177907438777 -> 0.7582242922134098 on epoch=65, global_step=1050
05/25/2022 20:16:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=66
05/25/2022 20:16:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/25/2022 20:16:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.38 on epoch=67
05/25/2022 20:16:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
05/25/2022 20:16:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.28 on epoch=68
05/25/2022 20:16:26 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.7641634489746116 on epoch=68
05/25/2022 20:16:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7582242922134098 -> 0.7641634489746116 on epoch=68, global_step=1100
05/25/2022 20:16:28 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
05/25/2022 20:16:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
05/25/2022 20:16:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=70
05/25/2022 20:16:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=71
05/25/2022 20:16:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
05/25/2022 20:16:41 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.7653767711464147 on epoch=71
05/25/2022 20:16:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7641634489746116 -> 0.7653767711464147 on epoch=71, global_step=1150
05/25/2022 20:16:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
05/25/2022 20:16:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=73
05/25/2022 20:16:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/25/2022 20:16:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=74
05/25/2022 20:16:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=74
05/25/2022 20:16:57 - INFO - __main__ - Global step 1200 Train loss 0.34 Classification-F1 0.7688756994224719 on epoch=74
05/25/2022 20:16:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7653767711464147 -> 0.7688756994224719 on epoch=74, global_step=1200
05/25/2022 20:17:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=75
05/25/2022 20:17:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=76
05/25/2022 20:17:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
05/25/2022 20:17:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=77
05/25/2022 20:17:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=78
05/25/2022 20:17:13 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.7715258330693774 on epoch=78
05/25/2022 20:17:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7688756994224719 -> 0.7715258330693774 on epoch=78, global_step=1250
05/25/2022 20:17:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=78
05/25/2022 20:17:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=79
05/25/2022 20:17:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=79
05/25/2022 20:17:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
05/25/2022 20:17:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=81
05/25/2022 20:17:29 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.7748742309666679 on epoch=81
05/25/2022 20:17:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7715258330693774 -> 0.7748742309666679 on epoch=81, global_step=1300
05/25/2022 20:17:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=81
05/25/2022 20:17:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=82
05/25/2022 20:17:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
05/25/2022 20:17:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.33 on epoch=83
05/25/2022 20:17:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=84
05/25/2022 20:17:45 - INFO - __main__ - Global step 1350 Train loss 0.27 Classification-F1 0.7883016341147793 on epoch=84
05/25/2022 20:17:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7748742309666679 -> 0.7883016341147793 on epoch=84, global_step=1350
05/25/2022 20:17:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/25/2022 20:17:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
05/25/2022 20:17:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=86
05/25/2022 20:17:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
05/25/2022 20:17:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=87
05/25/2022 20:18:00 - INFO - __main__ - Global step 1400 Train loss 0.26 Classification-F1 0.7861978753765657 on epoch=87
05/25/2022 20:18:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
05/25/2022 20:18:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
05/25/2022 20:18:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=89
05/25/2022 20:18:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
05/25/2022 20:18:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=90
05/25/2022 20:18:16 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7830250031736835 on epoch=90
05/25/2022 20:18:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=91
05/25/2022 20:18:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/25/2022 20:18:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.27 on epoch=92
05/25/2022 20:18:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
05/25/2022 20:18:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/25/2022 20:18:31 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.7815127891832913 on epoch=93
05/25/2022 20:18:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=94
05/25/2022 20:18:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=94
05/25/2022 20:18:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
05/25/2022 20:18:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=96
05/25/2022 20:18:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=96
05/25/2022 20:18:47 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.7794609521564163 on epoch=96
05/25/2022 20:18:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=97
05/25/2022 20:18:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=98
05/25/2022 20:18:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=98
05/25/2022 20:18:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/25/2022 20:18:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=99
05/25/2022 20:19:02 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.7832757520257521 on epoch=99
05/25/2022 20:19:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
05/25/2022 20:19:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=101
05/25/2022 20:19:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=101
05/25/2022 20:19:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
05/25/2022 20:19:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=103
05/25/2022 20:19:18 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.7871231968469857 on epoch=103
05/25/2022 20:19:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.16 on epoch=103
05/25/2022 20:19:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/25/2022 20:19:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
05/25/2022 20:19:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=105
05/25/2022 20:19:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.29 on epoch=106
05/25/2022 20:19:35 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.7944679991924086 on epoch=106
05/25/2022 20:19:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7883016341147793 -> 0.7944679991924086 on epoch=106, global_step=1700
05/25/2022 20:19:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=106
05/25/2022 20:19:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=107
05/25/2022 20:19:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
05/25/2022 20:19:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=108
05/25/2022 20:19:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=109
05/25/2022 20:19:51 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.807141884578766 on epoch=109
05/25/2022 20:19:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7944679991924086 -> 0.807141884578766 on epoch=109, global_step=1750
05/25/2022 20:19:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
05/25/2022 20:19:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
05/25/2022 20:19:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=111
05/25/2022 20:20:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/25/2022 20:20:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=112
05/25/2022 20:20:07 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.804023898661903 on epoch=112
05/25/2022 20:20:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=113
05/25/2022 20:20:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
05/25/2022 20:20:14 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/25/2022 20:20:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=114
05/25/2022 20:20:19 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/25/2022 20:20:23 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.8040480742096612 on epoch=115
05/25/2022 20:20:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=116
05/25/2022 20:20:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/25/2022 20:20:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
05/25/2022 20:20:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=118
05/25/2022 20:20:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
05/25/2022 20:20:39 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.7879002624368119 on epoch=118
05/25/2022 20:20:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=119
05/25/2022 20:20:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=119
05/25/2022 20:20:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=120
05/25/2022 20:20:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/25/2022 20:20:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=121
05/25/2022 20:20:55 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.7750545571694705 on epoch=121
05/25/2022 20:20:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=122
05/25/2022 20:21:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=123
05/25/2022 20:21:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=123
05/25/2022 20:21:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/25/2022 20:21:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=124
05/25/2022 20:21:11 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7998417169684774 on epoch=124
05/25/2022 20:21:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=125
05/25/2022 20:21:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
05/25/2022 20:21:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
05/25/2022 20:21:21 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=127
05/25/2022 20:21:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/25/2022 20:21:27 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.8003555298510685 on epoch=128
05/25/2022 20:21:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=128
05/25/2022 20:21:32 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
05/25/2022 20:21:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=129
05/25/2022 20:21:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=130
05/25/2022 20:21:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.27 on epoch=131
05/25/2022 20:21:43 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7894761575486792 on epoch=131
05/25/2022 20:21:45 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=131
05/25/2022 20:21:48 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/25/2022 20:21:50 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/25/2022 20:21:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/25/2022 20:21:55 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
05/25/2022 20:21:59 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.801117917104807 on epoch=134
05/25/2022 20:22:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/25/2022 20:22:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/25/2022 20:22:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=136
05/25/2022 20:22:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
05/25/2022 20:22:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=137
05/25/2022 20:22:15 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.8062654350570051 on epoch=137
05/25/2022 20:22:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.08 on epoch=138
05/25/2022 20:22:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=138
05/25/2022 20:22:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
05/25/2022 20:22:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/25/2022 20:22:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=140
05/25/2022 20:22:31 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.7996405080563649 on epoch=140
05/25/2022 20:22:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=141
05/25/2022 20:22:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
05/25/2022 20:22:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
05/25/2022 20:22:41 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
05/25/2022 20:22:44 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
05/25/2022 20:22:47 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.8097250709082572 on epoch=143
05/25/2022 20:22:47 - INFO - __main__ - Saving model with best Classification-F1: 0.807141884578766 -> 0.8097250709082572 on epoch=143, global_step=2300
05/25/2022 20:22:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=144
05/25/2022 20:22:52 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=144
05/25/2022 20:22:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/25/2022 20:22:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=146
05/25/2022 20:23:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/25/2022 20:23:03 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.799377419797324 on epoch=146
05/25/2022 20:23:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
05/25/2022 20:23:08 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
05/25/2022 20:23:11 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
05/25/2022 20:23:13 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
05/25/2022 20:23:16 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
05/25/2022 20:23:19 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.8069807374636538 on epoch=149
05/25/2022 20:23:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
05/25/2022 20:23:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
05/25/2022 20:23:27 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=151
05/25/2022 20:23:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/25/2022 20:23:32 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
05/25/2022 20:23:36 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7922164257925852 on epoch=153
05/25/2022 20:23:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.11 on epoch=153
05/25/2022 20:23:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/25/2022 20:23:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
05/25/2022 20:23:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=155
05/25/2022 20:23:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.18 on epoch=156
05/25/2022 20:23:52 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.7853337327475257 on epoch=156
05/25/2022 20:23:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
05/25/2022 20:23:57 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=157
05/25/2022 20:23:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
05/25/2022 20:24:02 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
05/25/2022 20:24:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/25/2022 20:24:08 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.8098994223214077 on epoch=159
05/25/2022 20:24:08 - INFO - __main__ - Saving model with best Classification-F1: 0.8097250709082572 -> 0.8098994223214077 on epoch=159, global_step=2550
05/25/2022 20:24:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
05/25/2022 20:24:13 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=160
05/25/2022 20:24:15 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=161
05/25/2022 20:24:18 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=161
05/25/2022 20:24:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
05/25/2022 20:24:24 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.7872996821052803 on epoch=162
05/25/2022 20:24:27 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
05/25/2022 20:24:29 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
05/25/2022 20:24:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
05/25/2022 20:24:34 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/25/2022 20:24:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/25/2022 20:24:40 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.8193325868009479 on epoch=165
05/25/2022 20:24:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8098994223214077 -> 0.8193325868009479 on epoch=165, global_step=2650
05/25/2022 20:24:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/25/2022 20:24:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
05/25/2022 20:24:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.17 on epoch=167
05/25/2022 20:24:50 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
05/25/2022 20:24:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/25/2022 20:24:56 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.8137820941649074 on epoch=168
05/25/2022 20:24:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
05/25/2022 20:25:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
05/25/2022 20:25:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=170
05/25/2022 20:25:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=171
05/25/2022 20:25:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
05/25/2022 20:25:13 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.8107453274728755 on epoch=171
05/25/2022 20:25:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.11 on epoch=172
05/25/2022 20:25:18 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=173
05/25/2022 20:25:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/25/2022 20:25:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/25/2022 20:25:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
05/25/2022 20:25:29 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.8052684903748735 on epoch=174
05/25/2022 20:25:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.17 on epoch=175
05/25/2022 20:25:34 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=176
05/25/2022 20:25:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
05/25/2022 20:25:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=177
05/25/2022 20:25:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/25/2022 20:25:45 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.8182803394985345 on epoch=178
05/25/2022 20:25:47 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=178
05/25/2022 20:25:50 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/25/2022 20:25:52 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
05/25/2022 20:25:55 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
05/25/2022 20:25:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
05/25/2022 20:26:01 - INFO - __main__ - Global step 2900 Train loss 0.10 Classification-F1 0.8103605193940571 on epoch=181
05/25/2022 20:26:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=181
05/25/2022 20:26:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
05/25/2022 20:26:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/25/2022 20:26:11 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
05/25/2022 20:26:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=184
05/25/2022 20:26:17 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.8009895393531656 on epoch=184
05/25/2022 20:26:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/25/2022 20:26:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
05/25/2022 20:26:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=186
05/25/2022 20:26:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/25/2022 20:26:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.10 on epoch=187
05/25/2022 20:26:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:26:31 - INFO - __main__ - Printing 3 examples
05/25/2022 20:26:31 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:26:31 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:26:31 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:26:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:26:31 - INFO - __main__ - Printing 3 examples
05/25/2022 20:26:31 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 20:26:31 - INFO - __main__ - ['others']
05/25/2022 20:26:31 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:26:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:26:32 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:26:33 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.8092555341641319 on epoch=187
05/25/2022 20:26:33 - INFO - __main__ - save last model!
05/25/2022 20:26:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 20:26:34 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 20:26:34 - INFO - __main__ - Printing 3 examples
05/25/2022 20:26:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 20:26:34 - INFO - __main__ - ['others']
05/25/2022 20:26:34 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 20:26:34 - INFO - __main__ - ['others']
05/25/2022 20:26:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 20:26:34 - INFO - __main__ - ['others']
05/25/2022 20:26:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:26:36 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:26:41 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 20:26:48 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:26:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:26:49 - INFO - __main__ - Starting training!
05/25/2022 20:27:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_13_0.3_8_predictions.txt
05/25/2022 20:27:57 - INFO - __main__ - Classification-F1 on test data: 0.5028
05/25/2022 20:27:57 - INFO - __main__ - prefix=emo_64_13, lr=0.3, bsz=8, dev_performance=0.8193325868009479, test_performance=0.5027621123729826
05/25/2022 20:27:57 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.2, bsz=8 ...
05/25/2022 20:27:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:27:58 - INFO - __main__ - Printing 3 examples
05/25/2022 20:27:58 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:27:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:27:58 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:27:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:27:58 - INFO - __main__ - Printing 3 examples
05/25/2022 20:27:58 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ -  [emo] why because you don't want to i want to
05/25/2022 20:27:58 - INFO - __main__ - ['others']
05/25/2022 20:27:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:27:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:27:59 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:28:14 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:28:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:28:15 - INFO - __main__ - Starting training!
05/25/2022 20:28:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.53 on epoch=0
05/25/2022 20:28:21 - INFO - __main__ - Step 20 Global step 20 Train loss 3.50 on epoch=1
05/25/2022 20:28:23 - INFO - __main__ - Step 30 Global step 30 Train loss 3.05 on epoch=1
05/25/2022 20:28:26 - INFO - __main__ - Step 40 Global step 40 Train loss 2.84 on epoch=2
05/25/2022 20:28:28 - INFO - __main__ - Step 50 Global step 50 Train loss 2.42 on epoch=3
05/25/2022 20:28:32 - INFO - __main__ - Global step 50 Train loss 3.27 Classification-F1 0.006590424500872262 on epoch=3
05/25/2022 20:28:32 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.006590424500872262 on epoch=3, global_step=50
05/25/2022 20:28:35 - INFO - __main__ - Step 60 Global step 60 Train loss 2.42 on epoch=3
05/25/2022 20:28:37 - INFO - __main__ - Step 70 Global step 70 Train loss 2.00 on epoch=4
05/25/2022 20:28:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.93 on epoch=4
05/25/2022 20:28:42 - INFO - __main__ - Step 90 Global step 90 Train loss 1.82 on epoch=5
05/25/2022 20:28:45 - INFO - __main__ - Step 100 Global step 100 Train loss 1.52 on epoch=6
05/25/2022 20:28:48 - INFO - __main__ - Global step 100 Train loss 1.94 Classification-F1 0.18795455320151097 on epoch=6
05/25/2022 20:28:48 - INFO - __main__ - Saving model with best Classification-F1: 0.006590424500872262 -> 0.18795455320151097 on epoch=6, global_step=100
05/25/2022 20:28:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.42 on epoch=6
05/25/2022 20:28:53 - INFO - __main__ - Step 120 Global step 120 Train loss 1.09 on epoch=7
05/25/2022 20:28:56 - INFO - __main__ - Step 130 Global step 130 Train loss 1.28 on epoch=8
05/25/2022 20:28:58 - INFO - __main__ - Step 140 Global step 140 Train loss 1.25 on epoch=8
05/25/2022 20:29:01 - INFO - __main__ - Step 150 Global step 150 Train loss 1.09 on epoch=9
05/25/2022 20:29:04 - INFO - __main__ - Global step 150 Train loss 1.22 Classification-F1 0.378007662835249 on epoch=9
05/25/2022 20:29:04 - INFO - __main__ - Saving model with best Classification-F1: 0.18795455320151097 -> 0.378007662835249 on epoch=9, global_step=150
05/25/2022 20:29:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=9
05/25/2022 20:29:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=10
05/25/2022 20:29:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.88 on epoch=11
05/25/2022 20:29:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=11
05/25/2022 20:29:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.94 on epoch=12
05/25/2022 20:29:20 - INFO - __main__ - Global step 200 Train loss 0.91 Classification-F1 0.515467330684722 on epoch=12
05/25/2022 20:29:20 - INFO - __main__ - Saving model with best Classification-F1: 0.378007662835249 -> 0.515467330684722 on epoch=12, global_step=200
05/25/2022 20:29:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.82 on epoch=13
05/25/2022 20:29:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.83 on epoch=13
05/25/2022 20:29:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=14
05/25/2022 20:29:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.72 on epoch=14
05/25/2022 20:29:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.88 on epoch=15
05/25/2022 20:29:36 - INFO - __main__ - Global step 250 Train loss 0.80 Classification-F1 0.5490790581415654 on epoch=15
05/25/2022 20:29:36 - INFO - __main__ - Saving model with best Classification-F1: 0.515467330684722 -> 0.5490790581415654 on epoch=15, global_step=250
05/25/2022 20:29:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=16
05/25/2022 20:29:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/25/2022 20:29:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=17
05/25/2022 20:29:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.76 on epoch=18
05/25/2022 20:29:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=18
05/25/2022 20:29:52 - INFO - __main__ - Global step 300 Train loss 0.74 Classification-F1 0.5498265583885404 on epoch=18
05/25/2022 20:29:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5490790581415654 -> 0.5498265583885404 on epoch=18, global_step=300
05/25/2022 20:29:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=19
05/25/2022 20:29:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.74 on epoch=19
05/25/2022 20:30:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=20
05/25/2022 20:30:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=21
05/25/2022 20:30:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.70 on epoch=21
05/25/2022 20:30:09 - INFO - __main__ - Global step 350 Train loss 0.75 Classification-F1 0.5488212801434882 on epoch=21
05/25/2022 20:30:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=22
05/25/2022 20:30:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.71 on epoch=23
05/25/2022 20:30:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=23
05/25/2022 20:30:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.71 on epoch=24
05/25/2022 20:30:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.63 on epoch=24
05/25/2022 20:30:25 - INFO - __main__ - Global step 400 Train loss 0.69 Classification-F1 0.6062067802327542 on epoch=24
05/25/2022 20:30:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5498265583885404 -> 0.6062067802327542 on epoch=24, global_step=400
05/25/2022 20:30:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=25
05/25/2022 20:30:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.69 on epoch=26
05/25/2022 20:30:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.60 on epoch=26
05/25/2022 20:30:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=27
05/25/2022 20:30:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=28
05/25/2022 20:30:40 - INFO - __main__ - Global step 450 Train loss 0.67 Classification-F1 0.618370548317336 on epoch=28
05/25/2022 20:30:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6062067802327542 -> 0.618370548317336 on epoch=28, global_step=450
05/25/2022 20:30:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.63 on epoch=28
05/25/2022 20:30:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=29
05/25/2022 20:30:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.66 on epoch=29
05/25/2022 20:30:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.63 on epoch=30
05/25/2022 20:30:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=31
05/25/2022 20:30:56 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.6276027499005004 on epoch=31
05/25/2022 20:30:56 - INFO - __main__ - Saving model with best Classification-F1: 0.618370548317336 -> 0.6276027499005004 on epoch=31, global_step=500
05/25/2022 20:30:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=31
05/25/2022 20:31:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=32
05/25/2022 20:31:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=33
05/25/2022 20:31:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=33
05/25/2022 20:31:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=34
05/25/2022 20:31:12 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.6624451130265083 on epoch=34
05/25/2022 20:31:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6276027499005004 -> 0.6624451130265083 on epoch=34, global_step=550
05/25/2022 20:31:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=34
05/25/2022 20:31:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.63 on epoch=35
05/25/2022 20:31:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=36
05/25/2022 20:31:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.58 on epoch=36
05/25/2022 20:31:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=37
05/25/2022 20:31:28 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.6837458228905597 on epoch=37
05/25/2022 20:31:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6624451130265083 -> 0.6837458228905597 on epoch=37, global_step=600
05/25/2022 20:31:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=38
05/25/2022 20:31:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=38
05/25/2022 20:31:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=39
05/25/2022 20:31:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=39
05/25/2022 20:31:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=40
05/25/2022 20:31:44 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.6828962201464684 on epoch=40
05/25/2022 20:31:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=41
05/25/2022 20:31:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=41
05/25/2022 20:31:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=42
05/25/2022 20:31:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=43
05/25/2022 20:31:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.55 on epoch=43
05/25/2022 20:32:00 - INFO - __main__ - Global step 700 Train loss 0.52 Classification-F1 0.7100167766624147 on epoch=43
05/25/2022 20:32:00 - INFO - __main__ - Saving model with best Classification-F1: 0.6837458228905597 -> 0.7100167766624147 on epoch=43, global_step=700
05/25/2022 20:32:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=44
05/25/2022 20:32:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=44
05/25/2022 20:32:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=45
05/25/2022 20:32:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=46
05/25/2022 20:32:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=46
05/25/2022 20:32:15 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.6677415403786322 on epoch=46
05/25/2022 20:32:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=47
05/25/2022 20:32:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=48
05/25/2022 20:32:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=48
05/25/2022 20:32:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.58 on epoch=49
05/25/2022 20:32:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=49
05/25/2022 20:32:31 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.7281200308089842 on epoch=49
05/25/2022 20:32:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7100167766624147 -> 0.7281200308089842 on epoch=49, global_step=800
05/25/2022 20:32:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.51 on epoch=50
05/25/2022 20:32:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=51
05/25/2022 20:32:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.54 on epoch=51
05/25/2022 20:32:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=52
05/25/2022 20:32:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=53
05/25/2022 20:32:47 - INFO - __main__ - Global step 850 Train loss 0.48 Classification-F1 0.6752737842124802 on epoch=53
05/25/2022 20:32:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=53
05/25/2022 20:32:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
05/25/2022 20:32:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=54
05/25/2022 20:32:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=55
05/25/2022 20:32:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=56
05/25/2022 20:33:03 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.7063870853196696 on epoch=56
05/25/2022 20:33:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
05/25/2022 20:33:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=57
05/25/2022 20:33:10 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
05/25/2022 20:33:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
05/25/2022 20:33:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=59
05/25/2022 20:33:18 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.724402742524114 on epoch=59
05/25/2022 20:33:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=59
05/25/2022 20:33:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=60
05/25/2022 20:33:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.39 on epoch=61
05/25/2022 20:33:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=61
05/25/2022 20:33:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
05/25/2022 20:33:34 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.7358063172068183 on epoch=62
05/25/2022 20:33:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7281200308089842 -> 0.7358063172068183 on epoch=62, global_step=1000
05/25/2022 20:33:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=63
05/25/2022 20:33:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=63
05/25/2022 20:33:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=64
05/25/2022 20:33:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
05/25/2022 20:33:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=65
05/25/2022 20:33:50 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.7316835884831885 on epoch=65
05/25/2022 20:33:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.42 on epoch=66
05/25/2022 20:33:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=66
05/25/2022 20:33:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.39 on epoch=67
05/25/2022 20:34:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
05/25/2022 20:34:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=68
05/25/2022 20:34:06 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.738608511690855 on epoch=68
05/25/2022 20:34:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7358063172068183 -> 0.738608511690855 on epoch=68, global_step=1100
05/25/2022 20:34:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=69
05/25/2022 20:34:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=69
05/25/2022 20:34:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=70
05/25/2022 20:34:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/25/2022 20:34:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.35 on epoch=71
05/25/2022 20:34:22 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.7489355074759217 on epoch=71
05/25/2022 20:34:22 - INFO - __main__ - Saving model with best Classification-F1: 0.738608511690855 -> 0.7489355074759217 on epoch=71, global_step=1150
05/25/2022 20:34:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=72
05/25/2022 20:34:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=73
05/25/2022 20:34:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.40 on epoch=73
05/25/2022 20:34:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
05/25/2022 20:34:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=74
05/25/2022 20:34:38 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7600572797241331 on epoch=74
05/25/2022 20:34:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7489355074759217 -> 0.7600572797241331 on epoch=74, global_step=1200
05/25/2022 20:34:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
05/25/2022 20:34:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=76
05/25/2022 20:34:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=76
05/25/2022 20:34:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/25/2022 20:34:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
05/25/2022 20:34:54 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.7459292114266834 on epoch=78
05/25/2022 20:34:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=78
05/25/2022 20:34:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=79
05/25/2022 20:35:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.34 on epoch=79
05/25/2022 20:35:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=80
05/25/2022 20:35:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
05/25/2022 20:35:09 - INFO - __main__ - Global step 1300 Train loss 0.36 Classification-F1 0.7406028104672527 on epoch=81
05/25/2022 20:35:12 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
05/25/2022 20:35:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
05/25/2022 20:35:17 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=83
05/25/2022 20:35:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=83
05/25/2022 20:35:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=84
05/25/2022 20:35:25 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.7613437879206246 on epoch=84
05/25/2022 20:35:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7600572797241331 -> 0.7613437879206246 on epoch=84, global_step=1350
05/25/2022 20:35:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=84
05/25/2022 20:35:30 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=85
05/25/2022 20:35:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=86
05/25/2022 20:35:35 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
05/25/2022 20:35:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=87
05/25/2022 20:35:41 - INFO - __main__ - Global step 1400 Train loss 0.28 Classification-F1 0.7600572797241331 on epoch=87
05/25/2022 20:35:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=88
05/25/2022 20:35:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=88
05/25/2022 20:35:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=89
05/25/2022 20:35:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=89
05/25/2022 20:35:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=90
05/25/2022 20:35:57 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.7637228623851361 on epoch=90
05/25/2022 20:35:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7613437879206246 -> 0.7637228623851361 on epoch=90, global_step=1450
05/25/2022 20:35:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=91
05/25/2022 20:36:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=91
05/25/2022 20:36:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=92
05/25/2022 20:36:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=93
05/25/2022 20:36:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=93
05/25/2022 20:36:12 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.7651783129662806 on epoch=93
05/25/2022 20:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7637228623851361 -> 0.7651783129662806 on epoch=93, global_step=1500
05/25/2022 20:36:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=94
05/25/2022 20:36:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=94
05/25/2022 20:36:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=95
05/25/2022 20:36:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=96
05/25/2022 20:36:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=96
05/25/2022 20:36:28 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.7614699464339868 on epoch=96
05/25/2022 20:36:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=97
05/25/2022 20:36:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=98
05/25/2022 20:36:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=98
05/25/2022 20:36:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=99
05/25/2022 20:36:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.29 on epoch=99
05/25/2022 20:36:44 - INFO - __main__ - Global step 1600 Train loss 0.31 Classification-F1 0.7788095700165618 on epoch=99
05/25/2022 20:36:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7651783129662806 -> 0.7788095700165618 on epoch=99, global_step=1600
05/25/2022 20:36:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=100
05/25/2022 20:36:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=101
05/25/2022 20:36:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=101
05/25/2022 20:36:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.33 on epoch=102
05/25/2022 20:36:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=103
05/25/2022 20:37:00 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.7739579579466851 on epoch=103
05/25/2022 20:37:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=103
05/25/2022 20:37:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=104
05/25/2022 20:37:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.32 on epoch=104
05/25/2022 20:37:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
05/25/2022 20:37:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=106
05/25/2022 20:37:15 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.7739579579466851 on epoch=106
05/25/2022 20:37:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.28 on epoch=106
05/25/2022 20:37:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.33 on epoch=107
05/25/2022 20:37:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=108
05/25/2022 20:37:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
05/25/2022 20:37:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=109
05/25/2022 20:37:31 - INFO - __main__ - Global step 1750 Train loss 0.29 Classification-F1 0.7735452780787287 on epoch=109
05/25/2022 20:37:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.27 on epoch=109
05/25/2022 20:37:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=110
05/25/2022 20:37:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
05/25/2022 20:37:41 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
05/25/2022 20:37:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=112
05/25/2022 20:37:47 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.7748803783424878 on epoch=112
05/25/2022 20:37:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
05/25/2022 20:37:52 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=113
05/25/2022 20:37:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=114
05/25/2022 20:37:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=114
05/25/2022 20:37:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.33 on epoch=115
05/25/2022 20:38:03 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.7842490842490841 on epoch=115
05/25/2022 20:38:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7788095700165618 -> 0.7842490842490841 on epoch=115, global_step=1850
05/25/2022 20:38:05 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=116
05/25/2022 20:38:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=116
05/25/2022 20:38:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.30 on epoch=117
05/25/2022 20:38:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=118
05/25/2022 20:38:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=118
05/25/2022 20:38:18 - INFO - __main__ - Global step 1900 Train loss 0.26 Classification-F1 0.7831119373340488 on epoch=118
05/25/2022 20:38:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
05/25/2022 20:38:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=119
05/25/2022 20:38:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/25/2022 20:38:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=121
05/25/2022 20:38:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=121
05/25/2022 20:38:34 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.7737600546222166 on epoch=121
05/25/2022 20:38:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=122
05/25/2022 20:38:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=123
05/25/2022 20:38:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
05/25/2022 20:38:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=124
05/25/2022 20:38:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=124
05/25/2022 20:38:50 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.779724755150287 on epoch=124
05/25/2022 20:38:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=125
05/25/2022 20:38:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=126
05/25/2022 20:38:57 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
05/25/2022 20:39:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.29 on epoch=127
05/25/2022 20:39:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=128
05/25/2022 20:39:06 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7771779732334226 on epoch=128
05/25/2022 20:39:08 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
05/25/2022 20:39:11 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.35 on epoch=129
05/25/2022 20:39:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=129
05/25/2022 20:39:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=130
05/25/2022 20:39:18 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=131
05/25/2022 20:39:21 - INFO - __main__ - Global step 2100 Train loss 0.23 Classification-F1 0.7749278499278499 on epoch=131
05/25/2022 20:39:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
05/25/2022 20:39:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.23 on epoch=132
05/25/2022 20:39:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=133
05/25/2022 20:39:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.25 on epoch=133
05/25/2022 20:39:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.24 on epoch=134
05/25/2022 20:39:37 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.7880979630636498 on epoch=134
05/25/2022 20:39:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7842490842490841 -> 0.7880979630636498 on epoch=134, global_step=2150
05/25/2022 20:39:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=134
05/25/2022 20:39:42 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
05/25/2022 20:39:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.26 on epoch=136
05/25/2022 20:39:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.21 on epoch=136
05/25/2022 20:39:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.23 on epoch=137
05/25/2022 20:39:53 - INFO - __main__ - Global step 2200 Train loss 0.23 Classification-F1 0.7888861031028307 on epoch=137
05/25/2022 20:39:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7880979630636498 -> 0.7888861031028307 on epoch=137, global_step=2200
05/25/2022 20:39:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=138
05/25/2022 20:39:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
05/25/2022 20:40:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=139
05/25/2022 20:40:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
05/25/2022 20:40:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.32 on epoch=140
05/25/2022 20:40:09 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7880979630636498 on epoch=140
05/25/2022 20:40:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=141
05/25/2022 20:40:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
05/25/2022 20:40:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.24 on epoch=142
05/25/2022 20:40:19 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=143
05/25/2022 20:40:21 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=143
05/25/2022 20:40:25 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7846934238410105 on epoch=143
05/25/2022 20:40:27 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.22 on epoch=144
05/25/2022 20:40:29 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.25 on epoch=144
05/25/2022 20:40:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.29 on epoch=145
05/25/2022 20:40:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
05/25/2022 20:40:37 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=146
05/25/2022 20:40:40 - INFO - __main__ - Global step 2350 Train loss 0.23 Classification-F1 0.784628888420757 on epoch=146
05/25/2022 20:40:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.24 on epoch=147
05/25/2022 20:40:45 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
05/25/2022 20:40:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
05/25/2022 20:40:50 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.20 on epoch=149
05/25/2022 20:40:53 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
05/25/2022 20:40:56 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.7926139131362717 on epoch=149
05/25/2022 20:40:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7888861031028307 -> 0.7926139131362717 on epoch=149, global_step=2400
05/25/2022 20:40:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.26 on epoch=150
05/25/2022 20:41:01 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=151
05/25/2022 20:41:04 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/25/2022 20:41:06 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.23 on epoch=152
05/25/2022 20:41:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.21 on epoch=153
05/25/2022 20:41:12 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.7931852839114826 on epoch=153
05/25/2022 20:41:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7926139131362717 -> 0.7931852839114826 on epoch=153, global_step=2450
05/25/2022 20:41:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=153
05/25/2022 20:41:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.19 on epoch=154
05/25/2022 20:41:20 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=154
05/25/2022 20:41:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=155
05/25/2022 20:41:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
05/25/2022 20:41:28 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.7929679758344337 on epoch=156
05/25/2022 20:41:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
05/25/2022 20:41:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.26 on epoch=157
05/25/2022 20:41:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=158
05/25/2022 20:41:38 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=158
05/25/2022 20:41:40 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=159
05/25/2022 20:41:44 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7967688275922102 on epoch=159
05/25/2022 20:41:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7931852839114826 -> 0.7967688275922102 on epoch=159, global_step=2550
05/25/2022 20:41:46 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=159
05/25/2022 20:41:49 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.21 on epoch=160
05/25/2022 20:41:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
05/25/2022 20:41:54 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=161
05/25/2022 20:41:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/25/2022 20:42:00 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.8016785448850667 on epoch=162
05/25/2022 20:42:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7967688275922102 -> 0.8016785448850667 on epoch=162, global_step=2600
05/25/2022 20:42:02 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/25/2022 20:42:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=163
05/25/2022 20:42:07 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.16 on epoch=164
05/25/2022 20:42:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
05/25/2022 20:42:12 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=165
05/25/2022 20:42:15 - INFO - __main__ - Global step 2650 Train loss 0.16 Classification-F1 0.7924752920035939 on epoch=165
05/25/2022 20:42:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.24 on epoch=166
05/25/2022 20:42:20 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/25/2022 20:42:23 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.22 on epoch=167
05/25/2022 20:42:25 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
05/25/2022 20:42:28 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
05/25/2022 20:42:31 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.7956965999575588 on epoch=168
05/25/2022 20:42:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=169
05/25/2022 20:42:36 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
05/25/2022 20:42:39 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=170
05/25/2022 20:42:41 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=171
05/25/2022 20:42:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
05/25/2022 20:42:47 - INFO - __main__ - Global step 2750 Train loss 0.15 Classification-F1 0.7861823895118362 on epoch=171
05/25/2022 20:42:49 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=172
05/25/2022 20:42:52 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
05/25/2022 20:42:54 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/25/2022 20:42:57 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.22 on epoch=174
05/25/2022 20:42:59 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/25/2022 20:43:03 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.7933501623100707 on epoch=174
05/25/2022 20:43:05 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=175
05/25/2022 20:43:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
05/25/2022 20:43:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
05/25/2022 20:43:13 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
05/25/2022 20:43:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
05/25/2022 20:43:19 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.7808092753505496 on epoch=178
05/25/2022 20:43:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.25 on epoch=178
05/25/2022 20:43:24 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.22 on epoch=179
05/25/2022 20:43:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
05/25/2022 20:43:29 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=180
05/25/2022 20:43:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.27 on epoch=181
05/25/2022 20:43:35 - INFO - __main__ - Global step 2900 Train loss 0.21 Classification-F1 0.7923934494255702 on epoch=181
05/25/2022 20:43:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
05/25/2022 20:43:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/25/2022 20:43:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
05/25/2022 20:43:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=183
05/25/2022 20:43:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.23 on epoch=184
05/25/2022 20:43:50 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.785623524861526 on epoch=184
05/25/2022 20:43:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=184
05/25/2022 20:43:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=185
05/25/2022 20:43:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=186
05/25/2022 20:44:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=186
05/25/2022 20:44:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
05/25/2022 20:44:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:44:04 - INFO - __main__ - Printing 3 examples
05/25/2022 20:44:04 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 20:44:04 - INFO - __main__ - ['sad']
05/25/2022 20:44:04 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 20:44:04 - INFO - __main__ - ['sad']
05/25/2022 20:44:04 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 20:44:04 - INFO - __main__ - ['sad']
05/25/2022 20:44:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:44:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:44:05 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:44:05 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:44:05 - INFO - __main__ - Printing 3 examples
05/25/2022 20:44:05 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 20:44:05 - INFO - __main__ - ['sad']
05/25/2022 20:44:05 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 20:44:05 - INFO - __main__ - ['sad']
05/25/2022 20:44:05 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 20:44:05 - INFO - __main__ - ['sad']
05/25/2022 20:44:05 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:44:05 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:44:05 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:44:06 - INFO - __main__ - Global step 3000 Train loss 0.16 Classification-F1 0.7929661736113349 on epoch=187
05/25/2022 20:44:06 - INFO - __main__ - save last model!
05/25/2022 20:44:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 20:44:06 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 20:44:06 - INFO - __main__ - Printing 3 examples
05/25/2022 20:44:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 20:44:06 - INFO - __main__ - ['others']
05/25/2022 20:44:06 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 20:44:06 - INFO - __main__ - ['others']
05/25/2022 20:44:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 20:44:06 - INFO - __main__ - ['others']
05/25/2022 20:44:06 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:44:08 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:44:14 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 20:44:21 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:44:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:44:22 - INFO - __main__ - Starting training!
05/25/2022 20:45:31 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_13_0.2_8_predictions.txt
05/25/2022 20:45:31 - INFO - __main__ - Classification-F1 on test data: 0.3946
05/25/2022 20:45:32 - INFO - __main__ - prefix=emo_64_13, lr=0.2, bsz=8, dev_performance=0.8016785448850667, test_performance=0.39461038557584716
05/25/2022 20:45:32 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.5, bsz=8 ...
05/25/2022 20:45:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:45:33 - INFO - __main__ - Printing 3 examples
05/25/2022 20:45:33 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:45:33 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:45:33 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 20:45:33 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 20:45:33 - INFO - __main__ - Printing 3 examples
05/25/2022 20:45:33 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 20:45:33 - INFO - __main__ - ['sad']
05/25/2022 20:45:33 - INFO - __main__ - Tokenizing Input ...
05/25/2022 20:45:33 - INFO - __main__ - Tokenizing Output ...
05/25/2022 20:45:34 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 20:45:52 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 20:45:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 20:45:53 - INFO - __main__ - Starting training!
05/25/2022 20:45:56 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=0
05/25/2022 20:45:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.54 on epoch=1
05/25/2022 20:46:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.16 on epoch=1
05/25/2022 20:46:03 - INFO - __main__ - Step 40 Global step 40 Train loss 1.67 on epoch=2
05/25/2022 20:46:06 - INFO - __main__ - Step 50 Global step 50 Train loss 1.28 on epoch=3
05/25/2022 20:46:10 - INFO - __main__ - Global step 50 Train loss 2.34 Classification-F1 0.26813345647309694 on epoch=3
05/25/2022 20:46:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.26813345647309694 on epoch=3, global_step=50
05/25/2022 20:46:12 - INFO - __main__ - Step 60 Global step 60 Train loss 1.11 on epoch=3
05/25/2022 20:46:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.76 on epoch=4
05/25/2022 20:46:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.95 on epoch=4
05/25/2022 20:46:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=5
05/25/2022 20:46:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.67 on epoch=6
05/25/2022 20:46:25 - INFO - __main__ - Global step 100 Train loss 0.88 Classification-F1 0.5463923442734322 on epoch=6
05/25/2022 20:46:25 - INFO - __main__ - Saving model with best Classification-F1: 0.26813345647309694 -> 0.5463923442734322 on epoch=6, global_step=100
05/25/2022 20:46:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=6
05/25/2022 20:46:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.73 on epoch=7
05/25/2022 20:46:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.65 on epoch=8
05/25/2022 20:46:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.77 on epoch=8
05/25/2022 20:46:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=9
05/25/2022 20:46:41 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.6029553275744892 on epoch=9
05/25/2022 20:46:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5463923442734322 -> 0.6029553275744892 on epoch=9, global_step=150
05/25/2022 20:46:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=9
05/25/2022 20:46:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=10
05/25/2022 20:46:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=11
05/25/2022 20:46:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.73 on epoch=11
05/25/2022 20:46:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=12
05/25/2022 20:46:57 - INFO - __main__ - Global step 200 Train loss 0.62 Classification-F1 0.6354295214589333 on epoch=12
05/25/2022 20:46:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6029553275744892 -> 0.6354295214589333 on epoch=12, global_step=200
05/25/2022 20:46:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=13
05/25/2022 20:47:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.66 on epoch=13
05/25/2022 20:47:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.43 on epoch=14
05/25/2022 20:47:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.58 on epoch=14
05/25/2022 20:47:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=15
05/25/2022 20:47:12 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.6800369221724823 on epoch=15
05/25/2022 20:47:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6354295214589333 -> 0.6800369221724823 on epoch=15, global_step=250
05/25/2022 20:47:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.42 on epoch=16
05/25/2022 20:47:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=16
05/25/2022 20:47:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.42 on epoch=17
05/25/2022 20:47:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.50 on epoch=18
05/25/2022 20:47:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.53 on epoch=18
05/25/2022 20:47:28 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.6958332349826352 on epoch=18
05/25/2022 20:47:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6800369221724823 -> 0.6958332349826352 on epoch=18, global_step=300
05/25/2022 20:47:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=19
05/25/2022 20:47:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.54 on epoch=19
05/25/2022 20:47:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=20
05/25/2022 20:47:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=21
05/25/2022 20:47:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=21
05/25/2022 20:47:44 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.6692137450564417 on epoch=21
05/25/2022 20:47:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.41 on epoch=22
05/25/2022 20:47:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=23
05/25/2022 20:47:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=23
05/25/2022 20:47:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/25/2022 20:47:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=24
05/25/2022 20:48:00 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.6678880412924664 on epoch=24
05/25/2022 20:48:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.40 on epoch=25
05/25/2022 20:48:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=26
05/25/2022 20:48:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=26
05/25/2022 20:48:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/25/2022 20:48:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=28
05/25/2022 20:48:15 - INFO - __main__ - Global step 450 Train loss 0.39 Classification-F1 0.6981522936431794 on epoch=28
05/25/2022 20:48:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6958332349826352 -> 0.6981522936431794 on epoch=28, global_step=450
05/25/2022 20:48:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.39 on epoch=28
05/25/2022 20:48:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.32 on epoch=29
05/25/2022 20:48:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.40 on epoch=29
05/25/2022 20:48:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=30
05/25/2022 20:48:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=31
05/25/2022 20:48:31 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.7066417440458361 on epoch=31
05/25/2022 20:48:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6981522936431794 -> 0.7066417440458361 on epoch=31, global_step=500
05/25/2022 20:48:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=31
05/25/2022 20:48:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=32
05/25/2022 20:48:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=33
05/25/2022 20:48:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=33
05/25/2022 20:48:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=34
05/25/2022 20:48:47 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.7071698396642248 on epoch=34
05/25/2022 20:48:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7066417440458361 -> 0.7071698396642248 on epoch=34, global_step=550
05/25/2022 20:48:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=34
05/25/2022 20:48:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=35
05/25/2022 20:48:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=36
05/25/2022 20:48:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
05/25/2022 20:48:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=37
05/25/2022 20:49:03 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.7099970924546476 on epoch=37
05/25/2022 20:49:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7071698396642248 -> 0.7099970924546476 on epoch=37, global_step=600
05/25/2022 20:49:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=38
05/25/2022 20:49:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=38
05/25/2022 20:49:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=39
05/25/2022 20:49:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=39
05/25/2022 20:49:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.29 on epoch=40
05/25/2022 20:49:19 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.7481799079138469 on epoch=40
05/25/2022 20:49:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7099970924546476 -> 0.7481799079138469 on epoch=40, global_step=650
05/25/2022 20:49:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=41
05/25/2022 20:49:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=41
05/25/2022 20:49:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=42
05/25/2022 20:49:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=43
05/25/2022 20:49:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
05/25/2022 20:49:35 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.7231789065991361 on epoch=43
05/25/2022 20:49:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=44
05/25/2022 20:49:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=44
05/25/2022 20:49:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=45
05/25/2022 20:49:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=46
05/25/2022 20:49:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.35 on epoch=46
05/25/2022 20:49:50 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.7391171328671329 on epoch=46
05/25/2022 20:49:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=47
05/25/2022 20:49:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=48
05/25/2022 20:49:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/25/2022 20:50:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=49
05/25/2022 20:50:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=49
05/25/2022 20:50:06 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.7122494734116304 on epoch=49
05/25/2022 20:50:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=50
05/25/2022 20:50:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=51
05/25/2022 20:50:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/25/2022 20:50:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=52
05/25/2022 20:50:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.28 on epoch=53
05/25/2022 20:50:22 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.729030735312387 on epoch=53
05/25/2022 20:50:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=53
05/25/2022 20:50:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=54
05/25/2022 20:50:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=54
05/25/2022 20:50:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=55
05/25/2022 20:50:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=56
05/25/2022 20:50:38 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.7572142667012867 on epoch=56
05/25/2022 20:50:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7481799079138469 -> 0.7572142667012867 on epoch=56, global_step=900
05/25/2022 20:50:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=56
05/25/2022 20:50:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=57
05/25/2022 20:50:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/25/2022 20:50:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=58
05/25/2022 20:50:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=59
05/25/2022 20:50:53 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.7585462287104624 on epoch=59
05/25/2022 20:50:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7572142667012867 -> 0.7585462287104624 on epoch=59, global_step=950
05/25/2022 20:50:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/25/2022 20:50:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=60
05/25/2022 20:51:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=61
05/25/2022 20:51:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=61
05/25/2022 20:51:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=62
05/25/2022 20:51:09 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.7395410998815154 on epoch=62
05/25/2022 20:51:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=63
05/25/2022 20:51:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=63
05/25/2022 20:51:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=64
05/25/2022 20:51:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=64
05/25/2022 20:51:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=65
05/25/2022 20:51:25 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.7668363868283614 on epoch=65
05/25/2022 20:51:25 - INFO - __main__ - Saving model with best Classification-F1: 0.7585462287104624 -> 0.7668363868283614 on epoch=65, global_step=1050
05/25/2022 20:51:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=66
05/25/2022 20:51:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=66
05/25/2022 20:51:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
05/25/2022 20:51:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=68
05/25/2022 20:51:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=68
05/25/2022 20:51:41 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.7495337764927801 on epoch=68
05/25/2022 20:51:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=69
05/25/2022 20:51:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=69
05/25/2022 20:51:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/25/2022 20:51:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=71
05/25/2022 20:51:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=71
05/25/2022 20:51:57 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.7590701564622427 on epoch=71
05/25/2022 20:51:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=72
05/25/2022 20:52:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=73
05/25/2022 20:52:04 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=73
05/25/2022 20:52:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=74
05/25/2022 20:52:09 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=74
05/25/2022 20:52:12 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.770133651081099 on epoch=74
05/25/2022 20:52:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7668363868283614 -> 0.770133651081099 on epoch=74, global_step=1200
05/25/2022 20:52:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=75
05/25/2022 20:52:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=76
05/25/2022 20:52:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=76
05/25/2022 20:52:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=77
05/25/2022 20:52:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.10 on epoch=78
05/25/2022 20:52:28 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.7791795176495484 on epoch=78
05/25/2022 20:52:28 - INFO - __main__ - Saving model with best Classification-F1: 0.770133651081099 -> 0.7791795176495484 on epoch=78, global_step=1250
05/25/2022 20:52:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=78
05/25/2022 20:52:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=79
05/25/2022 20:52:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=79
05/25/2022 20:52:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=80
05/25/2022 20:52:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.07 on epoch=81
05/25/2022 20:52:44 - INFO - __main__ - Global step 1300 Train loss 0.12 Classification-F1 0.7407011423480607 on epoch=81
05/25/2022 20:52:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=81
05/25/2022 20:52:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=82
05/25/2022 20:52:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/25/2022 20:52:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=83
05/25/2022 20:52:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=84
05/25/2022 20:53:00 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.7413328192705473 on epoch=84
05/25/2022 20:53:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=84
05/25/2022 20:53:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=85
05/25/2022 20:53:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=86
05/25/2022 20:53:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=86
05/25/2022 20:53:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=87
05/25/2022 20:53:16 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.7669232683375018 on epoch=87
05/25/2022 20:53:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=88
05/25/2022 20:53:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=88
05/25/2022 20:53:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.07 on epoch=89
05/25/2022 20:53:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
05/25/2022 20:53:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=90
05/25/2022 20:53:32 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7325402967405922 on epoch=90
05/25/2022 20:53:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=91
05/25/2022 20:53:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=91
05/25/2022 20:53:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=92
05/25/2022 20:53:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=93
05/25/2022 20:53:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=93
05/25/2022 20:53:48 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7612232200458056 on epoch=93
05/25/2022 20:53:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=94
05/25/2022 20:53:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=94
05/25/2022 20:53:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/25/2022 20:53:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=96
05/25/2022 20:54:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=96
05/25/2022 20:54:04 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.738774786906951 on epoch=96
05/25/2022 20:54:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.12 on epoch=97
05/25/2022 20:54:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=98
05/25/2022 20:54:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=98
05/25/2022 20:54:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=99
05/25/2022 20:54:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=99
05/25/2022 20:54:20 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7459861083283243 on epoch=99
05/25/2022 20:54:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
05/25/2022 20:54:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=101
05/25/2022 20:54:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=101
05/25/2022 20:54:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=102
05/25/2022 20:54:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=103
05/25/2022 20:54:36 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7743527035734828 on epoch=103
05/25/2022 20:54:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=103
05/25/2022 20:54:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=104
05/25/2022 20:54:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=104
05/25/2022 20:54:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=105
05/25/2022 20:54:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=106
05/25/2022 20:54:52 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.7309751334222505 on epoch=106
05/25/2022 20:54:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=106
05/25/2022 20:54:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=107
05/25/2022 20:54:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=108
05/25/2022 20:55:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=108
05/25/2022 20:55:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
05/25/2022 20:55:08 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7615023538552951 on epoch=109
05/25/2022 20:55:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.06 on epoch=109
05/25/2022 20:55:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=110
05/25/2022 20:55:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.04 on epoch=111
05/25/2022 20:55:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=111
05/25/2022 20:55:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=112
05/25/2022 20:55:24 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.7510747591163986 on epoch=112
05/25/2022 20:55:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=113
05/25/2022 20:55:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=113
05/25/2022 20:55:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=114
05/25/2022 20:55:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=114
05/25/2022 20:55:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=115
05/25/2022 20:55:40 - INFO - __main__ - Global step 1850 Train loss 0.10 Classification-F1 0.7489412666620864 on epoch=115
05/25/2022 20:55:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=116
05/25/2022 20:55:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
05/25/2022 20:55:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/25/2022 20:55:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=118
05/25/2022 20:55:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=118
05/25/2022 20:55:56 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7614799995680253 on epoch=118
05/25/2022 20:55:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=119
05/25/2022 20:56:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=119
05/25/2022 20:56:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
05/25/2022 20:56:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=121
05/25/2022 20:56:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=121
05/25/2022 20:56:12 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.7539072101102176 on epoch=121
05/25/2022 20:56:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/25/2022 20:56:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
05/25/2022 20:56:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
05/25/2022 20:56:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=124
05/25/2022 20:56:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=124
05/25/2022 20:56:28 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.7642084106369821 on epoch=124
05/25/2022 20:56:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/25/2022 20:56:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.02 on epoch=126
05/25/2022 20:56:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=126
05/25/2022 20:56:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=127
05/25/2022 20:56:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=128
05/25/2022 20:56:44 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.7414000637678593 on epoch=128
05/25/2022 20:56:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
05/25/2022 20:56:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=129
05/25/2022 20:56:51 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=129
05/25/2022 20:56:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=130
05/25/2022 20:56:56 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/25/2022 20:57:00 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.7635452944921025 on epoch=131
05/25/2022 20:57:02 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=131
05/25/2022 20:57:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=132
05/25/2022 20:57:07 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.08 on epoch=133
05/25/2022 20:57:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=133
05/25/2022 20:57:12 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.01 on epoch=134
05/25/2022 20:57:16 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.7427050130396986 on epoch=134
05/25/2022 20:57:18 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
05/25/2022 20:57:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=135
05/25/2022 20:57:23 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.02 on epoch=136
05/25/2022 20:57:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
05/25/2022 20:57:28 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=137
05/25/2022 20:57:32 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7682418982310872 on epoch=137
05/25/2022 20:57:34 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/25/2022 20:57:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/25/2022 20:57:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=139
05/25/2022 20:57:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=139
05/25/2022 20:57:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.09 on epoch=140
05/25/2022 20:57:48 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.7693382707525044 on epoch=140
05/25/2022 20:57:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/25/2022 20:57:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
05/25/2022 20:57:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=142
05/25/2022 20:57:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
05/25/2022 20:58:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
05/25/2022 20:58:04 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7477322479457426 on epoch=143
05/25/2022 20:58:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=144
05/25/2022 20:58:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
05/25/2022 20:58:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
05/25/2022 20:58:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=146
05/25/2022 20:58:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.03 on epoch=146
05/25/2022 20:58:21 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.7685735716345381 on epoch=146
05/25/2022 20:58:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=147
05/25/2022 20:58:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=148
05/25/2022 20:58:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=148
05/25/2022 20:58:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=149
05/25/2022 20:58:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
05/25/2022 20:58:37 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.7771424412755763 on epoch=149
05/25/2022 20:58:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=150
05/25/2022 20:58:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=151
05/25/2022 20:58:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=151
05/25/2022 20:58:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=152
05/25/2022 20:58:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/25/2022 20:58:53 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.7487586687406831 on epoch=153
05/25/2022 20:58:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/25/2022 20:58:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=154
05/25/2022 20:59:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=154
05/25/2022 20:59:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/25/2022 20:59:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=156
05/25/2022 20:59:09 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7717432607243341 on epoch=156
05/25/2022 20:59:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/25/2022 20:59:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 20:59:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=158
05/25/2022 20:59:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
05/25/2022 20:59:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=159
05/25/2022 20:59:26 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.7484303286899274 on epoch=159
05/25/2022 20:59:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=159
05/25/2022 20:59:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=160
05/25/2022 20:59:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=161
05/25/2022 20:59:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.06 on epoch=161
05/25/2022 20:59:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=162
05/25/2022 20:59:42 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7397983006934337 on epoch=162
05/25/2022 20:59:44 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/25/2022 20:59:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=163
05/25/2022 20:59:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.01 on epoch=164
05/25/2022 20:59:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=164
05/25/2022 20:59:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=165
05/25/2022 20:59:58 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7655963160506077 on epoch=165
05/25/2022 21:00:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=166
05/25/2022 21:00:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=166
05/25/2022 21:00:06 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/25/2022 21:00:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/25/2022 21:00:11 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=168
05/25/2022 21:00:14 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.7758293090449702 on epoch=168
05/25/2022 21:00:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
05/25/2022 21:00:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/25/2022 21:00:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
05/25/2022 21:00:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=171
05/25/2022 21:00:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=171
05/25/2022 21:00:31 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.7553081983590275 on epoch=171
05/25/2022 21:00:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/25/2022 21:00:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
05/25/2022 21:00:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=173
05/25/2022 21:00:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=174
05/25/2022 21:00:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/25/2022 21:00:47 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.7698404820494716 on epoch=174
05/25/2022 21:00:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=175
05/25/2022 21:00:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/25/2022 21:00:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/25/2022 21:00:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
05/25/2022 21:01:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=178
05/25/2022 21:01:03 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.7652251720433539 on epoch=178
05/25/2022 21:01:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/25/2022 21:01:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/25/2022 21:01:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=179
05/25/2022 21:01:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=180
05/25/2022 21:01:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/25/2022 21:01:20 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.7590128184869953 on epoch=181
05/25/2022 21:01:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=181
05/25/2022 21:01:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/25/2022 21:01:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=183
05/25/2022 21:01:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 21:01:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.01 on epoch=184
05/25/2022 21:01:36 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.7417113381715151 on epoch=184
05/25/2022 21:01:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/25/2022 21:01:41 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=185
05/25/2022 21:01:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/25/2022 21:01:46 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/25/2022 21:01:49 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/25/2022 21:01:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:01:50 - INFO - __main__ - Printing 3 examples
05/25/2022 21:01:50 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:01:50 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:01:50 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:01:50 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:01:50 - INFO - __main__ - Printing 3 examples
05/25/2022 21:01:50 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:01:50 - INFO - __main__ - ['sad']
05/25/2022 21:01:50 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:01:50 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:01:51 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:01:52 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.750447488926336 on epoch=187
05/25/2022 21:01:52 - INFO - __main__ - save last model!
05/25/2022 21:01:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 21:01:52 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 21:01:52 - INFO - __main__ - Printing 3 examples
05/25/2022 21:01:52 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 21:01:52 - INFO - __main__ - ['others']
05/25/2022 21:01:52 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 21:01:52 - INFO - __main__ - ['others']
05/25/2022 21:01:52 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 21:01:52 - INFO - __main__ - ['others']
05/25/2022 21:01:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:01:54 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:02:00 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 21:02:06 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:02:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:02:07 - INFO - __main__ - Starting training!
05/25/2022 21:03:16 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_21_0.5_8_predictions.txt
05/25/2022 21:03:16 - INFO - __main__ - Classification-F1 on test data: 0.4853
05/25/2022 21:03:17 - INFO - __main__ - prefix=emo_64_21, lr=0.5, bsz=8, dev_performance=0.7791795176495484, test_performance=0.4853205987950483
05/25/2022 21:03:17 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.4, bsz=8 ...
05/25/2022 21:03:17 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:03:17 - INFO - __main__ - Printing 3 examples
05/25/2022 21:03:17 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:03:17 - INFO - __main__ - ['sad']
05/25/2022 21:03:17 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:03:17 - INFO - __main__ - ['sad']
05/25/2022 21:03:17 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:03:17 - INFO - __main__ - ['sad']
05/25/2022 21:03:17 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:03:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:03:18 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:03:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:03:18 - INFO - __main__ - Printing 3 examples
05/25/2022 21:03:18 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:03:18 - INFO - __main__ - ['sad']
05/25/2022 21:03:18 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:03:18 - INFO - __main__ - ['sad']
05/25/2022 21:03:18 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:03:18 - INFO - __main__ - ['sad']
05/25/2022 21:03:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:03:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:03:18 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:03:34 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:03:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:03:35 - INFO - __main__ - Starting training!
05/25/2022 21:03:38 - INFO - __main__ - Step 10 Global step 10 Train loss 3.83 on epoch=0
05/25/2022 21:03:41 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=1
05/25/2022 21:03:43 - INFO - __main__ - Step 30 Global step 30 Train loss 2.52 on epoch=1
05/25/2022 21:03:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.98 on epoch=2
05/25/2022 21:03:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.48 on epoch=3
05/25/2022 21:03:52 - INFO - __main__ - Global step 50 Train loss 2.50 Classification-F1 0.17786130829609093 on epoch=3
05/25/2022 21:03:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17786130829609093 on epoch=3, global_step=50
05/25/2022 21:03:54 - INFO - __main__ - Step 60 Global step 60 Train loss 1.53 on epoch=3
05/25/2022 21:03:57 - INFO - __main__ - Step 70 Global step 70 Train loss 1.09 on epoch=4
05/25/2022 21:03:59 - INFO - __main__ - Step 80 Global step 80 Train loss 1.26 on epoch=4
05/25/2022 21:04:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
05/25/2022 21:04:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=6
05/25/2022 21:04:08 - INFO - __main__ - Global step 100 Train loss 1.13 Classification-F1 0.4984008185150257 on epoch=6
05/25/2022 21:04:08 - INFO - __main__ - Saving model with best Classification-F1: 0.17786130829609093 -> 0.4984008185150257 on epoch=6, global_step=100
05/25/2022 21:04:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=6
05/25/2022 21:04:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=7
05/25/2022 21:04:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.72 on epoch=8
05/25/2022 21:04:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=8
05/25/2022 21:04:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=9
05/25/2022 21:04:24 - INFO - __main__ - Global step 150 Train loss 0.75 Classification-F1 0.5065206344998232 on epoch=9
05/25/2022 21:04:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4984008185150257 -> 0.5065206344998232 on epoch=9, global_step=150
05/25/2022 21:04:27 - INFO - __main__ - Step 160 Global step 160 Train loss 0.71 on epoch=9
05/25/2022 21:04:29 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=10
05/25/2022 21:04:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.67 on epoch=11
05/25/2022 21:04:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=11
05/25/2022 21:04:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.68 on epoch=12
05/25/2022 21:04:40 - INFO - __main__ - Global step 200 Train loss 0.69 Classification-F1 0.6098527082673424 on epoch=12
05/25/2022 21:04:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5065206344998232 -> 0.6098527082673424 on epoch=12, global_step=200
05/25/2022 21:04:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.66 on epoch=13
05/25/2022 21:04:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.62 on epoch=13
05/25/2022 21:04:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=14
05/25/2022 21:04:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=14
05/25/2022 21:04:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=15
05/25/2022 21:04:56 - INFO - __main__ - Global step 250 Train loss 0.59 Classification-F1 0.6231259187596588 on epoch=15
05/25/2022 21:04:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6098527082673424 -> 0.6231259187596588 on epoch=15, global_step=250
05/25/2022 21:04:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=16
05/25/2022 21:05:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=16
05/25/2022 21:05:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=17
05/25/2022 21:05:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=18
05/25/2022 21:05:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=18
05/25/2022 21:05:12 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.6659303898136466 on epoch=18
05/25/2022 21:05:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6231259187596588 -> 0.6659303898136466 on epoch=18, global_step=300
05/25/2022 21:05:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=19
05/25/2022 21:05:17 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=19
05/25/2022 21:05:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=20
05/25/2022 21:05:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=21
05/25/2022 21:05:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=21
05/25/2022 21:05:28 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.6468931866084358 on epoch=21
05/25/2022 21:05:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=22
05/25/2022 21:05:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/25/2022 21:05:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=23
05/25/2022 21:05:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=24
05/25/2022 21:05:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
05/25/2022 21:05:44 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.6927885341914514 on epoch=24
05/25/2022 21:05:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6659303898136466 -> 0.6927885341914514 on epoch=24, global_step=400
05/25/2022 21:05:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=25
05/25/2022 21:05:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=26
05/25/2022 21:05:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=26
05/25/2022 21:05:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
05/25/2022 21:05:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=28
05/25/2022 21:06:00 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.6917676805082376 on epoch=28
05/25/2022 21:06:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
05/25/2022 21:06:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=29
05/25/2022 21:06:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=29
05/25/2022 21:06:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=30
05/25/2022 21:06:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/25/2022 21:06:16 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.6947432715735551 on epoch=31
05/25/2022 21:06:16 - INFO - __main__ - Saving model with best Classification-F1: 0.6927885341914514 -> 0.6947432715735551 on epoch=31, global_step=500
05/25/2022 21:06:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=31
05/25/2022 21:06:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
05/25/2022 21:06:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
05/25/2022 21:06:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/25/2022 21:06:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
05/25/2022 21:06:32 - INFO - __main__ - Global step 550 Train loss 0.42 Classification-F1 0.7170356781584432 on epoch=34
05/25/2022 21:06:32 - INFO - __main__ - Saving model with best Classification-F1: 0.6947432715735551 -> 0.7170356781584432 on epoch=34, global_step=550
05/25/2022 21:06:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
05/25/2022 21:06:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=35
05/25/2022 21:06:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=36
05/25/2022 21:06:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=36
05/25/2022 21:06:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.31 on epoch=37
05/25/2022 21:06:48 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.7255159852591242 on epoch=37
05/25/2022 21:06:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7170356781584432 -> 0.7255159852591242 on epoch=37, global_step=600
05/25/2022 21:06:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=38
05/25/2022 21:06:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/25/2022 21:06:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.31 on epoch=39
05/25/2022 21:06:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/25/2022 21:07:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=40
05/25/2022 21:07:04 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.733601469426992 on epoch=40
05/25/2022 21:07:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7255159852591242 -> 0.733601469426992 on epoch=40, global_step=650
05/25/2022 21:07:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=41
05/25/2022 21:07:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=41
05/25/2022 21:07:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=42
05/25/2022 21:07:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
05/25/2022 21:07:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.32 on epoch=43
05/25/2022 21:07:20 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.7400568700402587 on epoch=43
05/25/2022 21:07:20 - INFO - __main__ - Saving model with best Classification-F1: 0.733601469426992 -> 0.7400568700402587 on epoch=43, global_step=700
05/25/2022 21:07:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=44
05/25/2022 21:07:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=44
05/25/2022 21:07:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=45
05/25/2022 21:07:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.18 on epoch=46
05/25/2022 21:07:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
05/25/2022 21:07:36 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.7256598579904678 on epoch=46
05/25/2022 21:07:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=47
05/25/2022 21:07:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
05/25/2022 21:07:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=48
05/25/2022 21:07:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/25/2022 21:07:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=49
05/25/2022 21:07:52 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.7190220060381888 on epoch=49
05/25/2022 21:07:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=50
05/25/2022 21:07:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.15 on epoch=51
05/25/2022 21:08:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=51
05/25/2022 21:08:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=52
05/25/2022 21:08:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=53
05/25/2022 21:08:08 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.7142243516697153 on epoch=53
05/25/2022 21:08:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.29 on epoch=53
05/25/2022 21:08:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=54
05/25/2022 21:08:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=54
05/25/2022 21:08:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.30 on epoch=55
05/25/2022 21:08:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=56
05/25/2022 21:08:24 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.7690231881408353 on epoch=56
05/25/2022 21:08:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7400568700402587 -> 0.7690231881408353 on epoch=56, global_step=900
05/25/2022 21:08:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=56
05/25/2022 21:08:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=57
05/25/2022 21:08:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=58
05/25/2022 21:08:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=58
05/25/2022 21:08:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=59
05/25/2022 21:08:40 - INFO - __main__ - Global step 950 Train loss 0.24 Classification-F1 0.7681844570153369 on epoch=59
05/25/2022 21:08:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
05/25/2022 21:08:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=60
05/25/2022 21:08:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
05/25/2022 21:08:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
05/25/2022 21:08:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=62
05/25/2022 21:08:56 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.728830449320736 on epoch=62
05/25/2022 21:08:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
05/25/2022 21:09:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=63
05/25/2022 21:09:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/25/2022 21:09:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/25/2022 21:09:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/25/2022 21:09:12 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.7792387752943133 on epoch=65
05/25/2022 21:09:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7690231881408353 -> 0.7792387752943133 on epoch=65, global_step=1050
05/25/2022 21:09:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.14 on epoch=66
05/25/2022 21:09:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=66
05/25/2022 21:09:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=67
05/25/2022 21:09:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=68
05/25/2022 21:09:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/25/2022 21:09:28 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7866432878060785 on epoch=68
05/25/2022 21:09:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7792387752943133 -> 0.7866432878060785 on epoch=68, global_step=1100
05/25/2022 21:09:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=69
05/25/2022 21:09:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=69
05/25/2022 21:09:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/25/2022 21:09:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=71
05/25/2022 21:09:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=71
05/25/2022 21:09:44 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.7574425794020119 on epoch=71
05/25/2022 21:09:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.12 on epoch=72
05/25/2022 21:09:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=73
05/25/2022 21:09:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=73
05/25/2022 21:09:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=74
05/25/2022 21:09:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
05/25/2022 21:10:00 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7521703011158003 on epoch=74
05/25/2022 21:10:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
05/25/2022 21:10:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
05/25/2022 21:10:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/25/2022 21:10:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=77
05/25/2022 21:10:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
05/25/2022 21:10:16 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.7592315663519682 on epoch=78
05/25/2022 21:10:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
05/25/2022 21:10:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=79
05/25/2022 21:10:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=79
05/25/2022 21:10:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/25/2022 21:10:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=81
05/25/2022 21:10:32 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.7570614957198476 on epoch=81
05/25/2022 21:10:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=81
05/25/2022 21:10:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.13 on epoch=82
05/25/2022 21:10:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/25/2022 21:10:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
05/25/2022 21:10:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=84
05/25/2022 21:10:48 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.7404223874135393 on epoch=84
05/25/2022 21:10:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
05/25/2022 21:10:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=85
05/25/2022 21:10:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=86
05/25/2022 21:10:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=86
05/25/2022 21:11:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=87
05/25/2022 21:11:04 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7760186298350314 on epoch=87
05/25/2022 21:11:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=88
05/25/2022 21:11:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=88
05/25/2022 21:11:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
05/25/2022 21:11:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=89
05/25/2022 21:11:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
05/25/2022 21:11:20 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.7398524856605899 on epoch=90
05/25/2022 21:11:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=91
05/25/2022 21:11:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/25/2022 21:11:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/25/2022 21:11:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=93
05/25/2022 21:11:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.09 on epoch=93
05/25/2022 21:11:36 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.7447396771864857 on epoch=93
05/25/2022 21:11:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.08 on epoch=94
05/25/2022 21:11:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=94
05/25/2022 21:11:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=95
05/25/2022 21:11:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=96
05/25/2022 21:11:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
05/25/2022 21:11:52 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.741170234425883 on epoch=96
05/25/2022 21:11:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=97
05/25/2022 21:11:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=98
05/25/2022 21:12:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=98
05/25/2022 21:12:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=99
05/25/2022 21:12:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=99
05/25/2022 21:12:08 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.7788480289407242 on epoch=99
05/25/2022 21:12:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/25/2022 21:12:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=101
05/25/2022 21:12:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
05/25/2022 21:12:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
05/25/2022 21:12:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=103
05/25/2022 21:12:24 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.7553506952957485 on epoch=103
05/25/2022 21:12:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=103
05/25/2022 21:12:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
05/25/2022 21:12:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
05/25/2022 21:12:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=105
05/25/2022 21:12:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=106
05/25/2022 21:12:40 - INFO - __main__ - Global step 1700 Train loss 0.14 Classification-F1 0.7662870505558865 on epoch=106
05/25/2022 21:12:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
05/25/2022 21:12:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=107
05/25/2022 21:12:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=108
05/25/2022 21:12:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=108
05/25/2022 21:12:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=109
05/25/2022 21:12:56 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.7452395488109774 on epoch=109
05/25/2022 21:12:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=109
05/25/2022 21:13:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=110
05/25/2022 21:13:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=111
05/25/2022 21:13:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/25/2022 21:13:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=112
05/25/2022 21:13:12 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.750380921599409 on epoch=112
05/25/2022 21:13:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=113
05/25/2022 21:13:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=113
05/25/2022 21:13:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
05/25/2022 21:13:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
05/25/2022 21:13:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/25/2022 21:13:28 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7477664190294608 on epoch=115
05/25/2022 21:13:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=116
05/25/2022 21:13:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=116
05/25/2022 21:13:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=117
05/25/2022 21:13:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
05/25/2022 21:13:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
05/25/2022 21:13:44 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.7454974457148668 on epoch=118
05/25/2022 21:13:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=119
05/25/2022 21:13:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=119
05/25/2022 21:13:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=120
05/25/2022 21:13:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=121
05/25/2022 21:13:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=121
05/25/2022 21:14:00 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.7433336679815553 on epoch=121
05/25/2022 21:14:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=122
05/25/2022 21:14:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/25/2022 21:14:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=123
05/25/2022 21:14:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=124
05/25/2022 21:14:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=124
05/25/2022 21:14:16 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.7597514965722331 on epoch=124
05/25/2022 21:14:19 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/25/2022 21:14:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=126
05/25/2022 21:14:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=126
05/25/2022 21:14:26 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=127
05/25/2022 21:14:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=128
05/25/2022 21:14:32 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.7631148244738508 on epoch=128
05/25/2022 21:14:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/25/2022 21:14:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.11 on epoch=129
05/25/2022 21:14:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=129
05/25/2022 21:14:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=130
05/25/2022 21:14:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
05/25/2022 21:14:48 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.7568971145263635 on epoch=131
05/25/2022 21:14:51 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
05/25/2022 21:14:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=132
05/25/2022 21:14:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.05 on epoch=133
05/25/2022 21:14:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
05/25/2022 21:15:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=134
05/25/2022 21:15:04 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.7354597063155843 on epoch=134
05/25/2022 21:15:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=134
05/25/2022 21:15:09 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
05/25/2022 21:15:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=136
05/25/2022 21:15:14 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=136
05/25/2022 21:15:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=137
05/25/2022 21:15:20 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.7679393213226228 on epoch=137
05/25/2022 21:15:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=138
05/25/2022 21:15:25 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=138
05/25/2022 21:15:28 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/25/2022 21:15:30 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=139
05/25/2022 21:15:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
05/25/2022 21:15:36 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7636588487515982 on epoch=140
05/25/2022 21:15:39 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=141
05/25/2022 21:15:41 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.06 on epoch=141
05/25/2022 21:15:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=142
05/25/2022 21:15:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
05/25/2022 21:15:49 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
05/25/2022 21:15:52 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7656849241073841 on epoch=143
05/25/2022 21:15:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/25/2022 21:15:57 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=144
05/25/2022 21:16:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
05/25/2022 21:16:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=146
05/25/2022 21:16:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/25/2022 21:16:08 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7400086649212269 on epoch=146
05/25/2022 21:16:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
05/25/2022 21:16:13 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.03 on epoch=148
05/25/2022 21:16:16 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=148
05/25/2022 21:16:18 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/25/2022 21:16:21 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
05/25/2022 21:16:24 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.736723054223899 on epoch=149
05/25/2022 21:16:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
05/25/2022 21:16:29 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
05/25/2022 21:16:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
05/25/2022 21:16:34 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
05/25/2022 21:16:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=153
05/25/2022 21:16:40 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.7527460262031325 on epoch=153
05/25/2022 21:16:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=153
05/25/2022 21:16:45 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=154
05/25/2022 21:16:48 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=154
05/25/2022 21:16:50 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/25/2022 21:16:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.01 on epoch=156
05/25/2022 21:16:56 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7425529515228727 on epoch=156
05/25/2022 21:16:59 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
05/25/2022 21:17:01 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 21:17:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/25/2022 21:17:06 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=158
05/25/2022 21:17:09 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=159
05/25/2022 21:17:12 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.749265742990388 on epoch=159
05/25/2022 21:17:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
05/25/2022 21:17:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=160
05/25/2022 21:17:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=161
05/25/2022 21:17:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
05/25/2022 21:17:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.05 on epoch=162
05/25/2022 21:17:28 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.7582701903480875 on epoch=162
05/25/2022 21:17:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
05/25/2022 21:17:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/25/2022 21:17:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/25/2022 21:17:38 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.03 on epoch=164
05/25/2022 21:17:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/25/2022 21:17:44 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.754565719417538 on epoch=165
05/25/2022 21:17:47 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
05/25/2022 21:17:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
05/25/2022 21:17:52 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
05/25/2022 21:17:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.03 on epoch=168
05/25/2022 21:17:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/25/2022 21:18:00 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7353903997441693 on epoch=168
05/25/2022 21:18:03 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=169
05/25/2022 21:18:05 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.01 on epoch=169
05/25/2022 21:18:08 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
05/25/2022 21:18:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/25/2022 21:18:13 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
05/25/2022 21:18:16 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7598662605373682 on epoch=171
05/25/2022 21:18:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/25/2022 21:18:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/25/2022 21:18:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
05/25/2022 21:18:26 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/25/2022 21:18:29 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/25/2022 21:18:32 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.6022601294112266 on epoch=174
05/25/2022 21:18:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/25/2022 21:18:37 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=176
05/25/2022 21:18:40 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/25/2022 21:18:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
05/25/2022 21:18:45 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=178
05/25/2022 21:18:48 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7402433485208186 on epoch=178
05/25/2022 21:18:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=178
05/25/2022 21:18:53 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
05/25/2022 21:18:56 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=179
05/25/2022 21:18:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/25/2022 21:19:01 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=181
05/25/2022 21:19:04 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.5995752911852026 on epoch=181
05/25/2022 21:19:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=181
05/25/2022 21:19:09 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=182
05/25/2022 21:19:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/25/2022 21:19:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
05/25/2022 21:19:17 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/25/2022 21:19:20 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.7296053231857142 on epoch=184
05/25/2022 21:19:23 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
05/25/2022 21:19:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/25/2022 21:19:28 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
05/25/2022 21:19:31 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=186
05/25/2022 21:19:33 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/25/2022 21:19:34 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:19:34 - INFO - __main__ - Printing 3 examples
05/25/2022 21:19:34 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:19:34 - INFO - __main__ - ['sad']
05/25/2022 21:19:34 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:19:34 - INFO - __main__ - ['sad']
05/25/2022 21:19:34 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:19:34 - INFO - __main__ - ['sad']
05/25/2022 21:19:34 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:19:35 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:19:35 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:19:35 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:19:35 - INFO - __main__ - Printing 3 examples
05/25/2022 21:19:35 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:19:35 - INFO - __main__ - ['sad']
05/25/2022 21:19:35 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:19:35 - INFO - __main__ - ['sad']
05/25/2022 21:19:35 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:19:35 - INFO - __main__ - ['sad']
05/25/2022 21:19:35 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:19:35 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:19:35 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:19:36 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.7452840044401406 on epoch=187
05/25/2022 21:19:36 - INFO - __main__ - save last model!
05/25/2022 21:19:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 21:19:37 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 21:19:37 - INFO - __main__ - Printing 3 examples
05/25/2022 21:19:37 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 21:19:37 - INFO - __main__ - ['others']
05/25/2022 21:19:37 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 21:19:37 - INFO - __main__ - ['others']
05/25/2022 21:19:37 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 21:19:37 - INFO - __main__ - ['others']
05/25/2022 21:19:37 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:19:39 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:19:44 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 21:19:54 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:19:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:19:55 - INFO - __main__ - Starting training!
05/25/2022 21:20:57 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_21_0.4_8_predictions.txt
05/25/2022 21:20:57 - INFO - __main__ - Classification-F1 on test data: 0.3729
05/25/2022 21:20:58 - INFO - __main__ - prefix=emo_64_21, lr=0.4, bsz=8, dev_performance=0.7866432878060785, test_performance=0.37292120831539943
05/25/2022 21:20:58 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.3, bsz=8 ...
05/25/2022 21:20:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:20:59 - INFO - __main__ - Printing 3 examples
05/25/2022 21:20:59 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:20:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:20:59 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:20:59 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:20:59 - INFO - __main__ - Printing 3 examples
05/25/2022 21:20:59 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:20:59 - INFO - __main__ - ['sad']
05/25/2022 21:20:59 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:20:59 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:20:59 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:21:15 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:21:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:21:16 - INFO - __main__ - Starting training!
05/25/2022 21:21:19 - INFO - __main__ - Step 10 Global step 10 Train loss 4.36 on epoch=0
05/25/2022 21:21:22 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=1
05/25/2022 21:21:24 - INFO - __main__ - Step 30 Global step 30 Train loss 2.58 on epoch=1
05/25/2022 21:21:27 - INFO - __main__ - Step 40 Global step 40 Train loss 2.20 on epoch=2
05/25/2022 21:21:29 - INFO - __main__ - Step 50 Global step 50 Train loss 1.91 on epoch=3
05/25/2022 21:21:33 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.079570629210917 on epoch=3
05/25/2022 21:21:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.079570629210917 on epoch=3, global_step=50
05/25/2022 21:21:35 - INFO - __main__ - Step 60 Global step 60 Train loss 1.78 on epoch=3
05/25/2022 21:21:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=4
05/25/2022 21:21:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.25 on epoch=4
05/25/2022 21:21:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.23 on epoch=5
05/25/2022 21:21:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=6
05/25/2022 21:21:49 - INFO - __main__ - Global step 100 Train loss 1.32 Classification-F1 0.47388443419894555 on epoch=6
05/25/2022 21:21:49 - INFO - __main__ - Saving model with best Classification-F1: 0.079570629210917 -> 0.47388443419894555 on epoch=6, global_step=100
05/25/2022 21:21:51 - INFO - __main__ - Step 110 Global step 110 Train loss 1.03 on epoch=6
05/25/2022 21:21:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.81 on epoch=7
05/25/2022 21:21:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=8
05/25/2022 21:21:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=8
05/25/2022 21:22:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=9
05/25/2022 21:22:05 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.49708051788704566 on epoch=9
05/25/2022 21:22:05 - INFO - __main__ - Saving model with best Classification-F1: 0.47388443419894555 -> 0.49708051788704566 on epoch=9, global_step=150
05/25/2022 21:22:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=9
05/25/2022 21:22:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=10
05/25/2022 21:22:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.63 on epoch=11
05/25/2022 21:22:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=11
05/25/2022 21:22:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=12
05/25/2022 21:22:21 - INFO - __main__ - Global step 200 Train loss 0.75 Classification-F1 0.5356770833333333 on epoch=12
05/25/2022 21:22:21 - INFO - __main__ - Saving model with best Classification-F1: 0.49708051788704566 -> 0.5356770833333333 on epoch=12, global_step=200
05/25/2022 21:22:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.64 on epoch=13
05/25/2022 21:22:26 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=13
05/25/2022 21:22:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=14
05/25/2022 21:22:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.62 on epoch=14
05/25/2022 21:22:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=15
05/25/2022 21:22:37 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.624030272163705 on epoch=15
05/25/2022 21:22:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5356770833333333 -> 0.624030272163705 on epoch=15, global_step=250
05/25/2022 21:22:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=16
05/25/2022 21:22:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=16
05/25/2022 21:22:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.57 on epoch=17
05/25/2022 21:22:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=18
05/25/2022 21:22:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=18
05/25/2022 21:22:53 - INFO - __main__ - Global step 300 Train loss 0.59 Classification-F1 0.6301513816093257 on epoch=18
05/25/2022 21:22:53 - INFO - __main__ - Saving model with best Classification-F1: 0.624030272163705 -> 0.6301513816093257 on epoch=18, global_step=300
05/25/2022 21:22:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.52 on epoch=19
05/25/2022 21:22:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=19
05/25/2022 21:23:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=20
05/25/2022 21:23:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=21
05/25/2022 21:23:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.69 on epoch=21
05/25/2022 21:23:09 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.6146033485341846 on epoch=21
05/25/2022 21:23:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=22
05/25/2022 21:23:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=23
05/25/2022 21:23:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=23
05/25/2022 21:23:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=24
05/25/2022 21:23:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=24
05/25/2022 21:23:24 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.6366187439250033 on epoch=24
05/25/2022 21:23:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6301513816093257 -> 0.6366187439250033 on epoch=24, global_step=400
05/25/2022 21:23:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=25
05/25/2022 21:23:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=26
05/25/2022 21:23:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.65 on epoch=26
05/25/2022 21:23:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=27
05/25/2022 21:23:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=28
05/25/2022 21:23:40 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.638999785960896 on epoch=28
05/25/2022 21:23:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6366187439250033 -> 0.638999785960896 on epoch=28, global_step=450
05/25/2022 21:23:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=28
05/25/2022 21:23:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=29
05/25/2022 21:23:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=29
05/25/2022 21:23:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=30
05/25/2022 21:23:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/25/2022 21:23:56 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.6627477595937442 on epoch=31
05/25/2022 21:23:56 - INFO - __main__ - Saving model with best Classification-F1: 0.638999785960896 -> 0.6627477595937442 on epoch=31, global_step=500
05/25/2022 21:23:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.55 on epoch=31
05/25/2022 21:24:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
05/25/2022 21:24:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=33
05/25/2022 21:24:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
05/25/2022 21:24:09 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=34
05/25/2022 21:24:12 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.6590578007025456 on epoch=34
05/25/2022 21:24:15 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=34
05/25/2022 21:24:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=35
05/25/2022 21:24:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=36
05/25/2022 21:24:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=36
05/25/2022 21:24:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/25/2022 21:24:28 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.6937639877508364 on epoch=37
05/25/2022 21:24:28 - INFO - __main__ - Saving model with best Classification-F1: 0.6627477595937442 -> 0.6937639877508364 on epoch=37, global_step=600
05/25/2022 21:24:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=38
05/25/2022 21:24:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=38
05/25/2022 21:24:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=39
05/25/2022 21:24:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=39
05/25/2022 21:24:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
05/25/2022 21:24:44 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.7090800252641899 on epoch=40
05/25/2022 21:24:44 - INFO - __main__ - Saving model with best Classification-F1: 0.6937639877508364 -> 0.7090800252641899 on epoch=40, global_step=650
05/25/2022 21:24:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=41
05/25/2022 21:24:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=41
05/25/2022 21:24:52 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=42
05/25/2022 21:24:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.31 on epoch=43
05/25/2022 21:24:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=43
05/25/2022 21:25:00 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.7288453793877907 on epoch=43
05/25/2022 21:25:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7090800252641899 -> 0.7288453793877907 on epoch=43, global_step=700
05/25/2022 21:25:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=44
05/25/2022 21:25:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.50 on epoch=44
05/25/2022 21:25:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=45
05/25/2022 21:25:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=46
05/25/2022 21:25:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=46
05/25/2022 21:25:16 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.7218292697679083 on epoch=46
05/25/2022 21:25:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=47
05/25/2022 21:25:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=48
05/25/2022 21:25:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=48
05/25/2022 21:25:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=49
05/25/2022 21:25:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=49
05/25/2022 21:25:32 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7185590863074678 on epoch=49
05/25/2022 21:25:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=50
05/25/2022 21:25:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=51
05/25/2022 21:25:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=51
05/25/2022 21:25:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=52
05/25/2022 21:25:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=53
05/25/2022 21:25:48 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.7228073321633981 on epoch=53
05/25/2022 21:25:50 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
05/25/2022 21:25:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=54
05/25/2022 21:25:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
05/25/2022 21:25:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=55
05/25/2022 21:26:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/25/2022 21:26:04 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.7232405251483129 on epoch=56
05/25/2022 21:26:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
05/25/2022 21:26:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=57
05/25/2022 21:26:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=58
05/25/2022 21:26:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=58
05/25/2022 21:26:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=59
05/25/2022 21:26:20 - INFO - __main__ - Global step 950 Train loss 0.30 Classification-F1 0.7279045359402503 on epoch=59
05/25/2022 21:26:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=59
05/25/2022 21:26:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
05/25/2022 21:26:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=61
05/25/2022 21:26:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=61
05/25/2022 21:26:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=62
05/25/2022 21:26:36 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.7294492435668907 on epoch=62
05/25/2022 21:26:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7288453793877907 -> 0.7294492435668907 on epoch=62, global_step=1000
05/25/2022 21:26:38 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
05/25/2022 21:26:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
05/25/2022 21:26:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=64
05/25/2022 21:26:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
05/25/2022 21:26:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
05/25/2022 21:26:52 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.7457955351992278 on epoch=65
05/25/2022 21:26:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7294492435668907 -> 0.7457955351992278 on epoch=65, global_step=1050
05/25/2022 21:26:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=66
05/25/2022 21:26:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=66
05/25/2022 21:26:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=67
05/25/2022 21:27:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=68
05/25/2022 21:27:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=68
05/25/2022 21:27:08 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.7320545957003431 on epoch=68
05/25/2022 21:27:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
05/25/2022 21:27:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
05/25/2022 21:27:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=70
05/25/2022 21:27:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
05/25/2022 21:27:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=71
05/25/2022 21:27:24 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.7292886354579462 on epoch=71
05/25/2022 21:27:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
05/25/2022 21:27:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=73
05/25/2022 21:27:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=73
05/25/2022 21:27:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/25/2022 21:27:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
05/25/2022 21:27:40 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.701199987161952 on epoch=74
05/25/2022 21:27:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=75
05/25/2022 21:27:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
05/25/2022 21:27:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
05/25/2022 21:27:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/25/2022 21:27:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=78
05/25/2022 21:27:56 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.7200140013032896 on epoch=78
05/25/2022 21:27:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=78
05/25/2022 21:28:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=79
05/25/2022 21:28:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=79
05/25/2022 21:28:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=80
05/25/2022 21:28:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=81
05/25/2022 21:28:12 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.7251391975932286 on epoch=81
05/25/2022 21:28:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=81
05/25/2022 21:28:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=82
05/25/2022 21:28:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
05/25/2022 21:28:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=83
05/25/2022 21:28:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
05/25/2022 21:28:28 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.739412121151932 on epoch=84
05/25/2022 21:28:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=84
05/25/2022 21:28:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/25/2022 21:28:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=86
05/25/2022 21:28:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=86
05/25/2022 21:28:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
05/25/2022 21:28:43 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.747806428507476 on epoch=87
05/25/2022 21:28:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7457955351992278 -> 0.747806428507476 on epoch=87, global_step=1400
05/25/2022 21:28:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=88
05/25/2022 21:28:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
05/25/2022 21:28:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
05/25/2022 21:28:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=89
05/25/2022 21:28:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/25/2022 21:28:59 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7480808766086922 on epoch=90
05/25/2022 21:28:59 - INFO - __main__ - Saving model with best Classification-F1: 0.747806428507476 -> 0.7480808766086922 on epoch=90, global_step=1450
05/25/2022 21:29:02 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=91
05/25/2022 21:29:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=91
05/25/2022 21:29:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
05/25/2022 21:29:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=93
05/25/2022 21:29:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=93
05/25/2022 21:29:15 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.7447359884420148 on epoch=93
05/25/2022 21:29:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=94
05/25/2022 21:29:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
05/25/2022 21:29:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=95
05/25/2022 21:29:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=96
05/25/2022 21:29:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=96
05/25/2022 21:29:31 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.7307571147581289 on epoch=96
05/25/2022 21:29:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=97
05/25/2022 21:29:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
05/25/2022 21:29:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/25/2022 21:29:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/25/2022 21:29:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=99
05/25/2022 21:29:47 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.727936851776887 on epoch=99
05/25/2022 21:29:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/25/2022 21:29:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=101
05/25/2022 21:29:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=101
05/25/2022 21:29:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=102
05/25/2022 21:30:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
05/25/2022 21:30:03 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.7331945278111245 on epoch=103
05/25/2022 21:30:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=103
05/25/2022 21:30:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=104
05/25/2022 21:30:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=104
05/25/2022 21:30:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=105
05/25/2022 21:30:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=106
05/25/2022 21:30:19 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.7623659472738546 on epoch=106
05/25/2022 21:30:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7480808766086922 -> 0.7623659472738546 on epoch=106, global_step=1700
05/25/2022 21:30:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
05/25/2022 21:30:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=107
05/25/2022 21:30:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=108
05/25/2022 21:30:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
05/25/2022 21:30:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=109
05/25/2022 21:30:35 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.7644039696585658 on epoch=109
05/25/2022 21:30:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7623659472738546 -> 0.7644039696585658 on epoch=109, global_step=1750
05/25/2022 21:30:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=109
05/25/2022 21:30:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=110
05/25/2022 21:30:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=111
05/25/2022 21:30:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=111
05/25/2022 21:30:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
05/25/2022 21:30:51 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.7787906691263735 on epoch=112
05/25/2022 21:30:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7644039696585658 -> 0.7787906691263735 on epoch=112, global_step=1800
05/25/2022 21:30:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=113
05/25/2022 21:30:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.21 on epoch=113
05/25/2022 21:30:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
05/25/2022 21:31:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
05/25/2022 21:31:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=115
05/25/2022 21:31:07 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7529064968215265 on epoch=115
05/25/2022 21:31:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.15 on epoch=116
05/25/2022 21:31:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
05/25/2022 21:31:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
05/25/2022 21:31:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=118
05/25/2022 21:31:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
05/25/2022 21:31:23 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.7491912978486587 on epoch=118
05/25/2022 21:31:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=119
05/25/2022 21:31:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=119
05/25/2022 21:31:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=120
05/25/2022 21:31:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=121
05/25/2022 21:31:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
05/25/2022 21:31:39 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7727543032446238 on epoch=121
05/25/2022 21:31:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/25/2022 21:31:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/25/2022 21:31:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=123
05/25/2022 21:31:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=124
05/25/2022 21:31:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
05/25/2022 21:31:55 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.7659196218844377 on epoch=124
05/25/2022 21:31:58 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=125
05/25/2022 21:32:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=126
05/25/2022 21:32:03 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
05/25/2022 21:32:05 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/25/2022 21:32:08 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
05/25/2022 21:32:11 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.754178363045252 on epoch=128
05/25/2022 21:32:14 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.16 on epoch=128
05/25/2022 21:32:16 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
05/25/2022 21:32:19 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=129
05/25/2022 21:32:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
05/25/2022 21:32:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=131
05/25/2022 21:32:27 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7546602779085286 on epoch=131
05/25/2022 21:32:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
05/25/2022 21:32:32 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=132
05/25/2022 21:32:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=133
05/25/2022 21:32:37 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=133
05/25/2022 21:32:40 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
05/25/2022 21:32:43 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.7524431943422816 on epoch=134
05/25/2022 21:32:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
05/25/2022 21:32:48 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=135
05/25/2022 21:32:51 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=136
05/25/2022 21:32:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/25/2022 21:32:56 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=137
05/25/2022 21:32:59 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.7368492510951384 on epoch=137
05/25/2022 21:33:02 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
05/25/2022 21:33:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/25/2022 21:33:07 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=139
05/25/2022 21:33:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
05/25/2022 21:33:12 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=140
05/25/2022 21:33:15 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7534782270396081 on epoch=140
05/25/2022 21:33:18 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=141
05/25/2022 21:33:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
05/25/2022 21:33:23 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=142
05/25/2022 21:33:26 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=143
05/25/2022 21:33:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=143
05/25/2022 21:33:32 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.7505390530316401 on epoch=143
05/25/2022 21:33:34 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.07 on epoch=144
05/25/2022 21:33:37 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=144
05/25/2022 21:33:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=145
05/25/2022 21:33:42 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=146
05/25/2022 21:33:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=146
05/25/2022 21:33:48 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.7436645857706903 on epoch=146
05/25/2022 21:33:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
05/25/2022 21:33:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
05/25/2022 21:33:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=148
05/25/2022 21:33:58 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=149
05/25/2022 21:34:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
05/25/2022 21:34:04 - INFO - __main__ - Global step 2400 Train loss 0.14 Classification-F1 0.7665341993209039 on epoch=149
05/25/2022 21:34:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
05/25/2022 21:34:09 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/25/2022 21:34:12 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
05/25/2022 21:34:14 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
05/25/2022 21:34:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
05/25/2022 21:34:20 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.7539677836497697 on epoch=153
05/25/2022 21:34:23 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.16 on epoch=153
05/25/2022 21:34:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=154
05/25/2022 21:34:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
05/25/2022 21:34:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/25/2022 21:34:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=156
05/25/2022 21:34:36 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7620013125992287 on epoch=156
05/25/2022 21:34:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
05/25/2022 21:34:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
05/25/2022 21:34:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.07 on epoch=158
05/25/2022 21:34:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
05/25/2022 21:34:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
05/25/2022 21:34:53 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.752462947743318 on epoch=159
05/25/2022 21:34:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
05/25/2022 21:34:58 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.11 on epoch=160
05/25/2022 21:35:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/25/2022 21:35:03 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.11 on epoch=161
05/25/2022 21:35:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
05/25/2022 21:35:09 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7299903550421739 on epoch=162
05/25/2022 21:35:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/25/2022 21:35:14 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
05/25/2022 21:35:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/25/2022 21:35:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=164
05/25/2022 21:35:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/25/2022 21:35:25 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.7596176592136578 on epoch=165
05/25/2022 21:35:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=166
05/25/2022 21:35:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
05/25/2022 21:35:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=167
05/25/2022 21:35:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
05/25/2022 21:35:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/25/2022 21:35:41 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.7683016122299264 on epoch=168
05/25/2022 21:35:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=169
05/25/2022 21:35:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=169
05/25/2022 21:35:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
05/25/2022 21:35:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=171
05/25/2022 21:35:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=171
05/25/2022 21:35:57 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.754872641864905 on epoch=171
05/25/2022 21:36:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/25/2022 21:36:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
05/25/2022 21:36:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
05/25/2022 21:36:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/25/2022 21:36:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/25/2022 21:36:13 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.7434150143579926 on epoch=174
05/25/2022 21:36:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
05/25/2022 21:36:18 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/25/2022 21:36:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/25/2022 21:36:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=177
05/25/2022 21:36:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/25/2022 21:36:29 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7424291852419225 on epoch=178
05/25/2022 21:36:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=178
05/25/2022 21:36:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=179
05/25/2022 21:36:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
05/25/2022 21:36:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=180
05/25/2022 21:36:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
05/25/2022 21:36:46 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.7359577039616179 on epoch=181
05/25/2022 21:36:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
05/25/2022 21:36:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=182
05/25/2022 21:36:53 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/25/2022 21:36:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 21:36:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/25/2022 21:37:02 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7466685237313316 on epoch=184
05/25/2022 21:37:04 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=184
05/25/2022 21:37:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/25/2022 21:37:09 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
05/25/2022 21:37:12 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
05/25/2022 21:37:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
05/25/2022 21:37:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:37:16 - INFO - __main__ - Printing 3 examples
05/25/2022 21:37:16 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:37:16 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:37:16 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:37:16 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:37:16 - INFO - __main__ - Printing 3 examples
05/25/2022 21:37:16 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:37:16 - INFO - __main__ - ['sad']
05/25/2022 21:37:16 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:37:16 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:37:16 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:37:18 - INFO - __main__ - Global step 3000 Train loss 0.08 Classification-F1 0.7413222610636285 on epoch=187
05/25/2022 21:37:18 - INFO - __main__ - save last model!
05/25/2022 21:37:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 21:37:18 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 21:37:18 - INFO - __main__ - Printing 3 examples
05/25/2022 21:37:18 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 21:37:18 - INFO - __main__ - ['others']
05/25/2022 21:37:18 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 21:37:18 - INFO - __main__ - ['others']
05/25/2022 21:37:18 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 21:37:18 - INFO - __main__ - ['others']
05/25/2022 21:37:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:37:20 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:37:26 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 21:37:36 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:37:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:37:36 - INFO - __main__ - Starting training!
05/25/2022 21:38:40 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_21_0.3_8_predictions.txt
05/25/2022 21:38:40 - INFO - __main__ - Classification-F1 on test data: 0.4415
05/25/2022 21:38:41 - INFO - __main__ - prefix=emo_64_21, lr=0.3, bsz=8, dev_performance=0.7787906691263735, test_performance=0.4414896620445701
05/25/2022 21:38:41 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.2, bsz=8 ...
05/25/2022 21:38:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:38:42 - INFO - __main__ - Printing 3 examples
05/25/2022 21:38:42 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:38:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:38:42 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:38:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:38:42 - INFO - __main__ - Printing 3 examples
05/25/2022 21:38:42 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
05/25/2022 21:38:42 - INFO - __main__ - ['sad']
05/25/2022 21:38:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:38:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:38:42 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:38:58 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:38:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:38:59 - INFO - __main__ - Starting training!
05/25/2022 21:39:02 - INFO - __main__ - Step 10 Global step 10 Train loss 4.62 on epoch=0
05/25/2022 21:39:05 - INFO - __main__ - Step 20 Global step 20 Train loss 3.23 on epoch=1
05/25/2022 21:39:07 - INFO - __main__ - Step 30 Global step 30 Train loss 2.98 on epoch=1
05/25/2022 21:39:10 - INFO - __main__ - Step 40 Global step 40 Train loss 2.46 on epoch=2
05/25/2022 21:39:12 - INFO - __main__ - Step 50 Global step 50 Train loss 2.20 on epoch=3
05/25/2022 21:39:16 - INFO - __main__ - Global step 50 Train loss 3.10 Classification-F1 0.008179603202227635 on epoch=3
05/25/2022 21:39:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.008179603202227635 on epoch=3, global_step=50
05/25/2022 21:39:19 - INFO - __main__ - Step 60 Global step 60 Train loss 2.24 on epoch=3
05/25/2022 21:39:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.85 on epoch=4
05/25/2022 21:39:24 - INFO - __main__ - Step 80 Global step 80 Train loss 1.87 on epoch=4
05/25/2022 21:39:26 - INFO - __main__ - Step 90 Global step 90 Train loss 1.82 on epoch=5
05/25/2022 21:39:29 - INFO - __main__ - Step 100 Global step 100 Train loss 1.34 on epoch=6
05/25/2022 21:39:32 - INFO - __main__ - Global step 100 Train loss 1.82 Classification-F1 0.16468631137882336 on epoch=6
05/25/2022 21:39:32 - INFO - __main__ - Saving model with best Classification-F1: 0.008179603202227635 -> 0.16468631137882336 on epoch=6, global_step=100
05/25/2022 21:39:35 - INFO - __main__ - Step 110 Global step 110 Train loss 1.41 on epoch=6
05/25/2022 21:39:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.17 on epoch=7
05/25/2022 21:39:40 - INFO - __main__ - Step 130 Global step 130 Train loss 1.02 on epoch=8
05/25/2022 21:39:42 - INFO - __main__ - Step 140 Global step 140 Train loss 1.20 on epoch=8
05/25/2022 21:39:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.95 on epoch=9
05/25/2022 21:39:48 - INFO - __main__ - Global step 150 Train loss 1.15 Classification-F1 0.4025475526233066 on epoch=9
05/25/2022 21:39:48 - INFO - __main__ - Saving model with best Classification-F1: 0.16468631137882336 -> 0.4025475526233066 on epoch=9, global_step=150
05/25/2022 21:39:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.85 on epoch=9
05/25/2022 21:39:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.85 on epoch=10
05/25/2022 21:39:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=11
05/25/2022 21:39:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.83 on epoch=11
05/25/2022 21:40:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.66 on epoch=12
05/25/2022 21:40:04 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.49852789631143035 on epoch=12
05/25/2022 21:40:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4025475526233066 -> 0.49852789631143035 on epoch=12, global_step=200
05/25/2022 21:40:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=13
05/25/2022 21:40:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.84 on epoch=13
05/25/2022 21:40:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.75 on epoch=14
05/25/2022 21:40:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=14
05/25/2022 21:40:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.86 on epoch=15
05/25/2022 21:40:20 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.5215515827456125 on epoch=15
05/25/2022 21:40:20 - INFO - __main__ - Saving model with best Classification-F1: 0.49852789631143035 -> 0.5215515827456125 on epoch=15, global_step=250
05/25/2022 21:40:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.73 on epoch=16
05/25/2022 21:40:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=16
05/25/2022 21:40:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=17
05/25/2022 21:40:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=18
05/25/2022 21:40:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.78 on epoch=18
05/25/2022 21:40:36 - INFO - __main__ - Global step 300 Train loss 0.69 Classification-F1 0.5849845835420575 on epoch=18
05/25/2022 21:40:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5215515827456125 -> 0.5849845835420575 on epoch=18, global_step=300
05/25/2022 21:40:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=19
05/25/2022 21:40:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=19
05/25/2022 21:40:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.85 on epoch=20
05/25/2022 21:40:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=21
05/25/2022 21:40:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.76 on epoch=21
05/25/2022 21:40:52 - INFO - __main__ - Global step 350 Train loss 0.68 Classification-F1 0.5811771331262567 on epoch=21
05/25/2022 21:40:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=22
05/25/2022 21:40:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=23
05/25/2022 21:41:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.66 on epoch=23
05/25/2022 21:41:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=24
05/25/2022 21:41:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.66 on epoch=24
05/25/2022 21:41:08 - INFO - __main__ - Global step 400 Train loss 0.61 Classification-F1 0.5696567270252471 on epoch=24
05/25/2022 21:41:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=25
05/25/2022 21:41:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=26
05/25/2022 21:41:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.60 on epoch=26
05/25/2022 21:41:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=27
05/25/2022 21:41:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=28
05/25/2022 21:41:24 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.6226013667557652 on epoch=28
05/25/2022 21:41:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5849845835420575 -> 0.6226013667557652 on epoch=28, global_step=450
05/25/2022 21:41:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.50 on epoch=28
05/25/2022 21:41:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=29
05/25/2022 21:41:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.60 on epoch=29
05/25/2022 21:41:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=30
05/25/2022 21:41:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=31
05/25/2022 21:41:40 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.6387709991158267 on epoch=31
05/25/2022 21:41:40 - INFO - __main__ - Saving model with best Classification-F1: 0.6226013667557652 -> 0.6387709991158267 on epoch=31, global_step=500
05/25/2022 21:41:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.55 on epoch=31
05/25/2022 21:41:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=32
05/25/2022 21:41:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=33
05/25/2022 21:41:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.59 on epoch=33
05/25/2022 21:41:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=34
05/25/2022 21:41:56 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.6491829452461154 on epoch=34
05/25/2022 21:41:56 - INFO - __main__ - Saving model with best Classification-F1: 0.6387709991158267 -> 0.6491829452461154 on epoch=34, global_step=550
05/25/2022 21:41:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=34
05/25/2022 21:42:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=35
05/25/2022 21:42:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=36
05/25/2022 21:42:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=36
05/25/2022 21:42:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=37
05/25/2022 21:42:11 - INFO - __main__ - Global step 600 Train loss 0.54 Classification-F1 0.6889763858431244 on epoch=37
05/25/2022 21:42:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6491829452461154 -> 0.6889763858431244 on epoch=37, global_step=600
05/25/2022 21:42:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=38
05/25/2022 21:42:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=38
05/25/2022 21:42:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=39
05/25/2022 21:42:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=39
05/25/2022 21:42:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=40
05/25/2022 21:42:27 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.6870150067990651 on epoch=40
05/25/2022 21:42:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=41
05/25/2022 21:42:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=41
05/25/2022 21:42:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
05/25/2022 21:42:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=43
05/25/2022 21:42:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=43
05/25/2022 21:42:43 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.6976993424149682 on epoch=43
05/25/2022 21:42:43 - INFO - __main__ - Saving model with best Classification-F1: 0.6889763858431244 -> 0.6976993424149682 on epoch=43, global_step=700
05/25/2022 21:42:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=44
05/25/2022 21:42:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=44
05/25/2022 21:42:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=45
05/25/2022 21:42:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.35 on epoch=46
05/25/2022 21:42:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=46
05/25/2022 21:42:59 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.6812397486589067 on epoch=46
05/25/2022 21:43:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=47
05/25/2022 21:43:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
05/25/2022 21:43:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=48
05/25/2022 21:43:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=49
05/25/2022 21:43:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=49
05/25/2022 21:43:14 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.7141860616922596 on epoch=49
05/25/2022 21:43:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6976993424149682 -> 0.7141860616922596 on epoch=49, global_step=800
05/25/2022 21:43:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.55 on epoch=50
05/25/2022 21:43:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
05/25/2022 21:43:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=51
05/25/2022 21:43:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=52
05/25/2022 21:43:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
05/25/2022 21:43:30 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.6900651997138448 on epoch=53
05/25/2022 21:43:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=53
05/25/2022 21:43:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=54
05/25/2022 21:43:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.49 on epoch=54
05/25/2022 21:43:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=55
05/25/2022 21:43:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
05/25/2022 21:43:46 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.7088622719589672 on epoch=56
05/25/2022 21:43:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=56
05/25/2022 21:43:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=57
05/25/2022 21:43:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=58
05/25/2022 21:43:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=58
05/25/2022 21:43:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
05/25/2022 21:44:02 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.7051546093441988 on epoch=59
05/25/2022 21:44:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=59
05/25/2022 21:44:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=60
05/25/2022 21:44:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=61
05/25/2022 21:44:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=61
05/25/2022 21:44:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
05/25/2022 21:44:18 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.7220316499377964 on epoch=62
05/25/2022 21:44:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7141860616922596 -> 0.7220316499377964 on epoch=62, global_step=1000
05/25/2022 21:44:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=63
05/25/2022 21:44:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=63
05/25/2022 21:44:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.33 on epoch=64
05/25/2022 21:44:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=64
05/25/2022 21:44:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=65
05/25/2022 21:44:34 - INFO - __main__ - Global step 1050 Train loss 0.39 Classification-F1 0.7322574965974165 on epoch=65
05/25/2022 21:44:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7220316499377964 -> 0.7322574965974165 on epoch=65, global_step=1050
05/25/2022 21:44:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
05/25/2022 21:44:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=66
05/25/2022 21:44:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=67
05/25/2022 21:44:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=68
05/25/2022 21:44:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
05/25/2022 21:44:50 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.7148638317282775 on epoch=68
05/25/2022 21:44:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=69
05/25/2022 21:44:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=69
05/25/2022 21:44:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=70
05/25/2022 21:44:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
05/25/2022 21:45:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=71
05/25/2022 21:45:05 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.6977105328724034 on epoch=71
05/25/2022 21:45:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=72
05/25/2022 21:45:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
05/25/2022 21:45:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
05/25/2022 21:45:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=74
05/25/2022 21:45:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
05/25/2022 21:45:21 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7154111644657863 on epoch=74
05/25/2022 21:45:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=75
05/25/2022 21:45:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=76
05/25/2022 21:45:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=76
05/25/2022 21:45:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=77
05/25/2022 21:45:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.28 on epoch=78
05/25/2022 21:45:37 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.7067133406915258 on epoch=78
05/25/2022 21:45:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=78
05/25/2022 21:45:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=79
05/25/2022 21:45:44 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
05/25/2022 21:45:47 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=80
05/25/2022 21:45:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.30 on epoch=81
05/25/2022 21:45:53 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.71361797565151 on epoch=81
05/25/2022 21:45:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=81
05/25/2022 21:45:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=82
05/25/2022 21:46:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=83
05/25/2022 21:46:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
05/25/2022 21:46:05 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/25/2022 21:46:08 - INFO - __main__ - Global step 1350 Train loss 0.33 Classification-F1 0.736384370770856 on epoch=84
05/25/2022 21:46:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7322574965974165 -> 0.736384370770856 on epoch=84, global_step=1350
05/25/2022 21:46:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=84
05/25/2022 21:46:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.37 on epoch=85
05/25/2022 21:46:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/25/2022 21:46:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=86
05/25/2022 21:46:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=87
05/25/2022 21:46:24 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7405141432229758 on epoch=87
05/25/2022 21:46:24 - INFO - __main__ - Saving model with best Classification-F1: 0.736384370770856 -> 0.7405141432229758 on epoch=87, global_step=1400
05/25/2022 21:46:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=88
05/25/2022 21:46:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=88
05/25/2022 21:46:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=89
05/25/2022 21:46:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=89
05/25/2022 21:46:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=90
05/25/2022 21:46:40 - INFO - __main__ - Global step 1450 Train loss 0.28 Classification-F1 0.7353261186073183 on epoch=90
05/25/2022 21:46:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
05/25/2022 21:46:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=91
05/25/2022 21:46:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=92
05/25/2022 21:46:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=93
05/25/2022 21:46:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=93
05/25/2022 21:46:57 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.7460082344051087 on epoch=93
05/25/2022 21:46:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7405141432229758 -> 0.7460082344051087 on epoch=93, global_step=1500
05/25/2022 21:46:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
05/25/2022 21:47:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=94
05/25/2022 21:47:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=95
05/25/2022 21:47:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
05/25/2022 21:47:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=96
05/25/2022 21:47:13 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7324113217073999 on epoch=96
05/25/2022 21:47:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=97
05/25/2022 21:47:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=98
05/25/2022 21:47:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=98
05/25/2022 21:47:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=99
05/25/2022 21:47:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
05/25/2022 21:47:30 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.7355118368223578 on epoch=99
05/25/2022 21:47:32 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=100
05/25/2022 21:47:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
05/25/2022 21:47:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
05/25/2022 21:47:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.28 on epoch=102
05/25/2022 21:47:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=103
05/25/2022 21:47:46 - INFO - __main__ - Global step 1650 Train loss 0.26 Classification-F1 0.7387152896195606 on epoch=103
05/25/2022 21:47:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=103
05/25/2022 21:47:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
05/25/2022 21:47:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=104
05/25/2022 21:47:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=105
05/25/2022 21:47:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=106
05/25/2022 21:48:02 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.7293076726197247 on epoch=106
05/25/2022 21:48:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.31 on epoch=106
05/25/2022 21:48:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=107
05/25/2022 21:48:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=108
05/25/2022 21:48:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.28 on epoch=108
05/25/2022 21:48:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=109
05/25/2022 21:48:18 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.740782684119175 on epoch=109
05/25/2022 21:48:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=109
05/25/2022 21:48:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.28 on epoch=110
05/25/2022 21:48:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=111
05/25/2022 21:48:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=111
05/25/2022 21:48:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=112
05/25/2022 21:48:34 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7442005880659941 on epoch=112
05/25/2022 21:48:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=113
05/25/2022 21:48:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=113
05/25/2022 21:48:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=114
05/25/2022 21:48:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=114
05/25/2022 21:48:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=115
05/25/2022 21:48:50 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.7493252643385031 on epoch=115
05/25/2022 21:48:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7460082344051087 -> 0.7493252643385031 on epoch=115, global_step=1850
05/25/2022 21:48:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/25/2022 21:48:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=116
05/25/2022 21:48:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=117
05/25/2022 21:49:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
05/25/2022 21:49:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.25 on epoch=118
05/25/2022 21:49:06 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7343518655020205 on epoch=118
05/25/2022 21:49:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.15 on epoch=119
05/25/2022 21:49:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
05/25/2022 21:49:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=120
05/25/2022 21:49:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/25/2022 21:49:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
05/25/2022 21:49:22 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7285443617126786 on epoch=121
05/25/2022 21:49:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
05/25/2022 21:49:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
05/25/2022 21:49:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=123
05/25/2022 21:49:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
05/25/2022 21:49:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=124
05/25/2022 21:49:39 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7404988289410025 on epoch=124
05/25/2022 21:49:41 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=125
05/25/2022 21:49:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
05/25/2022 21:49:46 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.28 on epoch=126
05/25/2022 21:49:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
05/25/2022 21:49:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.14 on epoch=128
05/25/2022 21:49:55 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7555321828263177 on epoch=128
05/25/2022 21:49:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7493252643385031 -> 0.7555321828263177 on epoch=128, global_step=2050
05/25/2022 21:49:57 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=128
05/25/2022 21:50:00 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.13 on epoch=129
05/25/2022 21:50:02 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.28 on epoch=129
05/25/2022 21:50:05 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=130
05/25/2022 21:50:07 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=131
05/25/2022 21:50:11 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7476253183499713 on epoch=131
05/25/2022 21:50:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.25 on epoch=131
05/25/2022 21:50:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=132
05/25/2022 21:50:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
05/25/2022 21:50:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=133
05/25/2022 21:50:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=134
05/25/2022 21:50:27 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.7497791932546788 on epoch=134
05/25/2022 21:50:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.24 on epoch=134
05/25/2022 21:50:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
05/25/2022 21:50:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
05/25/2022 21:50:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
05/25/2022 21:50:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=137
05/25/2022 21:50:43 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7554349685820274 on epoch=137
05/25/2022 21:50:45 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=138
05/25/2022 21:50:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
05/25/2022 21:50:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
05/25/2022 21:50:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=139
05/25/2022 21:50:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.25 on epoch=140
05/25/2022 21:50:59 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.7558458226424565 on epoch=140
05/25/2022 21:50:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7555321828263177 -> 0.7558458226424565 on epoch=140, global_step=2250
05/25/2022 21:51:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.11 on epoch=141
05/25/2022 21:51:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
05/25/2022 21:51:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=142
05/25/2022 21:51:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.16 on epoch=143
05/25/2022 21:51:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/25/2022 21:51:15 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.7598161688386577 on epoch=143
05/25/2022 21:51:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7558458226424565 -> 0.7598161688386577 on epoch=143, global_step=2300
05/25/2022 21:51:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.13 on epoch=144
05/25/2022 21:51:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=144
05/25/2022 21:51:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.23 on epoch=145
05/25/2022 21:51:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
05/25/2022 21:51:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
05/25/2022 21:51:31 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.7502076514948831 on epoch=146
05/25/2022 21:51:34 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.26 on epoch=147
05/25/2022 21:51:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=148
05/25/2022 21:51:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
05/25/2022 21:51:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.07 on epoch=149
05/25/2022 21:51:44 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.18 on epoch=149
05/25/2022 21:51:47 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.7455738564113138 on epoch=149
05/25/2022 21:51:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
05/25/2022 21:51:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/25/2022 21:51:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=151
05/25/2022 21:51:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
05/25/2022 21:52:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
05/25/2022 21:52:03 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.7453057975180084 on epoch=153
05/25/2022 21:52:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
05/25/2022 21:52:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.13 on epoch=154
05/25/2022 21:52:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=154
05/25/2022 21:52:13 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
05/25/2022 21:52:16 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/25/2022 21:52:19 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7489220992966831 on epoch=156
05/25/2022 21:52:22 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.21 on epoch=156
05/25/2022 21:52:24 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.24 on epoch=157
05/25/2022 21:52:27 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/25/2022 21:52:30 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.21 on epoch=158
05/25/2022 21:52:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
05/25/2022 21:52:35 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7439020109128714 on epoch=159
05/25/2022 21:52:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=159
05/25/2022 21:52:41 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/25/2022 21:52:43 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
05/25/2022 21:52:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.21 on epoch=161
05/25/2022 21:52:48 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=162
05/25/2022 21:52:52 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.7503016455614693 on epoch=162
05/25/2022 21:52:54 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/25/2022 21:52:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=163
05/25/2022 21:52:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
05/25/2022 21:53:02 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
05/25/2022 21:53:04 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
05/25/2022 21:53:08 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.7546105333448148 on epoch=165
05/25/2022 21:53:10 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=166
05/25/2022 21:53:13 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=166
05/25/2022 21:53:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=167
05/25/2022 21:53:18 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=168
05/25/2022 21:53:20 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
05/25/2022 21:53:24 - INFO - __main__ - Global step 2700 Train loss 0.15 Classification-F1 0.7506229893981495 on epoch=168
05/25/2022 21:53:26 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.11 on epoch=169
05/25/2022 21:53:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
05/25/2022 21:53:31 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.12 on epoch=170
05/25/2022 21:53:34 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=171
05/25/2022 21:53:36 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=171
05/25/2022 21:53:40 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.7478624680267016 on epoch=171
05/25/2022 21:53:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/25/2022 21:53:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/25/2022 21:53:47 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/25/2022 21:53:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
05/25/2022 21:53:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=174
05/25/2022 21:53:56 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7470132146709817 on epoch=174
05/25/2022 21:53:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.23 on epoch=175
05/25/2022 21:54:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.11 on epoch=176
05/25/2022 21:54:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=176
05/25/2022 21:54:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/25/2022 21:54:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/25/2022 21:54:12 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.7427651277334812 on epoch=178
05/25/2022 21:54:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=178
05/25/2022 21:54:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=179
05/25/2022 21:54:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=179
05/25/2022 21:54:22 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=180
05/25/2022 21:54:24 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/25/2022 21:54:28 - INFO - __main__ - Global step 2900 Train loss 0.13 Classification-F1 0.7420065932938249 on epoch=181
05/25/2022 21:54:31 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.21 on epoch=181
05/25/2022 21:54:33 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
05/25/2022 21:54:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
05/25/2022 21:54:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.13 on epoch=183
05/25/2022 21:54:41 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.16 on epoch=184
05/25/2022 21:54:44 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.737868258493704 on epoch=184
05/25/2022 21:54:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
05/25/2022 21:54:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.24 on epoch=185
05/25/2022 21:54:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.10 on epoch=186
05/25/2022 21:54:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=186
05/25/2022 21:54:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
05/25/2022 21:54:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:54:58 - INFO - __main__ - Printing 3 examples
05/25/2022 21:54:58 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:54:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:54:58 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:54:58 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:54:58 - INFO - __main__ - Printing 3 examples
05/25/2022 21:54:58 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 21:54:58 - INFO - __main__ - ['happy']
05/25/2022 21:54:58 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:54:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:54:59 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:55:00 - INFO - __main__ - Global step 3000 Train loss 0.18 Classification-F1 0.7471651231464663 on epoch=187
05/25/2022 21:55:00 - INFO - __main__ - save last model!
05/25/2022 21:55:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 21:55:00 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 21:55:00 - INFO - __main__ - Printing 3 examples
05/25/2022 21:55:00 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 21:55:00 - INFO - __main__ - ['others']
05/25/2022 21:55:00 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 21:55:00 - INFO - __main__ - ['others']
05/25/2022 21:55:00 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 21:55:00 - INFO - __main__ - ['others']
05/25/2022 21:55:00 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:55:02 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:55:08 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 21:55:15 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:55:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:55:16 - INFO - __main__ - Starting training!
05/25/2022 21:56:22 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_21_0.2_8_predictions.txt
05/25/2022 21:56:22 - INFO - __main__ - Classification-F1 on test data: 0.3097
05/25/2022 21:56:22 - INFO - __main__ - prefix=emo_64_21, lr=0.2, bsz=8, dev_performance=0.7598161688386577, test_performance=0.3096535876192864
05/25/2022 21:56:22 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.5, bsz=8 ...
05/25/2022 21:56:23 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:56:23 - INFO - __main__ - Printing 3 examples
05/25/2022 21:56:23 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 21:56:23 - INFO - __main__ - ['happy']
05/25/2022 21:56:23 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 21:56:23 - INFO - __main__ - ['happy']
05/25/2022 21:56:23 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 21:56:23 - INFO - __main__ - ['happy']
05/25/2022 21:56:23 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:56:23 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:56:24 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 21:56:24 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 21:56:24 - INFO - __main__ - Printing 3 examples
05/25/2022 21:56:24 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 21:56:24 - INFO - __main__ - ['happy']
05/25/2022 21:56:24 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 21:56:24 - INFO - __main__ - ['happy']
05/25/2022 21:56:24 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 21:56:24 - INFO - __main__ - ['happy']
05/25/2022 21:56:24 - INFO - __main__ - Tokenizing Input ...
05/25/2022 21:56:24 - INFO - __main__ - Tokenizing Output ...
05/25/2022 21:56:24 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 21:56:43 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 21:56:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 21:56:44 - INFO - __main__ - Starting training!
05/25/2022 21:56:47 - INFO - __main__ - Step 10 Global step 10 Train loss 3.76 on epoch=0
05/25/2022 21:56:49 - INFO - __main__ - Step 20 Global step 20 Train loss 2.53 on epoch=1
05/25/2022 21:56:52 - INFO - __main__ - Step 30 Global step 30 Train loss 2.18 on epoch=1
05/25/2022 21:56:54 - INFO - __main__ - Step 40 Global step 40 Train loss 1.54 on epoch=2
05/25/2022 21:56:57 - INFO - __main__ - Step 50 Global step 50 Train loss 1.37 on epoch=3
05/25/2022 21:57:00 - INFO - __main__ - Global step 50 Train loss 2.28 Classification-F1 0.2699323262652163 on epoch=3
05/25/2022 21:57:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2699323262652163 on epoch=3, global_step=50
05/25/2022 21:57:03 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=3
05/25/2022 21:57:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.96 on epoch=4
05/25/2022 21:57:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=4
05/25/2022 21:57:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=5
05/25/2022 21:57:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=6
05/25/2022 21:57:16 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.514630444480774 on epoch=6
05/25/2022 21:57:16 - INFO - __main__ - Saving model with best Classification-F1: 0.2699323262652163 -> 0.514630444480774 on epoch=6, global_step=100
05/25/2022 21:57:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.74 on epoch=6
05/25/2022 21:57:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.58 on epoch=7
05/25/2022 21:57:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.68 on epoch=8
05/25/2022 21:57:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.68 on epoch=8
05/25/2022 21:57:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=9
05/25/2022 21:57:33 - INFO - __main__ - Global step 150 Train loss 0.65 Classification-F1 0.5275200701040252 on epoch=9
05/25/2022 21:57:33 - INFO - __main__ - Saving model with best Classification-F1: 0.514630444480774 -> 0.5275200701040252 on epoch=9, global_step=150
05/25/2022 21:57:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.62 on epoch=9
05/25/2022 21:57:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.57 on epoch=10
05/25/2022 21:57:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=11
05/25/2022 21:57:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=11
05/25/2022 21:57:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=12
05/25/2022 21:57:49 - INFO - __main__ - Global step 200 Train loss 0.57 Classification-F1 0.6304314505023914 on epoch=12
05/25/2022 21:57:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5275200701040252 -> 0.6304314505023914 on epoch=12, global_step=200
05/25/2022 21:57:51 - INFO - __main__ - Step 210 Global step 210 Train loss 0.61 on epoch=13
05/25/2022 21:57:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=13
05/25/2022 21:57:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=14
05/25/2022 21:57:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.56 on epoch=14
05/25/2022 21:58:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=15
05/25/2022 21:58:05 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.6742636348295153 on epoch=15
05/25/2022 21:58:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6304314505023914 -> 0.6742636348295153 on epoch=15, global_step=250
05/25/2022 21:58:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=16
05/25/2022 21:58:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=16
05/25/2022 21:58:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=17
05/25/2022 21:58:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=18
05/25/2022 21:58:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=18
05/25/2022 21:58:21 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.6529455576875659 on epoch=18
05/25/2022 21:58:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=19
05/25/2022 21:58:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=19
05/25/2022 21:58:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=20
05/25/2022 21:58:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=21
05/25/2022 21:58:33 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=21
05/25/2022 21:58:37 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.7034277433822066 on epoch=21
05/25/2022 21:58:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6742636348295153 -> 0.7034277433822066 on epoch=21, global_step=350
05/25/2022 21:58:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=22
05/25/2022 21:58:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=23
05/25/2022 21:58:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=23
05/25/2022 21:58:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
05/25/2022 21:58:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=24
05/25/2022 21:58:53 - INFO - __main__ - Global step 400 Train loss 0.42 Classification-F1 0.7317292018530135 on epoch=24
05/25/2022 21:58:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7034277433822066 -> 0.7317292018530135 on epoch=24, global_step=400
05/25/2022 21:58:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=25
05/25/2022 21:58:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=26
05/25/2022 21:59:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=26
05/25/2022 21:59:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.29 on epoch=27
05/25/2022 21:59:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=28
05/25/2022 21:59:09 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.7482562912204221 on epoch=28
05/25/2022 21:59:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7317292018530135 -> 0.7482562912204221 on epoch=28, global_step=450
05/25/2022 21:59:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=28
05/25/2022 21:59:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
05/25/2022 21:59:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.29 on epoch=29
05/25/2022 21:59:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=30
05/25/2022 21:59:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
05/25/2022 21:59:25 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.7448649280929867 on epoch=31
05/25/2022 21:59:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=31
05/25/2022 21:59:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=32
05/25/2022 21:59:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
05/25/2022 21:59:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/25/2022 21:59:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=34
05/25/2022 21:59:42 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.7301409025148522 on epoch=34
05/25/2022 21:59:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/25/2022 21:59:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=35
05/25/2022 21:59:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.32 on epoch=36
05/25/2022 21:59:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=36
05/25/2022 21:59:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=37
05/25/2022 21:59:58 - INFO - __main__ - Global step 600 Train loss 0.31 Classification-F1 0.7229958394514544 on epoch=37
05/25/2022 22:00:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=38
05/25/2022 22:00:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=38
05/25/2022 22:00:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=39
05/25/2022 22:00:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.28 on epoch=39
05/25/2022 22:00:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=40
05/25/2022 22:00:14 - INFO - __main__ - Global step 650 Train loss 0.31 Classification-F1 0.7658837751823013 on epoch=40
05/25/2022 22:00:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7482562912204221 -> 0.7658837751823013 on epoch=40, global_step=650
05/25/2022 22:00:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=41
05/25/2022 22:00:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=41
05/25/2022 22:00:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=42
05/25/2022 22:00:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=43
05/25/2022 22:00:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=43
05/25/2022 22:00:30 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.73784918431865 on epoch=43
05/25/2022 22:00:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.28 on epoch=44
05/25/2022 22:00:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=44
05/25/2022 22:00:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.29 on epoch=45
05/25/2022 22:00:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=46
05/25/2022 22:00:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=46
05/25/2022 22:00:46 - INFO - __main__ - Global step 750 Train loss 0.27 Classification-F1 0.7619389771088807 on epoch=46
05/25/2022 22:00:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=47
05/25/2022 22:00:51 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=48
05/25/2022 22:00:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/25/2022 22:00:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.27 on epoch=49
05/25/2022 22:00:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=49
05/25/2022 22:01:02 - INFO - __main__ - Global step 800 Train loss 0.26 Classification-F1 0.7716108419846608 on epoch=49
05/25/2022 22:01:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7658837751823013 -> 0.7716108419846608 on epoch=49, global_step=800
05/25/2022 22:01:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
05/25/2022 22:01:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=51
05/25/2022 22:01:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=51
05/25/2022 22:01:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=52
05/25/2022 22:01:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=53
05/25/2022 22:01:18 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.7717916182383681 on epoch=53
05/25/2022 22:01:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7716108419846608 -> 0.7717916182383681 on epoch=53, global_step=850
05/25/2022 22:01:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=53
05/25/2022 22:01:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=54
05/25/2022 22:01:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=54
05/25/2022 22:01:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=55
05/25/2022 22:01:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.17 on epoch=56
05/25/2022 22:01:34 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.7674152302914032 on epoch=56
05/25/2022 22:01:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=56
05/25/2022 22:01:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=57
05/25/2022 22:01:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=58
05/25/2022 22:01:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=58
05/25/2022 22:01:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=59
05/25/2022 22:01:50 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.7638365385333807 on epoch=59
05/25/2022 22:01:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=59
05/25/2022 22:01:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=60
05/25/2022 22:01:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=61
05/25/2022 22:02:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=61
05/25/2022 22:02:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=62
05/25/2022 22:02:06 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.759171816911683 on epoch=62
05/25/2022 22:02:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=63
05/25/2022 22:02:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=63
05/25/2022 22:02:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=64
05/25/2022 22:02:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=64
05/25/2022 22:02:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=65
05/25/2022 22:02:23 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.7665821015653307 on epoch=65
05/25/2022 22:02:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=66
05/25/2022 22:02:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=66
05/25/2022 22:02:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
05/25/2022 22:02:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=68
05/25/2022 22:02:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=68
05/25/2022 22:02:39 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7727495740055232 on epoch=68
05/25/2022 22:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7717916182383681 -> 0.7727495740055232 on epoch=68, global_step=1100
05/25/2022 22:02:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=69
05/25/2022 22:02:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=69
05/25/2022 22:02:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=70
05/25/2022 22:02:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=71
05/25/2022 22:02:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=71
05/25/2022 22:02:55 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.761609027018205 on epoch=71
05/25/2022 22:02:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=72
05/25/2022 22:03:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
05/25/2022 22:03:02 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=73
05/25/2022 22:03:05 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.12 on epoch=74
05/25/2022 22:03:07 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=74
05/25/2022 22:03:11 - INFO - __main__ - Global step 1200 Train loss 0.15 Classification-F1 0.773883321949892 on epoch=74
05/25/2022 22:03:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7727495740055232 -> 0.773883321949892 on epoch=74, global_step=1200
05/25/2022 22:03:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/25/2022 22:03:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=76
05/25/2022 22:03:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=76
05/25/2022 22:03:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=77
05/25/2022 22:03:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=78
05/25/2022 22:03:27 - INFO - __main__ - Global step 1250 Train loss 0.14 Classification-F1 0.7729816080403887 on epoch=78
05/25/2022 22:03:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.10 on epoch=78
05/25/2022 22:03:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=79
05/25/2022 22:03:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=79
05/25/2022 22:03:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=80
05/25/2022 22:03:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=81
05/25/2022 22:03:43 - INFO - __main__ - Global step 1300 Train loss 0.14 Classification-F1 0.7611686111250144 on epoch=81
05/25/2022 22:03:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=81
05/25/2022 22:03:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/25/2022 22:03:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=83
05/25/2022 22:03:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=83
05/25/2022 22:03:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=84
05/25/2022 22:03:59 - INFO - __main__ - Global step 1350 Train loss 0.13 Classification-F1 0.759665752766419 on epoch=84
05/25/2022 22:04:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.12 on epoch=84
05/25/2022 22:04:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
05/25/2022 22:04:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=86
05/25/2022 22:04:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=86
05/25/2022 22:04:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=87
05/25/2022 22:04:15 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.7629928336460791 on epoch=87
05/25/2022 22:04:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.13 on epoch=88
05/25/2022 22:04:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=88
05/25/2022 22:04:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=89
05/25/2022 22:04:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=89
05/25/2022 22:04:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.11 on epoch=90
05/25/2022 22:04:31 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7684911052785068 on epoch=90
05/25/2022 22:04:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=91
05/25/2022 22:04:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=91
05/25/2022 22:04:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=92
05/25/2022 22:04:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=93
05/25/2022 22:04:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/25/2022 22:04:47 - INFO - __main__ - Global step 1500 Train loss 0.12 Classification-F1 0.7318936462690497 on epoch=93
05/25/2022 22:04:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=94
05/25/2022 22:04:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=94
05/25/2022 22:04:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=95
05/25/2022 22:04:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=96
05/25/2022 22:05:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=96
05/25/2022 22:05:03 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.7605331268166134 on epoch=96
05/25/2022 22:05:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=97
05/25/2022 22:05:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=98
05/25/2022 22:05:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=98
05/25/2022 22:05:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=99
05/25/2022 22:05:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=99
05/25/2022 22:05:19 - INFO - __main__ - Global step 1600 Train loss 0.10 Classification-F1 0.7587440498052638 on epoch=99
05/25/2022 22:05:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
05/25/2022 22:05:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=101
05/25/2022 22:05:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=101
05/25/2022 22:05:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
05/25/2022 22:05:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=103
05/25/2022 22:05:35 - INFO - __main__ - Global step 1650 Train loss 0.12 Classification-F1 0.7686857830920445 on epoch=103
05/25/2022 22:05:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=103
05/25/2022 22:05:40 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=104
05/25/2022 22:05:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=104
05/25/2022 22:05:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=105
05/25/2022 22:05:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/25/2022 22:05:51 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.7386963261284507 on epoch=106
05/25/2022 22:05:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=106
05/25/2022 22:05:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
05/25/2022 22:05:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
05/25/2022 22:06:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=108
05/25/2022 22:06:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=109
05/25/2022 22:06:08 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.7387708891440234 on epoch=109
05/25/2022 22:06:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=109
05/25/2022 22:06:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=110
05/25/2022 22:06:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=111
05/25/2022 22:06:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
05/25/2022 22:06:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=112
05/25/2022 22:06:24 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.7431500776609954 on epoch=112
05/25/2022 22:06:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=113
05/25/2022 22:06:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=113
05/25/2022 22:06:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=114
05/25/2022 22:06:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=114
05/25/2022 22:06:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=115
05/25/2022 22:06:40 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.7633111588204329 on epoch=115
05/25/2022 22:06:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=116
05/25/2022 22:06:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=116
05/25/2022 22:06:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=117
05/25/2022 22:06:50 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=118
05/25/2022 22:06:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=118
05/25/2022 22:06:56 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.7578464749588201 on epoch=118
05/25/2022 22:06:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=119
05/25/2022 22:07:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
05/25/2022 22:07:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=120
05/25/2022 22:07:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=121
05/25/2022 22:07:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=121
05/25/2022 22:07:12 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.7539707842965128 on epoch=121
05/25/2022 22:07:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=122
05/25/2022 22:07:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=123
05/25/2022 22:07:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=123
05/25/2022 22:07:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=124
05/25/2022 22:07:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=124
05/25/2022 22:07:28 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.7927113908590625 on epoch=124
05/25/2022 22:07:28 - INFO - __main__ - Saving model with best Classification-F1: 0.773883321949892 -> 0.7927113908590625 on epoch=124, global_step=2000
05/25/2022 22:07:31 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
05/25/2022 22:07:33 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=126
05/25/2022 22:07:36 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=126
05/25/2022 22:07:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=127
05/25/2022 22:07:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=128
05/25/2022 22:07:44 - INFO - __main__ - Global step 2050 Train loss 0.07 Classification-F1 0.767752049947886 on epoch=128
05/25/2022 22:07:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=128
05/25/2022 22:07:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=129
05/25/2022 22:07:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.02 on epoch=129
05/25/2022 22:07:54 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
05/25/2022 22:07:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=131
05/25/2022 22:08:00 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7700625040280422 on epoch=131
05/25/2022 22:08:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.05 on epoch=131
05/25/2022 22:08:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/25/2022 22:08:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
05/25/2022 22:08:10 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
05/25/2022 22:08:13 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.04 on epoch=134
05/25/2022 22:08:16 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7683174867223663 on epoch=134
05/25/2022 22:08:19 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=134
05/25/2022 22:08:21 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
05/25/2022 22:08:24 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=136
05/25/2022 22:08:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=136
05/25/2022 22:08:29 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.05 on epoch=137
05/25/2022 22:08:32 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.7490651914897712 on epoch=137
05/25/2022 22:08:35 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
05/25/2022 22:08:37 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.03 on epoch=138
05/25/2022 22:08:40 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=139
05/25/2022 22:08:42 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.07 on epoch=139
05/25/2022 22:08:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.11 on epoch=140
05/25/2022 22:08:48 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.7514638921121306 on epoch=140
05/25/2022 22:08:51 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=141
05/25/2022 22:08:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=141
05/25/2022 22:08:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=142
05/25/2022 22:08:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/25/2022 22:09:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=143
05/25/2022 22:09:04 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.7684183583209235 on epoch=143
05/25/2022 22:09:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=144
05/25/2022 22:09:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
05/25/2022 22:09:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=145
05/25/2022 22:09:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=146
05/25/2022 22:09:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=146
05/25/2022 22:09:21 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.7608511884669446 on epoch=146
05/25/2022 22:09:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
05/25/2022 22:09:26 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
05/25/2022 22:09:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
05/25/2022 22:09:31 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=149
05/25/2022 22:09:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=149
05/25/2022 22:09:37 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.7738439885496182 on epoch=149
05/25/2022 22:09:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=150
05/25/2022 22:09:42 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.05 on epoch=151
05/25/2022 22:09:44 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
05/25/2022 22:09:47 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=152
05/25/2022 22:09:49 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
05/25/2022 22:09:53 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7595486559187863 on epoch=153
05/25/2022 22:09:55 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=153
05/25/2022 22:09:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=154
05/25/2022 22:10:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.08 on epoch=154
05/25/2022 22:10:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=155
05/25/2022 22:10:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=156
05/25/2022 22:10:09 - INFO - __main__ - Global step 2500 Train loss 0.04 Classification-F1 0.780150675911915 on epoch=156
05/25/2022 22:10:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=156
05/25/2022 22:10:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 22:10:16 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=158
05/25/2022 22:10:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=158
05/25/2022 22:10:21 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
05/25/2022 22:10:25 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7638768311012405 on epoch=159
05/25/2022 22:10:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.04 on epoch=159
05/25/2022 22:10:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/25/2022 22:10:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
05/25/2022 22:10:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/25/2022 22:10:37 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=162
05/25/2022 22:10:41 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.7583572141796073 on epoch=162
05/25/2022 22:10:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=163
05/25/2022 22:10:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
05/25/2022 22:10:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/25/2022 22:10:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/25/2022 22:10:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/25/2022 22:10:57 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.7588802398768303 on epoch=165
05/25/2022 22:10:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
05/25/2022 22:11:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=166
05/25/2022 22:11:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/25/2022 22:11:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
05/25/2022 22:11:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/25/2022 22:11:13 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.7882712933598506 on epoch=168
05/25/2022 22:11:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=169
05/25/2022 22:11:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/25/2022 22:11:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
05/25/2022 22:11:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=171
05/25/2022 22:11:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.06 on epoch=171
05/25/2022 22:11:29 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7450649632040068 on epoch=171
05/25/2022 22:11:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=172
05/25/2022 22:11:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
05/25/2022 22:11:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=173
05/25/2022 22:11:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
05/25/2022 22:11:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=174
05/25/2022 22:11:45 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.7579586429156104 on epoch=174
05/25/2022 22:11:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=175
05/25/2022 22:11:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
05/25/2022 22:11:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
05/25/2022 22:11:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=177
05/25/2022 22:11:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/25/2022 22:12:01 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.765367397257161 on epoch=178
05/25/2022 22:12:04 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=178
05/25/2022 22:12:06 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
05/25/2022 22:12:09 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
05/25/2022 22:12:11 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/25/2022 22:12:14 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=181
05/25/2022 22:12:17 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.7480522115387077 on epoch=181
05/25/2022 22:12:20 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/25/2022 22:12:22 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=182
05/25/2022 22:12:25 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=183
05/25/2022 22:12:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=183
05/25/2022 22:12:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
05/25/2022 22:12:33 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7428509166816266 on epoch=184
05/25/2022 22:12:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=184
05/25/2022 22:12:38 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=185
05/25/2022 22:12:41 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/25/2022 22:12:43 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/25/2022 22:12:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/25/2022 22:12:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:12:47 - INFO - __main__ - Printing 3 examples
05/25/2022 22:12:47 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:12:47 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:12:47 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:12:47 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:12:47 - INFO - __main__ - Printing 3 examples
05/25/2022 22:12:47 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:12:47 - INFO - __main__ - ['happy']
05/25/2022 22:12:47 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:12:48 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:12:48 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:12:49 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7701255464242 on epoch=187
05/25/2022 22:12:49 - INFO - __main__ - save last model!
05/25/2022 22:12:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 22:12:49 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 22:12:49 - INFO - __main__ - Printing 3 examples
05/25/2022 22:12:49 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 22:12:49 - INFO - __main__ - ['others']
05/25/2022 22:12:49 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 22:12:49 - INFO - __main__ - ['others']
05/25/2022 22:12:49 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 22:12:49 - INFO - __main__ - ['others']
05/25/2022 22:12:49 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:12:51 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:12:57 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 22:13:04 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:13:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:13:05 - INFO - __main__ - Starting training!
05/25/2022 22:14:10 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_42_0.5_8_predictions.txt
05/25/2022 22:14:10 - INFO - __main__ - Classification-F1 on test data: 0.5342
05/25/2022 22:14:10 - INFO - __main__ - prefix=emo_64_42, lr=0.5, bsz=8, dev_performance=0.7927113908590625, test_performance=0.5342018281000034
05/25/2022 22:14:10 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.4, bsz=8 ...
05/25/2022 22:14:11 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:14:11 - INFO - __main__ - Printing 3 examples
05/25/2022 22:14:11 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:14:11 - INFO - __main__ - ['happy']
05/25/2022 22:14:11 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:14:11 - INFO - __main__ - ['happy']
05/25/2022 22:14:11 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:14:11 - INFO - __main__ - ['happy']
05/25/2022 22:14:11 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:14:11 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:14:12 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:14:12 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:14:12 - INFO - __main__ - Printing 3 examples
05/25/2022 22:14:12 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:14:12 - INFO - __main__ - ['happy']
05/25/2022 22:14:12 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:14:12 - INFO - __main__ - ['happy']
05/25/2022 22:14:12 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:14:12 - INFO - __main__ - ['happy']
05/25/2022 22:14:12 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:14:12 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:14:12 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:14:28 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:14:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:14:29 - INFO - __main__ - Starting training!
05/25/2022 22:14:32 - INFO - __main__ - Step 10 Global step 10 Train loss 4.11 on epoch=0
05/25/2022 22:14:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=1
05/25/2022 22:14:37 - INFO - __main__ - Step 30 Global step 30 Train loss 2.46 on epoch=1
05/25/2022 22:14:40 - INFO - __main__ - Step 40 Global step 40 Train loss 1.99 on epoch=2
05/25/2022 22:14:42 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=3
05/25/2022 22:14:46 - INFO - __main__ - Global step 50 Train loss 2.68 Classification-F1 0.09765937540759123 on epoch=3
05/25/2022 22:14:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.09765937540759123 on epoch=3, global_step=50
05/25/2022 22:14:48 - INFO - __main__ - Step 60 Global step 60 Train loss 1.25 on epoch=3
05/25/2022 22:14:51 - INFO - __main__ - Step 70 Global step 70 Train loss 1.23 on epoch=4
05/25/2022 22:14:53 - INFO - __main__ - Step 80 Global step 80 Train loss 0.99 on epoch=4
05/25/2022 22:14:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=5
05/25/2022 22:14:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.86 on epoch=6
05/25/2022 22:15:02 - INFO - __main__ - Global step 100 Train loss 1.04 Classification-F1 0.43582438795062295 on epoch=6
05/25/2022 22:15:02 - INFO - __main__ - Saving model with best Classification-F1: 0.09765937540759123 -> 0.43582438795062295 on epoch=6, global_step=100
05/25/2022 22:15:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=6
05/25/2022 22:15:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=7
05/25/2022 22:15:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=8
05/25/2022 22:15:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.71 on epoch=8
05/25/2022 22:15:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.61 on epoch=9
05/25/2022 22:15:18 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.5126710199004976 on epoch=9
05/25/2022 22:15:18 - INFO - __main__ - Saving model with best Classification-F1: 0.43582438795062295 -> 0.5126710199004976 on epoch=9, global_step=150
05/25/2022 22:15:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.74 on epoch=9
05/25/2022 22:15:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.64 on epoch=10
05/25/2022 22:15:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.68 on epoch=11
05/25/2022 22:15:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.67 on epoch=11
05/25/2022 22:15:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=12
05/25/2022 22:15:34 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.5408839282236095 on epoch=12
05/25/2022 22:15:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5126710199004976 -> 0.5408839282236095 on epoch=12, global_step=200
05/25/2022 22:15:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.63 on epoch=13
05/25/2022 22:15:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=13
05/25/2022 22:15:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.61 on epoch=14
05/25/2022 22:15:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.75 on epoch=14
05/25/2022 22:15:46 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=15
05/25/2022 22:15:50 - INFO - __main__ - Global step 250 Train loss 0.63 Classification-F1 0.6203600352557851 on epoch=15
05/25/2022 22:15:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5408839282236095 -> 0.6203600352557851 on epoch=15, global_step=250
05/25/2022 22:15:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=16
05/25/2022 22:15:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=16
05/25/2022 22:15:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=17
05/25/2022 22:16:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=18
05/25/2022 22:16:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=18
05/25/2022 22:16:06 - INFO - __main__ - Global step 300 Train loss 0.54 Classification-F1 0.6374745063954002 on epoch=18
05/25/2022 22:16:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6203600352557851 -> 0.6374745063954002 on epoch=18, global_step=300
05/25/2022 22:16:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=19
05/25/2022 22:16:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=19
05/25/2022 22:16:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=20
05/25/2022 22:16:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.57 on epoch=21
05/25/2022 22:16:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
05/25/2022 22:16:22 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.699713803935665 on epoch=21
05/25/2022 22:16:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6374745063954002 -> 0.699713803935665 on epoch=21, global_step=350
05/25/2022 22:16:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=22
05/25/2022 22:16:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=23
05/25/2022 22:16:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=23
05/25/2022 22:16:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=24
05/25/2022 22:16:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=24
05/25/2022 22:16:38 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.7370421810459136 on epoch=24
05/25/2022 22:16:38 - INFO - __main__ - Saving model with best Classification-F1: 0.699713803935665 -> 0.7370421810459136 on epoch=24, global_step=400
05/25/2022 22:16:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=25
05/25/2022 22:16:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
05/25/2022 22:16:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=26
05/25/2022 22:16:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.39 on epoch=27
05/25/2022 22:16:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=28
05/25/2022 22:16:54 - INFO - __main__ - Global step 450 Train loss 0.42 Classification-F1 0.7292304175980697 on epoch=28
05/25/2022 22:16:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/25/2022 22:16:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
05/25/2022 22:17:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=29
05/25/2022 22:17:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
05/25/2022 22:17:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=31
05/25/2022 22:17:10 - INFO - __main__ - Global step 500 Train loss 0.41 Classification-F1 0.7523622284438699 on epoch=31
05/25/2022 22:17:10 - INFO - __main__ - Saving model with best Classification-F1: 0.7370421810459136 -> 0.7523622284438699 on epoch=31, global_step=500
05/25/2022 22:17:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=31
05/25/2022 22:17:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.50 on epoch=32
05/25/2022 22:17:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=33
05/25/2022 22:17:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=33
05/25/2022 22:17:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=34
05/25/2022 22:17:26 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.7149231526304345 on epoch=34
05/25/2022 22:17:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
05/25/2022 22:17:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.34 on epoch=35
05/25/2022 22:17:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=36
05/25/2022 22:17:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
05/25/2022 22:17:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=37
05/25/2022 22:17:42 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6907579492681339 on epoch=37
05/25/2022 22:17:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=38
05/25/2022 22:17:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=38
05/25/2022 22:17:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=39
05/25/2022 22:17:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.31 on epoch=39
05/25/2022 22:17:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=40
05/25/2022 22:17:58 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.7407038778381863 on epoch=40
05/25/2022 22:18:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=41
05/25/2022 22:18:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=41
05/25/2022 22:18:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.29 on epoch=42
05/25/2022 22:18:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.30 on epoch=43
05/25/2022 22:18:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
05/25/2022 22:18:14 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.7255530601395264 on epoch=43
05/25/2022 22:18:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=44
05/25/2022 22:18:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=44
05/25/2022 22:18:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/25/2022 22:18:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/25/2022 22:18:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=46
05/25/2022 22:18:30 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.7403473068273287 on epoch=46
05/25/2022 22:18:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=47
05/25/2022 22:18:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=48
05/25/2022 22:18:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/25/2022 22:18:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=49
05/25/2022 22:18:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=49
05/25/2022 22:18:46 - INFO - __main__ - Global step 800 Train loss 0.29 Classification-F1 0.7557262768530373 on epoch=49
05/25/2022 22:18:46 - INFO - __main__ - Saving model with best Classification-F1: 0.7523622284438699 -> 0.7557262768530373 on epoch=49, global_step=800
05/25/2022 22:18:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=50
05/25/2022 22:18:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=51
05/25/2022 22:18:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=51
05/25/2022 22:18:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/25/2022 22:18:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=53
05/25/2022 22:19:02 - INFO - __main__ - Global step 850 Train loss 0.24 Classification-F1 0.7386981096723154 on epoch=53
05/25/2022 22:19:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/25/2022 22:19:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=54
05/25/2022 22:19:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=54
05/25/2022 22:19:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=55
05/25/2022 22:19:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=56
05/25/2022 22:19:18 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.7757379727847423 on epoch=56
05/25/2022 22:19:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7557262768530373 -> 0.7757379727847423 on epoch=56, global_step=900
05/25/2022 22:19:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=56
05/25/2022 22:19:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=57
05/25/2022 22:19:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
05/25/2022 22:19:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
05/25/2022 22:19:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=59
05/25/2022 22:19:34 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.7503592227805761 on epoch=59
05/25/2022 22:19:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=59
05/25/2022 22:19:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=60
05/25/2022 22:19:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/25/2022 22:19:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=61
05/25/2022 22:19:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=62
05/25/2022 22:19:50 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.7562076531156636 on epoch=62
05/25/2022 22:19:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=63
05/25/2022 22:19:55 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=63
05/25/2022 22:19:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=64
05/25/2022 22:20:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
05/25/2022 22:20:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=65
05/25/2022 22:20:06 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7474879900877096 on epoch=65
05/25/2022 22:20:08 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=66
05/25/2022 22:20:11 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=66
05/25/2022 22:20:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=67
05/25/2022 22:20:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
05/25/2022 22:20:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/25/2022 22:20:22 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.7238134346273881 on epoch=68
05/25/2022 22:20:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=69
05/25/2022 22:20:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=69
05/25/2022 22:20:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=70
05/25/2022 22:20:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
05/25/2022 22:20:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=71
05/25/2022 22:20:38 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.7351523092709734 on epoch=71
05/25/2022 22:20:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=72
05/25/2022 22:20:43 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=73
05/25/2022 22:20:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/25/2022 22:20:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=74
05/25/2022 22:20:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=74
05/25/2022 22:20:53 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7339258769031535 on epoch=74
05/25/2022 22:20:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=75
05/25/2022 22:20:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=76
05/25/2022 22:21:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=76
05/25/2022 22:21:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.11 on epoch=77
05/25/2022 22:21:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=78
05/25/2022 22:21:09 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.7541975832886345 on epoch=78
05/25/2022 22:21:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=78
05/25/2022 22:21:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.18 on epoch=79
05/25/2022 22:21:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/25/2022 22:21:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=80
05/25/2022 22:21:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=81
05/25/2022 22:21:25 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.7512508958452505 on epoch=81
05/25/2022 22:21:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=81
05/25/2022 22:21:30 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
05/25/2022 22:21:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
05/25/2022 22:21:35 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=83
05/25/2022 22:21:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=84
05/25/2022 22:21:41 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.7464858138963217 on epoch=84
05/25/2022 22:21:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/25/2022 22:21:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.13 on epoch=85
05/25/2022 22:21:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
05/25/2022 22:21:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=86
05/25/2022 22:21:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=87
05/25/2022 22:21:57 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7386908151568863 on epoch=87
05/25/2022 22:22:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=88
05/25/2022 22:22:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=88
05/25/2022 22:22:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=89
05/25/2022 22:22:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
05/25/2022 22:22:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/25/2022 22:22:13 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.7477388341394432 on epoch=90
05/25/2022 22:22:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
05/25/2022 22:22:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
05/25/2022 22:22:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=92
05/25/2022 22:22:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=93
05/25/2022 22:22:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/25/2022 22:22:29 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.7375559515050808 on epoch=93
05/25/2022 22:22:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=94
05/25/2022 22:22:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
05/25/2022 22:22:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=95
05/25/2022 22:22:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=96
05/25/2022 22:22:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=96
05/25/2022 22:22:45 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.7418199507656924 on epoch=96
05/25/2022 22:22:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
05/25/2022 22:22:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=98
05/25/2022 22:22:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/25/2022 22:22:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
05/25/2022 22:22:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=99
05/25/2022 22:23:01 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7433920858553211 on epoch=99
05/25/2022 22:23:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=100
05/25/2022 22:23:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=101
05/25/2022 22:23:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=101
05/25/2022 22:23:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=102
05/25/2022 22:23:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=103
05/25/2022 22:23:17 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7715682507583417 on epoch=103
05/25/2022 22:23:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=103
05/25/2022 22:23:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=104
05/25/2022 22:23:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=104
05/25/2022 22:23:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=105
05/25/2022 22:23:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/25/2022 22:23:33 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.7540123737879698 on epoch=106
05/25/2022 22:23:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=106
05/25/2022 22:23:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
05/25/2022 22:23:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=108
05/25/2022 22:23:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=108
05/25/2022 22:23:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=109
05/25/2022 22:23:49 - INFO - __main__ - Global step 1750 Train loss 0.11 Classification-F1 0.7811127202592559 on epoch=109
05/25/2022 22:23:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7757379727847423 -> 0.7811127202592559 on epoch=109, global_step=1750
05/25/2022 22:23:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=109
05/25/2022 22:23:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=110
05/25/2022 22:23:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=111
05/25/2022 22:24:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=111
05/25/2022 22:24:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
05/25/2022 22:24:05 - INFO - __main__ - Global step 1800 Train loss 0.11 Classification-F1 0.7817778384590466 on epoch=112
05/25/2022 22:24:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7811127202592559 -> 0.7817778384590466 on epoch=112, global_step=1800
05/25/2022 22:24:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=113
05/25/2022 22:24:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=113
05/25/2022 22:24:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/25/2022 22:24:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
05/25/2022 22:24:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
05/25/2022 22:24:22 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7722058995827212 on epoch=115
05/25/2022 22:24:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
05/25/2022 22:24:27 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/25/2022 22:24:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=117
05/25/2022 22:24:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
05/25/2022 22:24:34 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=118
05/25/2022 22:24:37 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.7682428782659197 on epoch=118
05/25/2022 22:24:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=119
05/25/2022 22:24:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=119
05/25/2022 22:24:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=120
05/25/2022 22:24:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=121
05/25/2022 22:24:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=121
05/25/2022 22:24:53 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.7468540791938505 on epoch=121
05/25/2022 22:24:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
05/25/2022 22:24:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
05/25/2022 22:25:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
05/25/2022 22:25:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=124
05/25/2022 22:25:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=124
05/25/2022 22:25:09 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.7651305694423404 on epoch=124
05/25/2022 22:25:12 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=125
05/25/2022 22:25:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=126
05/25/2022 22:25:17 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=126
05/25/2022 22:25:19 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=127
05/25/2022 22:25:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
05/25/2022 22:25:25 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.766523203323136 on epoch=128
05/25/2022 22:25:28 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/25/2022 22:25:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=129
05/25/2022 22:25:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=129
05/25/2022 22:25:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=130
05/25/2022 22:25:38 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/25/2022 22:25:42 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.7691719072771703 on epoch=131
05/25/2022 22:25:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=131
05/25/2022 22:25:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
05/25/2022 22:25:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
05/25/2022 22:25:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=133
05/25/2022 22:25:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=134
05/25/2022 22:25:58 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.7481493752366527 on epoch=134
05/25/2022 22:26:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/25/2022 22:26:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=135
05/25/2022 22:26:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
05/25/2022 22:26:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/25/2022 22:26:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/25/2022 22:26:14 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.7700355604652113 on epoch=137
05/25/2022 22:26:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=138
05/25/2022 22:26:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=138
05/25/2022 22:26:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.06 on epoch=139
05/25/2022 22:26:24 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=139
05/25/2022 22:26:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=140
05/25/2022 22:26:30 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7519678187428995 on epoch=140
05/25/2022 22:26:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=141
05/25/2022 22:26:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=141
05/25/2022 22:26:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
05/25/2022 22:26:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
05/25/2022 22:26:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=143
05/25/2022 22:26:45 - INFO - __main__ - Global step 2300 Train loss 0.10 Classification-F1 0.7516169055727145 on epoch=143
05/25/2022 22:26:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/25/2022 22:26:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=144
05/25/2022 22:26:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.09 on epoch=145
05/25/2022 22:26:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=146
05/25/2022 22:26:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
05/25/2022 22:27:02 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7544626611091831 on epoch=146
05/25/2022 22:27:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
05/25/2022 22:27:07 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
05/25/2022 22:27:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=148
05/25/2022 22:27:12 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=149
05/25/2022 22:27:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=149
05/25/2022 22:27:18 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.764173489908784 on epoch=149
05/25/2022 22:27:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=150
05/25/2022 22:27:23 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=151
05/25/2022 22:27:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=151
05/25/2022 22:27:28 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=152
05/25/2022 22:27:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
05/25/2022 22:27:34 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.7817433658396188 on epoch=153
05/25/2022 22:27:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
05/25/2022 22:27:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
05/25/2022 22:27:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
05/25/2022 22:27:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/25/2022 22:27:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
05/25/2022 22:27:50 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.7704694680752974 on epoch=156
05/25/2022 22:27:52 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
05/25/2022 22:27:55 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/25/2022 22:27:57 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
05/25/2022 22:28:00 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=158
05/25/2022 22:28:02 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
05/25/2022 22:28:05 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.7713726882329603 on epoch=159
05/25/2022 22:28:08 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
05/25/2022 22:28:11 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
05/25/2022 22:28:13 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
05/25/2022 22:28:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
05/25/2022 22:28:18 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=162
05/25/2022 22:28:22 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.7459793665600568 on epoch=162
05/25/2022 22:28:24 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=163
05/25/2022 22:28:27 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=163
05/25/2022 22:28:29 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
05/25/2022 22:28:32 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.09 on epoch=164
05/25/2022 22:28:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=165
05/25/2022 22:28:38 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7587235741138615 on epoch=165
05/25/2022 22:28:40 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=166
05/25/2022 22:28:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
05/25/2022 22:28:45 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.10 on epoch=167
05/25/2022 22:28:48 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
05/25/2022 22:28:50 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/25/2022 22:28:54 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7570382199031384 on epoch=168
05/25/2022 22:28:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
05/25/2022 22:28:59 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=169
05/25/2022 22:29:01 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=170
05/25/2022 22:29:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
05/25/2022 22:29:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
05/25/2022 22:29:10 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.7755208212571588 on epoch=171
05/25/2022 22:29:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
05/25/2022 22:29:15 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/25/2022 22:29:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=173
05/25/2022 22:29:20 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
05/25/2022 22:29:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/25/2022 22:29:26 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7601108122095982 on epoch=174
05/25/2022 22:29:28 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
05/25/2022 22:29:31 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
05/25/2022 22:29:33 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
05/25/2022 22:29:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
05/25/2022 22:29:39 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=178
05/25/2022 22:29:42 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.7826146526362493 on epoch=178
05/25/2022 22:29:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7817778384590466 -> 0.7826146526362493 on epoch=178, global_step=2850
05/25/2022 22:29:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/25/2022 22:29:47 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=179
05/25/2022 22:29:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/25/2022 22:29:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=180
05/25/2022 22:29:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
05/25/2022 22:29:58 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.7724704998332891 on epoch=181
05/25/2022 22:30:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=181
05/25/2022 22:30:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=182
05/25/2022 22:30:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/25/2022 22:30:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=183
05/25/2022 22:30:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
05/25/2022 22:30:14 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.750234632847082 on epoch=184
05/25/2022 22:30:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=184
05/25/2022 22:30:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.09 on epoch=185
05/25/2022 22:30:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=186
05/25/2022 22:30:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=186
05/25/2022 22:30:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
05/25/2022 22:30:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:30:28 - INFO - __main__ - Printing 3 examples
05/25/2022 22:30:28 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:30:28 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:30:28 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:30:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:30:28 - INFO - __main__ - Printing 3 examples
05/25/2022 22:30:28 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:30:28 - INFO - __main__ - ['happy']
05/25/2022 22:30:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:30:29 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:30:29 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:30:30 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7707486320034705 on epoch=187
05/25/2022 22:30:30 - INFO - __main__ - save last model!
05/25/2022 22:30:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 22:30:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 22:30:30 - INFO - __main__ - Printing 3 examples
05/25/2022 22:30:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 22:30:30 - INFO - __main__ - ['others']
05/25/2022 22:30:30 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 22:30:30 - INFO - __main__ - ['others']
05/25/2022 22:30:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 22:30:30 - INFO - __main__ - ['others']
05/25/2022 22:30:30 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:30:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:30:38 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 22:30:45 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:30:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:30:46 - INFO - __main__ - Starting training!
05/25/2022 22:31:51 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_42_0.4_8_predictions.txt
05/25/2022 22:31:51 - INFO - __main__ - Classification-F1 on test data: 0.3911
05/25/2022 22:31:51 - INFO - __main__ - prefix=emo_64_42, lr=0.4, bsz=8, dev_performance=0.7826146526362493, test_performance=0.39113604717508077
05/25/2022 22:31:51 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.3, bsz=8 ...
05/25/2022 22:31:52 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:31:52 - INFO - __main__ - Printing 3 examples
05/25/2022 22:31:52 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:31:52 - INFO - __main__ - ['happy']
05/25/2022 22:31:52 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:31:52 - INFO - __main__ - ['happy']
05/25/2022 22:31:52 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:31:52 - INFO - __main__ - ['happy']
05/25/2022 22:31:52 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:31:52 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:31:53 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:31:53 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:31:53 - INFO - __main__ - Printing 3 examples
05/25/2022 22:31:53 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:31:53 - INFO - __main__ - ['happy']
05/25/2022 22:31:53 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:31:53 - INFO - __main__ - ['happy']
05/25/2022 22:31:53 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:31:53 - INFO - __main__ - ['happy']
05/25/2022 22:31:53 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:31:53 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:31:53 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:32:10 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:32:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:32:11 - INFO - __main__ - Starting training!
05/25/2022 22:32:15 - INFO - __main__ - Step 10 Global step 10 Train loss 4.17 on epoch=0
05/25/2022 22:32:17 - INFO - __main__ - Step 20 Global step 20 Train loss 3.02 on epoch=1
05/25/2022 22:32:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.60 on epoch=1
05/25/2022 22:32:22 - INFO - __main__ - Step 40 Global step 40 Train loss 2.19 on epoch=2
05/25/2022 22:32:25 - INFO - __main__ - Step 50 Global step 50 Train loss 2.03 on epoch=3
05/25/2022 22:32:29 - INFO - __main__ - Global step 50 Train loss 2.80 Classification-F1 0.02566711895070104 on epoch=3
05/25/2022 22:32:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.02566711895070104 on epoch=3, global_step=50
05/25/2022 22:32:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.60 on epoch=3
05/25/2022 22:32:34 - INFO - __main__ - Step 70 Global step 70 Train loss 1.57 on epoch=4
05/25/2022 22:32:37 - INFO - __main__ - Step 80 Global step 80 Train loss 1.38 on epoch=4
05/25/2022 22:32:39 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=5
05/25/2022 22:32:42 - INFO - __main__ - Step 100 Global step 100 Train loss 1.07 on epoch=6
05/25/2022 22:32:46 - INFO - __main__ - Global step 100 Train loss 1.36 Classification-F1 0.3221192200156546 on epoch=6
05/25/2022 22:32:46 - INFO - __main__ - Saving model with best Classification-F1: 0.02566711895070104 -> 0.3221192200156546 on epoch=6, global_step=100
05/25/2022 22:32:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=6
05/25/2022 22:32:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.89 on epoch=7
05/25/2022 22:32:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.89 on epoch=8
05/25/2022 22:32:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=8
05/25/2022 22:32:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=9
05/25/2022 22:33:03 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.5111041799576495 on epoch=9
05/25/2022 22:33:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3221192200156546 -> 0.5111041799576495 on epoch=9, global_step=150
05/25/2022 22:33:05 - INFO - __main__ - Step 160 Global step 160 Train loss 0.78 on epoch=9
05/25/2022 22:33:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.66 on epoch=10
05/25/2022 22:33:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=11
05/25/2022 22:33:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.65 on epoch=11
05/25/2022 22:33:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=12
05/25/2022 22:33:19 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5280683239524703 on epoch=12
05/25/2022 22:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5111041799576495 -> 0.5280683239524703 on epoch=12, global_step=200
05/25/2022 22:33:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=13
05/25/2022 22:33:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.70 on epoch=13
05/25/2022 22:33:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=14
05/25/2022 22:33:30 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=14
05/25/2022 22:33:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=15
05/25/2022 22:33:36 - INFO - __main__ - Global step 250 Train loss 0.70 Classification-F1 0.5836887758028698 on epoch=15
05/25/2022 22:33:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5280683239524703 -> 0.5836887758028698 on epoch=15, global_step=250
05/25/2022 22:33:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=16
05/25/2022 22:33:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=16
05/25/2022 22:33:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.60 on epoch=17
05/25/2022 22:33:46 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=18
05/25/2022 22:33:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.62 on epoch=18
05/25/2022 22:33:53 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.5533966872913264 on epoch=18
05/25/2022 22:33:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=19
05/25/2022 22:33:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=19
05/25/2022 22:34:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=20
05/25/2022 22:34:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=21
05/25/2022 22:34:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=21
05/25/2022 22:34:09 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.6507380049014932 on epoch=21
05/25/2022 22:34:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5836887758028698 -> 0.6507380049014932 on epoch=21, global_step=350
05/25/2022 22:34:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=22
05/25/2022 22:34:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=23
05/25/2022 22:34:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=23
05/25/2022 22:34:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=24
05/25/2022 22:34:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=24
05/25/2022 22:34:26 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.6527325911334148 on epoch=24
05/25/2022 22:34:26 - INFO - __main__ - Saving model with best Classification-F1: 0.6507380049014932 -> 0.6527325911334148 on epoch=24, global_step=400
05/25/2022 22:34:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=25
05/25/2022 22:34:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.56 on epoch=26
05/25/2022 22:34:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=26
05/25/2022 22:34:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.53 on epoch=27
05/25/2022 22:34:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=28
05/25/2022 22:34:42 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.694214073309523 on epoch=28
05/25/2022 22:34:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6527325911334148 -> 0.694214073309523 on epoch=28, global_step=450
05/25/2022 22:34:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=28
05/25/2022 22:34:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=29
05/25/2022 22:34:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=29
05/25/2022 22:34:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=30
05/25/2022 22:34:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=31
05/25/2022 22:34:59 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.663056426903049 on epoch=31
05/25/2022 22:35:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=31
05/25/2022 22:35:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.53 on epoch=32
05/25/2022 22:35:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=33
05/25/2022 22:35:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=33
05/25/2022 22:35:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
05/25/2022 22:35:15 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.6914844166416618 on epoch=34
05/25/2022 22:35:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=34
05/25/2022 22:35:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
05/25/2022 22:35:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
05/25/2022 22:35:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=36
05/25/2022 22:35:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
05/25/2022 22:35:32 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.6951954551763797 on epoch=37
05/25/2022 22:35:32 - INFO - __main__ - Saving model with best Classification-F1: 0.694214073309523 -> 0.6951954551763797 on epoch=37, global_step=600
05/25/2022 22:35:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
05/25/2022 22:35:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=38
05/25/2022 22:35:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=39
05/25/2022 22:35:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=39
05/25/2022 22:35:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=40
05/25/2022 22:35:49 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.7175192558685216 on epoch=40
05/25/2022 22:35:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6951954551763797 -> 0.7175192558685216 on epoch=40, global_step=650
05/25/2022 22:35:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
05/25/2022 22:35:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=41
05/25/2022 22:35:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=42
05/25/2022 22:35:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
05/25/2022 22:36:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=43
05/25/2022 22:36:05 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.6910422265911953 on epoch=43
05/25/2022 22:36:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=44
05/25/2022 22:36:10 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=44
05/25/2022 22:36:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
05/25/2022 22:36:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
05/25/2022 22:36:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=46
05/25/2022 22:36:22 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.7067363333751988 on epoch=46
05/25/2022 22:36:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=47
05/25/2022 22:36:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.31 on epoch=48
05/25/2022 22:36:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
05/25/2022 22:36:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=49
05/25/2022 22:36:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=49
05/25/2022 22:36:38 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7253381645647445 on epoch=49
05/25/2022 22:36:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7175192558685216 -> 0.7253381645647445 on epoch=49, global_step=800
05/25/2022 22:36:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=50
05/25/2022 22:36:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=51
05/25/2022 22:36:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=51
05/25/2022 22:36:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=52
05/25/2022 22:36:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
05/25/2022 22:36:55 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.7323458826922298 on epoch=53
05/25/2022 22:36:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7253381645647445 -> 0.7323458826922298 on epoch=53, global_step=850
05/25/2022 22:36:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=53
05/25/2022 22:37:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=54
05/25/2022 22:37:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=54
05/25/2022 22:37:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=55
05/25/2022 22:37:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=56
05/25/2022 22:37:11 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7345666636093612 on epoch=56
05/25/2022 22:37:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7323458826922298 -> 0.7345666636093612 on epoch=56, global_step=900
05/25/2022 22:37:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=56
05/25/2022 22:37:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=57
05/25/2022 22:37:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/25/2022 22:37:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=58
05/25/2022 22:37:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=59
05/25/2022 22:37:28 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7482080413839085 on epoch=59
05/25/2022 22:37:28 - INFO - __main__ - Saving model with best Classification-F1: 0.7345666636093612 -> 0.7482080413839085 on epoch=59, global_step=950
05/25/2022 22:37:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.30 on epoch=59
05/25/2022 22:37:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
05/25/2022 22:37:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.33 on epoch=61
05/25/2022 22:37:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
05/25/2022 22:37:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
05/25/2022 22:37:44 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.7561119244247825 on epoch=62
05/25/2022 22:37:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7482080413839085 -> 0.7561119244247825 on epoch=62, global_step=1000
05/25/2022 22:37:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
05/25/2022 22:37:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=63
05/25/2022 22:37:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=64
05/25/2022 22:37:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=64
05/25/2022 22:37:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=65
05/25/2022 22:38:01 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.7530558227323033 on epoch=65
05/25/2022 22:38:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=66
05/25/2022 22:38:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=66
05/25/2022 22:38:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=67
05/25/2022 22:38:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=68
05/25/2022 22:38:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=68
05/25/2022 22:38:17 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.7658991698869329 on epoch=68
05/25/2022 22:38:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7561119244247825 -> 0.7658991698869329 on epoch=68, global_step=1100
05/25/2022 22:38:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=69
05/25/2022 22:38:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/25/2022 22:38:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=70
05/25/2022 22:38:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=71
05/25/2022 22:38:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=71
05/25/2022 22:38:34 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.7489253829752163 on epoch=71
05/25/2022 22:38:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=72
05/25/2022 22:38:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=73
05/25/2022 22:38:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
05/25/2022 22:38:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=74
05/25/2022 22:38:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=74
05/25/2022 22:38:50 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.741160899664998 on epoch=74
05/25/2022 22:38:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=75
05/25/2022 22:38:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=76
05/25/2022 22:38:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
05/25/2022 22:39:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=77
05/25/2022 22:39:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
05/25/2022 22:39:07 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.7337214874448916 on epoch=78
05/25/2022 22:39:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=78
05/25/2022 22:39:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
05/25/2022 22:39:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=79
05/25/2022 22:39:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=80
05/25/2022 22:39:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/25/2022 22:39:24 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.7450122234115651 on epoch=81
05/25/2022 22:39:26 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=81
05/25/2022 22:39:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
05/25/2022 22:39:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=83
05/25/2022 22:39:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=83
05/25/2022 22:39:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=84
05/25/2022 22:39:40 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7430235642400214 on epoch=84
05/25/2022 22:39:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=84
05/25/2022 22:39:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=85
05/25/2022 22:39:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=86
05/25/2022 22:39:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=86
05/25/2022 22:39:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=87
05/25/2022 22:39:57 - INFO - __main__ - Global step 1400 Train loss 0.21 Classification-F1 0.7652820111875329 on epoch=87
05/25/2022 22:39:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=88
05/25/2022 22:40:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=88
05/25/2022 22:40:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
05/25/2022 22:40:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
05/25/2022 22:40:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=90
05/25/2022 22:40:13 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.7649038461538462 on epoch=90
05/25/2022 22:40:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=91
05/25/2022 22:40:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
05/25/2022 22:40:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=92
05/25/2022 22:40:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=93
05/25/2022 22:40:26 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=93
05/25/2022 22:40:30 - INFO - __main__ - Global step 1500 Train loss 0.22 Classification-F1 0.766387328594067 on epoch=93
05/25/2022 22:40:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7658991698869329 -> 0.766387328594067 on epoch=93, global_step=1500
05/25/2022 22:40:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=94
05/25/2022 22:40:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=94
05/25/2022 22:40:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
05/25/2022 22:40:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
05/25/2022 22:40:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=96
05/25/2022 22:40:46 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.7469887898274947 on epoch=96
05/25/2022 22:40:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=97
05/25/2022 22:40:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
05/25/2022 22:40:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=98
05/25/2022 22:40:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/25/2022 22:40:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
05/25/2022 22:41:03 - INFO - __main__ - Global step 1600 Train loss 0.17 Classification-F1 0.7909335999030656 on epoch=99
05/25/2022 22:41:03 - INFO - __main__ - Saving model with best Classification-F1: 0.766387328594067 -> 0.7909335999030656 on epoch=99, global_step=1600
05/25/2022 22:41:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
05/25/2022 22:41:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
05/25/2022 22:41:10 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/25/2022 22:41:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
05/25/2022 22:41:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=103
05/25/2022 22:41:19 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.7764991036391169 on epoch=103
05/25/2022 22:41:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
05/25/2022 22:41:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
05/25/2022 22:41:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/25/2022 22:41:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
05/25/2022 22:41:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
05/25/2022 22:41:36 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.7668059497117652 on epoch=106
05/25/2022 22:41:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=106
05/25/2022 22:41:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
05/25/2022 22:41:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
05/25/2022 22:41:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=108
05/25/2022 22:41:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=109
05/25/2022 22:41:52 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.7661848961362709 on epoch=109
05/25/2022 22:41:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=109
05/25/2022 22:41:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
05/25/2022 22:42:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
05/25/2022 22:42:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=111
05/25/2022 22:42:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=112
05/25/2022 22:42:09 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.7833915978603503 on epoch=112
05/25/2022 22:42:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=113
05/25/2022 22:42:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=113
05/25/2022 22:42:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=114
05/25/2022 22:42:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=114
05/25/2022 22:42:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/25/2022 22:42:25 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.7786106016470673 on epoch=115
05/25/2022 22:42:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/25/2022 22:42:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=116
05/25/2022 22:42:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=117
05/25/2022 22:42:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=118
05/25/2022 22:42:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=118
05/25/2022 22:42:42 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.7754949844274562 on epoch=118
05/25/2022 22:42:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=119
05/25/2022 22:42:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
05/25/2022 22:42:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
05/25/2022 22:42:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/25/2022 22:42:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=121
05/25/2022 22:42:58 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.7818557394513295 on epoch=121
05/25/2022 22:43:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=122
05/25/2022 22:43:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.23 on epoch=123
05/25/2022 22:43:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=123
05/25/2022 22:43:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=124
05/25/2022 22:43:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=124
05/25/2022 22:43:15 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.7771157049792534 on epoch=124
05/25/2022 22:43:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
05/25/2022 22:43:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/25/2022 22:43:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=126
05/25/2022 22:43:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
05/25/2022 22:43:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=128
05/25/2022 22:43:31 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.7552265743268002 on epoch=128
05/25/2022 22:43:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/25/2022 22:43:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=129
05/25/2022 22:43:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
05/25/2022 22:43:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.20 on epoch=130
05/25/2022 22:43:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
05/25/2022 22:43:48 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.7755226389063088 on epoch=131
05/25/2022 22:43:50 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=131
05/25/2022 22:43:53 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
05/25/2022 22:43:56 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
05/25/2022 22:43:58 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
05/25/2022 22:44:01 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=134
05/25/2022 22:44:04 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.761309467463623 on epoch=134
05/25/2022 22:44:07 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
05/25/2022 22:44:10 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.15 on epoch=135
05/25/2022 22:44:12 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.24 on epoch=136
05/25/2022 22:44:15 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=136
05/25/2022 22:44:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
05/25/2022 22:44:21 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7511403782942424 on epoch=137
05/25/2022 22:44:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
05/25/2022 22:44:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
05/25/2022 22:44:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=139
05/25/2022 22:44:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
05/25/2022 22:44:34 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=140
05/25/2022 22:44:37 - INFO - __main__ - Global step 2250 Train loss 0.13 Classification-F1 0.7677256179672817 on epoch=140
05/25/2022 22:44:40 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
05/25/2022 22:44:43 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.11 on epoch=141
05/25/2022 22:44:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/25/2022 22:44:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
05/25/2022 22:44:50 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=143
05/25/2022 22:44:54 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.7542277630644016 on epoch=143
05/25/2022 22:44:57 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/25/2022 22:44:59 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
05/25/2022 22:45:02 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
05/25/2022 22:45:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.14 on epoch=146
05/25/2022 22:45:07 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
05/25/2022 22:45:10 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.771841100505334 on epoch=146
05/25/2022 22:45:13 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
05/25/2022 22:45:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=148
05/25/2022 22:45:18 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
05/25/2022 22:45:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/25/2022 22:45:23 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
05/25/2022 22:45:27 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.7689554520830068 on epoch=149
05/25/2022 22:45:30 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=150
05/25/2022 22:45:32 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=151
05/25/2022 22:45:35 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
05/25/2022 22:45:37 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=152
05/25/2022 22:45:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
05/25/2022 22:45:44 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.7619911426332775 on epoch=153
05/25/2022 22:45:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=153
05/25/2022 22:45:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
05/25/2022 22:45:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=154
05/25/2022 22:45:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
05/25/2022 22:45:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
05/25/2022 22:46:00 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7555902249014954 on epoch=156
05/25/2022 22:46:03 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=156
05/25/2022 22:46:05 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
05/25/2022 22:46:08 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
05/25/2022 22:46:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
05/25/2022 22:46:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
05/25/2022 22:46:17 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.7685690158895903 on epoch=159
05/25/2022 22:46:19 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=159
05/25/2022 22:46:22 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=160
05/25/2022 22:46:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.14 on epoch=161
05/25/2022 22:46:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
05/25/2022 22:46:29 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=162
05/25/2022 22:46:33 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7513661942395369 on epoch=162
05/25/2022 22:46:36 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
05/25/2022 22:46:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.14 on epoch=163
05/25/2022 22:46:41 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
05/25/2022 22:46:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
05/25/2022 22:46:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
05/25/2022 22:46:49 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.7713818828015289 on epoch=165
05/25/2022 22:46:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=166
05/25/2022 22:46:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/25/2022 22:46:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/25/2022 22:47:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.15 on epoch=168
05/25/2022 22:47:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
05/25/2022 22:47:06 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.7844692707442853 on epoch=168
05/25/2022 22:47:09 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
05/25/2022 22:47:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=169
05/25/2022 22:47:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
05/25/2022 22:47:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=171
05/25/2022 22:47:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.14 on epoch=171
05/25/2022 22:47:22 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.7737797670096676 on epoch=171
05/25/2022 22:47:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
05/25/2022 22:47:27 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=173
05/25/2022 22:47:30 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=173
05/25/2022 22:47:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
05/25/2022 22:47:35 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=174
05/25/2022 22:47:38 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7666892968309267 on epoch=174
05/25/2022 22:47:41 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=175
05/25/2022 22:47:43 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
05/25/2022 22:47:46 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/25/2022 22:47:48 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.11 on epoch=177
05/25/2022 22:47:51 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
05/25/2022 22:47:54 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.7682127249726871 on epoch=178
05/25/2022 22:47:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.06 on epoch=178
05/25/2022 22:47:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
05/25/2022 22:48:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/25/2022 22:48:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=180
05/25/2022 22:48:07 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
05/25/2022 22:48:10 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7712227227509056 on epoch=181
05/25/2022 22:48:13 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.11 on epoch=181
05/25/2022 22:48:15 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=182
05/25/2022 22:48:18 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/25/2022 22:48:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 22:48:22 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
05/25/2022 22:48:26 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.7660901313641555 on epoch=184
05/25/2022 22:48:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/25/2022 22:48:31 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
05/25/2022 22:48:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
05/25/2022 22:48:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=186
05/25/2022 22:48:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
05/25/2022 22:48:39 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:48:39 - INFO - __main__ - Printing 3 examples
05/25/2022 22:48:39 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:48:39 - INFO - __main__ - ['happy']
05/25/2022 22:48:39 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:48:39 - INFO - __main__ - ['happy']
05/25/2022 22:48:39 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:48:39 - INFO - __main__ - ['happy']
05/25/2022 22:48:39 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:48:39 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:48:40 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:48:40 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:48:40 - INFO - __main__ - Printing 3 examples
05/25/2022 22:48:40 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:48:40 - INFO - __main__ - ['happy']
05/25/2022 22:48:40 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:48:40 - INFO - __main__ - ['happy']
05/25/2022 22:48:40 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:48:40 - INFO - __main__ - ['happy']
05/25/2022 22:48:40 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:48:40 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:48:40 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:48:41 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7704945239411952 on epoch=187
05/25/2022 22:48:41 - INFO - __main__ - save last model!
05/25/2022 22:48:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 22:48:42 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 22:48:42 - INFO - __main__ - Printing 3 examples
05/25/2022 22:48:42 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 22:48:42 - INFO - __main__ - ['others']
05/25/2022 22:48:42 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 22:48:42 - INFO - __main__ - ['others']
05/25/2022 22:48:42 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 22:48:42 - INFO - __main__ - ['others']
05/25/2022 22:48:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:48:44 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:48:49 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 22:48:56 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:48:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:48:57 - INFO - __main__ - Starting training!
05/25/2022 22:50:02 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_42_0.3_8_predictions.txt
05/25/2022 22:50:02 - INFO - __main__ - Classification-F1 on test data: 0.4727
05/25/2022 22:50:02 - INFO - __main__ - prefix=emo_64_42, lr=0.3, bsz=8, dev_performance=0.7909335999030656, test_performance=0.4726550356793543
05/25/2022 22:50:02 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.2, bsz=8 ...
05/25/2022 22:50:03 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:50:03 - INFO - __main__ - Printing 3 examples
05/25/2022 22:50:03 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
05/25/2022 22:50:03 - INFO - __main__ - ['happy']
05/25/2022 22:50:03 - INFO - __main__ -  [emo] your right i'm always right i am impressed
05/25/2022 22:50:03 - INFO - __main__ - ['happy']
05/25/2022 22:50:03 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
05/25/2022 22:50:03 - INFO - __main__ - ['happy']
05/25/2022 22:50:03 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:50:03 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:50:04 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 22:50:04 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 22:50:04 - INFO - __main__ - Printing 3 examples
05/25/2022 22:50:04 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
05/25/2022 22:50:04 - INFO - __main__ - ['happy']
05/25/2022 22:50:04 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
05/25/2022 22:50:04 - INFO - __main__ - ['happy']
05/25/2022 22:50:04 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
05/25/2022 22:50:04 - INFO - __main__ - ['happy']
05/25/2022 22:50:04 - INFO - __main__ - Tokenizing Input ...
05/25/2022 22:50:04 - INFO - __main__ - Tokenizing Output ...
05/25/2022 22:50:04 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 22:50:23 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 22:50:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 22:50:23 - INFO - __main__ - Starting training!
05/25/2022 22:50:27 - INFO - __main__ - Step 10 Global step 10 Train loss 4.32 on epoch=0
05/25/2022 22:50:29 - INFO - __main__ - Step 20 Global step 20 Train loss 3.34 on epoch=1
05/25/2022 22:50:32 - INFO - __main__ - Step 30 Global step 30 Train loss 2.87 on epoch=1
05/25/2022 22:50:34 - INFO - __main__ - Step 40 Global step 40 Train loss 2.54 on epoch=2
05/25/2022 22:50:37 - INFO - __main__ - Step 50 Global step 50 Train loss 2.37 on epoch=3
05/25/2022 22:50:40 - INFO - __main__ - Global step 50 Train loss 3.09 Classification-F1 0.0 on epoch=3
05/25/2022 22:50:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.0 on epoch=3, global_step=50
05/25/2022 22:50:43 - INFO - __main__ - Step 60 Global step 60 Train loss 1.99 on epoch=3
05/25/2022 22:50:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.11 on epoch=4
05/25/2022 22:50:48 - INFO - __main__ - Step 80 Global step 80 Train loss 1.81 on epoch=4
05/25/2022 22:50:50 - INFO - __main__ - Step 90 Global step 90 Train loss 1.60 on epoch=5
05/25/2022 22:50:53 - INFO - __main__ - Step 100 Global step 100 Train loss 1.46 on epoch=6
05/25/2022 22:50:56 - INFO - __main__ - Global step 100 Train loss 1.79 Classification-F1 0.19702331216577543 on epoch=6
05/25/2022 22:50:56 - INFO - __main__ - Saving model with best Classification-F1: 0.0 -> 0.19702331216577543 on epoch=6, global_step=100
05/25/2022 22:50:59 - INFO - __main__ - Step 110 Global step 110 Train loss 1.34 on epoch=6
05/25/2022 22:51:01 - INFO - __main__ - Step 120 Global step 120 Train loss 1.38 on epoch=7
05/25/2022 22:51:04 - INFO - __main__ - Step 130 Global step 130 Train loss 1.19 on epoch=8
05/25/2022 22:51:06 - INFO - __main__ - Step 140 Global step 140 Train loss 1.09 on epoch=8
05/25/2022 22:51:09 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=9
05/25/2022 22:51:12 - INFO - __main__ - Global step 150 Train loss 1.22 Classification-F1 0.38907201782607065 on epoch=9
05/25/2022 22:51:12 - INFO - __main__ - Saving model with best Classification-F1: 0.19702331216577543 -> 0.38907201782607065 on epoch=9, global_step=150
05/25/2022 22:51:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=9
05/25/2022 22:51:17 - INFO - __main__ - Step 170 Global step 170 Train loss 1.03 on epoch=10
05/25/2022 22:51:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.94 on epoch=11
05/25/2022 22:51:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.97 on epoch=11
05/25/2022 22:51:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=12
05/25/2022 22:51:28 - INFO - __main__ - Global step 200 Train loss 0.95 Classification-F1 0.5168170001725411 on epoch=12
05/25/2022 22:51:28 - INFO - __main__ - Saving model with best Classification-F1: 0.38907201782607065 -> 0.5168170001725411 on epoch=12, global_step=200
05/25/2022 22:51:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=13
05/25/2022 22:51:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=13
05/25/2022 22:51:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=14
05/25/2022 22:51:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=14
05/25/2022 22:51:40 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=15
05/25/2022 22:51:44 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5139112210025156 on epoch=15
05/25/2022 22:51:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.69 on epoch=16
05/25/2022 22:51:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=16
05/25/2022 22:51:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=17
05/25/2022 22:51:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.71 on epoch=18
05/25/2022 22:51:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=18
05/25/2022 22:52:00 - INFO - __main__ - Global step 300 Train loss 0.72 Classification-F1 0.5297431447016434 on epoch=18
05/25/2022 22:52:00 - INFO - __main__ - Saving model with best Classification-F1: 0.5168170001725411 -> 0.5297431447016434 on epoch=18, global_step=300
05/25/2022 22:52:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.67 on epoch=19
05/25/2022 22:52:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=19
05/25/2022 22:52:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=20
05/25/2022 22:52:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.56 on epoch=21
05/25/2022 22:52:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.67 on epoch=21
05/25/2022 22:52:16 - INFO - __main__ - Global step 350 Train loss 0.65 Classification-F1 0.5786376919720284 on epoch=21
05/25/2022 22:52:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5297431447016434 -> 0.5786376919720284 on epoch=21, global_step=350
05/25/2022 22:52:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=22
05/25/2022 22:52:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=23
05/25/2022 22:52:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=23
05/25/2022 22:52:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.61 on epoch=24
05/25/2022 22:52:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=24
05/25/2022 22:52:32 - INFO - __main__ - Global step 400 Train loss 0.62 Classification-F1 0.5888088694770248 on epoch=24
05/25/2022 22:52:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5786376919720284 -> 0.5888088694770248 on epoch=24, global_step=400
05/25/2022 22:52:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=25
05/25/2022 22:52:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.60 on epoch=26
05/25/2022 22:52:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.70 on epoch=26
05/25/2022 22:52:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=27
05/25/2022 22:52:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=28
05/25/2022 22:52:48 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.6050081717206417 on epoch=28
05/25/2022 22:52:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5888088694770248 -> 0.6050081717206417 on epoch=28, global_step=450
05/25/2022 22:52:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=28
05/25/2022 22:52:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=29
05/25/2022 22:52:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=29
05/25/2022 22:52:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=30
05/25/2022 22:53:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=31
05/25/2022 22:53:04 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.6112206133406833 on epoch=31
05/25/2022 22:53:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6050081717206417 -> 0.6112206133406833 on epoch=31, global_step=500
05/25/2022 22:53:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=31
05/25/2022 22:53:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=32
05/25/2022 22:53:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.50 on epoch=33
05/25/2022 22:53:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.57 on epoch=33
05/25/2022 22:53:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=34
05/25/2022 22:53:19 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.6584703701574659 on epoch=34
05/25/2022 22:53:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6112206133406833 -> 0.6584703701574659 on epoch=34, global_step=550
05/25/2022 22:53:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=34
05/25/2022 22:53:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=35
05/25/2022 22:53:27 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=36
05/25/2022 22:53:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=36
05/25/2022 22:53:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=37
05/25/2022 22:53:35 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.6726285252716294 on epoch=37
05/25/2022 22:53:35 - INFO - __main__ - Saving model with best Classification-F1: 0.6584703701574659 -> 0.6726285252716294 on epoch=37, global_step=600
05/25/2022 22:53:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=38
05/25/2022 22:53:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.52 on epoch=38
05/25/2022 22:53:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=39
05/25/2022 22:53:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=39
05/25/2022 22:53:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.38 on epoch=40
05/25/2022 22:53:51 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.6903897889052528 on epoch=40
05/25/2022 22:53:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6726285252716294 -> 0.6903897889052528 on epoch=40, global_step=650
05/25/2022 22:53:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=41
05/25/2022 22:53:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=41
05/25/2022 22:53:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=42
05/25/2022 22:54:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=43
05/25/2022 22:54:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=43
05/25/2022 22:54:07 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.6921110214887463 on epoch=43
05/25/2022 22:54:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6903897889052528 -> 0.6921110214887463 on epoch=43, global_step=700
05/25/2022 22:54:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=44
05/25/2022 22:54:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=44
05/25/2022 22:54:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.45 on epoch=45
05/25/2022 22:54:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=46
05/25/2022 22:54:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=46
05/25/2022 22:54:23 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.7187502735400173 on epoch=46
05/25/2022 22:54:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6921110214887463 -> 0.7187502735400173 on epoch=46, global_step=750
05/25/2022 22:54:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=47
05/25/2022 22:54:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=48
05/25/2022 22:54:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=48
05/25/2022 22:54:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=49
05/25/2022 22:54:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
05/25/2022 22:54:39 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.7283140606670018 on epoch=49
05/25/2022 22:54:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7187502735400173 -> 0.7283140606670018 on epoch=49, global_step=800
05/25/2022 22:54:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=50
05/25/2022 22:54:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=51
05/25/2022 22:54:47 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
05/25/2022 22:54:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=52
05/25/2022 22:54:52 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=53
05/25/2022 22:54:55 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.7236694677871148 on epoch=53
05/25/2022 22:54:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=53
05/25/2022 22:55:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=54
05/25/2022 22:55:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
05/25/2022 22:55:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=55
05/25/2022 22:55:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=56
05/25/2022 22:55:11 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.7181984482464798 on epoch=56
05/25/2022 22:55:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
05/25/2022 22:55:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
05/25/2022 22:55:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=58
05/25/2022 22:55:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
05/25/2022 22:55:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=59
05/25/2022 22:55:27 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.722869283412747 on epoch=59
05/25/2022 22:55:29 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=59
05/25/2022 22:55:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
05/25/2022 22:55:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
05/25/2022 22:55:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=61
05/25/2022 22:55:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=62
05/25/2022 22:55:43 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.7223286769612223 on epoch=62
05/25/2022 22:55:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
05/25/2022 22:55:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
05/25/2022 22:55:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=64
05/25/2022 22:55:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
05/25/2022 22:55:55 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=65
05/25/2022 22:55:59 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.7318635160914956 on epoch=65
05/25/2022 22:55:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7283140606670018 -> 0.7318635160914956 on epoch=65, global_step=1050
05/25/2022 22:56:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
05/25/2022 22:56:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
05/25/2022 22:56:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=67
05/25/2022 22:56:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=68
05/25/2022 22:56:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=68
05/25/2022 22:56:14 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.7356756520220438 on epoch=68
05/25/2022 22:56:14 - INFO - __main__ - Saving model with best Classification-F1: 0.7318635160914956 -> 0.7356756520220438 on epoch=68, global_step=1100
05/25/2022 22:56:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
05/25/2022 22:56:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.42 on epoch=69
05/25/2022 22:56:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=70
05/25/2022 22:56:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=71
05/25/2022 22:56:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
05/25/2022 22:56:30 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.7354124250988834 on epoch=71
05/25/2022 22:56:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
05/25/2022 22:56:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.36 on epoch=73
05/25/2022 22:56:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=73
05/25/2022 22:56:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.38 on epoch=74
05/25/2022 22:56:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=74
05/25/2022 22:56:46 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.7209562841530055 on epoch=74
05/25/2022 22:56:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=75
05/25/2022 22:56:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=76
05/25/2022 22:56:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
05/25/2022 22:56:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=77
05/25/2022 22:56:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=78
05/25/2022 22:57:02 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.7332690728043854 on epoch=78
05/25/2022 22:57:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=78
05/25/2022 22:57:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.36 on epoch=79
05/25/2022 22:57:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=79
05/25/2022 22:57:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=80
05/25/2022 22:57:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=81
05/25/2022 22:57:18 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.7357113349561398 on epoch=81
05/25/2022 22:57:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7356756520220438 -> 0.7357113349561398 on epoch=81, global_step=1300
05/25/2022 22:57:21 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.30 on epoch=81
05/25/2022 22:57:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.31 on epoch=82
05/25/2022 22:57:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.32 on epoch=83
05/25/2022 22:57:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=83
05/25/2022 22:57:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
05/25/2022 22:57:34 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.7470727333083287 on epoch=84
05/25/2022 22:57:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7357113349561398 -> 0.7470727333083287 on epoch=84, global_step=1350
05/25/2022 22:57:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=84
05/25/2022 22:57:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=85
05/25/2022 22:57:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=86
05/25/2022 22:57:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
05/25/2022 22:57:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=87
05/25/2022 22:57:50 - INFO - __main__ - Global step 1400 Train loss 0.29 Classification-F1 0.7449621206428265 on epoch=87
05/25/2022 22:57:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=88
05/25/2022 22:57:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=88
05/25/2022 22:57:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=89
05/25/2022 22:58:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=89
05/25/2022 22:58:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
05/25/2022 22:58:06 - INFO - __main__ - Global step 1450 Train loss 0.29 Classification-F1 0.7511892949770331 on epoch=90
05/25/2022 22:58:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7470727333083287 -> 0.7511892949770331 on epoch=90, global_step=1450
05/25/2022 22:58:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=91
05/25/2022 22:58:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=91
05/25/2022 22:58:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=92
05/25/2022 22:58:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.35 on epoch=93
05/25/2022 22:58:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.27 on epoch=93
05/25/2022 22:58:22 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.7673904768338584 on epoch=93
05/25/2022 22:58:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7511892949770331 -> 0.7673904768338584 on epoch=93, global_step=1500
05/25/2022 22:58:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
05/25/2022 22:58:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=94
05/25/2022 22:58:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=95
05/25/2022 22:58:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
05/25/2022 22:58:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=96
05/25/2022 22:58:38 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7505697959822606 on epoch=96
05/25/2022 22:58:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=97
05/25/2022 22:58:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=98
05/25/2022 22:58:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=98
05/25/2022 22:58:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=99
05/25/2022 22:58:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=99
05/25/2022 22:58:54 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.7589019232213958 on epoch=99
05/25/2022 22:58:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.28 on epoch=100
05/25/2022 22:58:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
05/25/2022 22:59:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.29 on epoch=101
05/25/2022 22:59:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
05/25/2022 22:59:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=103
05/25/2022 22:59:10 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7597960511820341 on epoch=103
05/25/2022 22:59:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=103
05/25/2022 22:59:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.27 on epoch=104
05/25/2022 22:59:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=104
05/25/2022 22:59:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=105
05/25/2022 22:59:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
05/25/2022 22:59:26 - INFO - __main__ - Global step 1700 Train loss 0.27 Classification-F1 0.7551826347734136 on epoch=106
05/25/2022 22:59:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=106
05/25/2022 22:59:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
05/25/2022 22:59:33 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=108
05/25/2022 22:59:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=108
05/25/2022 22:59:38 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
05/25/2022 22:59:41 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7750902473462453 on epoch=109
05/25/2022 22:59:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7673904768338584 -> 0.7750902473462453 on epoch=109, global_step=1750
05/25/2022 22:59:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=109
05/25/2022 22:59:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.32 on epoch=110
05/25/2022 22:59:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=111
05/25/2022 22:59:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
05/25/2022 22:59:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
05/25/2022 22:59:57 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.7560305918514874 on epoch=112
05/25/2022 23:00:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
05/25/2022 23:00:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=113
05/25/2022 23:00:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
05/25/2022 23:00:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=114
05/25/2022 23:00:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=115
05/25/2022 23:00:13 - INFO - __main__ - Global step 1850 Train loss 0.26 Classification-F1 0.731134486616501 on epoch=115
05/25/2022 23:00:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=116
05/25/2022 23:00:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=116
05/25/2022 23:00:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=117
05/25/2022 23:00:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
05/25/2022 23:00:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
05/25/2022 23:00:29 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.7344887736649222 on epoch=118
05/25/2022 23:00:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
05/25/2022 23:00:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.29 on epoch=119
05/25/2022 23:00:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
05/25/2022 23:00:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
05/25/2022 23:00:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=121
05/25/2022 23:00:45 - INFO - __main__ - Global step 1950 Train loss 0.22 Classification-F1 0.7513757997631583 on epoch=121
05/25/2022 23:00:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.22 on epoch=122
05/25/2022 23:00:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=123
05/25/2022 23:00:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=123
05/25/2022 23:00:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
05/25/2022 23:00:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=124
05/25/2022 23:01:01 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.7640077998286954 on epoch=124
05/25/2022 23:01:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=125
05/25/2022 23:01:06 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.24 on epoch=126
05/25/2022 23:01:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
05/25/2022 23:01:11 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
05/25/2022 23:01:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=128
05/25/2022 23:01:17 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.7633308813317744 on epoch=128
05/25/2022 23:01:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=128
05/25/2022 23:01:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
05/25/2022 23:01:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.22 on epoch=129
05/25/2022 23:01:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=130
05/25/2022 23:01:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=131
05/25/2022 23:01:33 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7554779158040027 on epoch=131
05/25/2022 23:01:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=131
05/25/2022 23:01:38 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=132
05/25/2022 23:01:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.24 on epoch=133
05/25/2022 23:01:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.17 on epoch=133
05/25/2022 23:01:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=134
05/25/2022 23:01:49 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.7757825748845228 on epoch=134
05/25/2022 23:01:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7750902473462453 -> 0.7757825748845228 on epoch=134, global_step=2150
05/25/2022 23:01:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/25/2022 23:01:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.20 on epoch=135
05/25/2022 23:01:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.22 on epoch=136
05/25/2022 23:01:59 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
05/25/2022 23:02:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=137
05/25/2022 23:02:04 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7549190606475147 on epoch=137
05/25/2022 23:02:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=138
05/25/2022 23:02:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=138
05/25/2022 23:02:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
05/25/2022 23:02:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.13 on epoch=139
05/25/2022 23:02:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.17 on epoch=140
05/25/2022 23:02:20 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.7637138188608776 on epoch=140
05/25/2022 23:02:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=141
05/25/2022 23:02:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
05/25/2022 23:02:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
05/25/2022 23:02:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
05/25/2022 23:02:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=143
05/25/2022 23:02:36 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.7637138188608776 on epoch=143
05/25/2022 23:02:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=144
05/25/2022 23:02:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=144
05/25/2022 23:02:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
05/25/2022 23:02:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=146
05/25/2022 23:02:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
05/25/2022 23:02:52 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.7682211398227441 on epoch=146
05/25/2022 23:02:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
05/25/2022 23:02:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
05/25/2022 23:03:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
05/25/2022 23:03:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.16 on epoch=149
05/25/2022 23:03:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=149
05/25/2022 23:03:08 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.7584825027114557 on epoch=149
05/25/2022 23:03:11 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
05/25/2022 23:03:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
05/25/2022 23:03:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
05/25/2022 23:03:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=152
05/25/2022 23:03:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=153
05/25/2022 23:03:24 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.763907110117165 on epoch=153
05/25/2022 23:03:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=153
05/25/2022 23:03:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
05/25/2022 23:03:31 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/25/2022 23:03:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=155
05/25/2022 23:03:36 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=156
05/25/2022 23:03:40 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7643799581598625 on epoch=156
05/25/2022 23:03:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
05/25/2022 23:03:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=157
05/25/2022 23:03:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
05/25/2022 23:03:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.18 on epoch=158
05/25/2022 23:03:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=159
05/25/2022 23:03:56 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7687000329801131 on epoch=159
05/25/2022 23:03:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=159
05/25/2022 23:04:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
05/25/2022 23:04:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/25/2022 23:04:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=161
05/25/2022 23:04:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=162
05/25/2022 23:04:12 - INFO - __main__ - Global step 2600 Train loss 0.14 Classification-F1 0.7648128752967462 on epoch=162
05/25/2022 23:04:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.20 on epoch=163
05/25/2022 23:04:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
05/25/2022 23:04:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.10 on epoch=164
05/25/2022 23:04:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
05/25/2022 23:04:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
05/25/2022 23:04:28 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.7643129770992365 on epoch=165
05/25/2022 23:04:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.18 on epoch=166
05/25/2022 23:04:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=166
05/25/2022 23:04:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
05/25/2022 23:04:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=168
05/25/2022 23:04:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/25/2022 23:04:44 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.76677022417154 on epoch=168
05/25/2022 23:04:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.22 on epoch=169
05/25/2022 23:04:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
05/25/2022 23:04:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
05/25/2022 23:04:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.23 on epoch=171
05/25/2022 23:04:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
05/25/2022 23:05:00 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.7848287912804042 on epoch=171
05/25/2022 23:05:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7757825748845228 -> 0.7848287912804042 on epoch=171, global_step=2750
05/25/2022 23:05:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.15 on epoch=172
05/25/2022 23:05:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=173
05/25/2022 23:05:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
05/25/2022 23:05:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/25/2022 23:05:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
05/25/2022 23:05:15 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.768193886337543 on epoch=174
05/25/2022 23:05:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.15 on epoch=175
05/25/2022 23:05:20 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
05/25/2022 23:05:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.15 on epoch=176
05/25/2022 23:05:25 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.17 on epoch=177
05/25/2022 23:05:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=178
05/25/2022 23:05:31 - INFO - __main__ - Global step 2850 Train loss 0.16 Classification-F1 0.7597525163765138 on epoch=178
05/25/2022 23:05:34 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
05/25/2022 23:05:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
05/25/2022 23:05:39 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=179
05/25/2022 23:05:41 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=180
05/25/2022 23:05:44 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=181
05/25/2022 23:05:47 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.7650967555364838 on epoch=181
05/25/2022 23:05:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
05/25/2022 23:05:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
05/25/2022 23:05:55 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
05/25/2022 23:05:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=183
05/25/2022 23:06:00 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.13 on epoch=184
05/25/2022 23:06:03 - INFO - __main__ - Global step 2950 Train loss 0.15 Classification-F1 0.7599061705627834 on epoch=184
05/25/2022 23:06:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
05/25/2022 23:06:08 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
05/25/2022 23:06:11 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
05/25/2022 23:06:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=186
05/25/2022 23:06:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
05/25/2022 23:06:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:06:18 - INFO - __main__ - Printing 3 examples
05/25/2022 23:06:18 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:06:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:06:18 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:06:18 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:06:18 - INFO - __main__ - Printing 3 examples
05/25/2022 23:06:18 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:06:18 - INFO - __main__ - ['others']
05/25/2022 23:06:18 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:06:18 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:06:18 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:06:20 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.768256086269811 on epoch=187
05/25/2022 23:06:20 - INFO - __main__ - save last model!
05/25/2022 23:06:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:06:20 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:06:20 - INFO - __main__ - Printing 3 examples
05/25/2022 23:06:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:06:20 - INFO - __main__ - ['others']
05/25/2022 23:06:20 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:06:20 - INFO - __main__ - ['others']
05/25/2022 23:06:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:06:20 - INFO - __main__ - ['others']
05/25/2022 23:06:20 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:06:22 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:06:27 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:06:34 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:06:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:06:35 - INFO - __main__ - Starting training!
05/25/2022 23:07:41 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_42_0.2_8_predictions.txt
05/25/2022 23:07:41 - INFO - __main__ - Classification-F1 on test data: 0.4843
05/25/2022 23:07:41 - INFO - __main__ - prefix=emo_64_42, lr=0.2, bsz=8, dev_performance=0.7848287912804042, test_performance=0.484328658620208
05/25/2022 23:07:41 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.5, bsz=8 ...
05/25/2022 23:07:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:07:42 - INFO - __main__ - Printing 3 examples
05/25/2022 23:07:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:07:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:07:42 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:07:42 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:07:42 - INFO - __main__ - Printing 3 examples
05/25/2022 23:07:42 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:07:42 - INFO - __main__ - ['others']
05/25/2022 23:07:42 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:07:42 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:07:43 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:07:59 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:08:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:08:00 - INFO - __main__ - Starting training!
05/25/2022 23:08:03 - INFO - __main__ - Step 10 Global step 10 Train loss 3.99 on epoch=0
05/25/2022 23:08:05 - INFO - __main__ - Step 20 Global step 20 Train loss 2.78 on epoch=1
05/25/2022 23:08:08 - INFO - __main__ - Step 30 Global step 30 Train loss 2.18 on epoch=1
05/25/2022 23:08:10 - INFO - __main__ - Step 40 Global step 40 Train loss 1.89 on epoch=2
05/25/2022 23:08:13 - INFO - __main__ - Step 50 Global step 50 Train loss 1.56 on epoch=3
05/25/2022 23:08:16 - INFO - __main__ - Global step 50 Train loss 2.48 Classification-F1 0.3962011322011322 on epoch=3
05/25/2022 23:08:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3962011322011322 on epoch=3, global_step=50
05/25/2022 23:08:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.17 on epoch=3
05/25/2022 23:08:21 - INFO - __main__ - Step 70 Global step 70 Train loss 1.08 on epoch=4
05/25/2022 23:08:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=4
05/25/2022 23:08:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.82 on epoch=5
05/25/2022 23:08:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.78 on epoch=6
05/25/2022 23:08:32 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.50013211636989 on epoch=6
05/25/2022 23:08:32 - INFO - __main__ - Saving model with best Classification-F1: 0.3962011322011322 -> 0.50013211636989 on epoch=6, global_step=100
05/25/2022 23:08:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=6
05/25/2022 23:08:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.83 on epoch=7
05/25/2022 23:08:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.76 on epoch=8
05/25/2022 23:08:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=8
05/25/2022 23:08:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.73 on epoch=9
05/25/2022 23:08:47 - INFO - __main__ - Global step 150 Train loss 0.78 Classification-F1 0.5524977634532173 on epoch=9
05/25/2022 23:08:47 - INFO - __main__ - Saving model with best Classification-F1: 0.50013211636989 -> 0.5524977634532173 on epoch=9, global_step=150
05/25/2022 23:08:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=9
05/25/2022 23:08:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=10
05/25/2022 23:08:55 - INFO - __main__ - Step 180 Global step 180 Train loss 0.66 on epoch=11
05/25/2022 23:08:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=11
05/25/2022 23:09:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.69 on epoch=12
05/25/2022 23:09:03 - INFO - __main__ - Global step 200 Train loss 0.63 Classification-F1 0.6616057522815771 on epoch=12
05/25/2022 23:09:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5524977634532173 -> 0.6616057522815771 on epoch=12, global_step=200
05/25/2022 23:09:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=13
05/25/2022 23:09:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=13
05/25/2022 23:09:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.60 on epoch=14
05/25/2022 23:09:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=14
05/25/2022 23:09:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=15
05/25/2022 23:09:19 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.7125457028324744 on epoch=15
05/25/2022 23:09:19 - INFO - __main__ - Saving model with best Classification-F1: 0.6616057522815771 -> 0.7125457028324744 on epoch=15, global_step=250
05/25/2022 23:09:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=16
05/25/2022 23:09:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=16
05/25/2022 23:09:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=17
05/25/2022 23:09:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=18
05/25/2022 23:09:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=18
05/25/2022 23:09:34 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.6475966119297197 on epoch=18
05/25/2022 23:09:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=19
05/25/2022 23:09:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=19
05/25/2022 23:09:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=20
05/25/2022 23:09:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=21
05/25/2022 23:09:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=21
05/25/2022 23:09:50 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.6688458975461825 on epoch=21
05/25/2022 23:09:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.56 on epoch=22
05/25/2022 23:09:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=23
05/25/2022 23:09:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.34 on epoch=23
05/25/2022 23:10:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=24
05/25/2022 23:10:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.40 on epoch=24
05/25/2022 23:10:06 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.728565207958842 on epoch=24
05/25/2022 23:10:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7125457028324744 -> 0.728565207958842 on epoch=24, global_step=400
05/25/2022 23:10:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.48 on epoch=25
05/25/2022 23:10:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=26
05/25/2022 23:10:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=26
05/25/2022 23:10:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=27
05/25/2022 23:10:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=28
05/25/2022 23:10:22 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.7113141801021289 on epoch=28
05/25/2022 23:10:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=28
05/25/2022 23:10:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=29
05/25/2022 23:10:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=29
05/25/2022 23:10:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
05/25/2022 23:10:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=31
05/25/2022 23:10:37 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.7147996806386056 on epoch=31
05/25/2022 23:10:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.37 on epoch=31
05/25/2022 23:10:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
05/25/2022 23:10:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
05/25/2022 23:10:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=33
05/25/2022 23:10:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.30 on epoch=34
05/25/2022 23:10:53 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.7771276481802798 on epoch=34
05/25/2022 23:10:53 - INFO - __main__ - Saving model with best Classification-F1: 0.728565207958842 -> 0.7771276481802798 on epoch=34, global_step=550
05/25/2022 23:10:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=34
05/25/2022 23:10:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=35
05/25/2022 23:11:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
05/25/2022 23:11:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.31 on epoch=36
05/25/2022 23:11:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=37
05/25/2022 23:11:09 - INFO - __main__ - Global step 600 Train loss 0.32 Classification-F1 0.7704277009617058 on epoch=37
05/25/2022 23:11:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
05/25/2022 23:11:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
05/25/2022 23:11:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=39
05/25/2022 23:11:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=39
05/25/2022 23:11:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=40
05/25/2022 23:11:25 - INFO - __main__ - Global step 650 Train loss 0.30 Classification-F1 0.7567509032787192 on epoch=40
05/25/2022 23:11:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=41
05/25/2022 23:11:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=41
05/25/2022 23:11:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.32 on epoch=42
05/25/2022 23:11:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=43
05/25/2022 23:11:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=43
05/25/2022 23:11:40 - INFO - __main__ - Global step 700 Train loss 0.29 Classification-F1 0.7381344206165952 on epoch=43
05/25/2022 23:11:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=44
05/25/2022 23:11:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=44
05/25/2022 23:11:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=45
05/25/2022 23:11:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=46
05/25/2022 23:11:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=46
05/25/2022 23:11:56 - INFO - __main__ - Global step 750 Train loss 0.28 Classification-F1 0.7896754874595223 on epoch=46
05/25/2022 23:11:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7771276481802798 -> 0.7896754874595223 on epoch=46, global_step=750
05/25/2022 23:11:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=47
05/25/2022 23:12:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=48
05/25/2022 23:12:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=48
05/25/2022 23:12:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=49
05/25/2022 23:12:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=49
05/25/2022 23:12:12 - INFO - __main__ - Global step 800 Train loss 0.24 Classification-F1 0.7649592690417873 on epoch=49
05/25/2022 23:12:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=50
05/25/2022 23:12:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=51
05/25/2022 23:12:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=51
05/25/2022 23:12:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=52
05/25/2022 23:12:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=53
05/25/2022 23:12:27 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.7604399510284775 on epoch=53
05/25/2022 23:12:30 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=53
05/25/2022 23:12:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=54
05/25/2022 23:12:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=54
05/25/2022 23:12:37 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=55
05/25/2022 23:12:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
05/25/2022 23:12:43 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.7565484156821369 on epoch=56
05/25/2022 23:12:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=56
05/25/2022 23:12:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
05/25/2022 23:12:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=58
05/25/2022 23:12:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=58
05/25/2022 23:12:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=59
05/25/2022 23:12:59 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.7814725995664782 on epoch=59
05/25/2022 23:13:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=59
05/25/2022 23:13:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=60
05/25/2022 23:13:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=61
05/25/2022 23:13:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=61
05/25/2022 23:13:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=62
05/25/2022 23:13:15 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.7807339000064484 on epoch=62
05/25/2022 23:13:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=63
05/25/2022 23:13:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=63
05/25/2022 23:13:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=64
05/25/2022 23:13:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=64
05/25/2022 23:13:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=65
05/25/2022 23:13:30 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.775698581025525 on epoch=65
05/25/2022 23:13:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=66
05/25/2022 23:13:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=66
05/25/2022 23:13:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=67
05/25/2022 23:13:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=68
05/25/2022 23:13:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=68
05/25/2022 23:13:46 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.761563039263507 on epoch=68
05/25/2022 23:13:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=69
05/25/2022 23:13:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=69
05/25/2022 23:13:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=70
05/25/2022 23:13:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=71
05/25/2022 23:13:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=71
05/25/2022 23:14:02 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.7604555112429129 on epoch=71
05/25/2022 23:14:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=72
05/25/2022 23:14:07 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=73
05/25/2022 23:14:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=73
05/25/2022 23:14:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=74
05/25/2022 23:14:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=74
05/25/2022 23:14:18 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.7836448299573147 on epoch=74
05/25/2022 23:14:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=75
05/25/2022 23:14:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=76
05/25/2022 23:14:25 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=76
05/25/2022 23:14:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=77
05/25/2022 23:14:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=78
05/25/2022 23:14:34 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.7469782876521786 on epoch=78
05/25/2022 23:14:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=78
05/25/2022 23:14:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=79
05/25/2022 23:14:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=79
05/25/2022 23:14:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=80
05/25/2022 23:14:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=81
05/25/2022 23:14:49 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.7432630749532158 on epoch=81
05/25/2022 23:14:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=81
05/25/2022 23:14:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=82
05/25/2022 23:14:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.07 on epoch=83
05/25/2022 23:14:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=83
05/25/2022 23:15:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=84
05/25/2022 23:15:05 - INFO - __main__ - Global step 1350 Train loss 0.12 Classification-F1 0.7567053743117845 on epoch=84
05/25/2022 23:15:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=84
05/25/2022 23:15:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=85
05/25/2022 23:15:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=86
05/25/2022 23:15:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=86
05/25/2022 23:15:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.10 on epoch=87
05/25/2022 23:15:21 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.7875104427736006 on epoch=87
05/25/2022 23:15:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.08 on epoch=88
05/25/2022 23:15:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=88
05/25/2022 23:15:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=89
05/25/2022 23:15:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=89
05/25/2022 23:15:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
05/25/2022 23:15:37 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.7892177051213857 on epoch=90
05/25/2022 23:15:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=91
05/25/2022 23:15:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=91
05/25/2022 23:15:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=92
05/25/2022 23:15:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=93
05/25/2022 23:15:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=93
05/25/2022 23:15:53 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.7533438006562306 on epoch=93
05/25/2022 23:15:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=94
05/25/2022 23:15:58 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/25/2022 23:16:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=95
05/25/2022 23:16:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=96
05/25/2022 23:16:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=96
05/25/2022 23:16:09 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.756386431242732 on epoch=96
05/25/2022 23:16:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=97
05/25/2022 23:16:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=98
05/25/2022 23:16:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=98
05/25/2022 23:16:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.08 on epoch=99
05/25/2022 23:16:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=99
05/25/2022 23:16:24 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.7746231769059974 on epoch=99
05/25/2022 23:16:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=100
05/25/2022 23:16:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=101
05/25/2022 23:16:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=101
05/25/2022 23:16:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=102
05/25/2022 23:16:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.10 on epoch=103
05/25/2022 23:16:41 - INFO - __main__ - Global step 1650 Train loss 0.08 Classification-F1 0.7683929937892494 on epoch=103
05/25/2022 23:16:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=103
05/25/2022 23:16:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=104
05/25/2022 23:16:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=104
05/25/2022 23:16:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=105
05/25/2022 23:16:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=106
05/25/2022 23:16:56 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.7846693974589223 on epoch=106
05/25/2022 23:16:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=106
05/25/2022 23:17:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=107
05/25/2022 23:17:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=108
05/25/2022 23:17:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=108
05/25/2022 23:17:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=109
05/25/2022 23:17:13 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.7961915808805761 on epoch=109
05/25/2022 23:17:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7896754874595223 -> 0.7961915808805761 on epoch=109, global_step=1750
05/25/2022 23:17:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=109
05/25/2022 23:17:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=110
05/25/2022 23:17:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
05/25/2022 23:17:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=111
05/25/2022 23:17:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=112
05/25/2022 23:17:28 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.8041590936629104 on epoch=112
05/25/2022 23:17:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7961915808805761 -> 0.8041590936629104 on epoch=112, global_step=1800
05/25/2022 23:17:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=113
05/25/2022 23:17:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=113
05/25/2022 23:17:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=114
05/25/2022 23:17:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=114
05/25/2022 23:17:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=115
05/25/2022 23:17:44 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.8013833836554526 on epoch=115
05/25/2022 23:17:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=116
05/25/2022 23:17:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=116
05/25/2022 23:17:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=117
05/25/2022 23:17:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=118
05/25/2022 23:17:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=118
05/25/2022 23:18:00 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.7983780767957196 on epoch=118
05/25/2022 23:18:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/25/2022 23:18:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=119
05/25/2022 23:18:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=120
05/25/2022 23:18:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=121
05/25/2022 23:18:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=121
05/25/2022 23:18:16 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.765134299619112 on epoch=121
05/25/2022 23:18:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=122
05/25/2022 23:18:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
05/25/2022 23:18:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=123
05/25/2022 23:18:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=124
05/25/2022 23:18:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/25/2022 23:18:32 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.7892156862745098 on epoch=124
05/25/2022 23:18:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.07 on epoch=125
05/25/2022 23:18:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=126
05/25/2022 23:18:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=126
05/25/2022 23:18:42 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.01 on epoch=127
05/25/2022 23:18:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=128
05/25/2022 23:18:48 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.7822505565218958 on epoch=128
05/25/2022 23:18:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.06 on epoch=128
05/25/2022 23:18:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.04 on epoch=129
05/25/2022 23:18:55 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=129
05/25/2022 23:18:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=130
05/25/2022 23:19:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
05/25/2022 23:19:04 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.7855164093394343 on epoch=131
05/25/2022 23:19:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.03 on epoch=131
05/25/2022 23:19:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.03 on epoch=132
05/25/2022 23:19:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=133
05/25/2022 23:19:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=133
05/25/2022 23:19:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=134
05/25/2022 23:19:20 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.7864211550940947 on epoch=134
05/25/2022 23:19:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=134
05/25/2022 23:19:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=135
05/25/2022 23:19:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=136
05/25/2022 23:19:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=136
05/25/2022 23:19:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
05/25/2022 23:19:35 - INFO - __main__ - Global step 2200 Train loss 0.05 Classification-F1 0.7852232389821132 on epoch=137
05/25/2022 23:19:38 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=138
05/25/2022 23:19:40 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=138
05/25/2022 23:19:43 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=139
05/25/2022 23:19:45 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=139
05/25/2022 23:19:48 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=140
05/25/2022 23:19:51 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.7697129509571545 on epoch=140
05/25/2022 23:19:54 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/25/2022 23:19:56 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=141
05/25/2022 23:19:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=142
05/25/2022 23:20:01 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=143
05/25/2022 23:20:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
05/25/2022 23:20:07 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.7980931307418597 on epoch=143
05/25/2022 23:20:10 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=144
05/25/2022 23:20:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=144
05/25/2022 23:20:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.04 on epoch=145
05/25/2022 23:20:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=146
05/25/2022 23:20:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
05/25/2022 23:20:23 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.7825396520228063 on epoch=146
05/25/2022 23:20:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=147
05/25/2022 23:20:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=148
05/25/2022 23:20:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.03 on epoch=148
05/25/2022 23:20:33 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=149
05/25/2022 23:20:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
05/25/2022 23:20:39 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.8032844604246577 on epoch=149
05/25/2022 23:20:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.06 on epoch=150
05/25/2022 23:20:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=151
05/25/2022 23:20:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.07 on epoch=151
05/25/2022 23:20:49 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=152
05/25/2022 23:20:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=153
05/25/2022 23:20:55 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.7884461723849027 on epoch=153
05/25/2022 23:20:57 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/25/2022 23:21:00 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=154
05/25/2022 23:21:02 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.16 on epoch=154
05/25/2022 23:21:05 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=155
05/25/2022 23:21:07 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=156
05/25/2022 23:21:11 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7961754804773001 on epoch=156
05/25/2022 23:21:13 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=156
05/25/2022 23:21:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=157
05/25/2022 23:21:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=158
05/25/2022 23:21:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
05/25/2022 23:21:23 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.03 on epoch=159
05/25/2022 23:21:27 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.7846462845971813 on epoch=159
05/25/2022 23:21:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/25/2022 23:21:31 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/25/2022 23:21:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=161
05/25/2022 23:21:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=161
05/25/2022 23:21:39 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=162
05/25/2022 23:21:42 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.8057361400760594 on epoch=162
05/25/2022 23:21:42 - INFO - __main__ - Saving model with best Classification-F1: 0.8041590936629104 -> 0.8057361400760594 on epoch=162, global_step=2600
05/25/2022 23:21:45 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
05/25/2022 23:21:47 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
05/25/2022 23:21:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
05/25/2022 23:21:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=164
05/25/2022 23:21:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.03 on epoch=165
05/25/2022 23:21:58 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.8020638091757184 on epoch=165
05/25/2022 23:22:00 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=166
05/25/2022 23:22:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
05/25/2022 23:22:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=167
05/25/2022 23:22:08 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
05/25/2022 23:22:10 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/25/2022 23:22:13 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.8306243105023522 on epoch=168
05/25/2022 23:22:14 - INFO - __main__ - Saving model with best Classification-F1: 0.8057361400760594 -> 0.8306243105023522 on epoch=168, global_step=2700
05/25/2022 23:22:16 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
05/25/2022 23:22:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
05/25/2022 23:22:21 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=170
05/25/2022 23:22:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=171
05/25/2022 23:22:26 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=171
05/25/2022 23:22:29 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.8315879510216198 on epoch=171
05/25/2022 23:22:29 - INFO - __main__ - Saving model with best Classification-F1: 0.8306243105023522 -> 0.8315879510216198 on epoch=171, global_step=2750
05/25/2022 23:22:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/25/2022 23:22:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
05/25/2022 23:22:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.01 on epoch=173
05/25/2022 23:22:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=174
05/25/2022 23:22:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.00 on epoch=174
05/25/2022 23:22:45 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.82348068253913 on epoch=174
05/25/2022 23:22:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/25/2022 23:22:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=176
05/25/2022 23:22:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=176
05/25/2022 23:22:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=177
05/25/2022 23:22:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
05/25/2022 23:23:00 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.7971401042350983 on epoch=178
05/25/2022 23:23:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/25/2022 23:23:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
05/25/2022 23:23:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=179
05/25/2022 23:23:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
05/25/2022 23:23:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=181
05/25/2022 23:23:16 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.8000453250453251 on epoch=181
05/25/2022 23:23:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=181
05/25/2022 23:23:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
05/25/2022 23:23:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
05/25/2022 23:23:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
05/25/2022 23:23:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=184
05/25/2022 23:23:32 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.7940643061632365 on epoch=184
05/25/2022 23:23:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=184
05/25/2022 23:23:36 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=185
05/25/2022 23:23:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.01 on epoch=186
05/25/2022 23:23:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
05/25/2022 23:23:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=187
05/25/2022 23:23:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:23:45 - INFO - __main__ - Printing 3 examples
05/25/2022 23:23:45 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:23:45 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:23:45 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:23:45 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:23:45 - INFO - __main__ - Printing 3 examples
05/25/2022 23:23:45 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:23:45 - INFO - __main__ - ['others']
05/25/2022 23:23:45 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:23:46 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:23:46 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:23:47 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.7957590472151669 on epoch=187
05/25/2022 23:23:47 - INFO - __main__ - save last model!
05/25/2022 23:23:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:23:47 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:23:47 - INFO - __main__ - Printing 3 examples
05/25/2022 23:23:47 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:23:47 - INFO - __main__ - ['others']
05/25/2022 23:23:47 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:23:47 - INFO - __main__ - ['others']
05/25/2022 23:23:47 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:23:47 - INFO - __main__ - ['others']
05/25/2022 23:23:47 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:23:49 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:23:55 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:24:04 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:24:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:24:05 - INFO - __main__ - Starting training!
05/25/2022 23:25:08 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_87_0.5_8_predictions.txt
05/25/2022 23:25:08 - INFO - __main__ - Classification-F1 on test data: 0.4955
05/25/2022 23:25:08 - INFO - __main__ - prefix=emo_64_87, lr=0.5, bsz=8, dev_performance=0.8315879510216198, test_performance=0.4954831378815132
05/25/2022 23:25:08 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.4, bsz=8 ...
05/25/2022 23:25:09 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:25:09 - INFO - __main__ - Printing 3 examples
05/25/2022 23:25:09 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:25:09 - INFO - __main__ - ['others']
05/25/2022 23:25:09 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:25:09 - INFO - __main__ - ['others']
05/25/2022 23:25:09 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:25:09 - INFO - __main__ - ['others']
05/25/2022 23:25:09 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:25:09 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:25:10 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:25:10 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:25:10 - INFO - __main__ - Printing 3 examples
05/25/2022 23:25:10 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:25:10 - INFO - __main__ - ['others']
05/25/2022 23:25:10 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:25:10 - INFO - __main__ - ['others']
05/25/2022 23:25:10 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:25:10 - INFO - __main__ - ['others']
05/25/2022 23:25:10 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:25:10 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:25:10 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:25:29 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:25:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:25:30 - INFO - __main__ - Starting training!
05/25/2022 23:25:33 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=0
05/25/2022 23:25:35 - INFO - __main__ - Step 20 Global step 20 Train loss 2.94 on epoch=1
05/25/2022 23:25:38 - INFO - __main__ - Step 30 Global step 30 Train loss 2.40 on epoch=1
05/25/2022 23:25:40 - INFO - __main__ - Step 40 Global step 40 Train loss 2.10 on epoch=2
05/25/2022 23:25:43 - INFO - __main__ - Step 50 Global step 50 Train loss 1.72 on epoch=3
05/25/2022 23:25:47 - INFO - __main__ - Global step 50 Train loss 2.70 Classification-F1 0.18068528270509976 on epoch=3
05/25/2022 23:25:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.18068528270509976 on epoch=3, global_step=50
05/25/2022 23:25:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.44 on epoch=3
05/25/2022 23:25:52 - INFO - __main__ - Step 70 Global step 70 Train loss 1.22 on epoch=4
05/25/2022 23:25:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.05 on epoch=4
05/25/2022 23:25:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=5
05/25/2022 23:25:59 - INFO - __main__ - Step 100 Global step 100 Train loss 0.83 on epoch=6
05/25/2022 23:26:03 - INFO - __main__ - Global step 100 Train loss 1.08 Classification-F1 0.5210849520694381 on epoch=6
05/25/2022 23:26:03 - INFO - __main__ - Saving model with best Classification-F1: 0.18068528270509976 -> 0.5210849520694381 on epoch=6, global_step=100
05/25/2022 23:26:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.66 on epoch=6
05/25/2022 23:26:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=7
05/25/2022 23:26:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=8
05/25/2022 23:26:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.69 on epoch=8
05/25/2022 23:26:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.66 on epoch=9
05/25/2022 23:26:19 - INFO - __main__ - Global step 150 Train loss 0.74 Classification-F1 0.5208069060194682 on epoch=9
05/25/2022 23:26:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=9
05/25/2022 23:26:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.69 on epoch=10
05/25/2022 23:26:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=11
05/25/2022 23:26:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=11
05/25/2022 23:26:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=12
05/25/2022 23:26:35 - INFO - __main__ - Global step 200 Train loss 0.67 Classification-F1 0.6182685231247859 on epoch=12
05/25/2022 23:26:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5210849520694381 -> 0.6182685231247859 on epoch=12, global_step=200
05/25/2022 23:26:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=13
05/25/2022 23:26:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=13
05/25/2022 23:26:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=14
05/25/2022 23:26:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=14
05/25/2022 23:26:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.61 on epoch=15
05/25/2022 23:26:51 - INFO - __main__ - Global step 250 Train loss 0.65 Classification-F1 0.646244358478401 on epoch=15
05/25/2022 23:26:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6182685231247859 -> 0.646244358478401 on epoch=15, global_step=250
05/25/2022 23:26:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=16
05/25/2022 23:26:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.55 on epoch=16
05/25/2022 23:26:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=17
05/25/2022 23:27:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=18
05/25/2022 23:27:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.42 on epoch=18
05/25/2022 23:27:07 - INFO - __main__ - Global step 300 Train loss 0.56 Classification-F1 0.6688609662689413 on epoch=18
05/25/2022 23:27:07 - INFO - __main__ - Saving model with best Classification-F1: 0.646244358478401 -> 0.6688609662689413 on epoch=18, global_step=300
05/25/2022 23:27:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.59 on epoch=19
05/25/2022 23:27:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=19
05/25/2022 23:27:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=20
05/25/2022 23:27:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=21
05/25/2022 23:27:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=21
05/25/2022 23:27:23 - INFO - __main__ - Global step 350 Train loss 0.52 Classification-F1 0.647575540853159 on epoch=21
05/25/2022 23:27:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=22
05/25/2022 23:27:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=23
05/25/2022 23:27:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=23
05/25/2022 23:27:33 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=24
05/25/2022 23:27:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=24
05/25/2022 23:27:39 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.7233515543645473 on epoch=24
05/25/2022 23:27:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6688609662689413 -> 0.7233515543645473 on epoch=24, global_step=400
05/25/2022 23:27:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=25
05/25/2022 23:27:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=26
05/25/2022 23:27:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=26
05/25/2022 23:27:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.46 on epoch=27
05/25/2022 23:27:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=28
05/25/2022 23:27:55 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.6962211226756185 on epoch=28
05/25/2022 23:27:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=28
05/25/2022 23:28:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.57 on epoch=29
05/25/2022 23:28:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=29
05/25/2022 23:28:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=30
05/25/2022 23:28:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=31
05/25/2022 23:28:11 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.7519817364995431 on epoch=31
05/25/2022 23:28:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7233515543645473 -> 0.7519817364995431 on epoch=31, global_step=500
05/25/2022 23:28:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=31
05/25/2022 23:28:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=32
05/25/2022 23:28:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=33
05/25/2022 23:28:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=33
05/25/2022 23:28:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=34
05/25/2022 23:28:27 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.7417951574569221 on epoch=34
05/25/2022 23:28:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.38 on epoch=34
05/25/2022 23:28:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=35
05/25/2022 23:28:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
05/25/2022 23:28:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
05/25/2022 23:28:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
05/25/2022 23:28:43 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.7592524385783193 on epoch=37
05/25/2022 23:28:43 - INFO - __main__ - Saving model with best Classification-F1: 0.7519817364995431 -> 0.7592524385783193 on epoch=37, global_step=600
05/25/2022 23:28:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=38
05/25/2022 23:28:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=38
05/25/2022 23:28:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=39
05/25/2022 23:28:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=39
05/25/2022 23:28:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
05/25/2022 23:28:59 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.7702743396405369 on epoch=40
05/25/2022 23:28:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7592524385783193 -> 0.7702743396405369 on epoch=40, global_step=650
05/25/2022 23:29:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
05/25/2022 23:29:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=41
05/25/2022 23:29:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=42
05/25/2022 23:29:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=43
05/25/2022 23:29:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=43
05/25/2022 23:29:15 - INFO - __main__ - Global step 700 Train loss 0.36 Classification-F1 0.7728422619047619 on epoch=43
05/25/2022 23:29:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7702743396405369 -> 0.7728422619047619 on epoch=43, global_step=700
05/25/2022 23:29:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=44
05/25/2022 23:29:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=44
05/25/2022 23:29:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=45
05/25/2022 23:29:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=46
05/25/2022 23:29:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=46
05/25/2022 23:29:31 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.7624720805413875 on epoch=46
05/25/2022 23:29:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
05/25/2022 23:29:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=48
05/25/2022 23:29:39 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
05/25/2022 23:29:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=49
05/25/2022 23:29:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.34 on epoch=49
05/25/2022 23:29:47 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.7732348079155145 on epoch=49
05/25/2022 23:29:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7728422619047619 -> 0.7732348079155145 on epoch=49, global_step=800
05/25/2022 23:29:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=50
05/25/2022 23:29:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=51
05/25/2022 23:29:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.28 on epoch=51
05/25/2022 23:29:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=52
05/25/2022 23:30:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=53
05/25/2022 23:30:03 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.7675618625249836 on epoch=53
05/25/2022 23:30:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=53
05/25/2022 23:30:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=54
05/25/2022 23:30:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=54
05/25/2022 23:30:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=55
05/25/2022 23:30:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
05/25/2022 23:30:19 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.7817762450616781 on epoch=56
05/25/2022 23:30:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7732348079155145 -> 0.7817762450616781 on epoch=56, global_step=900
05/25/2022 23:30:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=56
05/25/2022 23:30:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=57
05/25/2022 23:30:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=58
05/25/2022 23:30:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=58
05/25/2022 23:30:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=59
05/25/2022 23:30:35 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.7664083914058644 on epoch=59
05/25/2022 23:30:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=59
05/25/2022 23:30:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=60
05/25/2022 23:30:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
05/25/2022 23:30:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=61
05/25/2022 23:30:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=62
05/25/2022 23:30:51 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.7768181892801445 on epoch=62
05/25/2022 23:30:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
05/25/2022 23:30:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=63
05/25/2022 23:30:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
05/25/2022 23:31:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=64
05/25/2022 23:31:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=65
05/25/2022 23:31:07 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.7975344967532467 on epoch=65
05/25/2022 23:31:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7817762450616781 -> 0.7975344967532467 on epoch=65, global_step=1050
05/25/2022 23:31:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=66
05/25/2022 23:31:13 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.16 on epoch=66
05/25/2022 23:31:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=67
05/25/2022 23:31:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
05/25/2022 23:31:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=68
05/25/2022 23:31:24 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.7898199560099819 on epoch=68
05/25/2022 23:31:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=69
05/25/2022 23:31:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=69
05/25/2022 23:31:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=70
05/25/2022 23:31:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=71
05/25/2022 23:31:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=71
05/25/2022 23:31:40 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.7778847953425956 on epoch=71
05/25/2022 23:31:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=72
05/25/2022 23:31:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=73
05/25/2022 23:31:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=73
05/25/2022 23:31:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
05/25/2022 23:31:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=74
05/25/2022 23:31:56 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.7831461526961272 on epoch=74
05/25/2022 23:31:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=75
05/25/2022 23:32:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=76
05/25/2022 23:32:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=76
05/25/2022 23:32:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
05/25/2022 23:32:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=78
05/25/2022 23:32:12 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.7976485739842595 on epoch=78
05/25/2022 23:32:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7975344967532467 -> 0.7976485739842595 on epoch=78, global_step=1250
05/25/2022 23:32:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=78
05/25/2022 23:32:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
05/25/2022 23:32:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=79
05/25/2022 23:32:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/25/2022 23:32:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=81
05/25/2022 23:32:28 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.776115711866507 on epoch=81
05/25/2022 23:32:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=81
05/25/2022 23:32:33 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=82
05/25/2022 23:32:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=83
05/25/2022 23:32:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=83
05/25/2022 23:32:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=84
05/25/2022 23:32:44 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.7912762837051426 on epoch=84
05/25/2022 23:32:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=84
05/25/2022 23:32:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=85
05/25/2022 23:32:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=86
05/25/2022 23:32:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=86
05/25/2022 23:32:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
05/25/2022 23:33:00 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.7873871688481273 on epoch=87
05/25/2022 23:33:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=88
05/25/2022 23:33:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=88
05/25/2022 23:33:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=89
05/25/2022 23:33:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=89
05/25/2022 23:33:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=90
05/25/2022 23:33:17 - INFO - __main__ - Global step 1450 Train loss 0.13 Classification-F1 0.7822059273134916 on epoch=90
05/25/2022 23:33:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=91
05/25/2022 23:33:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.12 on epoch=91
05/25/2022 23:33:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=92
05/25/2022 23:33:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=93
05/25/2022 23:33:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=93
05/25/2022 23:33:33 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.7861095067220488 on epoch=93
05/25/2022 23:33:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=94
05/25/2022 23:33:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
05/25/2022 23:33:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=95
05/25/2022 23:33:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=96
05/25/2022 23:33:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=96
05/25/2022 23:33:49 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.7631804247050752 on epoch=96
05/25/2022 23:33:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
05/25/2022 23:33:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=98
05/25/2022 23:33:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=98
05/25/2022 23:33:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=99
05/25/2022 23:34:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
05/25/2022 23:34:05 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.7896897494462902 on epoch=99
05/25/2022 23:34:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=100
05/25/2022 23:34:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=101
05/25/2022 23:34:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/25/2022 23:34:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=102
05/25/2022 23:34:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
05/25/2022 23:34:21 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.7896456940440411 on epoch=103
05/25/2022 23:34:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=103
05/25/2022 23:34:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=104
05/25/2022 23:34:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
05/25/2022 23:34:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=105
05/25/2022 23:34:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/25/2022 23:34:37 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.8199020510251243 on epoch=106
05/25/2022 23:34:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7976485739842595 -> 0.8199020510251243 on epoch=106, global_step=1700
05/25/2022 23:34:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=106
05/25/2022 23:34:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=107
05/25/2022 23:34:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/25/2022 23:34:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=108
05/25/2022 23:34:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=109
05/25/2022 23:34:53 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.8109409595509898 on epoch=109
05/25/2022 23:34:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
05/25/2022 23:34:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
05/25/2022 23:35:01 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=111
05/25/2022 23:35:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.05 on epoch=111
05/25/2022 23:35:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=112
05/25/2022 23:35:09 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.7961422258297258 on epoch=112
05/25/2022 23:35:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/25/2022 23:35:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=113
05/25/2022 23:35:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
05/25/2022 23:35:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=114
05/25/2022 23:35:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
05/25/2022 23:35:25 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.808698707988126 on epoch=115
05/25/2022 23:35:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=116
05/25/2022 23:35:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=116
05/25/2022 23:35:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=117
05/25/2022 23:35:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=118
05/25/2022 23:35:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
05/25/2022 23:35:42 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.8114595943047043 on epoch=118
05/25/2022 23:35:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
05/25/2022 23:35:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=119
05/25/2022 23:35:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=120
05/25/2022 23:35:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=121
05/25/2022 23:35:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=121
05/25/2022 23:35:58 - INFO - __main__ - Global step 1950 Train loss 0.07 Classification-F1 0.7980389610389611 on epoch=121
05/25/2022 23:36:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=122
05/25/2022 23:36:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=123
05/25/2022 23:36:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=123
05/25/2022 23:36:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=124
05/25/2022 23:36:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=124
05/25/2022 23:36:14 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.8019211704326208 on epoch=124
05/25/2022 23:36:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.08 on epoch=125
05/25/2022 23:36:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=126
05/25/2022 23:36:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.09 on epoch=126
05/25/2022 23:36:24 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=127
05/25/2022 23:36:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=128
05/25/2022 23:36:30 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.7893835308694295 on epoch=128
05/25/2022 23:36:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/25/2022 23:36:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=129
05/25/2022 23:36:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=129
05/25/2022 23:36:40 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=130
05/25/2022 23:36:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
05/25/2022 23:36:47 - INFO - __main__ - Global step 2100 Train loss 0.08 Classification-F1 0.7918013457553319 on epoch=131
05/25/2022 23:36:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=131
05/25/2022 23:36:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=132
05/25/2022 23:36:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
05/25/2022 23:36:56 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/25/2022 23:36:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=134
05/25/2022 23:37:02 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.7773023642709163 on epoch=134
05/25/2022 23:37:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=134
05/25/2022 23:37:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=135
05/25/2022 23:37:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.04 on epoch=136
05/25/2022 23:37:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
05/25/2022 23:37:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=137
05/25/2022 23:37:18 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.8159106578619296 on epoch=137
05/25/2022 23:37:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.09 on epoch=138
05/25/2022 23:37:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=138
05/25/2022 23:37:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
05/25/2022 23:37:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=139
05/25/2022 23:37:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.02 on epoch=140
05/25/2022 23:37:34 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.8125372752018596 on epoch=140
05/25/2022 23:37:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=141
05/25/2022 23:37:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=141
05/25/2022 23:37:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=142
05/25/2022 23:37:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
05/25/2022 23:37:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=143
05/25/2022 23:37:50 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.8003080801952298 on epoch=143
05/25/2022 23:37:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=144
05/25/2022 23:37:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=144
05/25/2022 23:37:57 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.06 on epoch=145
05/25/2022 23:38:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=146
05/25/2022 23:38:02 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
05/25/2022 23:38:06 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.8030521095819603 on epoch=146
05/25/2022 23:38:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
05/25/2022 23:38:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=148
05/25/2022 23:38:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=148
05/25/2022 23:38:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
05/25/2022 23:38:18 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
05/25/2022 23:38:21 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.807842687120762 on epoch=149
05/25/2022 23:38:24 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=150
05/25/2022 23:38:26 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=151
05/25/2022 23:38:29 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=151
05/25/2022 23:38:31 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.02 on epoch=152
05/25/2022 23:38:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=153
05/25/2022 23:38:37 - INFO - __main__ - Global step 2450 Train loss 0.06 Classification-F1 0.8009828513315731 on epoch=153
05/25/2022 23:38:40 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
05/25/2022 23:38:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
05/25/2022 23:38:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=154
05/25/2022 23:38:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=155
05/25/2022 23:38:50 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=156
05/25/2022 23:38:54 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.796496632996633 on epoch=156
05/25/2022 23:38:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.08 on epoch=156
05/25/2022 23:38:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=157
05/25/2022 23:39:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=158
05/25/2022 23:39:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=158
05/25/2022 23:39:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=159
05/25/2022 23:39:10 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.8027269666527282 on epoch=159
05/25/2022 23:39:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=159
05/25/2022 23:39:14 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
05/25/2022 23:39:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
05/25/2022 23:39:19 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.09 on epoch=161
05/25/2022 23:39:22 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
05/25/2022 23:39:25 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.7962188635320739 on epoch=162
05/25/2022 23:39:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
05/25/2022 23:39:30 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.12 on epoch=163
05/25/2022 23:39:32 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=164
05/25/2022 23:39:35 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/25/2022 23:39:37 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=165
05/25/2022 23:39:41 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.7998313830875454 on epoch=165
05/25/2022 23:39:43 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
05/25/2022 23:39:46 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
05/25/2022 23:39:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
05/25/2022 23:39:50 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=168
05/25/2022 23:39:53 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
05/25/2022 23:39:56 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.7991473925401191 on epoch=168
05/25/2022 23:39:59 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=169
05/25/2022 23:40:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=169
05/25/2022 23:40:04 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
05/25/2022 23:40:06 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
05/25/2022 23:40:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
05/25/2022 23:40:12 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.7936163159197434 on epoch=171
05/25/2022 23:40:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.01 on epoch=172
05/25/2022 23:40:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
05/25/2022 23:40:19 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
05/25/2022 23:40:22 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=174
05/25/2022 23:40:24 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=174
05/25/2022 23:40:28 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.8091999795576931 on epoch=174
05/25/2022 23:40:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
05/25/2022 23:40:32 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
05/25/2022 23:40:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
05/25/2022 23:40:37 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=177
05/25/2022 23:40:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=178
05/25/2022 23:40:43 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.8042929546087441 on epoch=178
05/25/2022 23:40:46 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
05/25/2022 23:40:48 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
05/25/2022 23:40:50 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=179
05/25/2022 23:40:53 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=180
05/25/2022 23:40:55 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=181
05/25/2022 23:40:59 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.8039957245968976 on epoch=181
05/25/2022 23:41:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=181
05/25/2022 23:41:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.02 on epoch=182
05/25/2022 23:41:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
05/25/2022 23:41:08 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
05/25/2022 23:41:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=184
05/25/2022 23:41:14 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.789222670439286 on epoch=184
05/25/2022 23:41:17 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=184
05/25/2022 23:41:19 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/25/2022 23:41:22 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
05/25/2022 23:41:24 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=186
05/25/2022 23:41:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/25/2022 23:41:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:41:28 - INFO - __main__ - Printing 3 examples
05/25/2022 23:41:28 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:41:28 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:41:28 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:41:28 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:41:28 - INFO - __main__ - Printing 3 examples
05/25/2022 23:41:28 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:41:28 - INFO - __main__ - ['others']
05/25/2022 23:41:28 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:41:28 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:41:29 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:41:30 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.8004757616874307 on epoch=187
05/25/2022 23:41:30 - INFO - __main__ - save last model!
05/25/2022 23:41:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:41:30 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:41:30 - INFO - __main__ - Printing 3 examples
05/25/2022 23:41:30 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:41:30 - INFO - __main__ - ['others']
05/25/2022 23:41:30 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:41:30 - INFO - __main__ - ['others']
05/25/2022 23:41:30 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:41:30 - INFO - __main__ - ['others']
05/25/2022 23:41:30 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:41:32 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:41:38 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:41:46 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:41:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:41:47 - INFO - __main__ - Starting training!
05/25/2022 23:42:55 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_87_0.4_8_predictions.txt
05/25/2022 23:42:55 - INFO - __main__ - Classification-F1 on test data: 0.5149
05/25/2022 23:42:56 - INFO - __main__ - prefix=emo_64_87, lr=0.4, bsz=8, dev_performance=0.8199020510251243, test_performance=0.5148655443830339
05/25/2022 23:42:56 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.3, bsz=8 ...
05/25/2022 23:42:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:42:57 - INFO - __main__ - Printing 3 examples
05/25/2022 23:42:57 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:42:57 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:42:57 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:42:57 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:42:57 - INFO - __main__ - Printing 3 examples
05/25/2022 23:42:57 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:42:57 - INFO - __main__ - ['others']
05/25/2022 23:42:57 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:42:57 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:42:57 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:43:16 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:43:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:43:17 - INFO - __main__ - Starting training!
05/25/2022 23:43:20 - INFO - __main__ - Step 10 Global step 10 Train loss 3.79 on epoch=0
05/25/2022 23:43:22 - INFO - __main__ - Step 20 Global step 20 Train loss 3.05 on epoch=1
05/25/2022 23:43:25 - INFO - __main__ - Step 30 Global step 30 Train loss 2.61 on epoch=1
05/25/2022 23:43:27 - INFO - __main__ - Step 40 Global step 40 Train loss 2.54 on epoch=2
05/25/2022 23:43:30 - INFO - __main__ - Step 50 Global step 50 Train loss 2.10 on epoch=3
05/25/2022 23:43:33 - INFO - __main__ - Global step 50 Train loss 2.82 Classification-F1 0.07083996651950566 on epoch=3
05/25/2022 23:43:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.07083996651950566 on epoch=3, global_step=50
05/25/2022 23:43:36 - INFO - __main__ - Step 60 Global step 60 Train loss 1.63 on epoch=3
05/25/2022 23:43:38 - INFO - __main__ - Step 70 Global step 70 Train loss 1.50 on epoch=4
05/25/2022 23:43:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.28 on epoch=4
05/25/2022 23:43:43 - INFO - __main__ - Step 90 Global step 90 Train loss 1.31 on epoch=5
05/25/2022 23:43:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=6
05/25/2022 23:43:49 - INFO - __main__ - Global step 100 Train loss 1.34 Classification-F1 0.3811752485073042 on epoch=6
05/25/2022 23:43:49 - INFO - __main__ - Saving model with best Classification-F1: 0.07083996651950566 -> 0.3811752485073042 on epoch=6, global_step=100
05/25/2022 23:43:51 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=6
05/25/2022 23:43:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=7
05/25/2022 23:43:56 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=8
05/25/2022 23:43:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.65 on epoch=8
05/25/2022 23:44:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
05/25/2022 23:44:04 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.5285345088855213 on epoch=9
05/25/2022 23:44:04 - INFO - __main__ - Saving model with best Classification-F1: 0.3811752485073042 -> 0.5285345088855213 on epoch=9, global_step=150
05/25/2022 23:44:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.79 on epoch=9
05/25/2022 23:44:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=10
05/25/2022 23:44:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=11
05/25/2022 23:44:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.72 on epoch=11
05/25/2022 23:44:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.72 on epoch=12
05/25/2022 23:44:20 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.5422630759016209 on epoch=12
05/25/2022 23:44:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5285345088855213 -> 0.5422630759016209 on epoch=12, global_step=200
05/25/2022 23:44:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=13
05/25/2022 23:44:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.64 on epoch=13
05/25/2022 23:44:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=14
05/25/2022 23:44:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=14
05/25/2022 23:44:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=15
05/25/2022 23:44:35 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.6053606936998162 on epoch=15
05/25/2022 23:44:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5422630759016209 -> 0.6053606936998162 on epoch=15, global_step=250
05/25/2022 23:44:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=16
05/25/2022 23:44:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=16
05/25/2022 23:44:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=17
05/25/2022 23:44:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.62 on epoch=18
05/25/2022 23:44:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.62 on epoch=18
05/25/2022 23:44:51 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.6418755060124554 on epoch=18
05/25/2022 23:44:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6053606936998162 -> 0.6418755060124554 on epoch=18, global_step=300
05/25/2022 23:44:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=19
05/25/2022 23:44:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=19
05/25/2022 23:44:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.65 on epoch=20
05/25/2022 23:45:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.58 on epoch=21
05/25/2022 23:45:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=21
05/25/2022 23:45:07 - INFO - __main__ - Global step 350 Train loss 0.57 Classification-F1 0.6520397197918206 on epoch=21
05/25/2022 23:45:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6418755060124554 -> 0.6520397197918206 on epoch=21, global_step=350
05/25/2022 23:45:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=22
05/25/2022 23:45:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.59 on epoch=23
05/25/2022 23:45:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=23
05/25/2022 23:45:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.56 on epoch=24
05/25/2022 23:45:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.57 on epoch=24
05/25/2022 23:45:22 - INFO - __main__ - Global step 400 Train loss 0.55 Classification-F1 0.664295745259911 on epoch=24
05/25/2022 23:45:22 - INFO - __main__ - Saving model with best Classification-F1: 0.6520397197918206 -> 0.664295745259911 on epoch=24, global_step=400
05/25/2022 23:45:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=25
05/25/2022 23:45:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=26
05/25/2022 23:45:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=26
05/25/2022 23:45:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=27
05/25/2022 23:45:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=28
05/25/2022 23:45:37 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.6559400110409127 on epoch=28
05/25/2022 23:45:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=28
05/25/2022 23:45:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=29
05/25/2022 23:45:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=29
05/25/2022 23:45:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=30
05/25/2022 23:45:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=31
05/25/2022 23:45:53 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.6789289997619151 on epoch=31
05/25/2022 23:45:53 - INFO - __main__ - Saving model with best Classification-F1: 0.664295745259911 -> 0.6789289997619151 on epoch=31, global_step=500
05/25/2022 23:45:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=31
05/25/2022 23:45:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=32
05/25/2022 23:46:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=33
05/25/2022 23:46:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=33
05/25/2022 23:46:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=34
05/25/2022 23:46:09 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.6826861130994989 on epoch=34
05/25/2022 23:46:09 - INFO - __main__ - Saving model with best Classification-F1: 0.6789289997619151 -> 0.6826861130994989 on epoch=34, global_step=550
05/25/2022 23:46:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=34
05/25/2022 23:46:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=35
05/25/2022 23:46:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=36
05/25/2022 23:46:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=36
05/25/2022 23:46:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=37
05/25/2022 23:46:24 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.7128388547133961 on epoch=37
05/25/2022 23:46:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6826861130994989 -> 0.7128388547133961 on epoch=37, global_step=600
05/25/2022 23:46:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=38
05/25/2022 23:46:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=38
05/25/2022 23:46:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=39
05/25/2022 23:46:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.39 on epoch=39
05/25/2022 23:46:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
05/25/2022 23:46:40 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.6764346118997282 on epoch=40
05/25/2022 23:46:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=41
05/25/2022 23:46:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=41
05/25/2022 23:46:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=42
05/25/2022 23:46:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=43
05/25/2022 23:46:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=43
05/25/2022 23:46:55 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.6864374082494953 on epoch=43
05/25/2022 23:46:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.52 on epoch=44
05/25/2022 23:47:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=44
05/25/2022 23:47:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=45
05/25/2022 23:47:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=46
05/25/2022 23:47:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=46
05/25/2022 23:47:11 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.6919793042571291 on epoch=46
05/25/2022 23:47:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
05/25/2022 23:47:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=48
05/25/2022 23:47:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.32 on epoch=48
05/25/2022 23:47:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
05/25/2022 23:47:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.33 on epoch=49
05/25/2022 23:47:26 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.7067494579144491 on epoch=49
05/25/2022 23:47:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.34 on epoch=50
05/25/2022 23:47:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
05/25/2022 23:47:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
05/25/2022 23:47:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.43 on epoch=52
05/25/2022 23:47:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=53
05/25/2022 23:47:42 - INFO - __main__ - Global step 850 Train loss 0.38 Classification-F1 0.7020314829291981 on epoch=53
05/25/2022 23:47:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
05/25/2022 23:47:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
05/25/2022 23:47:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=54
05/25/2022 23:47:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=55
05/25/2022 23:47:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
05/25/2022 23:47:57 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.7233936484630287 on epoch=56
05/25/2022 23:47:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7128388547133961 -> 0.7233936484630287 on epoch=56, global_step=900
05/25/2022 23:48:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
05/25/2022 23:48:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
05/25/2022 23:48:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=58
05/25/2022 23:48:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=58
05/25/2022 23:48:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=59
05/25/2022 23:48:13 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.761782947115488 on epoch=59
05/25/2022 23:48:13 - INFO - __main__ - Saving model with best Classification-F1: 0.7233936484630287 -> 0.761782947115488 on epoch=59, global_step=950
05/25/2022 23:48:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=59
05/25/2022 23:48:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=60
05/25/2022 23:48:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=61
05/25/2022 23:48:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=61
05/25/2022 23:48:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=62
05/25/2022 23:48:28 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.7541671809482666 on epoch=62
05/25/2022 23:48:31 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
05/25/2022 23:48:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=63
05/25/2022 23:48:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
05/25/2022 23:48:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
05/25/2022 23:48:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=65
05/25/2022 23:48:44 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.7735867074102367 on epoch=65
05/25/2022 23:48:44 - INFO - __main__ - Saving model with best Classification-F1: 0.761782947115488 -> 0.7735867074102367 on epoch=65, global_step=1050
05/25/2022 23:48:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=66
05/25/2022 23:48:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=66
05/25/2022 23:48:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=67
05/25/2022 23:48:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=68
05/25/2022 23:48:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
05/25/2022 23:48:59 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.7690674878652144 on epoch=68
05/25/2022 23:49:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=69
05/25/2022 23:49:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=69
05/25/2022 23:49:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=70
05/25/2022 23:49:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=71
05/25/2022 23:49:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=71
05/25/2022 23:49:15 - INFO - __main__ - Global step 1150 Train loss 0.26 Classification-F1 0.7667106539645812 on epoch=71
05/25/2022 23:49:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=72
05/25/2022 23:49:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=73
05/25/2022 23:49:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=73
05/25/2022 23:49:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=74
05/25/2022 23:49:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=74
05/25/2022 23:49:31 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7725390065127173 on epoch=74
05/25/2022 23:49:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=75
05/25/2022 23:49:36 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=76
05/25/2022 23:49:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=76
05/25/2022 23:49:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
05/25/2022 23:49:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
05/25/2022 23:49:46 - INFO - __main__ - Global step 1250 Train loss 0.24 Classification-F1 0.7719377480346754 on epoch=78
05/25/2022 23:49:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=78
05/25/2022 23:49:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=79
05/25/2022 23:49:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=79
05/25/2022 23:49:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=80
05/25/2022 23:49:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
05/25/2022 23:50:02 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.7692583716519886 on epoch=81
05/25/2022 23:50:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
05/25/2022 23:50:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=82
05/25/2022 23:50:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=83
05/25/2022 23:50:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
05/25/2022 23:50:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=84
05/25/2022 23:50:17 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7762802427024422 on epoch=84
05/25/2022 23:50:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7735867074102367 -> 0.7762802427024422 on epoch=84, global_step=1350
05/25/2022 23:50:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.30 on epoch=84
05/25/2022 23:50:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=85
05/25/2022 23:50:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
05/25/2022 23:50:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=86
05/25/2022 23:50:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=87
05/25/2022 23:50:33 - INFO - __main__ - Global step 1400 Train loss 0.24 Classification-F1 0.7725292889905206 on epoch=87
05/25/2022 23:50:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=88
05/25/2022 23:50:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=88
05/25/2022 23:50:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=89
05/25/2022 23:50:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
05/25/2022 23:50:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
05/25/2022 23:50:49 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7719529300419088 on epoch=90
05/25/2022 23:50:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=91
05/25/2022 23:50:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=91
05/25/2022 23:50:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
05/25/2022 23:50:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=93
05/25/2022 23:51:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=93
05/25/2022 23:51:04 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.7825902413818557 on epoch=93
05/25/2022 23:51:04 - INFO - __main__ - Saving model with best Classification-F1: 0.7762802427024422 -> 0.7825902413818557 on epoch=93, global_step=1500
05/25/2022 23:51:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
05/25/2022 23:51:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
05/25/2022 23:51:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=95
05/25/2022 23:51:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
05/25/2022 23:51:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=96
05/25/2022 23:51:20 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.7700476593515103 on epoch=96
05/25/2022 23:51:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=97
05/25/2022 23:51:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=98
05/25/2022 23:51:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=98
05/25/2022 23:51:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
05/25/2022 23:51:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
05/25/2022 23:51:35 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.7902467499684463 on epoch=99
05/25/2022 23:51:35 - INFO - __main__ - Saving model with best Classification-F1: 0.7825902413818557 -> 0.7902467499684463 on epoch=99, global_step=1600
05/25/2022 23:51:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=100
05/25/2022 23:51:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=101
05/25/2022 23:51:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=101
05/25/2022 23:51:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
05/25/2022 23:51:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.22 on epoch=103
05/25/2022 23:51:51 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.7802109368945206 on epoch=103
05/25/2022 23:51:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
05/25/2022 23:51:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
05/25/2022 23:51:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=104
05/25/2022 23:52:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=105
05/25/2022 23:52:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
05/25/2022 23:52:06 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.7698240828823906 on epoch=106
05/25/2022 23:52:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=106
05/25/2022 23:52:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
05/25/2022 23:52:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=108
05/25/2022 23:52:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
05/25/2022 23:52:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=109
05/25/2022 23:52:22 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.7717896573858187 on epoch=109
05/25/2022 23:52:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=109
05/25/2022 23:52:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
05/25/2022 23:52:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=111
05/25/2022 23:52:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=111
05/25/2022 23:52:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.17 on epoch=112
05/25/2022 23:52:38 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.7640412468725929 on epoch=112
05/25/2022 23:52:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
05/25/2022 23:52:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=113
05/25/2022 23:52:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=114
05/25/2022 23:52:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=114
05/25/2022 23:52:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
05/25/2022 23:52:53 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.7861390569966336 on epoch=115
05/25/2022 23:52:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
05/25/2022 23:52:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/25/2022 23:53:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=117
05/25/2022 23:53:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=118
05/25/2022 23:53:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=118
05/25/2022 23:53:09 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.7842128669513484 on epoch=118
05/25/2022 23:53:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=119
05/25/2022 23:53:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=119
05/25/2022 23:53:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=120
05/25/2022 23:53:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
05/25/2022 23:53:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=121
05/25/2022 23:53:25 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.782947236148058 on epoch=121
05/25/2022 23:53:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=122
05/25/2022 23:53:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=123
05/25/2022 23:53:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.06 on epoch=123
05/25/2022 23:53:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.27 on epoch=124
05/25/2022 23:53:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
05/25/2022 23:53:41 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.8007208225126151 on epoch=124
05/25/2022 23:53:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7902467499684463 -> 0.8007208225126151 on epoch=124, global_step=2000
05/25/2022 23:53:43 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=125
05/25/2022 23:53:45 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=126
05/25/2022 23:53:48 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
05/25/2022 23:53:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=127
05/25/2022 23:53:53 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=128
05/25/2022 23:53:56 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.7784164135137622 on epoch=128
05/25/2022 23:53:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=128
05/25/2022 23:54:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=129
05/25/2022 23:54:03 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=129
05/25/2022 23:54:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=130
05/25/2022 23:54:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=131
05/25/2022 23:54:12 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7911777447362189 on epoch=131
05/25/2022 23:54:14 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
05/25/2022 23:54:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
05/25/2022 23:54:19 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.16 on epoch=133
05/25/2022 23:54:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.08 on epoch=133
05/25/2022 23:54:24 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=134
05/25/2022 23:54:27 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.7844609794275492 on epoch=134
05/25/2022 23:54:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=134
05/25/2022 23:54:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=135
05/25/2022 23:54:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
05/25/2022 23:54:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=136
05/25/2022 23:54:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
05/25/2022 23:54:43 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.7802841977306394 on epoch=137
05/25/2022 23:54:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=138
05/25/2022 23:54:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
05/25/2022 23:54:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=139
05/25/2022 23:54:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
05/25/2022 23:54:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
05/25/2022 23:54:59 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7787322051103762 on epoch=140
05/25/2022 23:55:02 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
05/25/2022 23:55:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=141
05/25/2022 23:55:07 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/25/2022 23:55:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
05/25/2022 23:55:12 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=143
05/25/2022 23:55:15 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.7954981693409088 on epoch=143
05/25/2022 23:55:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
05/25/2022 23:55:20 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.18 on epoch=144
05/25/2022 23:55:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=145
05/25/2022 23:55:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
05/25/2022 23:55:27 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
05/25/2022 23:55:31 - INFO - __main__ - Global step 2350 Train loss 0.11 Classification-F1 0.78882535646996 on epoch=146
05/25/2022 23:55:33 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=147
05/25/2022 23:55:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=148
05/25/2022 23:55:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
05/25/2022 23:55:41 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=149
05/25/2022 23:55:43 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=149
05/25/2022 23:55:47 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.7916722701061598 on epoch=149
05/25/2022 23:55:49 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
05/25/2022 23:55:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
05/25/2022 23:55:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
05/25/2022 23:55:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=152
05/25/2022 23:55:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
05/25/2022 23:56:03 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.7881811815635346 on epoch=153
05/25/2022 23:56:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.07 on epoch=153
05/25/2022 23:56:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
05/25/2022 23:56:10 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
05/25/2022 23:56:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
05/25/2022 23:56:15 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
05/25/2022 23:56:18 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.7859183506456044 on epoch=156
05/25/2022 23:56:21 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
05/25/2022 23:56:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
05/25/2022 23:56:26 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.17 on epoch=158
05/25/2022 23:56:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
05/25/2022 23:56:31 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=159
05/25/2022 23:56:34 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.777372795003678 on epoch=159
05/25/2022 23:56:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=159
05/25/2022 23:56:39 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
05/25/2022 23:56:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=161
05/25/2022 23:56:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=161
05/25/2022 23:56:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/25/2022 23:56:50 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.7946210826210827 on epoch=162
05/25/2022 23:56:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
05/25/2022 23:56:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=163
05/25/2022 23:56:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=164
05/25/2022 23:57:00 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
05/25/2022 23:57:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
05/25/2022 23:57:06 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.7995989720139268 on epoch=165
05/25/2022 23:57:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=166
05/25/2022 23:57:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=166
05/25/2022 23:57:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
05/25/2022 23:57:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
05/25/2022 23:57:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=168
05/25/2022 23:57:21 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.789833503655657 on epoch=168
05/25/2022 23:57:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
05/25/2022 23:57:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=169
05/25/2022 23:57:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
05/25/2022 23:57:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.12 on epoch=171
05/25/2022 23:57:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=171
05/25/2022 23:57:37 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.7731486061997797 on epoch=171
05/25/2022 23:57:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
05/25/2022 23:57:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=173
05/25/2022 23:57:44 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
05/25/2022 23:57:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
05/25/2022 23:57:49 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.12 on epoch=174
05/25/2022 23:57:53 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.7917989463485535 on epoch=174
05/25/2022 23:57:55 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=175
05/25/2022 23:57:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
05/25/2022 23:58:00 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=176
05/25/2022 23:58:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
05/25/2022 23:58:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.15 on epoch=178
05/25/2022 23:58:09 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.7965414181847131 on epoch=178
05/25/2022 23:58:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
05/25/2022 23:58:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
05/25/2022 23:58:16 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
05/25/2022 23:58:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
05/25/2022 23:58:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
05/25/2022 23:58:24 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7856699460778362 on epoch=181
05/25/2022 23:58:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
05/25/2022 23:58:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=182
05/25/2022 23:58:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=183
05/25/2022 23:58:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
05/25/2022 23:58:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
05/25/2022 23:58:40 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.7895716834688875 on epoch=184
05/25/2022 23:58:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
05/25/2022 23:58:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
05/25/2022 23:58:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.14 on epoch=186
05/25/2022 23:58:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
05/25/2022 23:58:52 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
05/25/2022 23:58:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:58:54 - INFO - __main__ - Printing 3 examples
05/25/2022 23:58:54 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:58:54 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:58:54 - INFO - __main__ - Loaded 256 examples from train data
05/25/2022 23:58:54 - INFO - __main__ - Start tokenizing ... 256 instances
05/25/2022 23:58:54 - INFO - __main__ - Printing 3 examples
05/25/2022 23:58:54 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/25/2022 23:58:54 - INFO - __main__ - ['others']
05/25/2022 23:58:54 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:58:54 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:58:54 - INFO - __main__ - Loaded 256 examples from dev data
05/25/2022 23:58:56 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.7787721014082901 on epoch=187
05/25/2022 23:58:56 - INFO - __main__ - save last model!
05/25/2022 23:58:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/25/2022 23:58:56 - INFO - __main__ - Start tokenizing ... 5509 instances
05/25/2022 23:58:56 - INFO - __main__ - Printing 3 examples
05/25/2022 23:58:56 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/25/2022 23:58:56 - INFO - __main__ - ['others']
05/25/2022 23:58:56 - INFO - __main__ -  [emo] what you like very little things ok
05/25/2022 23:58:56 - INFO - __main__ - ['others']
05/25/2022 23:58:56 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/25/2022 23:58:56 - INFO - __main__ - ['others']
05/25/2022 23:58:56 - INFO - __main__ - Tokenizing Input ...
05/25/2022 23:58:58 - INFO - __main__ - Tokenizing Output ...
05/25/2022 23:59:03 - INFO - __main__ - Loaded 5509 examples from test data
05/25/2022 23:59:10 - INFO - __main__ - load prompt embedding from ckpt
05/25/2022 23:59:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/25/2022 23:59:11 - INFO - __main__ - Starting training!
05/26/2022 00:00:29 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_87_0.3_8_predictions.txt
05/26/2022 00:00:29 - INFO - __main__ - Classification-F1 on test data: 0.4940
05/26/2022 00:00:30 - INFO - __main__ - prefix=emo_64_87, lr=0.3, bsz=8, dev_performance=0.8007208225126151, test_performance=0.49396689280443196
05/26/2022 00:00:30 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.2, bsz=8 ...
05/26/2022 00:00:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 00:00:31 - INFO - __main__ - Printing 3 examples
05/26/2022 00:00:31 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:00:31 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:00:31 - INFO - __main__ - Loaded 256 examples from train data
05/26/2022 00:00:31 - INFO - __main__ - Start tokenizing ... 256 instances
05/26/2022 00:00:31 - INFO - __main__ - Printing 3 examples
05/26/2022 00:00:31 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
05/26/2022 00:00:31 - INFO - __main__ - ['others']
05/26/2022 00:00:31 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:00:31 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:00:31 - INFO - __main__ - Loaded 256 examples from dev data
05/26/2022 00:00:50 - INFO - __main__ - load prompt embedding from ckpt
05/26/2022 00:00:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
05/26/2022 00:00:51 - INFO - __main__ - Starting training!
05/26/2022 00:00:54 - INFO - __main__ - Step 10 Global step 10 Train loss 4.66 on epoch=0
05/26/2022 00:00:56 - INFO - __main__ - Step 20 Global step 20 Train loss 3.50 on epoch=1
05/26/2022 00:00:59 - INFO - __main__ - Step 30 Global step 30 Train loss 3.03 on epoch=1
05/26/2022 00:01:01 - INFO - __main__ - Step 40 Global step 40 Train loss 2.83 on epoch=2
05/26/2022 00:01:03 - INFO - __main__ - Step 50 Global step 50 Train loss 2.60 on epoch=3
05/26/2022 00:01:07 - INFO - __main__ - Global step 50 Train loss 3.32 Classification-F1 0.004830917874396135 on epoch=3
05/26/2022 00:01:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.004830917874396135 on epoch=3, global_step=50
05/26/2022 00:01:10 - INFO - __main__ - Step 60 Global step 60 Train loss 2.31 on epoch=3
05/26/2022 00:01:12 - INFO - __main__ - Step 70 Global step 70 Train loss 2.21 on epoch=4
05/26/2022 00:01:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.89 on epoch=4
05/26/2022 00:01:17 - INFO - __main__ - Step 90 Global step 90 Train loss 1.75 on epoch=5
05/26/2022 00:01:19 - INFO - __main__ - Step 100 Global step 100 Train loss 1.60 on epoch=6
05/26/2022 00:01:23 - INFO - __main__ - Global step 100 Train loss 1.95 Classification-F1 0.2214075735934819 on epoch=6
05/26/2022 00:01:23 - INFO - __main__ - Saving model with best Classification-F1: 0.004830917874396135 -> 0.2214075735934819 on epoch=6, global_step=100
05/26/2022 00:01:25 - INFO - __main__ - Step 110 Global step 110 Train loss 1.42 on epoch=6
05/26/2022 00:01:27 - INFO - __main__ - Step 120 Global step 120 Train loss 1.42 on epoch=7
05/26/2022 00:01:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.19 on epoch=8
05/26/2022 00:01:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.03 on epoch=8
05/26/2022 00:01:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=9
05/26/2022 00:01:38 - INFO - __main__ - Global step 150 Train loss 1.23 Classification-F1 0.3897634321012796 on epoch=9
05/26/2022 00:01:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2214075735934819 -> 0.3897634321012796 on epoch=9, global_step=150
05/26/2022 00:01:41 - INFO - __main__ - Step 160 Global step 160 Train loss 0.91 on epoch=9
05/26/2022 00:01:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=10
05/26/2022 00:01:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=11
05/26/2022 00:01:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.88 on epoch=11
05/26/2022 00:01:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.74 on epoch=12
05/26/2022 00:01:54 - INFO - __main__ - Global step 200 Train loss 0.82 Classification-F1 0.5145174676166545 on epoch=12
05/26/2022 00:01:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3897634321012796 -> 0.5145174676166545 on epoch=12, global_step=200
05/26/2022 00:01:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=13
05/26/2022 00:01:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.78 on epoch=13
05/26/2022 00:02:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.76 on epoch=14
05/26/2022 00:02:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=14
05/26/2022 00:02:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.71 on epoch=15
05/26/2022 00:02:09 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.5241909147397832 on epoch=15
05/26/2022 00:02:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5145174676166545 -> 0.5241909147397832 on epoch=15, global_step=250
05/26/2022 00:02:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=16
05/26/2022 00:02:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.81 on epoch=16
05/26/2022 00:02:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.77 on epoch=17
05/26/2022 00:02:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=18
05/26/2022 00:02:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=18
05/26/2022 00:02:25 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.5369544369471927 on epoch=18
05/26/2022 00:02:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5241909147397832 -> 0.5369544369471927 on epoch=18, global_step=300
05/26/2022 00:02:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=19
05/26/2022 00:02:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=19
05/26/2022 00:02:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=20
05/26/2022 00:02:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.72 on epoch=21
05/26/2022 00:02:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=21
05/26/2022 00:02:40 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.5364123272714152 on epoch=21
05/26/2022 00:02:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.75 on epoch=22
05/26/2022 00:02:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=23
05/26/2022 00:02:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=23
05/26/2022 00:02:50 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=24
05/26/2022 00:02:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.61 on epoch=24
05/26/2022 00:02:56 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.5917407670030047 on epoch=24
05/26/2022 00:02:56 - INFO - __main__ - Saving model with best Classification-F1: 0.5369544369471927 -> 0.5917407670030047 on epoch=24, global_step=400
05/26/2022 00:02:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.66 on epoch=25
05/26/2022 00:03:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
05/26/2022 00:03:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=26
05/26/2022 00:03:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=27
05/26/2022 00:03:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=28
05/26/2022 00:03:11 - INFO - __main__ - Global step 450 Train loss 0.58 Classification-F1 0.6286179387984251 on epoch=28
05/26/2022 00:03:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5917407670030047 -> 0.6286179387984251 on epoch=28, global_step=450
05/26/2022 00:03:14 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=28
05/26/2022 00:03:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.72 on epoch=29
05/26/2022 00:03:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=29
05/26/2022 00:03:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.53 on epoch=30
05/26/2022 00:03:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=31
05/26/2022 00:03:27 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.6574731415231192 on epoch=31
05/26/2022 00:03:27 - INFO - __main__ - Saving model with best Classification-F1: 0.6286179387984251 -> 0.6574731415231192 on epoch=31, global_step=500
05/26/2022 00:03:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=31
05/26/2022 00:03:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=32
05/26/2022 00:03:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=33
05/26/2022 00:03:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
05/26/2022 00:03:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=34
05/26/2022 00:03:42 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.6497431987545937 on epoch=34
05/26/2022 00:03:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.59 on epoch=34
05/26/2022 00:03:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.60 on epoch=35
05/26/2022 00:03:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.61 on epoch=36
05/26/2022 00:03:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=36
05/26/2022 00:03:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=37
05/26/2022 00:03:58 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.6804595691483647 on epoch=37
05/26/2022 00:03:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6574731415231192 -> 0.6804595691483647 on epoch=37, global_step=600
05/26/2022 00:04:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=38
05/26/2022 00:04:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=38
05/26/2022 00:04:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=39
05/26/2022 00:04:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=39
05/26/2022 00:04:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=40
05/26/2022 00:04:13 - INFO - __main__ - Global step 650 Train loss 0.56 Classification-F1 0.692385461458503 on epoch=40
05/26/2022 00:04:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6804595691483647 -> 0.692385461458503 on epoch=40, global_step=650
05/26/2022 00:04:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=41
05/26/2022 00:04:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=41
05/26/2022 00:04:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=42
05/26/2022 00:04:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.59 on epoch=43
05/26/2022 00:04:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=43
05/26/2022 00:04:29 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.6579290263113793 on epoch=43
05/26/2022 00:04:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.58 on epoch=44
05/26/2022 00:04:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=44
05/26/2022 00:04:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=45
05/26/2022 00:04:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=46
05/26/2022 00:04:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=46
05/26/2022 00:04:44 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.6976942324232964 on epoch=46
05/26/2022 00:04:44 - INFO - __main__ - Saving model with best Classification-F1: 0.692385461458503 -> 0.6976942324232964 on epoch=46, global_step=750
05/26/2022 00:04:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=47
05/26/2022 00:04:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.57 on epoch=48
05/26/2022 00:04:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=48
05/26/2022 00:04:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.62 on epoch=49
05/26/2022 00:04:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=49
05/26/2022 00:05:00 - INFO - __main__ - Global step 800 Train loss 0.55 Classification-F1 0.6960994459392629 on epoch=49
05/26/2022 00:05:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=50
05/26/2022 00:05:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=51
05/26/2022 00:05:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=51
05/26/2022 00:05:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=52
05/26/2022 00:05:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=53
05/26/2022 00:05:15 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.6876047427518015 on epoch=53
05/26/2022 00:05:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=53
05/26/2022 00:05:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.48 on epoch=54
05/26/2022 00:05:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=54
05/26/2022 00:05:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=55
05/26/2022 00:05:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.52 on epoch=56
05/26/2022 00:05:31 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.7125688168917541 on epoch=56
05/26/2022 00:05:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6976942324232964 -> 0.7125688168917541 on epoch=56, global_step=900
05/26/2022 00:05:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
05/26/2022 00:05:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=57
05/26/2022 00:05:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.54 on epoch=58
05/26/2022 00:05:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
05/26/2022 00:05:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=59
05/26/2022 00:05:47 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.7408857760840457 on epoch=59
05/26/2022 00:05:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7125688168917541 -> 0.7408857760840457 on epoch=59, global_step=950
05/26/2022 00:05:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=59
05/26/2022 00:05:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=60
05/26/2022 00:05:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
05/26/2022 00:05:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.32 on epoch=61
05/26/2022 00:05:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
05/26/2022 00:06:02 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.7227562369312854 on epoch=62
05/26/2022 00:06:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=63
05/26/2022 00:06:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=63
05/26/2022 00:06:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=64
05/26/2022 00:06:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=64
05/26/2022 00:06:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=65
05/26/2022 00:06:18 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.745613632957643 on epoch=65
05/26/2022 00:06:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7408857760840457 -> 0.745613632957643 on epoch=65, global_step=1050
05/26/2022 00:06:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=66
05/26/2022 00:06:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
05/26/2022 00:06:25 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=67
05/26/2022 00:06:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=68
05/26/2022 00:06:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=68
05/26/2022 00:06:33 - INFO - __main__ - Global step 1100 Train loss 0.40 Classification-F1 0.7344266850495107 on epoch=68
05/26/2022 00:06:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
05/26/2022 00:06:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=69
05/26/2022 00:06:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
05/26/2022 00:06:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
05/26/2022 00:06:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=71
05/26/2022 00:06:49 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.722929812576006 on epoch=71
05/26/2022 00:06:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
05/26/2022 00:06:53 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=73
05/26/2022 00:06:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=73
05/26/2022 00:06:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.39 on epoch=74
05/26/2022 00:07:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=74
05/26/2022 00:07:04 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.7433042067432867 on epoch=74
05/26/2022 00:07:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.36 on epoch=75
05/26/2022 00:07:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=76
05/26/2022 00:07:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=76
05/26/2022 00:07:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=77
05/26/2022 00:07:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=78
05/26/2022 00:07:20 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.7332437629440615 on epoch=78
05/26/2022 00:07:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=78
05/26/2022 00:07:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=79
05/26/2022 00:07:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=79
05/26/2022 00:07:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
05/26/2022 00:07:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=81
05/26/2022 00:07:35 - INFO - __main__ - Global step 1300 Train loss 0.34 Classification-F1 0.7374881958473289 on epoch=81
05/26/2022 00:07:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=81
05/26/2022 00:07:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=82
05/26/2022 00:07:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=83
05/26/2022 00:07:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=83
05/26/2022 00:07:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=84
05/26/2022 00:07:51 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.7394181077320613 on epoch=84
05/26/2022 00:07:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=84
05/26/2022 00:07:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=85
05/26/2022 00:07:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=86
05/26/2022 00:08:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
05/26/2022 00:08:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=87
05/26/2022 00:08:06 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.757441238548622 on epoch=87
05/26/2022 00:08:06 - INFO - __main__ - Saving model with best Classification-F1: 0.745613632957643 -> 0.757441238548622 on epoch=87, global_step=1400
05/26/2022 00:08:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.34 on epoch=88
05/26/2022 00:08:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
05/26/2022 00:08:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=89
05/26/2022 00:08:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=89
05/26/2022 00:08:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=90
05/26/2022 00:08:22 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.7480214473849242 on epoch=90
05/26/2022 00:08:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=91
05/26/2022 00:08:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.36 on epoch=91
05/26/2022 00:08:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=92
05/26/2022 00:08:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=93
05/26/2022 00:08:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
05/26/2022 00:08:37 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.7510094009512315 on epoch=93
05/26/2022 00:08:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.31 on epoch=94
05/26/2022 00:08:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=94
05/26/2022 00:08:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
05/26/2022 00:08:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=96
05/26/2022 00:08:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=96
05/26/2022 00:08:53 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.7453807609291739 on epoch=96
05/26/2022 00:08:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=97
05/26/2022 00:08:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=98
05/26/2022 00:09:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=98
05/26/2022 00:09:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=99
05/26/2022 00:09:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=99
05/26/2022 00:09:09 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.7507086938744212 on epoch=99
05/26/2022 00:09:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=100
05/26/2022 00:09:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=101
05/26/2022 00:09:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=101
05/26/2022 00:09:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
05/26/2022 00:09:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=103
05/26/2022 00:09:24 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7584147325117901 on epoch=103
05/26/2022 00:09:24 - INFO - __main__ - Saving model with best Classification-F1: 0.757441238548622 -> 0.7584147325117901 on epoch=103, global_step=1650
05/26/2022 00:09:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
05/26/2022 00:09:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=104
05/26/2022 00:09:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=104
05/26/2022 00:09:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=105
05/26/2022 00:09:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
05/26/2022 00:09:39 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.7652915924352264 on epoch=106
05/26/2022 00:09:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7584147325117901 -> 0.7652915924352264 on epoch=106, global_step=1700
05/26/2022 00:09:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=106
05/26/2022 00:09:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=107
05/26/2022 00:09:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.30 on epoch=108
05/26/2022 00:09:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=108
05/26/2022 00:09:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=109
05/26/2022 00:09:55 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7917931296130234 on epoch=109
05/26/2022 00:09:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7652915924352264 -> 0.7917931296130234 on epoch=109, global_step=1750
05/26/2022 00:09:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
05/26/2022 00:10:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=110
05/26/2022 00:10:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=111
05/26/2022 00:10:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.31 on epoch=111
05/26/2022 00:10:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=112
05/26/2022 00:10:10 - INFO - __main__ - Global step 1800 Train loss 0.29 Classification-F1 0.7875536083472316 on epoch=112
05/26/2022 00:10:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
05/26/2022 00:10:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=113
05/26/2022 00:10:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=114
05/26/2022 00:10:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=114
05/26/2022 00:10:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=115
05/26/2022 00:10:26 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7812416954080517 on epoch=115
05/26/2022 00:10:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=116
05/26/2022 00:10:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
05/26/2022 00:10:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
05/26/2022 00:10:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=118
05/26/2022 00:10:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=118
05/26/2022 00:10:41 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.7890998256219082 on epoch=118
05/26/2022 00:10:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=119
05/26/2022 00:10:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=119
05/26/2022 00:10:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=120
05/26/2022 00:10:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
05/26/2022 00:10:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
05/26/2022 00:10:57 - INFO - __main__ - Global step 1950 Train loss 0.24 Classification-F1 0.7658520770010131 on epoch=121
05/26/2022 00:10:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=122
05/26/2022 00:11:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=123
05/26/2022 00:11:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=123
05/26/2022 00:11:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=124
05/26/2022 00:11:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=124
05/26/2022 00:11:12 - INFO - __main__ - Global step 2000 Train loss 0.25 Classification-F1 0.7803225572329131 on epoch=124
05/26/2022 00:11:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=125
05/26/2022 00:11:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.27 on epoch=126
05/26/2022 00:11:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.16 on epoch=126
05/26/2022 00:11:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=127
05/26/2022 00:11:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=128
05/26/2022 00:11:28 - INFO - __main__ - Global step 2050 Train loss 0.23 Classification-F1 0.7875953309137991 on epoch=128
05/26/2022 00:11:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
05/26/2022 00:11:33 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.19 on epoch=129
05/26/2022 00:11:35 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.21 on epoch=129
05/26/2022 00:11:38 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=130
05/26/2022 00:11:40 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
05/26/2022 00:11:44 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.8006143970748731 on epoch=131
05/26/2022 00:11:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7917931296130234 -> 0.8006143970748731 on epoch=131, global_step=2100
05/26/2022 00:11:46 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=131
05/26/2022 00:11:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
05/26/2022 00:11:51 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.17 on epoch=133
05/26/2022 00:11:53 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=133
05/26/2022 00:11:56 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=134
05/26/2022 00:11:59 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.7817004650079865 on epoch=134
05/26/2022 00:12:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=134
05/26/2022 00:12:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
05/26/2022 00:12:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.28 on epoch=136
05/26/2022 00:12:09 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.22 on epoch=136
05/26/2022 00:12:11 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
05/26/2022 00:12:15 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.8028869075452756 on epoch=137
05/26/2022 00:12:15 - INFO - __main__ - Saving model with best Classification-F1: 0.8006143970748731 -> 0.8028869075452756 on epoch=137, global_step=2200
05/26/2022 00:12:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.27 on epoch=138
05/26/2022 00:12:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=138
05/26/2022 00:12:22 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.18 on epoch=139
05/26/2022 00:12:25 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.21 on epoch=139
05/26/2022 00:12:27 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
05/26/2022 00:12:30 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.7899520156579299 on epoch=140
05/26/2022 00:12:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
05/26/2022 00:12:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
05/26/2022 00:12:38 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
05/26/2022 00:12:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=143
05/26/2022 00:12:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
05/26/2022 00:12:46 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.801654708488649 on epoch=143
05/26/2022 00:12:48 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
05/26/2022 00:12:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.26 on epoch=144
05/26/2022 00:12:53 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=145
05/26/2022 00:12:56 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=146
05/26/2022 00:12:58 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
05/26/2022 00:13:01 - INFO - __main__ - Global step 2350 Train loss 0.20 Classification-F1 0.7976019634907102 on epoch=146
05/26/2022 00:13:04 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=147
05/26/2022 00:13:06 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
05/26/2022 00:13:09 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.20 on epoch=148
05/26/2022 00:13:11 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=149
05/26/2022 00:13:14 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=149
05/26/2022 00:13:17 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.8061658642118578 on epoch=149
05/26/2022 00:13:17 - INFO - __main__ - Saving model with best Classification-F1: 0.8028869075452756 -> 0.8061658642118578 on epoch=149, global_step=2400
05/26/2022 00:13:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.25 on epoch=150
05/26/2022 00:13:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=151
05/26/2022 00:13:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=151
05/26/2022 00:13:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
05/26/2022 00:13:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
05/26/2022 00:13:33 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.8126154899663494 on epoch=153
05/26/2022 00:13:33 - INFO - __main__ - Saving model with best Classification-F1: 0.8061658642118578 -> 0.8126154899663494 on epoch=153, global_step=2450
05/26/2022 00:13:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=153
05/26/2022 00:13:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
05/26/2022 00:13:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=154
05/26/2022 00:13:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=155
05/26/2022 00:13:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.23 on epoch=156
05/26/2022 00:13:48 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7909618275605736 on epoch=156
05/26/2022 00:13:50 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
05/26/2022 00:13:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=157
05/26/2022 00:13:55 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=158
05/26/2022 00:13:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
05/26/2022 00:14:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
05/26/2022 00:14:04 - INFO - __main__ - Global step 2550 Train loss 0.15 Classification-F1 0.7973892756159103 on epoch=159
05/26/2022 00:14:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
05/26/2022 00:14:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=160
05/26/2022 00:14:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
05/26/2022 00:14:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
05/26/2022 00:14:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
05/26/2022 00:14:19 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.7887488328664799 on epoch=162
05/26/2022 00:14:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=163
05/26/2022 00:14:24 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.10 on epoch=163
05/26/2022 00:14:26 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.22 on epoch=164
05/26/2022 00:14:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
05/26/2022 00:14:31 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
05/26/2022 00:14:35 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.8009820078789266 on epoch=165
05/26/2022 00:14:37 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.23 on epoch=166
05/26/2022 00:14:40 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.16 on epoch=166
05/26/2022 00:14:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=167
05/26/2022 00:14:44 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=168
05/26/2022 00:14:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
05/26/2022 00:14:50 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.8022533057934916 on epoch=168
05/26/2022 00:14:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.17 on epoch=169
05/26/2022 00:14:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=169
05/26/2022 00:14:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=170
05/26/2022 00:15:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.15 on epoch=171
05/26/2022 00:15:02 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.12 on epoch=171
05/26/2022 00:15:06 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.7968366013238878 on epoch=171
05/26/2022 00:15:08 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
05/26/2022 00:15:11 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.19 on epoch=173
05/26/2022 00:15:13 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.17 on epoch=173
05/26/2022 00:15:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.15 on epoch=174
05/26/2022 00:15:18 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.17 on epoch=174
05/26/2022 00:15:21 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.8011736843853632 on epoch=174
05/26/2022 00:15:24 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.13 on epoch=175
05/26/2022 00:15:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.15 on epoch=176
05/26/2022 00:15:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.20 on epoch=176
05/26/2022 00:15:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
05/26/2022 00:15:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
05/26/2022 00:15:37 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.785345935303237 on epoch=178
05/26/2022 00:15:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=178
05/26/2022 00:15:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.15 on epoch=179
05/26/2022 00:15:44 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.22 on epoch=179
05/26/2022 00:15:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.13 on epoch=180
05/26/2022 00:15:49 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=181
05/26/2022 00:15:52 - INFO - __main__ - Global step 2900 Train loss 0.15 Classification-F1 0.7889820971867008 on epoch=181
05/26/2022 00:15:55 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.14 on epoch=181
05/26/2022 00:15:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=182
05/26/2022 00:16:00 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.18 on epoch=183
05/26/2022 00:16:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.19 on epoch=183
05/26/2022 00:16:04 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
05/26/2022 00:16:08 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.8052422428303074 on epoch=184
05/26/2022 00:16:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=184
05/26/2022 00:16:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
05/26/2022 00:16:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.09 on epoch=186
05/26/2022 00:16:18 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=186
05/26/2022 00:16:20 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=187
05/26/2022 00:16:23 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.7993746595468045 on epoch=187
05/26/2022 00:16:23 - INFO - __main__ - save last model!
05/26/2022 00:16:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
05/26/2022 00:16:24 - INFO - __main__ - Start tokenizing ... 5509 instances
05/26/2022 00:16:24 - INFO - __main__ - Printing 3 examples
05/26/2022 00:16:24 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
05/26/2022 00:16:24 - INFO - __main__ - ['others']
05/26/2022 00:16:24 - INFO - __main__ -  [emo] what you like very little things ok
05/26/2022 00:16:24 - INFO - __main__ - ['others']
05/26/2022 00:16:24 - INFO - __main__ -  [emo] yes how so i want to fuck babu
05/26/2022 00:16:24 - INFO - __main__ - ['others']
05/26/2022 00:16:24 - INFO - __main__ - Tokenizing Input ...
05/26/2022 00:16:26 - INFO - __main__ - Tokenizing Output ...
05/26/2022 00:16:31 - INFO - __main__ - Loaded 5509 examples from test data
05/26/2022 00:17:46 - INFO - __main__ - Saved prediction in models/T5-large-multitask-cls2cls-5e-1-4-20-down64shot/singletask-emo/emo_64_87_0.2_8_predictions.txt
05/26/2022 00:17:46 - INFO - __main__ - Classification-F1 on test data: 0.4095
05/26/2022 00:17:46 - INFO - __main__ - prefix=emo_64_87, lr=0.2, bsz=8, dev_performance=0.8126154899663494, test_performance=0.40953859885077454
