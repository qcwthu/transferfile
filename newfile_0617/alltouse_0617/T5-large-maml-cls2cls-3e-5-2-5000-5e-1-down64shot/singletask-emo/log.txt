05/21/2022 21:23:14 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:23:14 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo
05/21/2022 21:23:14 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:23:14 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo
05/21/2022 21:23:16 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:23:16 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:23:16 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:23:16 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:16 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/21/2022 21:23:16 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:23:16 - INFO - __main__ - Using 2 gpus
05/21/2022 21:23:16 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
05/21/2022 21:23:21 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.5, bsz=8 ...
06/13/2022 21:19:33 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/13/2022 21:19:33 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo
06/13/2022 21:19:33 - INFO - __main__ - Namespace(task_dir='data_64/emo/', task_name='emo', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/13/2022 21:19:33 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo
06/13/2022 21:19:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/13/2022 21:19:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/13/2022 21:19:34 - INFO - __main__ - args.device: cuda:0
06/13/2022 21:19:34 - INFO - __main__ - args.device: cuda:1
06/13/2022 21:19:34 - INFO - __main__ - Using 2 gpus
06/13/2022 21:19:34 - INFO - __main__ - Using 2 gpus
06/13/2022 21:19:34 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
06/13/2022 21:19:34 - INFO - __main__ - Fine-tuning the following samples: ['emo_64_100', 'emo_64_13', 'emo_64_21', 'emo_64_42', 'emo_64_87']
06/13/2022 21:19:39 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.5, bsz=8 ...
06/13/2022 21:19:40 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:19:40 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:19:40 - INFO - __main__ - Printing 3 examples
06/13/2022 21:19:40 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:19:40 - INFO - __main__ - Printing 3 examples
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:19:40 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:19:40 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:19:40 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:19:40 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:19:40 - INFO - __main__ - Printing 3 examples
06/13/2022 21:19:40 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:19:40 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:19:40 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:19:40 - INFO - __main__ - Printing 3 examples
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:19:40 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:19:40 - INFO - __main__ - ['others']
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:19:40 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:19:41 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:19:41 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:19:58 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:19:59 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:19:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:19:59 - INFO - __main__ - Starting training!
06/13/2022 21:20:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:20:05 - INFO - __main__ - Starting training!
06/13/2022 21:20:09 - INFO - __main__ - Step 10 Global step 10 Train loss 2.53 on epoch=0
06/13/2022 21:20:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.18 on epoch=1
06/13/2022 21:20:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.00 on epoch=1
06/13/2022 21:20:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=2
06/13/2022 21:20:19 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=3
06/13/2022 21:20:22 - INFO - __main__ - Global step 50 Train loss 1.34 Classification-F1 0.2174234034699151 on epoch=3
06/13/2022 21:20:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2174234034699151 on epoch=3, global_step=50
06/13/2022 21:20:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.85 on epoch=3
06/13/2022 21:20:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.92 on epoch=4
06/13/2022 21:20:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=4
06/13/2022 21:20:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=5
06/13/2022 21:20:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=6
06/13/2022 21:20:38 - INFO - __main__ - Global step 100 Train loss 0.87 Classification-F1 0.4242605924758042 on epoch=6
06/13/2022 21:20:38 - INFO - __main__ - Saving model with best Classification-F1: 0.2174234034699151 -> 0.4242605924758042 on epoch=6, global_step=100
06/13/2022 21:20:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=6
06/13/2022 21:20:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.70 on epoch=7
06/13/2022 21:20:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.80 on epoch=8
06/13/2022 21:20:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.77 on epoch=8
06/13/2022 21:20:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=9
06/13/2022 21:20:54 - INFO - __main__ - Global step 150 Train loss 0.78 Classification-F1 0.4364698379167941 on epoch=9
06/13/2022 21:20:54 - INFO - __main__ - Saving model with best Classification-F1: 0.4242605924758042 -> 0.4364698379167941 on epoch=9, global_step=150
06/13/2022 21:20:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.70 on epoch=9
06/13/2022 21:20:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.73 on epoch=10
06/13/2022 21:21:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.74 on epoch=11
06/13/2022 21:21:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.69 on epoch=11
06/13/2022 21:21:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.67 on epoch=12
06/13/2022 21:21:10 - INFO - __main__ - Global step 200 Train loss 0.71 Classification-F1 0.5690656262505002 on epoch=12
06/13/2022 21:21:10 - INFO - __main__ - Saving model with best Classification-F1: 0.4364698379167941 -> 0.5690656262505002 on epoch=12, global_step=200
06/13/2022 21:21:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.59 on epoch=13
06/13/2022 21:21:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.58 on epoch=13
06/13/2022 21:21:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.57 on epoch=14
06/13/2022 21:21:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.54 on epoch=14
06/13/2022 21:21:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=15
06/13/2022 21:21:26 - INFO - __main__ - Global step 250 Train loss 0.57 Classification-F1 0.5058112699876082 on epoch=15
06/13/2022 21:21:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=16
06/13/2022 21:21:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=16
06/13/2022 21:21:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=17
06/13/2022 21:21:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.63 on epoch=18
06/13/2022 21:21:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=18
06/13/2022 21:21:42 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.4618152812504793 on epoch=18
06/13/2022 21:21:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=19
06/13/2022 21:21:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=19
06/13/2022 21:21:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=20
06/13/2022 21:21:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=21
06/13/2022 21:21:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=21
06/13/2022 21:21:58 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.6250262953251647 on epoch=21
06/13/2022 21:21:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5690656262505002 -> 0.6250262953251647 on epoch=21, global_step=350
06/13/2022 21:22:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=22
06/13/2022 21:22:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=23
06/13/2022 21:22:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=23
06/13/2022 21:22:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=24
06/13/2022 21:22:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=24
06/13/2022 21:22:14 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.7213015897103412 on epoch=24
06/13/2022 21:22:14 - INFO - __main__ - Saving model with best Classification-F1: 0.6250262953251647 -> 0.7213015897103412 on epoch=24, global_step=400
06/13/2022 21:22:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=25
06/13/2022 21:22:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=26
06/13/2022 21:22:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=26
06/13/2022 21:22:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=27
06/13/2022 21:22:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=28
06/13/2022 21:22:30 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.6613835490962384 on epoch=28
06/13/2022 21:22:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=28
06/13/2022 21:22:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=29
06/13/2022 21:22:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=29
06/13/2022 21:22:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=30
06/13/2022 21:22:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=31
06/13/2022 21:22:46 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.6458666606498874 on epoch=31
06/13/2022 21:22:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=31
06/13/2022 21:22:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
06/13/2022 21:22:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=33
06/13/2022 21:22:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=33
06/13/2022 21:22:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=34
06/13/2022 21:23:02 - INFO - __main__ - Global step 550 Train loss 0.41 Classification-F1 0.7481238718278345 on epoch=34
06/13/2022 21:23:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7213015897103412 -> 0.7481238718278345 on epoch=34, global_step=550
06/13/2022 21:23:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=34
06/13/2022 21:23:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=35
06/13/2022 21:23:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=36
06/13/2022 21:23:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.39 on epoch=36
06/13/2022 21:23:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=37
06/13/2022 21:23:18 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.7299180715706752 on epoch=37
06/13/2022 21:23:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=38
06/13/2022 21:23:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=38
06/13/2022 21:23:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
06/13/2022 21:23:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
06/13/2022 21:23:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=40
06/13/2022 21:23:34 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.6252458206878654 on epoch=40
06/13/2022 21:23:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=41
06/13/2022 21:23:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=41
06/13/2022 21:23:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=42
06/13/2022 21:23:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=43
06/13/2022 21:23:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=43
06/13/2022 21:23:50 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.6597078360236256 on epoch=43
06/13/2022 21:23:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=44
06/13/2022 21:23:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=44
06/13/2022 21:23:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=45
06/13/2022 21:24:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=46
06/13/2022 21:24:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=46
06/13/2022 21:24:06 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7077983672101318 on epoch=46
06/13/2022 21:24:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=47
06/13/2022 21:24:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
06/13/2022 21:24:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
06/13/2022 21:24:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=49
06/13/2022 21:24:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/13/2022 21:24:21 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.7398050024600156 on epoch=49
06/13/2022 21:24:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
06/13/2022 21:24:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=51
06/13/2022 21:24:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=51
06/13/2022 21:24:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=52
06/13/2022 21:24:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
06/13/2022 21:24:38 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.7285391406971953 on epoch=53
06/13/2022 21:24:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.30 on epoch=53
06/13/2022 21:24:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=54
06/13/2022 21:24:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=54
06/13/2022 21:24:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=55
06/13/2022 21:24:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
06/13/2022 21:24:53 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.6783530172129982 on epoch=56
06/13/2022 21:24:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=56
06/13/2022 21:24:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
06/13/2022 21:25:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
06/13/2022 21:25:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=58
06/13/2022 21:25:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=59
06/13/2022 21:25:09 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.742814871387296 on epoch=59
06/13/2022 21:25:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
06/13/2022 21:25:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=60
06/13/2022 21:25:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=61
06/13/2022 21:25:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/13/2022 21:25:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=62
06/13/2022 21:25:25 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.7423732532852445 on epoch=62
06/13/2022 21:25:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=63
06/13/2022 21:25:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=63
06/13/2022 21:25:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=64
06/13/2022 21:25:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
06/13/2022 21:25:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
06/13/2022 21:25:41 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.765114775440095 on epoch=65
06/13/2022 21:25:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7481238718278345 -> 0.765114775440095 on epoch=65, global_step=1050
06/13/2022 21:25:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
06/13/2022 21:25:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=66
06/13/2022 21:25:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=67
06/13/2022 21:25:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.29 on epoch=68
06/13/2022 21:25:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=68
06/13/2022 21:25:57 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.6925861580365555 on epoch=68
06/13/2022 21:26:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=69
06/13/2022 21:26:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=69
06/13/2022 21:26:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=70
06/13/2022 21:26:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=71
06/13/2022 21:26:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
06/13/2022 21:26:13 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.7289504328470893 on epoch=71
06/13/2022 21:26:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=72
06/13/2022 21:26:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=73
06/13/2022 21:26:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
06/13/2022 21:26:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=74
06/13/2022 21:26:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=74
06/13/2022 21:26:29 - INFO - __main__ - Global step 1200 Train loss 0.24 Classification-F1 0.72031255985902 on epoch=74
06/13/2022 21:26:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=75
06/13/2022 21:26:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=76
06/13/2022 21:26:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=76
06/13/2022 21:26:39 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=77
06/13/2022 21:26:42 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=78
06/13/2022 21:26:45 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.7114782953220635 on epoch=78
06/13/2022 21:26:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=78
06/13/2022 21:26:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=79
06/13/2022 21:26:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=79
06/13/2022 21:26:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=80
06/13/2022 21:26:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
06/13/2022 21:27:01 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.7582187118527157 on epoch=81
06/13/2022 21:27:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
06/13/2022 21:27:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=82
06/13/2022 21:27:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=83
06/13/2022 21:27:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=83
06/13/2022 21:27:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
06/13/2022 21:27:17 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.7310602028428805 on epoch=84
06/13/2022 21:27:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.28 on epoch=84
06/13/2022 21:27:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=85
06/13/2022 21:27:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
06/13/2022 21:27:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
06/13/2022 21:27:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=87
06/13/2022 21:27:33 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.7573682148150234 on epoch=87
06/13/2022 21:27:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=88
06/13/2022 21:27:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
06/13/2022 21:27:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
06/13/2022 21:27:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
06/13/2022 21:27:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
06/13/2022 21:27:49 - INFO - __main__ - Global step 1450 Train loss 0.22 Classification-F1 0.689247311827957 on epoch=90
06/13/2022 21:27:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=91
06/13/2022 21:27:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=91
06/13/2022 21:27:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=92
06/13/2022 21:27:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
06/13/2022 21:28:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=93
06/13/2022 21:28:05 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.6782048148792699 on epoch=93
06/13/2022 21:28:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=94
06/13/2022 21:28:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=94
06/13/2022 21:28:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
06/13/2022 21:28:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
06/13/2022 21:28:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
06/13/2022 21:28:21 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.7405924877407613 on epoch=96
06/13/2022 21:28:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
06/13/2022 21:28:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=98
06/13/2022 21:28:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
06/13/2022 21:28:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
06/13/2022 21:28:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=99
06/13/2022 21:28:37 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.7078058325687108 on epoch=99
06/13/2022 21:28:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=100
06/13/2022 21:28:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
06/13/2022 21:28:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=101
06/13/2022 21:28:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=102
06/13/2022 21:28:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=103
06/13/2022 21:28:53 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.5899679007241194 on epoch=103
06/13/2022 21:28:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=103
06/13/2022 21:28:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
06/13/2022 21:29:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=104
06/13/2022 21:29:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=105
06/13/2022 21:29:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=106
06/13/2022 21:29:09 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.765136230869311 on epoch=106
06/13/2022 21:29:09 - INFO - __main__ - Saving model with best Classification-F1: 0.765114775440095 -> 0.765136230869311 on epoch=106, global_step=1700
06/13/2022 21:29:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=106
06/13/2022 21:29:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
06/13/2022 21:29:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=108
06/13/2022 21:29:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
06/13/2022 21:29:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=109
06/13/2022 21:29:25 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.7311984604477257 on epoch=109
06/13/2022 21:29:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=109
06/13/2022 21:29:30 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=110
06/13/2022 21:29:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
06/13/2022 21:29:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
06/13/2022 21:29:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=112
06/13/2022 21:29:41 - INFO - __main__ - Global step 1800 Train loss 0.14 Classification-F1 0.7788711178251205 on epoch=112
06/13/2022 21:29:41 - INFO - __main__ - Saving model with best Classification-F1: 0.765136230869311 -> 0.7788711178251205 on epoch=112, global_step=1800
06/13/2022 21:29:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=113
06/13/2022 21:29:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=113
06/13/2022 21:29:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
06/13/2022 21:29:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=114
06/13/2022 21:29:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
06/13/2022 21:29:57 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.732547915736077 on epoch=115
06/13/2022 21:29:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=116
06/13/2022 21:30:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
06/13/2022 21:30:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=117
06/13/2022 21:30:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=118
06/13/2022 21:30:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=118
06/13/2022 21:30:13 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.691136978081859 on epoch=118
06/13/2022 21:30:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=119
06/13/2022 21:30:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=119
06/13/2022 21:30:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=120
06/13/2022 21:30:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
06/13/2022 21:30:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=121
06/13/2022 21:30:29 - INFO - __main__ - Global step 1950 Train loss 0.12 Classification-F1 0.7051650978283898 on epoch=121
06/13/2022 21:30:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=122
06/13/2022 21:30:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=123
06/13/2022 21:30:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=123
06/13/2022 21:30:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
06/13/2022 21:30:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=124
06/13/2022 21:30:45 - INFO - __main__ - Global step 2000 Train loss 0.11 Classification-F1 0.7789965382316791 on epoch=124
06/13/2022 21:30:45 - INFO - __main__ - Saving model with best Classification-F1: 0.7788711178251205 -> 0.7789965382316791 on epoch=124, global_step=2000
06/13/2022 21:30:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=125
06/13/2022 21:30:50 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=126
06/13/2022 21:30:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=126
06/13/2022 21:30:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=127
06/13/2022 21:30:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=128
06/13/2022 21:31:01 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.8049132122873129 on epoch=128
06/13/2022 21:31:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7789965382316791 -> 0.8049132122873129 on epoch=128, global_step=2050
06/13/2022 21:31:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=128
06/13/2022 21:31:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
06/13/2022 21:31:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=129
06/13/2022 21:31:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=130
06/13/2022 21:31:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=131
06/13/2022 21:31:17 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7722031122031122 on epoch=131
06/13/2022 21:31:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.12 on epoch=131
06/13/2022 21:31:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
06/13/2022 21:31:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=133
06/13/2022 21:31:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=133
06/13/2022 21:31:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=134
06/13/2022 21:31:34 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.7672857641298555 on epoch=134
06/13/2022 21:31:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
06/13/2022 21:31:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.05 on epoch=135
06/13/2022 21:31:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=136
06/13/2022 21:31:44 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
06/13/2022 21:31:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=137
06/13/2022 21:31:50 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.7706502411526721 on epoch=137
06/13/2022 21:31:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
06/13/2022 21:31:55 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=138
06/13/2022 21:31:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=139
06/13/2022 21:32:00 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
06/13/2022 21:32:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
06/13/2022 21:32:06 - INFO - __main__ - Global step 2250 Train loss 0.09 Classification-F1 0.7916469192304432 on epoch=140
06/13/2022 21:32:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
06/13/2022 21:32:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
06/13/2022 21:32:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=142
06/13/2022 21:32:16 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
06/13/2022 21:32:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
06/13/2022 21:32:22 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.7674628188712695 on epoch=143
06/13/2022 21:32:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=144
06/13/2022 21:32:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.09 on epoch=144
06/13/2022 21:32:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=145
06/13/2022 21:32:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
06/13/2022 21:32:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=146
06/13/2022 21:32:38 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7859996283909327 on epoch=146
06/13/2022 21:32:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
06/13/2022 21:32:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.13 on epoch=148
06/13/2022 21:32:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=148
06/13/2022 21:32:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
06/13/2022 21:32:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
06/13/2022 21:32:54 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7402224791337695 on epoch=149
06/13/2022 21:32:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=150
06/13/2022 21:32:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
06/13/2022 21:33:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
06/13/2022 21:33:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=152
06/13/2022 21:33:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=153
06/13/2022 21:33:10 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.7586877418471549 on epoch=153
06/13/2022 21:33:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=153
06/13/2022 21:33:15 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=154
06/13/2022 21:33:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=154
06/13/2022 21:33:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=155
06/13/2022 21:33:23 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=156
06/13/2022 21:33:26 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7915931948798329 on epoch=156
06/13/2022 21:33:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=156
06/13/2022 21:33:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=157
06/13/2022 21:33:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
06/13/2022 21:33:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=158
06/13/2022 21:33:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
06/13/2022 21:33:42 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.7850433013804924 on epoch=159
06/13/2022 21:33:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
06/13/2022 21:33:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=160
06/13/2022 21:33:50 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
06/13/2022 21:33:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
06/13/2022 21:33:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=162
06/13/2022 21:33:58 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.735402022524759 on epoch=162
06/13/2022 21:34:01 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
06/13/2022 21:34:03 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.05 on epoch=163
06/13/2022 21:34:06 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=164
06/13/2022 21:34:08 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
06/13/2022 21:34:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=165
06/13/2022 21:34:14 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.7751202303678126 on epoch=165
06/13/2022 21:34:16 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
06/13/2022 21:34:19 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/13/2022 21:34:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=167
06/13/2022 21:34:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
06/13/2022 21:34:27 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.12 on epoch=168
06/13/2022 21:34:30 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.7504836347897963 on epoch=168
06/13/2022 21:34:32 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
06/13/2022 21:34:35 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
06/13/2022 21:34:37 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.17 on epoch=170
06/13/2022 21:34:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
06/13/2022 21:34:42 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
06/13/2022 21:34:46 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.7346487335115361 on epoch=171
06/13/2022 21:34:48 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
06/13/2022 21:34:51 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=173
06/13/2022 21:34:53 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.14 on epoch=173
06/13/2022 21:34:56 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
06/13/2022 21:34:58 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
06/13/2022 21:35:02 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.7808398343624062 on epoch=174
06/13/2022 21:35:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
06/13/2022 21:35:07 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
06/13/2022 21:35:10 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
06/13/2022 21:35:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=177
06/13/2022 21:35:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
06/13/2022 21:35:18 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.786396611343966 on epoch=178
06/13/2022 21:35:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=178
06/13/2022 21:35:23 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
06/13/2022 21:35:26 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
06/13/2022 21:35:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
06/13/2022 21:35:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
06/13/2022 21:35:34 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7449555067060988 on epoch=181
06/13/2022 21:35:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
06/13/2022 21:35:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.05 on epoch=182
06/13/2022 21:35:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/13/2022 21:35:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=183
06/13/2022 21:35:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.12 on epoch=184
06/13/2022 21:35:50 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7799678646526872 on epoch=184
06/13/2022 21:35:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
06/13/2022 21:35:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
06/13/2022 21:35:58 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
06/13/2022 21:36:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
06/13/2022 21:36:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=187
06/13/2022 21:36:04 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:36:04 - INFO - __main__ - Printing 3 examples
06/13/2022 21:36:04 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:04 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:04 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:04 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:36:04 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:36:04 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:36:04 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:36:04 - INFO - __main__ - Printing 3 examples
06/13/2022 21:36:04 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:04 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:04 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:36:04 - INFO - __main__ - ['others']
06/13/2022 21:36:05 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:36:05 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:36:05 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:36:06 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.7918332194121669 on epoch=187
06/13/2022 21:36:06 - INFO - __main__ - save last model!
06/13/2022 21:36:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 21:36:06 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 21:36:06 - INFO - __main__ - Printing 3 examples
06/13/2022 21:36:06 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 21:36:06 - INFO - __main__ - ['others']
06/13/2022 21:36:06 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 21:36:06 - INFO - __main__ - ['others']
06/13/2022 21:36:06 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 21:36:06 - INFO - __main__ - ['others']
06/13/2022 21:36:06 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:36:08 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:36:14 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 21:36:20 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:36:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:36:21 - INFO - __main__ - Starting training!
06/13/2022 21:37:26 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_100_0.5_8_predictions.txt
06/13/2022 21:37:26 - INFO - __main__ - Classification-F1 on test data: 0.4044
06/13/2022 21:37:27 - INFO - __main__ - prefix=emo_64_100, lr=0.5, bsz=8, dev_performance=0.8049132122873129, test_performance=0.4044411779667051
06/13/2022 21:37:27 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.4, bsz=8 ...
06/13/2022 21:37:27 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:37:27 - INFO - __main__ - Printing 3 examples
06/13/2022 21:37:27 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:37:27 - INFO - __main__ - ['others']
06/13/2022 21:37:27 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:37:27 - INFO - __main__ - ['others']
06/13/2022 21:37:27 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:37:27 - INFO - __main__ - ['others']
06/13/2022 21:37:27 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:37:28 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:37:28 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:37:28 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:37:28 - INFO - __main__ - Printing 3 examples
06/13/2022 21:37:28 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:37:28 - INFO - __main__ - ['others']
06/13/2022 21:37:28 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:37:28 - INFO - __main__ - ['others']
06/13/2022 21:37:28 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:37:28 - INFO - __main__ - ['others']
06/13/2022 21:37:28 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:37:28 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:37:28 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:37:43 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:37:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:37:44 - INFO - __main__ - Starting training!
06/13/2022 21:37:47 - INFO - __main__ - Step 10 Global step 10 Train loss 2.56 on epoch=0
06/13/2022 21:37:50 - INFO - __main__ - Step 20 Global step 20 Train loss 1.36 on epoch=1
06/13/2022 21:37:52 - INFO - __main__ - Step 30 Global step 30 Train loss 1.02 on epoch=1
06/13/2022 21:37:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=2
06/13/2022 21:37:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.88 on epoch=3
06/13/2022 21:38:00 - INFO - __main__ - Global step 50 Train loss 1.36 Classification-F1 0.21099773242630382 on epoch=3
06/13/2022 21:38:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21099773242630382 on epoch=3, global_step=50
06/13/2022 21:38:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.96 on epoch=3
06/13/2022 21:38:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.82 on epoch=4
06/13/2022 21:38:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=4
06/13/2022 21:38:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=5
06/13/2022 21:38:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.77 on epoch=6
06/13/2022 21:38:16 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.32858216557344466 on epoch=6
06/13/2022 21:38:16 - INFO - __main__ - Saving model with best Classification-F1: 0.21099773242630382 -> 0.32858216557344466 on epoch=6, global_step=100
06/13/2022 21:38:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=6
06/13/2022 21:38:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=7
06/13/2022 21:38:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=8
06/13/2022 21:38:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.75 on epoch=8
06/13/2022 21:38:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=9
06/13/2022 21:38:32 - INFO - __main__ - Global step 150 Train loss 0.81 Classification-F1 0.40403489640130863 on epoch=9
06/13/2022 21:38:32 - INFO - __main__ - Saving model with best Classification-F1: 0.32858216557344466 -> 0.40403489640130863 on epoch=9, global_step=150
06/13/2022 21:38:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.82 on epoch=9
06/13/2022 21:38:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=10
06/13/2022 21:38:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.71 on epoch=11
06/13/2022 21:38:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=11
06/13/2022 21:38:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=12
06/13/2022 21:38:47 - INFO - __main__ - Global step 200 Train loss 0.75 Classification-F1 0.5070432790289519 on epoch=12
06/13/2022 21:38:47 - INFO - __main__ - Saving model with best Classification-F1: 0.40403489640130863 -> 0.5070432790289519 on epoch=12, global_step=200
06/13/2022 21:38:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=13
06/13/2022 21:38:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=13
06/13/2022 21:38:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=14
06/13/2022 21:38:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=14
06/13/2022 21:39:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.60 on epoch=15
06/13/2022 21:39:03 - INFO - __main__ - Global step 250 Train loss 0.67 Classification-F1 0.49158469058057463 on epoch=15
06/13/2022 21:39:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=16
06/13/2022 21:39:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=16
06/13/2022 21:39:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.62 on epoch=17
06/13/2022 21:39:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=18
06/13/2022 21:39:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.67 on epoch=18
06/13/2022 21:39:19 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6269491446028811 on epoch=18
06/13/2022 21:39:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5070432790289519 -> 0.6269491446028811 on epoch=18, global_step=300
06/13/2022 21:39:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=19
06/13/2022 21:39:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.71 on epoch=19
06/13/2022 21:39:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=20
06/13/2022 21:39:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=21
06/13/2022 21:39:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=21
06/13/2022 21:39:34 - INFO - __main__ - Global step 350 Train loss 0.63 Classification-F1 0.4611095420977731 on epoch=21
06/13/2022 21:39:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.57 on epoch=22
06/13/2022 21:39:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=23
06/13/2022 21:39:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.53 on epoch=23
06/13/2022 21:39:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=24
06/13/2022 21:39:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=24
06/13/2022 21:39:50 - INFO - __main__ - Global step 400 Train loss 0.56 Classification-F1 0.6689785510138542 on epoch=24
06/13/2022 21:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6269491446028811 -> 0.6689785510138542 on epoch=24, global_step=400
06/13/2022 21:39:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=25
06/13/2022 21:39:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=26
06/13/2022 21:39:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=26
06/13/2022 21:40:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.63 on epoch=27
06/13/2022 21:40:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=28
06/13/2022 21:40:06 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.5836910503980901 on epoch=28
06/13/2022 21:40:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=28
06/13/2022 21:40:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=29
06/13/2022 21:40:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=29
06/13/2022 21:40:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=30
06/13/2022 21:40:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=31
06/13/2022 21:40:21 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.6678486819407812 on epoch=31
06/13/2022 21:40:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=31
06/13/2022 21:40:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=32
06/13/2022 21:40:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/13/2022 21:40:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=33
06/13/2022 21:40:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.46 on epoch=34
06/13/2022 21:40:37 - INFO - __main__ - Global step 550 Train loss 0.48 Classification-F1 0.7569173028037594 on epoch=34
06/13/2022 21:40:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6689785510138542 -> 0.7569173028037594 on epoch=34, global_step=550
06/13/2022 21:40:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=34
06/13/2022 21:40:42 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=35
06/13/2022 21:40:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=36
06/13/2022 21:40:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=36
06/13/2022 21:40:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=37
06/13/2022 21:40:53 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.6971489388376227 on epoch=37
06/13/2022 21:40:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=38
06/13/2022 21:40:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=38
06/13/2022 21:41:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=39
06/13/2022 21:41:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=39
06/13/2022 21:41:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
06/13/2022 21:41:08 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.5909746347367237 on epoch=40
06/13/2022 21:41:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=41
06/13/2022 21:41:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/13/2022 21:41:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.62 on epoch=42
06/13/2022 21:41:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=43
06/13/2022 21:41:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=43
06/13/2022 21:41:24 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.6334948965080155 on epoch=43
06/13/2022 21:41:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=44
06/13/2022 21:41:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=44
06/13/2022 21:41:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=45
06/13/2022 21:41:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=46
06/13/2022 21:41:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=46
06/13/2022 21:41:40 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.6117818403957328 on epoch=46
06/13/2022 21:41:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=47
06/13/2022 21:41:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=48
06/13/2022 21:41:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
06/13/2022 21:41:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=49
06/13/2022 21:41:52 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=49
06/13/2022 21:41:55 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.662908727270981 on epoch=49
06/13/2022 21:41:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=50
06/13/2022 21:42:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
06/13/2022 21:42:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=51
06/13/2022 21:42:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.53 on epoch=52
06/13/2022 21:42:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=53
06/13/2022 21:42:11 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.7307952969360737 on epoch=53
06/13/2022 21:42:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/13/2022 21:42:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=54
06/13/2022 21:42:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
06/13/2022 21:42:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
06/13/2022 21:42:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.28 on epoch=56
06/13/2022 21:42:26 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.7225767836704933 on epoch=56
06/13/2022 21:42:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=56
06/13/2022 21:42:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=57
06/13/2022 21:42:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.28 on epoch=58
06/13/2022 21:42:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=58
06/13/2022 21:42:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=59
06/13/2022 21:42:42 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.7728118840562916 on epoch=59
06/13/2022 21:42:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7569173028037594 -> 0.7728118840562916 on epoch=59, global_step=950
06/13/2022 21:42:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
06/13/2022 21:42:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=60
06/13/2022 21:42:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
06/13/2022 21:42:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=61
06/13/2022 21:42:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
06/13/2022 21:42:58 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.772302474265885 on epoch=62
06/13/2022 21:43:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=63
06/13/2022 21:43:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=63
06/13/2022 21:43:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=64
06/13/2022 21:43:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/13/2022 21:43:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=65
06/13/2022 21:43:13 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.723343674476062 on epoch=65
06/13/2022 21:43:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=66
06/13/2022 21:43:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=66
06/13/2022 21:43:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
06/13/2022 21:43:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=68
06/13/2022 21:43:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=68
06/13/2022 21:43:29 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.6462197106846201 on epoch=68
06/13/2022 21:43:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=69
06/13/2022 21:43:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=69
06/13/2022 21:43:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.31 on epoch=70
06/13/2022 21:43:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=71
06/13/2022 21:43:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
06/13/2022 21:43:44 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.6419514083603644 on epoch=71
06/13/2022 21:43:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/13/2022 21:43:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=73
06/13/2022 21:43:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/13/2022 21:43:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
06/13/2022 21:43:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=74
06/13/2022 21:44:00 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.6956815584764768 on epoch=74
06/13/2022 21:44:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=75
06/13/2022 21:44:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=76
06/13/2022 21:44:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=76
06/13/2022 21:44:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
06/13/2022 21:44:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.35 on epoch=78
06/13/2022 21:44:15 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.6464136890506754 on epoch=78
06/13/2022 21:44:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=78
06/13/2022 21:44:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=79
06/13/2022 21:44:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=79
06/13/2022 21:44:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=80
06/13/2022 21:44:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.33 on epoch=81
06/13/2022 21:44:31 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.670748692275601 on epoch=81
06/13/2022 21:44:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=81
06/13/2022 21:44:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=82
06/13/2022 21:44:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=83
06/13/2022 21:44:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.28 on epoch=83
06/13/2022 21:44:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=84
06/13/2022 21:44:47 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.7476607072604335 on epoch=84
06/13/2022 21:44:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.35 on epoch=84
06/13/2022 21:44:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
06/13/2022 21:44:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
06/13/2022 21:44:57 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=86
06/13/2022 21:44:59 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.28 on epoch=87
06/13/2022 21:45:02 - INFO - __main__ - Global step 1400 Train loss 0.27 Classification-F1 0.7256011552355044 on epoch=87
06/13/2022 21:45:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=88
06/13/2022 21:45:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=88
06/13/2022 21:45:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=89
06/13/2022 21:45:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=89
06/13/2022 21:45:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=90
06/13/2022 21:45:18 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.701869960680793 on epoch=90
06/13/2022 21:45:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.31 on epoch=91
06/13/2022 21:45:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.32 on epoch=91
06/13/2022 21:45:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=92
06/13/2022 21:45:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=93
06/13/2022 21:45:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
06/13/2022 21:45:34 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.6549007832186174 on epoch=93
06/13/2022 21:45:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
06/13/2022 21:45:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=94
06/13/2022 21:45:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=95
06/13/2022 21:45:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=96
06/13/2022 21:45:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
06/13/2022 21:45:49 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7312587291881025 on epoch=96
06/13/2022 21:45:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=97
06/13/2022 21:45:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
06/13/2022 21:45:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
06/13/2022 21:45:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=99
06/13/2022 21:46:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=99
06/13/2022 21:46:05 - INFO - __main__ - Global step 1600 Train loss 0.23 Classification-F1 0.7379530557162136 on epoch=99
06/13/2022 21:46:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
06/13/2022 21:46:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=101
06/13/2022 21:46:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=101
06/13/2022 21:46:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
06/13/2022 21:46:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
06/13/2022 21:46:21 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.7439067467326033 on epoch=103
06/13/2022 21:46:23 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
06/13/2022 21:46:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
06/13/2022 21:46:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=104
06/13/2022 21:46:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
06/13/2022 21:46:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
06/13/2022 21:46:36 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.740731272558028 on epoch=106
06/13/2022 21:46:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
06/13/2022 21:46:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
06/13/2022 21:46:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=108
06/13/2022 21:46:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
06/13/2022 21:46:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
06/13/2022 21:46:52 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.7979189122050785 on epoch=109
06/13/2022 21:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7728118840562916 -> 0.7979189122050785 on epoch=109, global_step=1750
06/13/2022 21:46:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=109
06/13/2022 21:46:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
06/13/2022 21:47:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
06/13/2022 21:47:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=111
06/13/2022 21:47:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=112
06/13/2022 21:47:08 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.7613576391557662 on epoch=112
06/13/2022 21:47:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.26 on epoch=113
06/13/2022 21:47:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=113
06/13/2022 21:47:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
06/13/2022 21:47:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=114
06/13/2022 21:47:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=115
06/13/2022 21:47:24 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.7576868199833565 on epoch=115
06/13/2022 21:47:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.31 on epoch=116
06/13/2022 21:47:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=116
06/13/2022 21:47:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=117
06/13/2022 21:47:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
06/13/2022 21:47:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
06/13/2022 21:47:39 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.6321035390386538 on epoch=118
06/13/2022 21:47:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
06/13/2022 21:47:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=119
06/13/2022 21:47:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=120
06/13/2022 21:47:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
06/13/2022 21:47:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.28 on epoch=121
06/13/2022 21:47:55 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.7096361312446862 on epoch=121
06/13/2022 21:47:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
06/13/2022 21:48:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
06/13/2022 21:48:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
06/13/2022 21:48:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=124
06/13/2022 21:48:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=124
06/13/2022 21:48:11 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.6872124582869855 on epoch=124
06/13/2022 21:48:13 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
06/13/2022 21:48:15 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
06/13/2022 21:48:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
06/13/2022 21:48:20 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
06/13/2022 21:48:23 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=128
06/13/2022 21:48:26 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.6403221515010132 on epoch=128
06/13/2022 21:48:29 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
06/13/2022 21:48:31 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=129
06/13/2022 21:48:34 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=129
06/13/2022 21:48:36 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=130
06/13/2022 21:48:39 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=131
06/13/2022 21:48:42 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7291313964165178 on epoch=131
06/13/2022 21:48:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=131
06/13/2022 21:48:47 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
06/13/2022 21:48:49 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
06/13/2022 21:48:52 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=133
06/13/2022 21:48:54 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=134
06/13/2022 21:48:58 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.7849361202493679 on epoch=134
06/13/2022 21:49:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=134
06/13/2022 21:49:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.14 on epoch=135
06/13/2022 21:49:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
06/13/2022 21:49:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=136
06/13/2022 21:49:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
06/13/2022 21:49:13 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7835818290002312 on epoch=137
06/13/2022 21:49:16 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
06/13/2022 21:49:18 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.28 on epoch=138
06/13/2022 21:49:21 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=139
06/13/2022 21:49:23 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
06/13/2022 21:49:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=140
06/13/2022 21:49:29 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.6944509337371115 on epoch=140
06/13/2022 21:49:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
06/13/2022 21:49:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=141
06/13/2022 21:49:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
06/13/2022 21:49:39 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
06/13/2022 21:49:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.10 on epoch=143
06/13/2022 21:49:45 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.7209366975421864 on epoch=143
06/13/2022 21:49:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
06/13/2022 21:49:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=144
06/13/2022 21:49:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.17 on epoch=145
06/13/2022 21:49:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=146
06/13/2022 21:49:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
06/13/2022 21:50:00 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.6211029230990954 on epoch=146
06/13/2022 21:50:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
06/13/2022 21:50:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
06/13/2022 21:50:07 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
06/13/2022 21:50:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
06/13/2022 21:50:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=149
06/13/2022 21:50:16 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.7183146658557747 on epoch=149
06/13/2022 21:50:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=150
06/13/2022 21:50:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=151
06/13/2022 21:50:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=151
06/13/2022 21:50:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=152
06/13/2022 21:50:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
06/13/2022 21:50:31 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7556502742864506 on epoch=153
06/13/2022 21:50:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=153
06/13/2022 21:50:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.20 on epoch=154
06/13/2022 21:50:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.22 on epoch=154
06/13/2022 21:50:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
06/13/2022 21:50:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
06/13/2022 21:50:47 - INFO - __main__ - Global step 2500 Train loss 0.16 Classification-F1 0.7751800611494388 on epoch=156
06/13/2022 21:50:49 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.13 on epoch=156
06/13/2022 21:50:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
06/13/2022 21:50:54 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
06/13/2022 21:50:57 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=158
06/13/2022 21:50:59 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
06/13/2022 21:51:03 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.6903097798259089 on epoch=159
06/13/2022 21:51:05 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.16 on epoch=159
06/13/2022 21:51:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
06/13/2022 21:51:10 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
06/13/2022 21:51:12 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
06/13/2022 21:51:15 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=162
06/13/2022 21:51:18 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7802362371020065 on epoch=162
06/13/2022 21:51:21 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=163
06/13/2022 21:51:23 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
06/13/2022 21:51:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
06/13/2022 21:51:28 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
06/13/2022 21:51:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
06/13/2022 21:51:34 - INFO - __main__ - Global step 2650 Train loss 0.12 Classification-F1 0.6834897434018931 on epoch=165
06/13/2022 21:51:36 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/13/2022 21:51:39 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/13/2022 21:51:41 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=167
06/13/2022 21:51:43 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=168
06/13/2022 21:51:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
06/13/2022 21:51:49 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.622746529169918 on epoch=168
06/13/2022 21:51:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
06/13/2022 21:51:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
06/13/2022 21:51:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
06/13/2022 21:51:59 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
06/13/2022 21:52:01 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.07 on epoch=171
06/13/2022 21:52:05 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.7799387881066426 on epoch=171
06/13/2022 21:52:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
06/13/2022 21:52:10 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.15 on epoch=173
06/13/2022 21:52:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.11 on epoch=173
06/13/2022 21:52:15 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=174
06/13/2022 21:52:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=174
06/13/2022 21:52:20 - INFO - __main__ - Global step 2800 Train loss 0.10 Classification-F1 0.6733046865124661 on epoch=174
06/13/2022 21:52:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=175
06/13/2022 21:52:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
06/13/2022 21:52:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
06/13/2022 21:52:30 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
06/13/2022 21:52:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
06/13/2022 21:52:36 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.7507762623224228 on epoch=178
06/13/2022 21:52:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.13 on epoch=178
06/13/2022 21:52:41 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.10 on epoch=179
06/13/2022 21:52:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=179
06/13/2022 21:52:46 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/13/2022 21:52:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
06/13/2022 21:52:52 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7491619101529946 on epoch=181
06/13/2022 21:52:54 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
06/13/2022 21:52:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
06/13/2022 21:52:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=183
06/13/2022 21:53:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.10 on epoch=183
06/13/2022 21:53:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
06/13/2022 21:53:07 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.7219146662365028 on epoch=184
06/13/2022 21:53:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.06 on epoch=184
06/13/2022 21:53:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
06/13/2022 21:53:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
06/13/2022 21:53:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
06/13/2022 21:53:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
06/13/2022 21:53:20 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:53:20 - INFO - __main__ - Printing 3 examples
06/13/2022 21:53:20 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:53:20 - INFO - __main__ - ['others']
06/13/2022 21:53:20 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:53:20 - INFO - __main__ - ['others']
06/13/2022 21:53:20 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:53:20 - INFO - __main__ - ['others']
06/13/2022 21:53:20 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:53:20 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:53:21 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:53:21 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:53:21 - INFO - __main__ - Printing 3 examples
06/13/2022 21:53:21 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:53:21 - INFO - __main__ - ['others']
06/13/2022 21:53:21 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:53:21 - INFO - __main__ - ['others']
06/13/2022 21:53:21 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:53:21 - INFO - __main__ - ['others']
06/13/2022 21:53:21 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:53:21 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:53:21 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:53:22 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.758869468552072 on epoch=187
06/13/2022 21:53:22 - INFO - __main__ - save last model!
06/13/2022 21:53:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 21:53:22 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 21:53:22 - INFO - __main__ - Printing 3 examples
06/13/2022 21:53:22 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 21:53:22 - INFO - __main__ - ['others']
06/13/2022 21:53:22 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 21:53:22 - INFO - __main__ - ['others']
06/13/2022 21:53:22 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 21:53:22 - INFO - __main__ - ['others']
06/13/2022 21:53:22 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:53:25 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:53:30 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 21:53:36 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:53:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:53:37 - INFO - __main__ - Starting training!
06/13/2022 21:54:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_100_0.4_8_predictions.txt
06/13/2022 21:54:43 - INFO - __main__ - Classification-F1 on test data: 0.2997
06/13/2022 21:54:43 - INFO - __main__ - prefix=emo_64_100, lr=0.4, bsz=8, dev_performance=0.7979189122050785, test_performance=0.29969179863951356
06/13/2022 21:54:43 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.3, bsz=8 ...
06/13/2022 21:54:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:54:44 - INFO - __main__ - Printing 3 examples
06/13/2022 21:54:44 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 21:54:44 - INFO - __main__ - ['others']
06/13/2022 21:54:44 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 21:54:44 - INFO - __main__ - ['others']
06/13/2022 21:54:44 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 21:54:44 - INFO - __main__ - ['others']
06/13/2022 21:54:44 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:54:44 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:54:45 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 21:54:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 21:54:45 - INFO - __main__ - Printing 3 examples
06/13/2022 21:54:45 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 21:54:45 - INFO - __main__ - ['others']
06/13/2022 21:54:45 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 21:54:45 - INFO - __main__ - ['others']
06/13/2022 21:54:45 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 21:54:45 - INFO - __main__ - ['others']
06/13/2022 21:54:45 - INFO - __main__ - Tokenizing Input ...
06/13/2022 21:54:45 - INFO - __main__ - Tokenizing Output ...
06/13/2022 21:54:45 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 21:55:00 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 21:55:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 21:55:01 - INFO - __main__ - Starting training!
06/13/2022 21:55:04 - INFO - __main__ - Step 10 Global step 10 Train loss 2.86 on epoch=0
06/13/2022 21:55:06 - INFO - __main__ - Step 20 Global step 20 Train loss 1.47 on epoch=1
06/13/2022 21:55:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=1
06/13/2022 21:55:11 - INFO - __main__ - Step 40 Global step 40 Train loss 1.00 on epoch=2
06/13/2022 21:55:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=3
06/13/2022 21:55:17 - INFO - __main__ - Global step 50 Train loss 1.49 Classification-F1 0.13067758749069247 on epoch=3
06/13/2022 21:55:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.13067758749069247 on epoch=3, global_step=50
06/13/2022 21:55:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=3
06/13/2022 21:55:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=4
06/13/2022 21:55:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.87 on epoch=4
06/13/2022 21:55:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.88 on epoch=5
06/13/2022 21:55:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=6
06/13/2022 21:55:33 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.2888917230895688 on epoch=6
06/13/2022 21:55:33 - INFO - __main__ - Saving model with best Classification-F1: 0.13067758749069247 -> 0.2888917230895688 on epoch=6, global_step=100
06/13/2022 21:55:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=6
06/13/2022 21:55:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=7
06/13/2022 21:55:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=8
06/13/2022 21:55:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=8
06/13/2022 21:55:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.77 on epoch=9
06/13/2022 21:55:48 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.4392452517452517 on epoch=9
06/13/2022 21:55:48 - INFO - __main__ - Saving model with best Classification-F1: 0.2888917230895688 -> 0.4392452517452517 on epoch=9, global_step=150
06/13/2022 21:55:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
06/13/2022 21:55:53 - INFO - __main__ - Step 170 Global step 170 Train loss 0.80 on epoch=10
06/13/2022 21:55:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=11
06/13/2022 21:55:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=11
06/13/2022 21:56:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=12
06/13/2022 21:56:04 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.4431722555144715 on epoch=12
06/13/2022 21:56:04 - INFO - __main__ - Saving model with best Classification-F1: 0.4392452517452517 -> 0.4431722555144715 on epoch=12, global_step=200
06/13/2022 21:56:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=13
06/13/2022 21:56:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=13
06/13/2022 21:56:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.63 on epoch=14
06/13/2022 21:56:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.59 on epoch=14
06/13/2022 21:56:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.69 on epoch=15
06/13/2022 21:56:20 - INFO - __main__ - Global step 250 Train loss 0.66 Classification-F1 0.5244607060788409 on epoch=15
06/13/2022 21:56:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4431722555144715 -> 0.5244607060788409 on epoch=15, global_step=250
06/13/2022 21:56:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=16
06/13/2022 21:56:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=16
06/13/2022 21:56:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.66 on epoch=17
06/13/2022 21:56:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=18
06/13/2022 21:56:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.62 on epoch=18
06/13/2022 21:56:36 - INFO - __main__ - Global step 300 Train loss 0.65 Classification-F1 0.44194858674775334 on epoch=18
06/13/2022 21:56:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=19
06/13/2022 21:56:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=19
06/13/2022 21:56:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.63 on epoch=20
06/13/2022 21:56:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=21
06/13/2022 21:56:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=21
06/13/2022 21:56:52 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.4521869418162445 on epoch=21
06/13/2022 21:56:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.64 on epoch=22
06/13/2022 21:56:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.58 on epoch=23
06/13/2022 21:56:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=23
06/13/2022 21:57:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=24
06/13/2022 21:57:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=24
06/13/2022 21:57:07 - INFO - __main__ - Global step 400 Train loss 0.62 Classification-F1 0.6544934744347182 on epoch=24
06/13/2022 21:57:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5244607060788409 -> 0.6544934744347182 on epoch=24, global_step=400
06/13/2022 21:57:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.59 on epoch=25
06/13/2022 21:57:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
06/13/2022 21:57:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=26
06/13/2022 21:57:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.57 on epoch=27
06/13/2022 21:57:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.49 on epoch=28
06/13/2022 21:57:23 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.6479275768278325 on epoch=28
06/13/2022 21:57:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=28
06/13/2022 21:57:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=29
06/13/2022 21:57:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.61 on epoch=29
06/13/2022 21:57:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=30
06/13/2022 21:57:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=31
06/13/2022 21:57:39 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.6914702668649321 on epoch=31
06/13/2022 21:57:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6544934744347182 -> 0.6914702668649321 on epoch=31, global_step=500
06/13/2022 21:57:42 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=31
06/13/2022 21:57:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=32
06/13/2022 21:57:47 - INFO - __main__ - Step 530 Global step 530 Train loss 0.54 on epoch=33
06/13/2022 21:57:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=33
06/13/2022 21:57:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=34
06/13/2022 21:57:55 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.6942850458543889 on epoch=34
06/13/2022 21:57:55 - INFO - __main__ - Saving model with best Classification-F1: 0.6914702668649321 -> 0.6942850458543889 on epoch=34, global_step=550
06/13/2022 21:57:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=34
06/13/2022 21:58:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=35
06/13/2022 21:58:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=36
06/13/2022 21:58:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=36
06/13/2022 21:58:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=37
06/13/2022 21:58:11 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.6961308127847763 on epoch=37
06/13/2022 21:58:11 - INFO - __main__ - Saving model with best Classification-F1: 0.6942850458543889 -> 0.6961308127847763 on epoch=37, global_step=600
06/13/2022 21:58:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.50 on epoch=38
06/13/2022 21:58:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=38
06/13/2022 21:58:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=39
06/13/2022 21:58:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=39
06/13/2022 21:58:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=40
06/13/2022 21:58:27 - INFO - __main__ - Global step 650 Train loss 0.48 Classification-F1 0.6654386642974993 on epoch=40
06/13/2022 21:58:29 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=41
06/13/2022 21:58:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=41
06/13/2022 21:58:34 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=42
06/13/2022 21:58:37 - INFO - __main__ - Step 690 Global step 690 Train loss 0.49 on epoch=43
06/13/2022 21:58:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=43
06/13/2022 21:58:43 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.6694405711318057 on epoch=43
06/13/2022 21:58:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.60 on epoch=44
06/13/2022 21:58:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=44
06/13/2022 21:58:50 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=45
06/13/2022 21:58:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=46
06/13/2022 21:58:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=46
06/13/2022 21:58:59 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.6667651030204046 on epoch=46
06/13/2022 21:59:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=47
06/13/2022 21:59:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=48
06/13/2022 21:59:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=48
06/13/2022 21:59:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/13/2022 21:59:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=49
06/13/2022 21:59:15 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.6984939977841759 on epoch=49
06/13/2022 21:59:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6961308127847763 -> 0.6984939977841759 on epoch=49, global_step=800
06/13/2022 21:59:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=50
06/13/2022 21:59:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=51
06/13/2022 21:59:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=51
06/13/2022 21:59:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.56 on epoch=52
06/13/2022 21:59:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.37 on epoch=53
06/13/2022 21:59:31 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.6932424366722502 on epoch=53
06/13/2022 21:59:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=53
06/13/2022 21:59:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=54
06/13/2022 21:59:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.41 on epoch=54
06/13/2022 21:59:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=55
06/13/2022 21:59:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=56
06/13/2022 21:59:47 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.7234331884331884 on epoch=56
06/13/2022 21:59:47 - INFO - __main__ - Saving model with best Classification-F1: 0.6984939977841759 -> 0.7234331884331884 on epoch=56, global_step=900
06/13/2022 21:59:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
06/13/2022 21:59:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/13/2022 21:59:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=58
06/13/2022 21:59:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=58
06/13/2022 22:00:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=59
06/13/2022 22:00:03 - INFO - __main__ - Global step 950 Train loss 0.38 Classification-F1 0.7281072144230039 on epoch=59
06/13/2022 22:00:03 - INFO - __main__ - Saving model with best Classification-F1: 0.7234331884331884 -> 0.7281072144230039 on epoch=59, global_step=950
06/13/2022 22:00:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/13/2022 22:00:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=60
06/13/2022 22:00:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=61
06/13/2022 22:00:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=61
06/13/2022 22:00:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=62
06/13/2022 22:00:19 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.7179503028433867 on epoch=62
06/13/2022 22:00:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=63
06/13/2022 22:00:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=63
06/13/2022 22:00:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=64
06/13/2022 22:00:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/13/2022 22:00:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
06/13/2022 22:00:35 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.712617009921003 on epoch=65
06/13/2022 22:00:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
06/13/2022 22:00:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=66
06/13/2022 22:00:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=67
06/13/2022 22:00:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/13/2022 22:00:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.40 on epoch=68
06/13/2022 22:00:51 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.6458309622909417 on epoch=68
06/13/2022 22:00:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=69
06/13/2022 22:00:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=69
06/13/2022 22:00:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=70
06/13/2022 22:01:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=71
06/13/2022 22:01:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
06/13/2022 22:01:07 - INFO - __main__ - Global step 1150 Train loss 0.33 Classification-F1 0.6232990179586964 on epoch=71
06/13/2022 22:01:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=72
06/13/2022 22:01:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=73
06/13/2022 22:01:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=73
06/13/2022 22:01:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=74
06/13/2022 22:01:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=74
06/13/2022 22:01:23 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.7141616707441568 on epoch=74
06/13/2022 22:01:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=75
06/13/2022 22:01:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=76
06/13/2022 22:01:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=76
06/13/2022 22:01:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=77
06/13/2022 22:01:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=78
06/13/2022 22:01:39 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.6598636034119906 on epoch=78
06/13/2022 22:01:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=78
06/13/2022 22:01:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=79
06/13/2022 22:01:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=79
06/13/2022 22:01:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=80
06/13/2022 22:01:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=81
06/13/2022 22:01:55 - INFO - __main__ - Global step 1300 Train loss 0.35 Classification-F1 0.7143829435756844 on epoch=81
06/13/2022 22:01:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=81
06/13/2022 22:02:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/13/2022 22:02:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=83
06/13/2022 22:02:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=83
06/13/2022 22:02:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=84
06/13/2022 22:02:11 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7435494017997228 on epoch=84
06/13/2022 22:02:11 - INFO - __main__ - Saving model with best Classification-F1: 0.7281072144230039 -> 0.7435494017997228 on epoch=84, global_step=1350
06/13/2022 22:02:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/13/2022 22:02:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=85
06/13/2022 22:02:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
06/13/2022 22:02:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.31 on epoch=86
06/13/2022 22:02:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=87
06/13/2022 22:02:27 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7592253910548612 on epoch=87
06/13/2022 22:02:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7435494017997228 -> 0.7592253910548612 on epoch=87, global_step=1400
06/13/2022 22:02:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=88
06/13/2022 22:02:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
06/13/2022 22:02:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.34 on epoch=89
06/13/2022 22:02:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=89
06/13/2022 22:02:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=90
06/13/2022 22:02:42 - INFO - __main__ - Global step 1450 Train loss 0.30 Classification-F1 0.705302681796497 on epoch=90
06/13/2022 22:02:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.28 on epoch=91
06/13/2022 22:02:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=91
06/13/2022 22:02:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=92
06/13/2022 22:02:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=93
06/13/2022 22:02:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=93
06/13/2022 22:02:59 - INFO - __main__ - Global step 1500 Train loss 0.30 Classification-F1 0.7496973245474754 on epoch=93
06/13/2022 22:03:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=94
06/13/2022 22:03:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.31 on epoch=94
06/13/2022 22:03:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=95
06/13/2022 22:03:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=96
06/13/2022 22:03:11 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=96
06/13/2022 22:03:14 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.7257705765518264 on epoch=96
06/13/2022 22:03:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=97
06/13/2022 22:03:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.19 on epoch=98
06/13/2022 22:03:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=98
06/13/2022 22:03:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=99
06/13/2022 22:03:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
06/13/2022 22:03:30 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.7583030730587943 on epoch=99
06/13/2022 22:03:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=100
06/13/2022 22:03:35 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=101
06/13/2022 22:03:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=101
06/13/2022 22:03:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=102
06/13/2022 22:03:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=103
06/13/2022 22:03:46 - INFO - __main__ - Global step 1650 Train loss 0.29 Classification-F1 0.7418919292548769 on epoch=103
06/13/2022 22:03:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=103
06/13/2022 22:03:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.28 on epoch=104
06/13/2022 22:03:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=104
06/13/2022 22:03:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=105
06/13/2022 22:03:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=106
06/13/2022 22:04:02 - INFO - __main__ - Global step 1700 Train loss 0.28 Classification-F1 0.7535087434356559 on epoch=106
06/13/2022 22:04:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=106
06/13/2022 22:04:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=107
06/13/2022 22:04:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.32 on epoch=108
06/13/2022 22:04:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
06/13/2022 22:04:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
06/13/2022 22:04:18 - INFO - __main__ - Global step 1750 Train loss 0.28 Classification-F1 0.7629011431392654 on epoch=109
06/13/2022 22:04:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7592253910548612 -> 0.7629011431392654 on epoch=109, global_step=1750
06/13/2022 22:04:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=109
06/13/2022 22:04:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=110
06/13/2022 22:04:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=111
06/13/2022 22:04:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=111
06/13/2022 22:04:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
06/13/2022 22:04:34 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.7513594364395433 on epoch=112
06/13/2022 22:04:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.28 on epoch=113
06/13/2022 22:04:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=113
06/13/2022 22:04:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=114
06/13/2022 22:04:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
06/13/2022 22:04:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
06/13/2022 22:04:50 - INFO - __main__ - Global step 1850 Train loss 0.24 Classification-F1 0.7034247805450181 on epoch=115
06/13/2022 22:04:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=116
06/13/2022 22:04:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=116
06/13/2022 22:04:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=117
06/13/2022 22:05:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
06/13/2022 22:05:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=118
06/13/2022 22:05:06 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.6721125886403505 on epoch=118
06/13/2022 22:05:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.27 on epoch=119
06/13/2022 22:05:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.27 on epoch=119
06/13/2022 22:05:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=120
06/13/2022 22:05:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=121
06/13/2022 22:05:19 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=121
06/13/2022 22:05:22 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.6992420672127913 on epoch=121
06/13/2022 22:05:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=122
06/13/2022 22:05:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
06/13/2022 22:05:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
06/13/2022 22:05:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=124
06/13/2022 22:05:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
06/13/2022 22:05:38 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.7053325385896402 on epoch=124
06/13/2022 22:05:40 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
06/13/2022 22:05:43 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=126
06/13/2022 22:05:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=126
06/13/2022 22:05:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.28 on epoch=127
06/13/2022 22:05:51 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.20 on epoch=128
06/13/2022 22:05:54 - INFO - __main__ - Global step 2050 Train loss 0.24 Classification-F1 0.7587273462503373 on epoch=128
06/13/2022 22:05:56 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.25 on epoch=128
06/13/2022 22:05:59 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
06/13/2022 22:06:01 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.24 on epoch=129
06/13/2022 22:06:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
06/13/2022 22:06:06 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.24 on epoch=131
06/13/2022 22:06:10 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7024547725085803 on epoch=131
06/13/2022 22:06:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=131
06/13/2022 22:06:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
06/13/2022 22:06:18 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
06/13/2022 22:06:20 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.15 on epoch=133
06/13/2022 22:06:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.23 on epoch=134
06/13/2022 22:06:26 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7921456911388768 on epoch=134
06/13/2022 22:06:26 - INFO - __main__ - Saving model with best Classification-F1: 0.7629011431392654 -> 0.7921456911388768 on epoch=134, global_step=2150
06/13/2022 22:06:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=134
06/13/2022 22:06:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
06/13/2022 22:06:33 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.30 on epoch=136
06/13/2022 22:06:36 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
06/13/2022 22:06:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.15 on epoch=137
06/13/2022 22:06:42 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.7786854746309005 on epoch=137
06/13/2022 22:06:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.22 on epoch=138
06/13/2022 22:06:47 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
06/13/2022 22:06:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
06/13/2022 22:06:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
06/13/2022 22:06:55 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=140
06/13/2022 22:06:58 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.7577832752522257 on epoch=140
06/13/2022 22:07:00 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=141
06/13/2022 22:07:03 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.21 on epoch=141
06/13/2022 22:07:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=142
06/13/2022 22:07:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=143
06/13/2022 22:07:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
06/13/2022 22:07:14 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.758964920474368 on epoch=143
06/13/2022 22:07:16 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=144
06/13/2022 22:07:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.35 on epoch=144
06/13/2022 22:07:21 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.11 on epoch=145
06/13/2022 22:07:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.20 on epoch=146
06/13/2022 22:07:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.17 on epoch=146
06/13/2022 22:07:30 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.6494723313401887 on epoch=146
06/13/2022 22:07:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
06/13/2022 22:07:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
06/13/2022 22:07:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
06/13/2022 22:07:40 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.25 on epoch=149
06/13/2022 22:07:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=149
06/13/2022 22:07:46 - INFO - __main__ - Global step 2400 Train loss 0.20 Classification-F1 0.7446958122226731 on epoch=149
06/13/2022 22:07:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
06/13/2022 22:07:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
06/13/2022 22:07:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.22 on epoch=151
06/13/2022 22:07:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.21 on epoch=152
06/13/2022 22:07:58 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=153
06/13/2022 22:08:02 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7063042091217134 on epoch=153
06/13/2022 22:08:04 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.14 on epoch=153
06/13/2022 22:08:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
06/13/2022 22:08:09 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=154
06/13/2022 22:08:12 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
06/13/2022 22:08:14 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
06/13/2022 22:08:17 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.7582311967791355 on epoch=156
06/13/2022 22:08:20 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
06/13/2022 22:08:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=157
06/13/2022 22:08:25 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.23 on epoch=158
06/13/2022 22:08:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
06/13/2022 22:08:30 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=159
06/13/2022 22:08:33 - INFO - __main__ - Global step 2550 Train loss 0.19 Classification-F1 0.7537741886139807 on epoch=159
06/13/2022 22:08:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.20 on epoch=159
06/13/2022 22:08:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
06/13/2022 22:08:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=161
06/13/2022 22:08:43 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
06/13/2022 22:08:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=162
06/13/2022 22:08:49 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.7487395292343777 on epoch=162
06/13/2022 22:08:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
06/13/2022 22:08:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=163
06/13/2022 22:08:57 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=164
06/13/2022 22:08:59 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=164
06/13/2022 22:09:02 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
06/13/2022 22:09:05 - INFO - __main__ - Global step 2650 Train loss 0.15 Classification-F1 0.7232774671352926 on epoch=165
06/13/2022 22:09:08 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/13/2022 22:09:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/13/2022 22:09:13 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
06/13/2022 22:09:15 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=168
06/13/2022 22:09:18 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.17 on epoch=168
06/13/2022 22:09:21 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.6851021112285648 on epoch=168
06/13/2022 22:09:24 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=169
06/13/2022 22:09:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.17 on epoch=169
06/13/2022 22:09:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
06/13/2022 22:09:31 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
06/13/2022 22:09:34 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
06/13/2022 22:09:37 - INFO - __main__ - Global step 2750 Train loss 0.16 Classification-F1 0.6589947585888738 on epoch=171
06/13/2022 22:09:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.13 on epoch=172
06/13/2022 22:09:42 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
06/13/2022 22:09:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
06/13/2022 22:09:47 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=174
06/13/2022 22:09:50 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.22 on epoch=174
06/13/2022 22:09:53 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.7068729801509366 on epoch=174
06/13/2022 22:09:56 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
06/13/2022 22:09:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
06/13/2022 22:10:01 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=176
06/13/2022 22:10:03 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.22 on epoch=177
06/13/2022 22:10:06 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
06/13/2022 22:10:09 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.6727298396549314 on epoch=178
06/13/2022 22:10:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=178
06/13/2022 22:10:14 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
06/13/2022 22:10:17 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
06/13/2022 22:10:19 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
06/13/2022 22:10:22 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
06/13/2022 22:10:25 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7017591378228887 on epoch=181
06/13/2022 22:10:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
06/13/2022 22:10:30 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
06/13/2022 22:10:32 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=183
06/13/2022 22:10:35 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
06/13/2022 22:10:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
06/13/2022 22:10:41 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.7682378935898561 on epoch=184
06/13/2022 22:10:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
06/13/2022 22:10:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=185
06/13/2022 22:10:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
06/13/2022 22:10:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.12 on epoch=186
06/13/2022 22:10:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
06/13/2022 22:10:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:10:55 - INFO - __main__ - Printing 3 examples
06/13/2022 22:10:55 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:10:55 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:10:55 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:10:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:10:55 - INFO - __main__ - Printing 3 examples
06/13/2022 22:10:55 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 22:10:55 - INFO - __main__ - ['others']
06/13/2022 22:10:55 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:10:55 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:10:56 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:10:57 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.7328670642880705 on epoch=187
06/13/2022 22:10:57 - INFO - __main__ - save last model!
06/13/2022 22:10:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 22:10:57 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 22:10:57 - INFO - __main__ - Printing 3 examples
06/13/2022 22:10:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 22:10:57 - INFO - __main__ - ['others']
06/13/2022 22:10:57 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 22:10:57 - INFO - __main__ - ['others']
06/13/2022 22:10:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 22:10:57 - INFO - __main__ - ['others']
06/13/2022 22:10:57 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:10:59 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:11:05 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 22:11:14 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:11:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:11:15 - INFO - __main__ - Starting training!
06/13/2022 22:12:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_100_0.3_8_predictions.txt
06/13/2022 22:12:17 - INFO - __main__ - Classification-F1 on test data: 0.3825
06/13/2022 22:12:17 - INFO - __main__ - prefix=emo_64_100, lr=0.3, bsz=8, dev_performance=0.7921456911388768, test_performance=0.38248323039448284
06/13/2022 22:12:17 - INFO - __main__ - Running ... prefix=emo_64_100, lr=0.2, bsz=8 ...
06/13/2022 22:12:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:12:18 - INFO - __main__ - Printing 3 examples
06/13/2022 22:12:18 - INFO - __main__ -  [emo] how cause yes am listening
06/13/2022 22:12:18 - INFO - __main__ - ['others']
06/13/2022 22:12:18 - INFO - __main__ -  [emo] ok that way i like living wwrong
06/13/2022 22:12:18 - INFO - __main__ - ['others']
06/13/2022 22:12:18 - INFO - __main__ -  [emo] as u feel to on ur mind depends whose mind your mindn
06/13/2022 22:12:18 - INFO - __main__ - ['others']
06/13/2022 22:12:18 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:12:19 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:12:19 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:12:19 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:12:19 - INFO - __main__ - Printing 3 examples
06/13/2022 22:12:19 - INFO - __main__ -  [emo] i loving nature great things happen when men meet mountains d cool
06/13/2022 22:12:19 - INFO - __main__ - ['others']
06/13/2022 22:12:19 - INFO - __main__ -  [emo] i'm a handsome boy of 16 squintingfacewithtongue u expected a lot from me smilingface how old are you
06/13/2022 22:12:19 - INFO - __main__ - ['others']
06/13/2022 22:12:19 - INFO - __main__ -  [emo] all i want is a real women live to chat with me i want my fountain of youth back love is whatever you want it to be in a way yes
06/13/2022 22:12:19 - INFO - __main__ - ['others']
06/13/2022 22:12:19 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:12:19 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:12:19 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:12:38 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:12:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:12:39 - INFO - __main__ - Starting training!
06/13/2022 22:12:42 - INFO - __main__ - Step 10 Global step 10 Train loss 3.18 on epoch=0
06/13/2022 22:12:45 - INFO - __main__ - Step 20 Global step 20 Train loss 1.74 on epoch=1
06/13/2022 22:12:47 - INFO - __main__ - Step 30 Global step 30 Train loss 1.43 on epoch=1
06/13/2022 22:12:50 - INFO - __main__ - Step 40 Global step 40 Train loss 1.27 on epoch=2
06/13/2022 22:12:52 - INFO - __main__ - Step 50 Global step 50 Train loss 1.01 on epoch=3
06/13/2022 22:12:56 - INFO - __main__ - Global step 50 Train loss 1.73 Classification-F1 0.1622674710910005 on epoch=3
06/13/2022 22:12:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1622674710910005 on epoch=3, global_step=50
06/13/2022 22:12:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=3
06/13/2022 22:13:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.95 on epoch=4
06/13/2022 22:13:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.89 on epoch=4
06/13/2022 22:13:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=5
06/13/2022 22:13:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.90 on epoch=6
06/13/2022 22:13:12 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.3347524154589372 on epoch=6
06/13/2022 22:13:12 - INFO - __main__ - Saving model with best Classification-F1: 0.1622674710910005 -> 0.3347524154589372 on epoch=6, global_step=100
06/13/2022 22:13:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=6
06/13/2022 22:13:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=7
06/13/2022 22:13:19 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=8
06/13/2022 22:13:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=8
06/13/2022 22:13:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.80 on epoch=9
06/13/2022 22:13:27 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.17388791734507225 on epoch=9
06/13/2022 22:13:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=9
06/13/2022 22:13:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.87 on epoch=10
06/13/2022 22:13:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.86 on epoch=11
06/13/2022 22:13:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.84 on epoch=11
06/13/2022 22:13:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=12
06/13/2022 22:13:43 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.21797364528707813 on epoch=12
06/13/2022 22:13:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.84 on epoch=13
06/13/2022 22:13:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.77 on epoch=13
06/13/2022 22:13:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.74 on epoch=14
06/13/2022 22:13:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.73 on epoch=14
06/13/2022 22:13:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.78 on epoch=15
06/13/2022 22:13:59 - INFO - __main__ - Global step 250 Train loss 0.77 Classification-F1 0.49551149943883843 on epoch=15
06/13/2022 22:13:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3347524154589372 -> 0.49551149943883843 on epoch=15, global_step=250
06/13/2022 22:14:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=16
06/13/2022 22:14:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.73 on epoch=16
06/13/2022 22:14:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=17
06/13/2022 22:14:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.70 on epoch=18
06/13/2022 22:14:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=18
06/13/2022 22:14:15 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.44445164399432696 on epoch=18
06/13/2022 22:14:17 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=19
06/13/2022 22:14:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=19
06/13/2022 22:14:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=20
06/13/2022 22:14:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.69 on epoch=21
06/13/2022 22:14:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=21
06/13/2022 22:14:31 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.2106842781141075 on epoch=21
06/13/2022 22:14:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.74 on epoch=22
06/13/2022 22:14:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=23
06/13/2022 22:14:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.72 on epoch=23
06/13/2022 22:14:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=24
06/13/2022 22:14:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=24
06/13/2022 22:14:47 - INFO - __main__ - Global step 400 Train loss 0.67 Classification-F1 0.605941260780389 on epoch=24
06/13/2022 22:14:47 - INFO - __main__ - Saving model with best Classification-F1: 0.49551149943883843 -> 0.605941260780389 on epoch=24, global_step=400
06/13/2022 22:14:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.59 on epoch=25
06/13/2022 22:14:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.68 on epoch=26
06/13/2022 22:14:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.64 on epoch=26
06/13/2022 22:14:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=27
06/13/2022 22:14:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.65 on epoch=28
06/13/2022 22:15:02 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.6524287959458593 on epoch=28
06/13/2022 22:15:02 - INFO - __main__ - Saving model with best Classification-F1: 0.605941260780389 -> 0.6524287959458593 on epoch=28, global_step=450
06/13/2022 22:15:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=28
06/13/2022 22:15:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.66 on epoch=29
06/13/2022 22:15:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.72 on epoch=29
06/13/2022 22:15:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=30
06/13/2022 22:15:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=31
06/13/2022 22:15:18 - INFO - __main__ - Global step 500 Train loss 0.61 Classification-F1 0.608380080591034 on epoch=31
06/13/2022 22:15:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=31
06/13/2022 22:15:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=32
06/13/2022 22:15:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.61 on epoch=33
06/13/2022 22:15:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=33
06/13/2022 22:15:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=34
06/13/2022 22:15:34 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.6766026899553885 on epoch=34
06/13/2022 22:15:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6524287959458593 -> 0.6766026899553885 on epoch=34, global_step=550
06/13/2022 22:15:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=34
06/13/2022 22:15:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=35
06/13/2022 22:15:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.50 on epoch=36
06/13/2022 22:15:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.52 on epoch=36
06/13/2022 22:15:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=37
06/13/2022 22:15:50 - INFO - __main__ - Global step 600 Train loss 0.54 Classification-F1 0.6736846433340042 on epoch=37
06/13/2022 22:15:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.52 on epoch=38
06/13/2022 22:15:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=38
06/13/2022 22:15:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.58 on epoch=39
06/13/2022 22:16:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=39
06/13/2022 22:16:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
06/13/2022 22:16:06 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.6319031920460492 on epoch=40
06/13/2022 22:16:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.52 on epoch=41
06/13/2022 22:16:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.59 on epoch=41
06/13/2022 22:16:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=42
06/13/2022 22:16:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.60 on epoch=43
06/13/2022 22:16:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=43
06/13/2022 22:16:22 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.6365716467834956 on epoch=43
06/13/2022 22:16:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=44
06/13/2022 22:16:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=44
06/13/2022 22:16:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=45
06/13/2022 22:16:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.51 on epoch=46
06/13/2022 22:16:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.54 on epoch=46
06/13/2022 22:16:38 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.6755891659117466 on epoch=46
06/13/2022 22:16:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=47
06/13/2022 22:16:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.54 on epoch=48
06/13/2022 22:16:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=48
06/13/2022 22:16:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=49
06/13/2022 22:16:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=49
06/13/2022 22:16:54 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.6494391227413387 on epoch=49
06/13/2022 22:16:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=50
06/13/2022 22:16:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.52 on epoch=51
06/13/2022 22:17:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=51
06/13/2022 22:17:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=52
06/13/2022 22:17:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.46 on epoch=53
06/13/2022 22:17:09 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.6044234193173055 on epoch=53
06/13/2022 22:17:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=53
06/13/2022 22:17:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.39 on epoch=54
06/13/2022 22:17:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=54
06/13/2022 22:17:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=55
06/13/2022 22:17:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=56
06/13/2022 22:17:25 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.6884913054201134 on epoch=56
06/13/2022 22:17:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6766026899553885 -> 0.6884913054201134 on epoch=56, global_step=900
06/13/2022 22:17:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=56
06/13/2022 22:17:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=57
06/13/2022 22:17:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/13/2022 22:17:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=58
06/13/2022 22:17:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=59
06/13/2022 22:17:41 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.707505185515006 on epoch=59
06/13/2022 22:17:41 - INFO - __main__ - Saving model with best Classification-F1: 0.6884913054201134 -> 0.707505185515006 on epoch=59, global_step=950
06/13/2022 22:17:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.47 on epoch=59
06/13/2022 22:17:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=60
06/13/2022 22:17:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=61
06/13/2022 22:17:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/13/2022 22:17:54 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=62
06/13/2022 22:17:57 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.6770526649928058 on epoch=62
06/13/2022 22:17:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=63
06/13/2022 22:18:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.32 on epoch=63
06/13/2022 22:18:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=64
06/13/2022 22:18:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=64
06/13/2022 22:18:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=65
06/13/2022 22:18:13 - INFO - __main__ - Global step 1050 Train loss 0.33 Classification-F1 0.6893423920601235 on epoch=65
06/13/2022 22:18:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=66
06/13/2022 22:18:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=66
06/13/2022 22:18:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=67
06/13/2022 22:18:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=68
06/13/2022 22:18:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=68
06/13/2022 22:18:29 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.6478601567209162 on epoch=68
06/13/2022 22:18:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=69
06/13/2022 22:18:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=69
06/13/2022 22:18:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=70
06/13/2022 22:18:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=71
06/13/2022 22:18:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=71
06/13/2022 22:18:45 - INFO - __main__ - Global step 1150 Train loss 0.32 Classification-F1 0.6896143711810399 on epoch=71
06/13/2022 22:18:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.32 on epoch=72
06/13/2022 22:18:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.34 on epoch=73
06/13/2022 22:18:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
06/13/2022 22:18:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=74
06/13/2022 22:18:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=74
06/13/2022 22:19:00 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7062298917582459 on epoch=74
06/13/2022 22:19:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=75
06/13/2022 22:19:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.34 on epoch=76
06/13/2022 22:19:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=76
06/13/2022 22:19:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=77
06/13/2022 22:19:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=78
06/13/2022 22:19:16 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.6774205066914553 on epoch=78
06/13/2022 22:19:19 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=78
06/13/2022 22:19:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=79
06/13/2022 22:19:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
06/13/2022 22:19:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.31 on epoch=80
06/13/2022 22:19:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
06/13/2022 22:19:32 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.6760762910263769 on epoch=81
06/13/2022 22:19:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.35 on epoch=81
06/13/2022 22:19:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.40 on epoch=82
06/13/2022 22:19:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=83
06/13/2022 22:19:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=83
06/13/2022 22:19:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=84
06/13/2022 22:19:48 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.7044560144333303 on epoch=84
06/13/2022 22:19:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=84
06/13/2022 22:19:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
06/13/2022 22:19:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=86
06/13/2022 22:19:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=86
06/13/2022 22:20:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=87
06/13/2022 22:20:04 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.7475905599735799 on epoch=87
06/13/2022 22:20:04 - INFO - __main__ - Saving model with best Classification-F1: 0.707505185515006 -> 0.7475905599735799 on epoch=87, global_step=1400
06/13/2022 22:20:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.27 on epoch=88
06/13/2022 22:20:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.36 on epoch=88
06/13/2022 22:20:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=89
06/13/2022 22:20:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
06/13/2022 22:20:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=90
06/13/2022 22:20:20 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.7081275069011271 on epoch=90
06/13/2022 22:20:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=91
06/13/2022 22:20:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=91
06/13/2022 22:20:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
06/13/2022 22:20:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=93
06/13/2022 22:20:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=93
06/13/2022 22:20:36 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.7175630225572167 on epoch=93
06/13/2022 22:20:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
06/13/2022 22:20:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
06/13/2022 22:20:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=95
06/13/2022 22:20:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=96
06/13/2022 22:20:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
06/13/2022 22:20:52 - INFO - __main__ - Global step 1550 Train loss 0.27 Classification-F1 0.6940654722504989 on epoch=96
06/13/2022 22:20:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=97
06/13/2022 22:20:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=98
06/13/2022 22:20:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
06/13/2022 22:21:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=99
06/13/2022 22:21:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=99
06/13/2022 22:21:08 - INFO - __main__ - Global step 1600 Train loss 0.27 Classification-F1 0.6814040299114926 on epoch=99
06/13/2022 22:21:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
06/13/2022 22:21:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=101
06/13/2022 22:21:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=101
06/13/2022 22:21:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=102
06/13/2022 22:21:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=103
06/13/2022 22:21:24 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.7062621746832273 on epoch=103
06/13/2022 22:21:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=103
06/13/2022 22:21:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=104
06/13/2022 22:21:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=104
06/13/2022 22:21:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=105
06/13/2022 22:21:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=106
06/13/2022 22:21:40 - INFO - __main__ - Global step 1700 Train loss 0.22 Classification-F1 0.742365064215458 on epoch=106
06/13/2022 22:21:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=106
06/13/2022 22:21:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=107
06/13/2022 22:21:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=108
06/13/2022 22:21:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=108
06/13/2022 22:21:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=109
06/13/2022 22:21:55 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.7191515590195647 on epoch=109
06/13/2022 22:21:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.32 on epoch=109
06/13/2022 22:22:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
06/13/2022 22:22:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=111
06/13/2022 22:22:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
06/13/2022 22:22:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
06/13/2022 22:22:11 - INFO - __main__ - Global step 1800 Train loss 0.24 Classification-F1 0.725605983803491 on epoch=112
06/13/2022 22:22:14 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
06/13/2022 22:22:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=113
06/13/2022 22:22:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.27 on epoch=114
06/13/2022 22:22:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
06/13/2022 22:22:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=115
06/13/2022 22:22:27 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.7540293689040073 on epoch=115
06/13/2022 22:22:27 - INFO - __main__ - Saving model with best Classification-F1: 0.7475905599735799 -> 0.7540293689040073 on epoch=115, global_step=1850
06/13/2022 22:22:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.24 on epoch=116
06/13/2022 22:22:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=116
06/13/2022 22:22:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=117
06/13/2022 22:22:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=118
06/13/2022 22:22:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=118
06/13/2022 22:22:43 - INFO - __main__ - Global step 1900 Train loss 0.25 Classification-F1 0.7216891627543036 on epoch=118
06/13/2022 22:22:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
06/13/2022 22:22:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=119
06/13/2022 22:22:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.17 on epoch=120
06/13/2022 22:22:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=121
06/13/2022 22:22:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=121
06/13/2022 22:22:59 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.6529853453662461 on epoch=121
06/13/2022 22:23:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
06/13/2022 22:23:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
06/13/2022 22:23:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
06/13/2022 22:23:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=124
06/13/2022 22:23:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=124
06/13/2022 22:23:15 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.726072850586373 on epoch=124
06/13/2022 22:23:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.25 on epoch=125
06/13/2022 22:23:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=126
06/13/2022 22:23:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.24 on epoch=126
06/13/2022 22:23:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
06/13/2022 22:23:28 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=128
06/13/2022 22:23:31 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.6951764612312096 on epoch=128
06/13/2022 22:23:34 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
06/13/2022 22:23:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.23 on epoch=129
06/13/2022 22:23:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
06/13/2022 22:23:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.15 on epoch=130
06/13/2022 22:23:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=131
06/13/2022 22:23:47 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.7118386752959653 on epoch=131
06/13/2022 22:23:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
06/13/2022 22:23:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
06/13/2022 22:23:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.10 on epoch=133
06/13/2022 22:23:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.23 on epoch=133
06/13/2022 22:23:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
06/13/2022 22:24:03 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.7472744290400914 on epoch=134
06/13/2022 22:24:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.26 on epoch=134
06/13/2022 22:24:08 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.16 on epoch=135
06/13/2022 22:24:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.21 on epoch=136
06/13/2022 22:24:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=136
06/13/2022 22:24:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.30 on epoch=137
06/13/2022 22:24:19 - INFO - __main__ - Global step 2200 Train loss 0.22 Classification-F1 0.7919870171862697 on epoch=137
06/13/2022 22:24:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7540293689040073 -> 0.7919870171862697 on epoch=137, global_step=2200
06/13/2022 22:24:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=138
06/13/2022 22:24:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
06/13/2022 22:24:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
06/13/2022 22:24:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=139
06/13/2022 22:24:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=140
06/13/2022 22:24:35 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.7354746605143768 on epoch=140
06/13/2022 22:24:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=141
06/13/2022 22:24:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.16 on epoch=141
06/13/2022 22:24:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=142
06/13/2022 22:24:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.20 on epoch=143
06/13/2022 22:24:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
06/13/2022 22:24:51 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.672705931167016 on epoch=143
06/13/2022 22:24:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=144
06/13/2022 22:24:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.20 on epoch=144
06/13/2022 22:24:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=145
06/13/2022 22:25:01 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.11 on epoch=146
06/13/2022 22:25:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=146
06/13/2022 22:25:07 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.7253878375184379 on epoch=146
06/13/2022 22:25:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
06/13/2022 22:25:12 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
06/13/2022 22:25:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=148
06/13/2022 22:25:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.19 on epoch=149
06/13/2022 22:25:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=149
06/13/2022 22:25:22 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.6684809678215033 on epoch=149
06/13/2022 22:25:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
06/13/2022 22:25:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.22 on epoch=151
06/13/2022 22:25:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
06/13/2022 22:25:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.19 on epoch=152
06/13/2022 22:25:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
06/13/2022 22:25:38 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.7718971013991827 on epoch=153
06/13/2022 22:25:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
06/13/2022 22:25:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.12 on epoch=154
06/13/2022 22:25:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.11 on epoch=154
06/13/2022 22:25:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=155
06/13/2022 22:25:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
06/13/2022 22:25:54 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.7426500023274216 on epoch=156
06/13/2022 22:25:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.16 on epoch=156
06/13/2022 22:25:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.15 on epoch=157
06/13/2022 22:26:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.16 on epoch=158
06/13/2022 22:26:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
06/13/2022 22:26:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.14 on epoch=159
06/13/2022 22:26:10 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7076914252381878 on epoch=159
06/13/2022 22:26:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.13 on epoch=159
06/13/2022 22:26:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=160
06/13/2022 22:26:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=161
06/13/2022 22:26:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=161
06/13/2022 22:26:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=162
06/13/2022 22:26:26 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.7380447450777875 on epoch=162
06/13/2022 22:26:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
06/13/2022 22:26:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=163
06/13/2022 22:26:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=164
06/13/2022 22:26:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.15 on epoch=164
06/13/2022 22:26:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
06/13/2022 22:26:42 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.739205706735404 on epoch=165
06/13/2022 22:26:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
06/13/2022 22:26:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=166
06/13/2022 22:26:49 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=167
06/13/2022 22:26:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=168
06/13/2022 22:26:54 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=168
06/13/2022 22:26:58 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.7149239435008864 on epoch=168
06/13/2022 22:27:00 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=169
06/13/2022 22:27:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.11 on epoch=169
06/13/2022 22:27:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=170
06/13/2022 22:27:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.18 on epoch=171
06/13/2022 22:27:10 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
06/13/2022 22:27:14 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.6819223046848738 on epoch=171
06/13/2022 22:27:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=172
06/13/2022 22:27:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.14 on epoch=173
06/13/2022 22:27:21 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=173
06/13/2022 22:27:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.12 on epoch=174
06/13/2022 22:27:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.20 on epoch=174
06/13/2022 22:27:30 - INFO - __main__ - Global step 2800 Train loss 0.15 Classification-F1 0.7489016716576323 on epoch=174
06/13/2022 22:27:32 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=175
06/13/2022 22:27:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
06/13/2022 22:27:37 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
06/13/2022 22:27:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=177
06/13/2022 22:27:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.14 on epoch=178
06/13/2022 22:27:46 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.7640235354268374 on epoch=178
06/13/2022 22:27:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=178
06/13/2022 22:27:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.11 on epoch=179
06/13/2022 22:27:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.16 on epoch=179
06/13/2022 22:27:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
06/13/2022 22:27:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
06/13/2022 22:28:01 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7635485459390569 on epoch=181
06/13/2022 22:28:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
06/13/2022 22:28:06 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
06/13/2022 22:28:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.12 on epoch=183
06/13/2022 22:28:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
06/13/2022 22:28:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
06/13/2022 22:28:17 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.7479999637478483 on epoch=184
06/13/2022 22:28:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
06/13/2022 22:28:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
06/13/2022 22:28:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.11 on epoch=186
06/13/2022 22:28:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.16 on epoch=186
06/13/2022 22:28:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
06/13/2022 22:28:31 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:28:31 - INFO - __main__ - Printing 3 examples
06/13/2022 22:28:31 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 22:28:31 - INFO - __main__ - ['others']
06/13/2022 22:28:31 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 22:28:31 - INFO - __main__ - ['others']
06/13/2022 22:28:31 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 22:28:31 - INFO - __main__ - ['others']
06/13/2022 22:28:31 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:28:31 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:28:32 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:28:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:28:32 - INFO - __main__ - Printing 3 examples
06/13/2022 22:28:32 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 22:28:32 - INFO - __main__ - ['others']
06/13/2022 22:28:32 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 22:28:32 - INFO - __main__ - ['others']
06/13/2022 22:28:32 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 22:28:32 - INFO - __main__ - ['others']
06/13/2022 22:28:32 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:28:32 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:28:32 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:28:33 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.743160813731746 on epoch=187
06/13/2022 22:28:33 - INFO - __main__ - save last model!
06/13/2022 22:28:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 22:28:33 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 22:28:33 - INFO - __main__ - Printing 3 examples
06/13/2022 22:28:33 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 22:28:33 - INFO - __main__ - ['others']
06/13/2022 22:28:33 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 22:28:33 - INFO - __main__ - ['others']
06/13/2022 22:28:33 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 22:28:33 - INFO - __main__ - ['others']
06/13/2022 22:28:33 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:28:36 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:28:41 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 22:28:48 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:28:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:28:48 - INFO - __main__ - Starting training!
06/13/2022 22:29:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_100_0.2_8_predictions.txt
06/13/2022 22:29:53 - INFO - __main__ - Classification-F1 on test data: 0.4666
06/13/2022 22:29:54 - INFO - __main__ - prefix=emo_64_100, lr=0.2, bsz=8, dev_performance=0.7919870171862697, test_performance=0.4665641361531438
06/13/2022 22:29:54 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.5, bsz=8 ...
06/13/2022 22:29:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:29:55 - INFO - __main__ - Printing 3 examples
06/13/2022 22:29:55 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:29:55 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:29:55 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:29:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:29:55 - INFO - __main__ - Printing 3 examples
06/13/2022 22:29:55 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 22:29:55 - INFO - __main__ - ['others']
06/13/2022 22:29:55 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:29:55 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:29:55 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:30:11 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:30:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:30:12 - INFO - __main__ - Starting training!
06/13/2022 22:30:15 - INFO - __main__ - Step 10 Global step 10 Train loss 2.61 on epoch=0
06/13/2022 22:30:17 - INFO - __main__ - Step 20 Global step 20 Train loss 1.86 on epoch=1
06/13/2022 22:30:20 - INFO - __main__ - Step 30 Global step 30 Train loss 1.67 on epoch=1
06/13/2022 22:30:22 - INFO - __main__ - Step 40 Global step 40 Train loss 1.45 on epoch=2
06/13/2022 22:30:24 - INFO - __main__ - Step 50 Global step 50 Train loss 1.22 on epoch=3
06/13/2022 22:30:28 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.14345522898154478 on epoch=3
06/13/2022 22:30:28 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.14345522898154478 on epoch=3, global_step=50
06/13/2022 22:30:31 - INFO - __main__ - Step 60 Global step 60 Train loss 1.10 on epoch=3
06/13/2022 22:30:33 - INFO - __main__ - Step 70 Global step 70 Train loss 1.04 on epoch=4
06/13/2022 22:30:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=4
06/13/2022 22:30:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.99 on epoch=5
06/13/2022 22:30:40 - INFO - __main__ - Step 100 Global step 100 Train loss 1.09 on epoch=6
06/13/2022 22:30:44 - INFO - __main__ - Global step 100 Train loss 1.05 Classification-F1 0.17346055479047806 on epoch=6
06/13/2022 22:30:44 - INFO - __main__ - Saving model with best Classification-F1: 0.14345522898154478 -> 0.17346055479047806 on epoch=6, global_step=100
06/13/2022 22:30:46 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=6
06/13/2022 22:30:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.92 on epoch=7
06/13/2022 22:30:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.79 on epoch=8
06/13/2022 22:30:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=8
06/13/2022 22:30:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.94 on epoch=9
06/13/2022 22:30:59 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.21753319485174324 on epoch=9
06/13/2022 22:30:59 - INFO - __main__ - Saving model with best Classification-F1: 0.17346055479047806 -> 0.21753319485174324 on epoch=9, global_step=150
06/13/2022 22:31:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.87 on epoch=9
06/13/2022 22:31:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.89 on epoch=10
06/13/2022 22:31:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.90 on epoch=11
06/13/2022 22:31:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=11
06/13/2022 22:31:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=12
06/13/2022 22:31:15 - INFO - __main__ - Global step 200 Train loss 0.90 Classification-F1 0.1 on epoch=12
06/13/2022 22:31:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=13
06/13/2022 22:31:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=13
06/13/2022 22:31:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.82 on epoch=14
06/13/2022 22:31:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.89 on epoch=14
06/13/2022 22:31:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
06/13/2022 22:31:30 - INFO - __main__ - Global step 250 Train loss 0.85 Classification-F1 0.13869137398549164 on epoch=15
06/13/2022 22:31:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.94 on epoch=16
06/13/2022 22:31:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.79 on epoch=16
06/13/2022 22:31:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.90 on epoch=17
06/13/2022 22:31:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.77 on epoch=18
06/13/2022 22:31:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.77 on epoch=18
06/13/2022 22:31:45 - INFO - __main__ - Global step 300 Train loss 0.83 Classification-F1 0.2097693629903281 on epoch=18
06/13/2022 22:31:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.81 on epoch=19
06/13/2022 22:31:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.81 on epoch=19
06/13/2022 22:31:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.83 on epoch=20
06/13/2022 22:31:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.78 on epoch=21
06/13/2022 22:31:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=21
06/13/2022 22:32:01 - INFO - __main__ - Global step 350 Train loss 0.79 Classification-F1 0.22546851356070308 on epoch=21
06/13/2022 22:32:01 - INFO - __main__ - Saving model with best Classification-F1: 0.21753319485174324 -> 0.22546851356070308 on epoch=21, global_step=350
06/13/2022 22:32:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.86 on epoch=22
06/13/2022 22:32:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.81 on epoch=23
06/13/2022 22:32:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.75 on epoch=23
06/13/2022 22:32:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.79 on epoch=24
06/13/2022 22:32:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=24
06/13/2022 22:32:17 - INFO - __main__ - Global step 400 Train loss 0.79 Classification-F1 0.46387373273521726 on epoch=24
06/13/2022 22:32:17 - INFO - __main__ - Saving model with best Classification-F1: 0.22546851356070308 -> 0.46387373273521726 on epoch=24, global_step=400
06/13/2022 22:32:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.75 on epoch=25
06/13/2022 22:32:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.77 on epoch=26
06/13/2022 22:32:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.69 on epoch=26
06/13/2022 22:32:26 - INFO - __main__ - Step 440 Global step 440 Train loss 0.61 on epoch=27
06/13/2022 22:32:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.70 on epoch=28
06/13/2022 22:32:32 - INFO - __main__ - Global step 450 Train loss 0.70 Classification-F1 0.37006237006236997 on epoch=28
06/13/2022 22:32:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.64 on epoch=28
06/13/2022 22:32:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.79 on epoch=29
06/13/2022 22:32:39 - INFO - __main__ - Step 480 Global step 480 Train loss 0.66 on epoch=29
06/13/2022 22:32:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.68 on epoch=30
06/13/2022 22:32:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.67 on epoch=31
06/13/2022 22:32:48 - INFO - __main__ - Global step 500 Train loss 0.69 Classification-F1 0.5107399630972831 on epoch=31
06/13/2022 22:32:48 - INFO - __main__ - Saving model with best Classification-F1: 0.46387373273521726 -> 0.5107399630972831 on epoch=31, global_step=500
06/13/2022 22:32:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.62 on epoch=31
06/13/2022 22:32:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=32
06/13/2022 22:32:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.57 on epoch=33
06/13/2022 22:32:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=33
06/13/2022 22:33:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=34
06/13/2022 22:33:03 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.5640233983242915 on epoch=34
06/13/2022 22:33:03 - INFO - __main__ - Saving model with best Classification-F1: 0.5107399630972831 -> 0.5640233983242915 on epoch=34, global_step=550
06/13/2022 22:33:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.69 on epoch=34
06/13/2022 22:33:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=35
06/13/2022 22:33:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=36
06/13/2022 22:33:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=36
06/13/2022 22:33:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.68 on epoch=37
06/13/2022 22:33:19 - INFO - __main__ - Global step 600 Train loss 0.62 Classification-F1 0.62896964375459 on epoch=37
06/13/2022 22:33:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5640233983242915 -> 0.62896964375459 on epoch=37, global_step=600
06/13/2022 22:33:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=38
06/13/2022 22:33:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=38
06/13/2022 22:33:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.60 on epoch=39
06/13/2022 22:33:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=39
06/13/2022 22:33:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=40
06/13/2022 22:33:34 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.47220666012681434 on epoch=40
06/13/2022 22:33:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.61 on epoch=41
06/13/2022 22:33:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=41
06/13/2022 22:33:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=42
06/13/2022 22:33:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=43
06/13/2022 22:33:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=43
06/13/2022 22:33:50 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.6477840040753948 on epoch=43
06/13/2022 22:33:50 - INFO - __main__ - Saving model with best Classification-F1: 0.62896964375459 -> 0.6477840040753948 on epoch=43, global_step=700
06/13/2022 22:33:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=44
06/13/2022 22:33:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=44
06/13/2022 22:33:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=45
06/13/2022 22:33:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=46
06/13/2022 22:34:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=46
06/13/2022 22:34:05 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.6744484126984127 on epoch=46
06/13/2022 22:34:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6477840040753948 -> 0.6744484126984127 on epoch=46, global_step=750
06/13/2022 22:34:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=47
06/13/2022 22:34:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=48
06/13/2022 22:34:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=48
06/13/2022 22:34:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/13/2022 22:34:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=49
06/13/2022 22:34:21 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.5892362209531524 on epoch=49
06/13/2022 22:34:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=50
06/13/2022 22:34:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=51
06/13/2022 22:34:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
06/13/2022 22:34:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=52
06/13/2022 22:34:33 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=53
06/13/2022 22:34:36 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.5673817840753586 on epoch=53
06/13/2022 22:34:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
06/13/2022 22:34:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
06/13/2022 22:34:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=54
06/13/2022 22:34:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.52 on epoch=55
06/13/2022 22:34:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
06/13/2022 22:34:52 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.7215957086811995 on epoch=56
06/13/2022 22:34:52 - INFO - __main__ - Saving model with best Classification-F1: 0.6744484126984127 -> 0.7215957086811995 on epoch=56, global_step=900
06/13/2022 22:34:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=56
06/13/2022 22:34:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=57
06/13/2022 22:34:59 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
06/13/2022 22:35:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.33 on epoch=58
06/13/2022 22:35:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=59
06/13/2022 22:35:07 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.7001740085933535 on epoch=59
06/13/2022 22:35:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.31 on epoch=59
06/13/2022 22:35:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=60
06/13/2022 22:35:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=61
06/13/2022 22:35:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=61
06/13/2022 22:35:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=62
06/13/2022 22:35:23 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.7116476933828385 on epoch=62
06/13/2022 22:35:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=63
06/13/2022 22:35:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=63
06/13/2022 22:35:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=64
06/13/2022 22:35:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=64
06/13/2022 22:35:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=65
06/13/2022 22:35:38 - INFO - __main__ - Global step 1050 Train loss 0.30 Classification-F1 0.7175393557829624 on epoch=65
06/13/2022 22:35:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=66
06/13/2022 22:35:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
06/13/2022 22:35:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
06/13/2022 22:35:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
06/13/2022 22:35:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=68
06/13/2022 22:35:53 - INFO - __main__ - Global step 1100 Train loss 0.29 Classification-F1 0.7181230579242119 on epoch=68
06/13/2022 22:35:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=69
06/13/2022 22:35:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=69
06/13/2022 22:36:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=70
06/13/2022 22:36:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=71
06/13/2022 22:36:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=71
06/13/2022 22:36:09 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.6772506775975209 on epoch=71
06/13/2022 22:36:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=72
06/13/2022 22:36:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=73
06/13/2022 22:36:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=73
06/13/2022 22:36:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=74
06/13/2022 22:36:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=74
06/13/2022 22:36:24 - INFO - __main__ - Global step 1200 Train loss 0.23 Classification-F1 0.7597477626565079 on epoch=74
06/13/2022 22:36:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7215957086811995 -> 0.7597477626565079 on epoch=74, global_step=1200
06/13/2022 22:36:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=75
06/13/2022 22:36:29 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=76
06/13/2022 22:36:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
06/13/2022 22:36:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=77
06/13/2022 22:36:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=78
06/13/2022 22:36:40 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.6524485276842247 on epoch=78
06/13/2022 22:36:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=78
06/13/2022 22:36:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=79
06/13/2022 22:36:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=79
06/13/2022 22:36:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=80
06/13/2022 22:36:52 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
06/13/2022 22:36:55 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.6979490321075664 on epoch=81
06/13/2022 22:36:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
06/13/2022 22:37:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=82
06/13/2022 22:37:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=83
06/13/2022 22:37:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=83
06/13/2022 22:37:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
06/13/2022 22:37:10 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.7351406963812533 on epoch=84
06/13/2022 22:37:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=84
06/13/2022 22:37:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=85
06/13/2022 22:37:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=86
06/13/2022 22:37:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=86
06/13/2022 22:37:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
06/13/2022 22:37:26 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.7472002768114315 on epoch=87
06/13/2022 22:37:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=88
06/13/2022 22:37:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=88
06/13/2022 22:37:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
06/13/2022 22:37:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=89
06/13/2022 22:37:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.32 on epoch=90
06/13/2022 22:37:41 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.7185711526346582 on epoch=90
06/13/2022 22:37:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=91
06/13/2022 22:37:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=91
06/13/2022 22:37:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=92
06/13/2022 22:37:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.10 on epoch=93
06/13/2022 22:37:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
06/13/2022 22:37:57 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.7428889747203701 on epoch=93
06/13/2022 22:37:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=94
06/13/2022 22:38:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
06/13/2022 22:38:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=95
06/13/2022 22:38:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=96
06/13/2022 22:38:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=96
06/13/2022 22:38:12 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.6885983054428055 on epoch=96
06/13/2022 22:38:14 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=97
06/13/2022 22:38:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=98
06/13/2022 22:38:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=98
06/13/2022 22:38:21 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=99
06/13/2022 22:38:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
06/13/2022 22:38:27 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.7128178749602435 on epoch=99
06/13/2022 22:38:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=100
06/13/2022 22:38:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
06/13/2022 22:38:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.16 on epoch=101
06/13/2022 22:38:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=102
06/13/2022 22:38:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=103
06/13/2022 22:38:43 - INFO - __main__ - Global step 1650 Train loss 0.17 Classification-F1 0.7249790226077388 on epoch=103
06/13/2022 22:38:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=103
06/13/2022 22:38:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=104
06/13/2022 22:38:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
06/13/2022 22:38:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
06/13/2022 22:38:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=106
06/13/2022 22:38:58 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.7317867694305045 on epoch=106
06/13/2022 22:39:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=106
06/13/2022 22:39:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=107
06/13/2022 22:39:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=108
06/13/2022 22:39:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=108
06/13/2022 22:39:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=109
06/13/2022 22:39:13 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.724211148751281 on epoch=109
06/13/2022 22:39:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.08 on epoch=109
06/13/2022 22:39:18 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
06/13/2022 22:39:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=111
06/13/2022 22:39:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
06/13/2022 22:39:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=112
06/13/2022 22:39:29 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.7889696973134273 on epoch=112
06/13/2022 22:39:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7597477626565079 -> 0.7889696973134273 on epoch=112, global_step=1800
06/13/2022 22:39:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=113
06/13/2022 22:39:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
06/13/2022 22:39:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
06/13/2022 22:39:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
06/13/2022 22:39:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=115
06/13/2022 22:39:44 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.7042274235494574 on epoch=115
06/13/2022 22:39:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
06/13/2022 22:39:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=116
06/13/2022 22:39:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=117
06/13/2022 22:39:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
06/13/2022 22:39:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=118
06/13/2022 22:40:00 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.7649753583261479 on epoch=118
06/13/2022 22:40:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
06/13/2022 22:40:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
06/13/2022 22:40:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=120
06/13/2022 22:40:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=121
06/13/2022 22:40:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.17 on epoch=121
06/13/2022 22:40:15 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.7405379565658311 on epoch=121
06/13/2022 22:40:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=122
06/13/2022 22:40:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
06/13/2022 22:40:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=123
06/13/2022 22:40:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=124
06/13/2022 22:40:27 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=124
06/13/2022 22:40:31 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.7324946340506193 on epoch=124
06/13/2022 22:40:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
06/13/2022 22:40:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=126
06/13/2022 22:40:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.17 on epoch=126
06/13/2022 22:40:40 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
06/13/2022 22:40:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.06 on epoch=128
06/13/2022 22:40:46 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.7471484752671875 on epoch=128
06/13/2022 22:40:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=128
06/13/2022 22:40:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=129
06/13/2022 22:40:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=129
06/13/2022 22:40:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=130
06/13/2022 22:40:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=131
06/13/2022 22:41:02 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.7373227654251022 on epoch=131
06/13/2022 22:41:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=131
06/13/2022 22:41:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
06/13/2022 22:41:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=133
06/13/2022 22:41:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
06/13/2022 22:41:15 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.26 on epoch=134
06/13/2022 22:41:18 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.7226762479608855 on epoch=134
06/13/2022 22:41:21 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.08 on epoch=134
06/13/2022 22:41:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
06/13/2022 22:41:26 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.05 on epoch=136
06/13/2022 22:41:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
06/13/2022 22:41:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=137
06/13/2022 22:41:34 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7311025929115821 on epoch=137
06/13/2022 22:41:37 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=138
06/13/2022 22:41:39 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
06/13/2022 22:41:42 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.07 on epoch=139
06/13/2022 22:41:44 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=139
06/13/2022 22:41:47 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
06/13/2022 22:41:50 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7215958795097994 on epoch=140
06/13/2022 22:41:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
06/13/2022 22:41:55 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
06/13/2022 22:41:58 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
06/13/2022 22:42:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
06/13/2022 22:42:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
06/13/2022 22:42:06 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.7325482677329831 on epoch=143
06/13/2022 22:42:08 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
06/13/2022 22:42:11 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
06/13/2022 22:42:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=145
06/13/2022 22:42:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=146
06/13/2022 22:42:18 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
06/13/2022 22:42:22 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.7424892152019618 on epoch=146
06/13/2022 22:42:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=147
06/13/2022 22:42:27 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=148
06/13/2022 22:42:29 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=148
06/13/2022 22:42:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
06/13/2022 22:42:34 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=149
06/13/2022 22:42:38 - INFO - __main__ - Global step 2400 Train loss 0.10 Classification-F1 0.7232984728960584 on epoch=149
06/13/2022 22:42:40 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=150
06/13/2022 22:42:43 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
06/13/2022 22:42:45 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
06/13/2022 22:42:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
06/13/2022 22:42:50 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
06/13/2022 22:42:53 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.7475047978337452 on epoch=153
06/13/2022 22:42:56 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=153
06/13/2022 22:42:58 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=154
06/13/2022 22:43:01 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
06/13/2022 22:43:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.13 on epoch=155
06/13/2022 22:43:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=156
06/13/2022 22:43:09 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7177697868721669 on epoch=156
06/13/2022 22:43:12 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.09 on epoch=156
06/13/2022 22:43:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
06/13/2022 22:43:17 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=158
06/13/2022 22:43:19 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=158
06/13/2022 22:43:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.16 on epoch=159
06/13/2022 22:43:25 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.7365495129820232 on epoch=159
06/13/2022 22:43:27 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
06/13/2022 22:43:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=160
06/13/2022 22:43:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.11 on epoch=161
06/13/2022 22:43:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
06/13/2022 22:43:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
06/13/2022 22:43:41 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.7277788696715584 on epoch=162
06/13/2022 22:43:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.08 on epoch=163
06/13/2022 22:43:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=163
06/13/2022 22:43:48 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
06/13/2022 22:43:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=164
06/13/2022 22:43:53 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
06/13/2022 22:43:57 - INFO - __main__ - Global step 2650 Train loss 0.09 Classification-F1 0.7406820684645845 on epoch=165
06/13/2022 22:43:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.07 on epoch=166
06/13/2022 22:44:02 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
06/13/2022 22:44:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.18 on epoch=167
06/13/2022 22:44:07 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.04 on epoch=168
06/13/2022 22:44:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
06/13/2022 22:44:13 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.7379807081896275 on epoch=168
06/13/2022 22:44:15 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.08 on epoch=169
06/13/2022 22:44:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
06/13/2022 22:44:20 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.08 on epoch=170
06/13/2022 22:44:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
06/13/2022 22:44:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=171
06/13/2022 22:44:28 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.7375469645566612 on epoch=171
06/13/2022 22:44:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.14 on epoch=172
06/13/2022 22:44:33 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
06/13/2022 22:44:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.13 on epoch=173
06/13/2022 22:44:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=174
06/13/2022 22:44:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=174
06/13/2022 22:44:44 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7529249290478436 on epoch=174
06/13/2022 22:44:47 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
06/13/2022 22:44:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=176
06/13/2022 22:44:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
06/13/2022 22:44:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
06/13/2022 22:44:57 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=178
06/13/2022 22:45:00 - INFO - __main__ - Global step 2850 Train loss 0.09 Classification-F1 0.693334782813156 on epoch=178
06/13/2022 22:45:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
06/13/2022 22:45:05 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=179
06/13/2022 22:45:08 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.06 on epoch=179
06/13/2022 22:45:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
06/13/2022 22:45:13 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=181
06/13/2022 22:45:16 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.7081969246031747 on epoch=181
06/13/2022 22:45:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.12 on epoch=181
06/13/2022 22:45:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
06/13/2022 22:45:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/13/2022 22:45:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
06/13/2022 22:45:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
06/13/2022 22:45:32 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7443720981928972 on epoch=184
06/13/2022 22:45:34 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=184
06/13/2022 22:45:37 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.11 on epoch=185
06/13/2022 22:45:39 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=186
06/13/2022 22:45:42 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
06/13/2022 22:45:44 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.16 on epoch=187
06/13/2022 22:45:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:45:46 - INFO - __main__ - Printing 3 examples
06/13/2022 22:45:46 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:45:46 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:45:46 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:45:46 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:45:46 - INFO - __main__ - Printing 3 examples
06/13/2022 22:45:46 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 22:45:46 - INFO - __main__ - ['others']
06/13/2022 22:45:46 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:45:46 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:45:46 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:45:48 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.7562062013494675 on epoch=187
06/13/2022 22:45:48 - INFO - __main__ - save last model!
06/13/2022 22:45:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 22:45:48 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 22:45:48 - INFO - __main__ - Printing 3 examples
06/13/2022 22:45:48 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 22:45:48 - INFO - __main__ - ['others']
06/13/2022 22:45:48 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 22:45:48 - INFO - __main__ - ['others']
06/13/2022 22:45:48 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 22:45:48 - INFO - __main__ - ['others']
06/13/2022 22:45:48 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:45:50 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:45:55 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 22:46:02 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:46:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:46:02 - INFO - __main__ - Starting training!
06/13/2022 22:47:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_13_0.5_8_predictions.txt
06/13/2022 22:47:07 - INFO - __main__ - Classification-F1 on test data: 0.4394
06/13/2022 22:47:08 - INFO - __main__ - prefix=emo_64_13, lr=0.5, bsz=8, dev_performance=0.7889696973134273, test_performance=0.4394475767163697
06/13/2022 22:47:08 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.4, bsz=8 ...
06/13/2022 22:47:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:47:09 - INFO - __main__ - Printing 3 examples
06/13/2022 22:47:09 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:47:09 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:47:09 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 22:47:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 22:47:09 - INFO - __main__ - Printing 3 examples
06/13/2022 22:47:09 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 22:47:09 - INFO - __main__ - ['others']
06/13/2022 22:47:09 - INFO - __main__ - Tokenizing Input ...
06/13/2022 22:47:09 - INFO - __main__ - Tokenizing Output ...
06/13/2022 22:47:09 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 22:47:24 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 22:47:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 22:47:25 - INFO - __main__ - Starting training!
06/13/2022 22:47:28 - INFO - __main__ - Step 10 Global step 10 Train loss 2.88 on epoch=0
06/13/2022 22:47:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.40 on epoch=1
06/13/2022 22:47:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.06 on epoch=1
06/13/2022 22:47:36 - INFO - __main__ - Step 40 Global step 40 Train loss 1.01 on epoch=2
06/13/2022 22:47:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.98 on epoch=3
06/13/2022 22:47:42 - INFO - __main__ - Global step 50 Train loss 1.47 Classification-F1 0.17626452494873546 on epoch=3
06/13/2022 22:47:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17626452494873546 on epoch=3, global_step=50
06/13/2022 22:47:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.94 on epoch=3
06/13/2022 22:47:47 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=4
06/13/2022 22:47:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=4
06/13/2022 22:47:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=5
06/13/2022 22:47:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.98 on epoch=6
06/13/2022 22:47:58 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.2270766672940586 on epoch=6
06/13/2022 22:47:58 - INFO - __main__ - Saving model with best Classification-F1: 0.17626452494873546 -> 0.2270766672940586 on epoch=6, global_step=100
06/13/2022 22:48:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.88 on epoch=6
06/13/2022 22:48:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.98 on epoch=7
06/13/2022 22:48:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.88 on epoch=8
06/13/2022 22:48:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.81 on epoch=8
06/13/2022 22:48:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.74 on epoch=9
06/13/2022 22:48:14 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.3040109025118072 on epoch=9
06/13/2022 22:48:14 - INFO - __main__ - Saving model with best Classification-F1: 0.2270766672940586 -> 0.3040109025118072 on epoch=9, global_step=150
06/13/2022 22:48:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.89 on epoch=9
06/13/2022 22:48:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.88 on epoch=10
06/13/2022 22:48:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.85 on epoch=11
06/13/2022 22:48:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.76 on epoch=11
06/13/2022 22:48:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=12
06/13/2022 22:48:30 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.5215386260101583 on epoch=12
06/13/2022 22:48:30 - INFO - __main__ - Saving model with best Classification-F1: 0.3040109025118072 -> 0.5215386260101583 on epoch=12, global_step=200
06/13/2022 22:48:33 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=13
06/13/2022 22:48:35 - INFO - __main__ - Step 220 Global step 220 Train loss 0.70 on epoch=13
06/13/2022 22:48:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.77 on epoch=14
06/13/2022 22:48:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=14
06/13/2022 22:48:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=15
06/13/2022 22:48:46 - INFO - __main__ - Global step 250 Train loss 0.75 Classification-F1 0.5456733036057108 on epoch=15
06/13/2022 22:48:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5215386260101583 -> 0.5456733036057108 on epoch=15, global_step=250
06/13/2022 22:48:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=16
06/13/2022 22:48:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=16
06/13/2022 22:48:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=17
06/13/2022 22:48:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.75 on epoch=18
06/13/2022 22:48:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=18
06/13/2022 22:49:02 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.6300340474889786 on epoch=18
06/13/2022 22:49:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5456733036057108 -> 0.6300340474889786 on epoch=18, global_step=300
06/13/2022 22:49:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.66 on epoch=19
06/13/2022 22:49:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=19
06/13/2022 22:49:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.64 on epoch=20
06/13/2022 22:49:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.64 on epoch=21
06/13/2022 22:49:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=21
06/13/2022 22:49:18 - INFO - __main__ - Global step 350 Train loss 0.63 Classification-F1 0.5305847338935574 on epoch=21
06/13/2022 22:49:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.67 on epoch=22
06/13/2022 22:49:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=23
06/13/2022 22:49:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=23
06/13/2022 22:49:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.64 on epoch=24
06/13/2022 22:49:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=24
06/13/2022 22:49:34 - INFO - __main__ - Global step 400 Train loss 0.58 Classification-F1 0.6251890115313861 on epoch=24
06/13/2022 22:49:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.62 on epoch=25
06/13/2022 22:49:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=26
06/13/2022 22:49:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.56 on epoch=26
06/13/2022 22:49:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.69 on epoch=27
06/13/2022 22:49:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=28
06/13/2022 22:49:49 - INFO - __main__ - Global step 450 Train loss 0.57 Classification-F1 0.5812394552270518 on epoch=28
06/13/2022 22:49:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=28
06/13/2022 22:49:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=29
06/13/2022 22:49:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=29
06/13/2022 22:49:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.59 on epoch=30
06/13/2022 22:50:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=31
06/13/2022 22:50:05 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.5790235484859046 on epoch=31
06/13/2022 22:50:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.47 on epoch=31
06/13/2022 22:50:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=32
06/13/2022 22:50:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=33
06/13/2022 22:50:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=33
06/13/2022 22:50:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.54 on epoch=34
06/13/2022 22:50:21 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.6463116866763469 on epoch=34
06/13/2022 22:50:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6300340474889786 -> 0.6463116866763469 on epoch=34, global_step=550
06/13/2022 22:50:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=34
06/13/2022 22:50:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.54 on epoch=35
06/13/2022 22:50:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=36
06/13/2022 22:50:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=36
06/13/2022 22:50:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=37
06/13/2022 22:50:37 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.7072147346803601 on epoch=37
06/13/2022 22:50:37 - INFO - __main__ - Saving model with best Classification-F1: 0.6463116866763469 -> 0.7072147346803601 on epoch=37, global_step=600
06/13/2022 22:50:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=38
06/13/2022 22:50:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=38
06/13/2022 22:50:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=39
06/13/2022 22:50:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
06/13/2022 22:50:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=40
06/13/2022 22:50:53 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.7003672745694023 on epoch=40
06/13/2022 22:50:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=41
06/13/2022 22:50:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=41
06/13/2022 22:51:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=42
06/13/2022 22:51:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
06/13/2022 22:51:05 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
06/13/2022 22:51:09 - INFO - __main__ - Global step 700 Train loss 0.40 Classification-F1 0.722823265505479 on epoch=43
06/13/2022 22:51:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7072147346803601 -> 0.722823265505479 on epoch=43, global_step=700
06/13/2022 22:51:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=44
06/13/2022 22:51:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
06/13/2022 22:51:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=45
06/13/2022 22:51:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=46
06/13/2022 22:51:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=46
06/13/2022 22:51:24 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.701573450530285 on epoch=46
06/13/2022 22:51:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=47
06/13/2022 22:51:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
06/13/2022 22:51:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=48
06/13/2022 22:51:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=49
06/13/2022 22:51:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/13/2022 22:51:40 - INFO - __main__ - Global step 800 Train loss 0.40 Classification-F1 0.7364493206124849 on epoch=49
06/13/2022 22:51:40 - INFO - __main__ - Saving model with best Classification-F1: 0.722823265505479 -> 0.7364493206124849 on epoch=49, global_step=800
06/13/2022 22:51:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=50
06/13/2022 22:51:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=51
06/13/2022 22:51:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/13/2022 22:51:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=52
06/13/2022 22:51:53 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=53
06/13/2022 22:51:56 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.6807033727921383 on epoch=53
06/13/2022 22:51:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/13/2022 22:52:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
06/13/2022 22:52:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=54
06/13/2022 22:52:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
06/13/2022 22:52:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=56
06/13/2022 22:52:12 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.759869721262624 on epoch=56
06/13/2022 22:52:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7364493206124849 -> 0.759869721262624 on epoch=56, global_step=900
06/13/2022 22:52:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=56
06/13/2022 22:52:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=57
06/13/2022 22:52:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/13/2022 22:52:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
06/13/2022 22:52:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=59
06/13/2022 22:52:28 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.7202724983463498 on epoch=59
06/13/2022 22:52:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=59
06/13/2022 22:52:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.35 on epoch=60
06/13/2022 22:52:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=61
06/13/2022 22:52:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=61
06/13/2022 22:52:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/13/2022 22:52:44 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.7500375391994096 on epoch=62
06/13/2022 22:52:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=63
06/13/2022 22:52:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
06/13/2022 22:52:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
06/13/2022 22:52:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=64
06/13/2022 22:52:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
06/13/2022 22:53:00 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.7228931808088493 on epoch=65
06/13/2022 22:53:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
06/13/2022 22:53:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=66
06/13/2022 22:53:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
06/13/2022 22:53:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=68
06/13/2022 22:53:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=68
06/13/2022 22:53:16 - INFO - __main__ - Global step 1100 Train loss 0.30 Classification-F1 0.755571721459572 on epoch=68
06/13/2022 22:53:18 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=69
06/13/2022 22:53:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=69
06/13/2022 22:53:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/13/2022 22:53:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.35 on epoch=71
06/13/2022 22:53:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
06/13/2022 22:53:32 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.7481351586169815 on epoch=71
06/13/2022 22:53:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=72
06/13/2022 22:53:37 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
06/13/2022 22:53:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/13/2022 22:53:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=74
06/13/2022 22:53:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=74
06/13/2022 22:53:47 - INFO - __main__ - Global step 1200 Train loss 0.28 Classification-F1 0.7338673686897819 on epoch=74
06/13/2022 22:53:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=75
06/13/2022 22:53:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=76
06/13/2022 22:53:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
06/13/2022 22:53:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=77
06/13/2022 22:54:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=78
06/13/2022 22:54:03 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.704198972112921 on epoch=78
06/13/2022 22:54:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=78
06/13/2022 22:54:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=79
06/13/2022 22:54:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=79
06/13/2022 22:54:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/13/2022 22:54:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=81
06/13/2022 22:54:19 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.7531490881729734 on epoch=81
06/13/2022 22:54:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=81
06/13/2022 22:54:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=82
06/13/2022 22:54:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=83
06/13/2022 22:54:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=83
06/13/2022 22:54:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=84
06/13/2022 22:54:35 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.7520624740963724 on epoch=84
06/13/2022 22:54:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=84
06/13/2022 22:54:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.30 on epoch=85
06/13/2022 22:54:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=86
06/13/2022 22:54:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=86
06/13/2022 22:54:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=87
06/13/2022 22:54:51 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.7163908825093036 on epoch=87
06/13/2022 22:54:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=88
06/13/2022 22:54:56 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=88
06/13/2022 22:54:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=89
06/13/2022 22:55:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=89
06/13/2022 22:55:03 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
06/13/2022 22:55:07 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.735742623533594 on epoch=90
06/13/2022 22:55:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=91
06/13/2022 22:55:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
06/13/2022 22:55:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=92
06/13/2022 22:55:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=93
06/13/2022 22:55:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
06/13/2022 22:55:23 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.7402614465154731 on epoch=93
06/13/2022 22:55:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
06/13/2022 22:55:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
06/13/2022 22:55:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=95
06/13/2022 22:55:33 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
06/13/2022 22:55:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=96
06/13/2022 22:55:39 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.7650098073521623 on epoch=96
06/13/2022 22:55:39 - INFO - __main__ - Saving model with best Classification-F1: 0.759869721262624 -> 0.7650098073521623 on epoch=96, global_step=1550
06/13/2022 22:55:41 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=97
06/13/2022 22:55:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=98
06/13/2022 22:55:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=98
06/13/2022 22:55:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=99
06/13/2022 22:55:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=99
06/13/2022 22:55:55 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.7585453580819075 on epoch=99
06/13/2022 22:55:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=100
06/13/2022 22:56:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
06/13/2022 22:56:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
06/13/2022 22:56:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.52 on epoch=102
06/13/2022 22:56:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
06/13/2022 22:56:11 - INFO - __main__ - Global step 1650 Train loss 0.28 Classification-F1 0.7191106018442734 on epoch=103
06/13/2022 22:56:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=103
06/13/2022 22:56:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
06/13/2022 22:56:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=104
06/13/2022 22:56:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
06/13/2022 22:56:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=106
06/13/2022 22:56:27 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.7285392717069734 on epoch=106
06/13/2022 22:56:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=106
06/13/2022 22:56:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=107
06/13/2022 22:56:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=108
06/13/2022 22:56:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
06/13/2022 22:56:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=109
06/13/2022 22:56:42 - INFO - __main__ - Global step 1750 Train loss 0.23 Classification-F1 0.7692187124277484 on epoch=109
06/13/2022 22:56:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7650098073521623 -> 0.7692187124277484 on epoch=109, global_step=1750
06/13/2022 22:56:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
06/13/2022 22:56:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
06/13/2022 22:56:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
06/13/2022 22:56:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=111
06/13/2022 22:56:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=112
06/13/2022 22:56:58 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.7410314779609949 on epoch=112
06/13/2022 22:57:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=113
06/13/2022 22:57:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=113
06/13/2022 22:57:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=114
06/13/2022 22:57:08 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=114
06/13/2022 22:57:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=115
06/13/2022 22:57:14 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.743375366895516 on epoch=115
06/13/2022 22:57:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.34 on epoch=116
06/13/2022 22:57:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
06/13/2022 22:57:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=117
06/13/2022 22:57:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
06/13/2022 22:57:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=118
06/13/2022 22:57:30 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.7492803050792448 on epoch=118
06/13/2022 22:57:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
06/13/2022 22:57:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
06/13/2022 22:57:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=120
06/13/2022 22:57:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=121
06/13/2022 22:57:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=121
06/13/2022 22:57:46 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.7621887387849423 on epoch=121
06/13/2022 22:57:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.25 on epoch=122
06/13/2022 22:57:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
06/13/2022 22:57:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=123
06/13/2022 22:57:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=124
06/13/2022 22:57:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=124
06/13/2022 22:58:02 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7590207927752542 on epoch=124
06/13/2022 22:58:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.21 on epoch=125
06/13/2022 22:58:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=126
06/13/2022 22:58:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=126
06/13/2022 22:58:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.21 on epoch=127
06/13/2022 22:58:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
06/13/2022 22:58:18 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.7457239942836194 on epoch=128
06/13/2022 22:58:20 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=128
06/13/2022 22:58:23 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
06/13/2022 22:58:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=129
06/13/2022 22:58:28 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
06/13/2022 22:58:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
06/13/2022 22:58:34 - INFO - __main__ - Global step 2100 Train loss 0.17 Classification-F1 0.7519450548870235 on epoch=131
06/13/2022 22:58:36 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=131
06/13/2022 22:58:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
06/13/2022 22:58:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
06/13/2022 22:58:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=133
06/13/2022 22:58:46 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
06/13/2022 22:58:50 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7607508559091389 on epoch=134
06/13/2022 22:58:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
06/13/2022 22:58:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=135
06/13/2022 22:58:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=136
06/13/2022 22:59:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=136
06/13/2022 22:59:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=137
06/13/2022 22:59:06 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7542756724779901 on epoch=137
06/13/2022 22:59:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.14 on epoch=138
06/13/2022 22:59:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
06/13/2022 22:59:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=139
06/13/2022 22:59:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=139
06/13/2022 22:59:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=140
06/13/2022 22:59:22 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.7201808789898174 on epoch=140
06/13/2022 22:59:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=141
06/13/2022 22:59:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=141
06/13/2022 22:59:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
06/13/2022 22:59:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=143
06/13/2022 22:59:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=143
06/13/2022 22:59:38 - INFO - __main__ - Global step 2300 Train loss 0.17 Classification-F1 0.756224290780142 on epoch=143
06/13/2022 22:59:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.18 on epoch=144
06/13/2022 22:59:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=144
06/13/2022 22:59:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=145
06/13/2022 22:59:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
06/13/2022 22:59:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
06/13/2022 22:59:53 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.7740164461466574 on epoch=146
06/13/2022 22:59:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7692187124277484 -> 0.7740164461466574 on epoch=146, global_step=2350
06/13/2022 22:59:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
06/13/2022 22:59:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.21 on epoch=148
06/13/2022 23:00:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.15 on epoch=148
06/13/2022 23:00:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=149
06/13/2022 23:00:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
06/13/2022 23:00:09 - INFO - __main__ - Global step 2400 Train loss 0.17 Classification-F1 0.7407809056301341 on epoch=149
06/13/2022 23:00:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
06/13/2022 23:00:14 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
06/13/2022 23:00:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
06/13/2022 23:00:19 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=152
06/13/2022 23:00:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=153
06/13/2022 23:00:25 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7537301548247566 on epoch=153
06/13/2022 23:00:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.15 on epoch=153
06/13/2022 23:00:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
06/13/2022 23:00:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=154
06/13/2022 23:00:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
06/13/2022 23:00:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
06/13/2022 23:00:41 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.7372946715574809 on epoch=156
06/13/2022 23:00:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=156
06/13/2022 23:00:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
06/13/2022 23:00:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
06/13/2022 23:00:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.13 on epoch=158
06/13/2022 23:00:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.13 on epoch=159
06/13/2022 23:00:57 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7837614651233779 on epoch=159
06/13/2022 23:00:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7740164461466574 -> 0.7837614651233779 on epoch=159, global_step=2550
06/13/2022 23:00:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=159
06/13/2022 23:01:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=160
06/13/2022 23:01:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
06/13/2022 23:01:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=161
06/13/2022 23:01:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=162
06/13/2022 23:01:13 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.7721147827212735 on epoch=162
06/13/2022 23:01:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=163
06/13/2022 23:01:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=163
06/13/2022 23:01:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.14 on epoch=164
06/13/2022 23:01:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
06/13/2022 23:01:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
06/13/2022 23:01:29 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.7638277108889899 on epoch=165
06/13/2022 23:01:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
06/13/2022 23:01:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.15 on epoch=166
06/13/2022 23:01:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=167
06/13/2022 23:01:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
06/13/2022 23:01:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
06/13/2022 23:01:45 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.7716328244511174 on epoch=168
06/13/2022 23:01:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=169
06/13/2022 23:01:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
06/13/2022 23:01:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
06/13/2022 23:01:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=171
06/13/2022 23:01:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.11 on epoch=171
06/13/2022 23:02:01 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.7612050361866363 on epoch=171
06/13/2022 23:02:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
06/13/2022 23:02:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=173
06/13/2022 23:02:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=173
06/13/2022 23:02:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.17 on epoch=174
06/13/2022 23:02:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
06/13/2022 23:02:17 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.7757699989091699 on epoch=174
06/13/2022 23:02:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
06/13/2022 23:02:22 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.08 on epoch=176
06/13/2022 23:02:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.23 on epoch=176
06/13/2022 23:02:27 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
06/13/2022 23:02:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
06/13/2022 23:02:33 - INFO - __main__ - Global step 2850 Train loss 0.13 Classification-F1 0.7631434273257981 on epoch=178
06/13/2022 23:02:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
06/13/2022 23:02:38 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
06/13/2022 23:02:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
06/13/2022 23:02:43 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=180
06/13/2022 23:02:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
06/13/2022 23:02:48 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.7690980807401911 on epoch=181
06/13/2022 23:02:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.10 on epoch=181
06/13/2022 23:02:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=182
06/13/2022 23:02:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/13/2022 23:02:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.12 on epoch=183
06/13/2022 23:03:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.11 on epoch=184
06/13/2022 23:03:04 - INFO - __main__ - Global step 2950 Train loss 0.09 Classification-F1 0.7604086834417083 on epoch=184
06/13/2022 23:03:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=184
06/13/2022 23:03:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
06/13/2022 23:03:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=186
06/13/2022 23:03:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
06/13/2022 23:03:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
06/13/2022 23:03:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:03:18 - INFO - __main__ - Printing 3 examples
06/13/2022 23:03:18 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 23:03:18 - INFO - __main__ - ['others']
06/13/2022 23:03:18 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 23:03:18 - INFO - __main__ - ['others']
06/13/2022 23:03:18 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 23:03:18 - INFO - __main__ - ['others']
06/13/2022 23:03:18 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:03:18 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:03:19 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:03:19 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:03:19 - INFO - __main__ - Printing 3 examples
06/13/2022 23:03:19 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 23:03:19 - INFO - __main__ - ['others']
06/13/2022 23:03:19 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 23:03:19 - INFO - __main__ - ['others']
06/13/2022 23:03:19 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 23:03:19 - INFO - __main__ - ['others']
06/13/2022 23:03:19 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:03:19 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:03:19 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:03:20 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.7865157109174612 on epoch=187
06/13/2022 23:03:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7837614651233779 -> 0.7865157109174612 on epoch=187, global_step=3000
06/13/2022 23:03:20 - INFO - __main__ - save last model!
06/13/2022 23:03:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 23:03:21 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 23:03:21 - INFO - __main__ - Printing 3 examples
06/13/2022 23:03:21 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 23:03:21 - INFO - __main__ - ['others']
06/13/2022 23:03:21 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 23:03:21 - INFO - __main__ - ['others']
06/13/2022 23:03:21 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 23:03:21 - INFO - __main__ - ['others']
06/13/2022 23:03:21 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:03:23 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:03:28 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 23:03:34 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:03:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:03:35 - INFO - __main__ - Starting training!
06/13/2022 23:04:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_13_0.4_8_predictions.txt
06/13/2022 23:04:41 - INFO - __main__ - Classification-F1 on test data: 0.3628
06/13/2022 23:04:41 - INFO - __main__ - prefix=emo_64_13, lr=0.4, bsz=8, dev_performance=0.7865157109174612, test_performance=0.36283526713240943
06/13/2022 23:04:41 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.3, bsz=8 ...
06/13/2022 23:04:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:04:42 - INFO - __main__ - Printing 3 examples
06/13/2022 23:04:42 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:04:42 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:04:42 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:04:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:04:42 - INFO - __main__ - Printing 3 examples
06/13/2022 23:04:42 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 23:04:42 - INFO - __main__ - ['others']
06/13/2022 23:04:42 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:04:43 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:04:43 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:04:58 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:04:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:04:59 - INFO - __main__ - Starting training!
06/13/2022 23:05:02 - INFO - __main__ - Step 10 Global step 10 Train loss 3.04 on epoch=0
06/13/2022 23:05:04 - INFO - __main__ - Step 20 Global step 20 Train loss 1.52 on epoch=1
06/13/2022 23:05:07 - INFO - __main__ - Step 30 Global step 30 Train loss 1.15 on epoch=1
06/13/2022 23:05:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=2
06/13/2022 23:05:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=3
06/13/2022 23:05:15 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.1 on epoch=3
06/13/2022 23:05:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=3, global_step=50
06/13/2022 23:05:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=3
06/13/2022 23:05:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.94 on epoch=4
06/13/2022 23:05:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=4
06/13/2022 23:05:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.93 on epoch=5
06/13/2022 23:05:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=6
06/13/2022 23:05:31 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.4099261570291135 on epoch=6
06/13/2022 23:05:31 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.4099261570291135 on epoch=6, global_step=100
06/13/2022 23:05:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.85 on epoch=6
06/13/2022 23:05:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.84 on epoch=7
06/13/2022 23:05:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=8
06/13/2022 23:05:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=8
06/13/2022 23:05:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=9
06/13/2022 23:05:47 - INFO - __main__ - Global step 150 Train loss 0.85 Classification-F1 0.3332017151454678 on epoch=9
06/13/2022 23:05:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=9
06/13/2022 23:05:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=10
06/13/2022 23:05:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.74 on epoch=11
06/13/2022 23:05:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=11
06/13/2022 23:05:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.82 on epoch=12
06/13/2022 23:06:03 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.5161495371068027 on epoch=12
06/13/2022 23:06:03 - INFO - __main__ - Saving model with best Classification-F1: 0.4099261570291135 -> 0.5161495371068027 on epoch=12, global_step=200
06/13/2022 23:06:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.68 on epoch=13
06/13/2022 23:06:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.73 on epoch=13
06/13/2022 23:06:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=14
06/13/2022 23:06:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=14
06/13/2022 23:06:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=15
06/13/2022 23:06:19 - INFO - __main__ - Global step 250 Train loss 0.69 Classification-F1 0.556729115397846 on epoch=15
06/13/2022 23:06:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5161495371068027 -> 0.556729115397846 on epoch=15, global_step=250
06/13/2022 23:06:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.74 on epoch=16
06/13/2022 23:06:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=16
06/13/2022 23:06:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.75 on epoch=17
06/13/2022 23:06:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=18
06/13/2022 23:06:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.65 on epoch=18
06/13/2022 23:06:35 - INFO - __main__ - Global step 300 Train loss 0.66 Classification-F1 0.4984026919191962 on epoch=18
06/13/2022 23:06:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
06/13/2022 23:06:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=19
06/13/2022 23:06:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=20
06/13/2022 23:06:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=21
06/13/2022 23:06:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=21
06/13/2022 23:06:50 - INFO - __main__ - Global step 350 Train loss 0.62 Classification-F1 0.6083172657453386 on epoch=21
06/13/2022 23:06:50 - INFO - __main__ - Saving model with best Classification-F1: 0.556729115397846 -> 0.6083172657453386 on epoch=21, global_step=350
06/13/2022 23:06:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.71 on epoch=22
06/13/2022 23:06:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.48 on epoch=23
06/13/2022 23:06:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.63 on epoch=23
06/13/2022 23:07:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=24
06/13/2022 23:07:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=24
06/13/2022 23:07:06 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.6760297992714872 on epoch=24
06/13/2022 23:07:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6083172657453386 -> 0.6760297992714872 on epoch=24, global_step=400
06/13/2022 23:07:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=25
06/13/2022 23:07:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.53 on epoch=26
06/13/2022 23:07:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=26
06/13/2022 23:07:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=27
06/13/2022 23:07:19 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=28
06/13/2022 23:07:22 - INFO - __main__ - Global step 450 Train loss 0.56 Classification-F1 0.568178652843653 on epoch=28
06/13/2022 23:07:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=28
06/13/2022 23:07:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.63 on epoch=29
06/13/2022 23:07:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=29
06/13/2022 23:07:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=30
06/13/2022 23:07:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=31
06/13/2022 23:07:38 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.647926748988276 on epoch=31
06/13/2022 23:07:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=31
06/13/2022 23:07:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.58 on epoch=32
06/13/2022 23:07:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=33
06/13/2022 23:07:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=33
06/13/2022 23:07:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=34
06/13/2022 23:07:54 - INFO - __main__ - Global step 550 Train loss 0.50 Classification-F1 0.6782433666857859 on epoch=34
06/13/2022 23:07:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6760297992714872 -> 0.6782433666857859 on epoch=34, global_step=550
06/13/2022 23:07:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=34
06/13/2022 23:07:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.51 on epoch=35
06/13/2022 23:08:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=36
06/13/2022 23:08:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=36
06/13/2022 23:08:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=37
06/13/2022 23:08:10 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.6964785214785214 on epoch=37
06/13/2022 23:08:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6782433666857859 -> 0.6964785214785214 on epoch=37, global_step=600
06/13/2022 23:08:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=38
06/13/2022 23:08:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=38
06/13/2022 23:08:17 - INFO - __main__ - Step 630 Global step 630 Train loss 0.52 on epoch=39
06/13/2022 23:08:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=39
06/13/2022 23:08:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=40
06/13/2022 23:08:25 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.6511046159797977 on epoch=40
06/13/2022 23:08:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=41
06/13/2022 23:08:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=41
06/13/2022 23:08:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=42
06/13/2022 23:08:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
06/13/2022 23:08:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=43
06/13/2022 23:08:41 - INFO - __main__ - Global step 700 Train loss 0.42 Classification-F1 0.6439775019274909 on epoch=43
06/13/2022 23:08:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.49 on epoch=44
06/13/2022 23:08:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
06/13/2022 23:08:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=45
06/13/2022 23:08:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=46
06/13/2022 23:08:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=46
06/13/2022 23:08:57 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.6153571428571428 on epoch=46
06/13/2022 23:09:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=47
06/13/2022 23:09:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/13/2022 23:09:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=48
06/13/2022 23:09:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/13/2022 23:09:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.37 on epoch=49
06/13/2022 23:09:13 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.7296662031677599 on epoch=49
06/13/2022 23:09:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6964785214785214 -> 0.7296662031677599 on epoch=49, global_step=800
06/13/2022 23:09:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=50
06/13/2022 23:09:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=51
06/13/2022 23:09:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=51
06/13/2022 23:09:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=52
06/13/2022 23:09:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=53
06/13/2022 23:09:29 - INFO - __main__ - Global step 850 Train loss 0.39 Classification-F1 0.6564311943392556 on epoch=53
06/13/2022 23:09:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/13/2022 23:09:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=54
06/13/2022 23:09:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=54
06/13/2022 23:09:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=55
06/13/2022 23:09:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=56
06/13/2022 23:09:45 - INFO - __main__ - Global step 900 Train loss 0.39 Classification-F1 0.726871153251733 on epoch=56
06/13/2022 23:09:47 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=56
06/13/2022 23:09:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/13/2022 23:09:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=58
06/13/2022 23:09:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/13/2022 23:09:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=59
06/13/2022 23:10:00 - INFO - __main__ - Global step 950 Train loss 0.34 Classification-F1 0.6936427924195581 on epoch=59
06/13/2022 23:10:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=59
06/13/2022 23:10:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=60
06/13/2022 23:10:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=61
06/13/2022 23:10:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=61
06/13/2022 23:10:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/13/2022 23:10:16 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.6832861172762651 on epoch=62
06/13/2022 23:10:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=63
06/13/2022 23:10:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=63
06/13/2022 23:10:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/13/2022 23:10:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=64
06/13/2022 23:10:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=65
06/13/2022 23:10:32 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.7035960929703867 on epoch=65
06/13/2022 23:10:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=66
06/13/2022 23:10:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=66
06/13/2022 23:10:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=67
06/13/2022 23:10:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
06/13/2022 23:10:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=68
06/13/2022 23:10:48 - INFO - __main__ - Global step 1100 Train loss 0.31 Classification-F1 0.7148497715111061 on epoch=68
06/13/2022 23:10:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=69
06/13/2022 23:10:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=69
06/13/2022 23:10:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=70
06/13/2022 23:10:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=71
06/13/2022 23:11:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=71
06/13/2022 23:11:04 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.6984741347751401 on epoch=71
06/13/2022 23:11:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=72
06/13/2022 23:11:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=73
06/13/2022 23:11:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=73
06/13/2022 23:11:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=74
06/13/2022 23:11:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
06/13/2022 23:11:20 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7520524770782104 on epoch=74
06/13/2022 23:11:20 - INFO - __main__ - Saving model with best Classification-F1: 0.7296662031677599 -> 0.7520524770782104 on epoch=74, global_step=1200
06/13/2022 23:11:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=75
06/13/2022 23:11:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
06/13/2022 23:11:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
06/13/2022 23:11:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=77
06/13/2022 23:11:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=78
06/13/2022 23:11:35 - INFO - __main__ - Global step 1250 Train loss 0.28 Classification-F1 0.7034784601022349 on epoch=78
06/13/2022 23:11:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=78
06/13/2022 23:11:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=79
06/13/2022 23:11:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=79
06/13/2022 23:11:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
06/13/2022 23:11:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=81
06/13/2022 23:11:51 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.7333804664935215 on epoch=81
06/13/2022 23:11:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=81
06/13/2022 23:11:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=82
06/13/2022 23:11:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
06/13/2022 23:12:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=83
06/13/2022 23:12:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=84
06/13/2022 23:12:07 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.7762959745625159 on epoch=84
06/13/2022 23:12:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7520524770782104 -> 0.7762959745625159 on epoch=84, global_step=1350
06/13/2022 23:12:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=84
06/13/2022 23:12:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=85
06/13/2022 23:12:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
06/13/2022 23:12:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
06/13/2022 23:12:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.21 on epoch=87
06/13/2022 23:12:23 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.7651530647881141 on epoch=87
06/13/2022 23:12:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=88
06/13/2022 23:12:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=88
06/13/2022 23:12:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=89
06/13/2022 23:12:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=89
06/13/2022 23:12:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
06/13/2022 23:12:39 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7505188646303629 on epoch=90
06/13/2022 23:12:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
06/13/2022 23:12:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=91
06/13/2022 23:12:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.31 on epoch=92
06/13/2022 23:12:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
06/13/2022 23:12:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.20 on epoch=93
06/13/2022 23:12:54 - INFO - __main__ - Global step 1500 Train loss 0.23 Classification-F1 0.7689295055424088 on epoch=93
06/13/2022 23:12:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
06/13/2022 23:12:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
06/13/2022 23:13:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=95
06/13/2022 23:13:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=96
06/13/2022 23:13:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
06/13/2022 23:13:10 - INFO - __main__ - Global step 1550 Train loss 0.24 Classification-F1 0.7552229306011693 on epoch=96
06/13/2022 23:13:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=97
06/13/2022 23:13:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=98
06/13/2022 23:13:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
06/13/2022 23:13:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
06/13/2022 23:13:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=99
06/13/2022 23:13:26 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.735521924704509 on epoch=99
06/13/2022 23:13:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
06/13/2022 23:13:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=101
06/13/2022 23:13:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
06/13/2022 23:13:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
06/13/2022 23:13:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
06/13/2022 23:13:42 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.7309457883998833 on epoch=103
06/13/2022 23:13:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
06/13/2022 23:13:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
06/13/2022 23:13:50 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=104
06/13/2022 23:13:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
06/13/2022 23:13:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=106
06/13/2022 23:13:58 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.7530055207572081 on epoch=106
06/13/2022 23:14:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=106
06/13/2022 23:14:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=107
06/13/2022 23:14:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
06/13/2022 23:14:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=108
06/13/2022 23:14:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=109
06/13/2022 23:14:14 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.7537944021152976 on epoch=109
06/13/2022 23:14:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=109
06/13/2022 23:14:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
06/13/2022 23:14:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=111
06/13/2022 23:14:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=111
06/13/2022 23:14:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=112
06/13/2022 23:14:30 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.7712818032565696 on epoch=112
06/13/2022 23:14:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=113
06/13/2022 23:14:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=113
06/13/2022 23:14:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=114
06/13/2022 23:14:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=114
06/13/2022 23:14:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=115
06/13/2022 23:14:46 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.7647850712372397 on epoch=115
06/13/2022 23:14:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
06/13/2022 23:14:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=116
06/13/2022 23:14:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=117
06/13/2022 23:14:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.14 on epoch=118
06/13/2022 23:14:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
06/13/2022 23:15:01 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.7390553532127436 on epoch=118
06/13/2022 23:15:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=119
06/13/2022 23:15:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=119
06/13/2022 23:15:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.15 on epoch=120
06/13/2022 23:15:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
06/13/2022 23:15:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=121
06/13/2022 23:15:17 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.7516057228823186 on epoch=121
06/13/2022 23:15:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=122
06/13/2022 23:15:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=123
06/13/2022 23:15:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=123
06/13/2022 23:15:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
06/13/2022 23:15:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=124
06/13/2022 23:15:33 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7349749112907007 on epoch=124
06/13/2022 23:15:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
06/13/2022 23:15:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=126
06/13/2022 23:15:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
06/13/2022 23:15:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
06/13/2022 23:15:46 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
06/13/2022 23:15:49 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.7182775278412862 on epoch=128
06/13/2022 23:15:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
06/13/2022 23:15:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
06/13/2022 23:15:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
06/13/2022 23:15:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.17 on epoch=130
06/13/2022 23:16:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=131
06/13/2022 23:16:05 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.7396805072861411 on epoch=131
06/13/2022 23:16:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
06/13/2022 23:16:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.14 on epoch=132
06/13/2022 23:16:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
06/13/2022 23:16:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
06/13/2022 23:16:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=134
06/13/2022 23:16:21 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.7757043111687074 on epoch=134
06/13/2022 23:16:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
06/13/2022 23:16:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=135
06/13/2022 23:16:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=136
06/13/2022 23:16:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=136
06/13/2022 23:16:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.20 on epoch=137
06/13/2022 23:16:37 - INFO - __main__ - Global step 2200 Train loss 0.15 Classification-F1 0.7799093502355005 on epoch=137
06/13/2022 23:16:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7762959745625159 -> 0.7799093502355005 on epoch=137, global_step=2200
06/13/2022 23:16:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=138
06/13/2022 23:16:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.18 on epoch=138
06/13/2022 23:16:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=139
06/13/2022 23:16:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=139
06/13/2022 23:16:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=140
06/13/2022 23:16:52 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.7709033511413308 on epoch=140
06/13/2022 23:16:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
06/13/2022 23:16:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
06/13/2022 23:17:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.11 on epoch=142
06/13/2022 23:17:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
06/13/2022 23:17:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=143
06/13/2022 23:17:08 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.7393613575332016 on epoch=143
06/13/2022 23:17:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.22 on epoch=144
06/13/2022 23:17:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=144
06/13/2022 23:17:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
06/13/2022 23:17:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
06/13/2022 23:17:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
06/13/2022 23:17:24 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.7290930344673857 on epoch=146
06/13/2022 23:17:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.12 on epoch=147
06/13/2022 23:17:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.11 on epoch=148
06/13/2022 23:17:32 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=148
06/13/2022 23:17:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.10 on epoch=149
06/13/2022 23:17:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=149
06/13/2022 23:17:40 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.7224965170348658 on epoch=149
06/13/2022 23:17:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=150
06/13/2022 23:17:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.12 on epoch=151
06/13/2022 23:17:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.15 on epoch=151
06/13/2022 23:17:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=152
06/13/2022 23:17:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.08 on epoch=153
06/13/2022 23:17:56 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7315818547315258 on epoch=153
06/13/2022 23:17:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
06/13/2022 23:18:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
06/13/2022 23:18:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=154
06/13/2022 23:18:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.22 on epoch=155
06/13/2022 23:18:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=156
06/13/2022 23:18:11 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.7579085580813243 on epoch=156
06/13/2022 23:18:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.11 on epoch=156
06/13/2022 23:18:16 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
06/13/2022 23:18:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=158
06/13/2022 23:18:21 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
06/13/2022 23:18:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=159
06/13/2022 23:18:27 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.7343630498283947 on epoch=159
06/13/2022 23:18:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=159
06/13/2022 23:18:32 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.13 on epoch=160
06/13/2022 23:18:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=161
06/13/2022 23:18:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
06/13/2022 23:18:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=162
06/13/2022 23:18:43 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7613511791753655 on epoch=162
06/13/2022 23:18:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
06/13/2022 23:18:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
06/13/2022 23:18:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.09 on epoch=164
06/13/2022 23:18:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=164
06/13/2022 23:18:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=165
06/13/2022 23:18:59 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.6230147310722851 on epoch=165
06/13/2022 23:19:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=166
06/13/2022 23:19:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.09 on epoch=166
06/13/2022 23:19:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
06/13/2022 23:19:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
06/13/2022 23:19:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
06/13/2022 23:19:15 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.5969741855418487 on epoch=168
06/13/2022 23:19:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=169
06/13/2022 23:19:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=169
06/13/2022 23:19:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=170
06/13/2022 23:19:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=171
06/13/2022 23:19:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=171
06/13/2022 23:19:31 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.7379215067662183 on epoch=171
06/13/2022 23:19:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=172
06/13/2022 23:19:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=173
06/13/2022 23:19:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.15 on epoch=173
06/13/2022 23:19:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=174
06/13/2022 23:19:44 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
06/13/2022 23:19:47 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7622450959249487 on epoch=174
06/13/2022 23:19:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
06/13/2022 23:19:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.14 on epoch=176
06/13/2022 23:19:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
06/13/2022 23:19:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
06/13/2022 23:19:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
06/13/2022 23:20:03 - INFO - __main__ - Global step 2850 Train loss 0.10 Classification-F1 0.751390310991015 on epoch=178
06/13/2022 23:20:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.08 on epoch=178
06/13/2022 23:20:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.12 on epoch=179
06/13/2022 23:20:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
06/13/2022 23:20:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=180
06/13/2022 23:20:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
06/13/2022 23:20:19 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.7638538050936008 on epoch=181
06/13/2022 23:20:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=181
06/13/2022 23:20:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
06/13/2022 23:20:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=183
06/13/2022 23:20:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.15 on epoch=183
06/13/2022 23:20:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.08 on epoch=184
06/13/2022 23:20:34 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.7461729263111988 on epoch=184
06/13/2022 23:20:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
06/13/2022 23:20:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
06/13/2022 23:20:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=186
06/13/2022 23:20:44 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.15 on epoch=186
06/13/2022 23:20:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=187
06/13/2022 23:20:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:20:48 - INFO - __main__ - Printing 3 examples
06/13/2022 23:20:48 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 23:20:48 - INFO - __main__ - ['others']
06/13/2022 23:20:48 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 23:20:48 - INFO - __main__ - ['others']
06/13/2022 23:20:48 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 23:20:48 - INFO - __main__ - ['others']
06/13/2022 23:20:48 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:20:48 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:20:49 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:20:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:20:49 - INFO - __main__ - Printing 3 examples
06/13/2022 23:20:49 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 23:20:49 - INFO - __main__ - ['others']
06/13/2022 23:20:49 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 23:20:49 - INFO - __main__ - ['others']
06/13/2022 23:20:49 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 23:20:49 - INFO - __main__ - ['others']
06/13/2022 23:20:49 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:20:49 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:20:49 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:20:50 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.7798995477541905 on epoch=187
06/13/2022 23:20:50 - INFO - __main__ - save last model!
06/13/2022 23:20:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 23:20:50 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 23:20:50 - INFO - __main__ - Printing 3 examples
06/13/2022 23:20:50 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 23:20:50 - INFO - __main__ - ['others']
06/13/2022 23:20:50 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 23:20:50 - INFO - __main__ - ['others']
06/13/2022 23:20:50 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 23:20:50 - INFO - __main__ - ['others']
06/13/2022 23:20:50 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:20:52 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:20:58 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 23:21:04 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:21:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:21:05 - INFO - __main__ - Starting training!
06/13/2022 23:22:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_13_0.3_8_predictions.txt
06/13/2022 23:22:10 - INFO - __main__ - Classification-F1 on test data: 0.3954
06/13/2022 23:22:10 - INFO - __main__ - prefix=emo_64_13, lr=0.3, bsz=8, dev_performance=0.7799093502355005, test_performance=0.39538686502319803
06/13/2022 23:22:10 - INFO - __main__ - Running ... prefix=emo_64_13, lr=0.2, bsz=8 ...
06/13/2022 23:22:11 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:22:11 - INFO - __main__ - Printing 3 examples
06/13/2022 23:22:11 - INFO - __main__ -  [emo] you picture you sent one to my phone you sent one to my phone
06/13/2022 23:22:11 - INFO - __main__ - ['others']
06/13/2022 23:22:11 - INFO - __main__ -  [emo] it's boring without you is not boring on a date no not on date
06/13/2022 23:22:11 - INFO - __main__ - ['others']
06/13/2022 23:22:11 - INFO - __main__ -  [emo] really  hmph yes i just didn't bother to find out before how can you call me without having my number
06/13/2022 23:22:11 - INFO - __main__ - ['others']
06/13/2022 23:22:11 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:22:11 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:22:12 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:22:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:22:12 - INFO - __main__ - Printing 3 examples
06/13/2022 23:22:12 - INFO - __main__ -  [emo] oh ur r so lucky smilingfacewithhearteyes oh really thanksgrinningfacewithsmilingeyes disappointedface
06/13/2022 23:22:12 - INFO - __main__ - ['others']
06/13/2022 23:22:12 - INFO - __main__ -  [emo] that's nothing smilingfacewithsmilingeyes you are welcome how is your day so far as same so before how about you sister
06/13/2022 23:22:12 - INFO - __main__ - ['others']
06/13/2022 23:22:12 - INFO - __main__ -  [emo] why because you don't want to i want to
06/13/2022 23:22:12 - INFO - __main__ - ['others']
06/13/2022 23:22:12 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:22:12 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:22:12 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:22:27 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:22:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:22:28 - INFO - __main__ - Starting training!
06/13/2022 23:22:31 - INFO - __main__ - Step 10 Global step 10 Train loss 3.26 on epoch=0
06/13/2022 23:22:33 - INFO - __main__ - Step 20 Global step 20 Train loss 1.91 on epoch=1
06/13/2022 23:22:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=1
06/13/2022 23:22:38 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=2
06/13/2022 23:22:41 - INFO - __main__ - Step 50 Global step 50 Train loss 1.00 on epoch=3
06/13/2022 23:22:44 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.16194174757281554 on epoch=3
06/13/2022 23:22:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16194174757281554 on epoch=3, global_step=50
06/13/2022 23:22:47 - INFO - __main__ - Step 60 Global step 60 Train loss 1.04 on epoch=3
06/13/2022 23:22:49 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=4
06/13/2022 23:22:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
06/13/2022 23:22:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.85 on epoch=5
06/13/2022 23:22:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=6
06/13/2022 23:23:00 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.28656015037593985 on epoch=6
06/13/2022 23:23:00 - INFO - __main__ - Saving model with best Classification-F1: 0.16194174757281554 -> 0.28656015037593985 on epoch=6, global_step=100
06/13/2022 23:23:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.93 on epoch=6
06/13/2022 23:23:05 - INFO - __main__ - Step 120 Global step 120 Train loss 0.82 on epoch=7
06/13/2022 23:23:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=8
06/13/2022 23:23:10 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=8
06/13/2022 23:23:13 - INFO - __main__ - Step 150 Global step 150 Train loss 0.83 on epoch=9
06/13/2022 23:23:16 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.20846770391324848 on epoch=9
06/13/2022 23:23:19 - INFO - __main__ - Step 160 Global step 160 Train loss 0.81 on epoch=9
06/13/2022 23:23:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.74 on epoch=10
06/13/2022 23:23:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.78 on epoch=11
06/13/2022 23:23:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.86 on epoch=11
06/13/2022 23:23:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.81 on epoch=12
06/13/2022 23:23:32 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.3804637992462222 on epoch=12
06/13/2022 23:23:32 - INFO - __main__ - Saving model with best Classification-F1: 0.28656015037593985 -> 0.3804637992462222 on epoch=12, global_step=200
06/13/2022 23:23:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=13
06/13/2022 23:23:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=13
06/13/2022 23:23:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=14
06/13/2022 23:23:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=14
06/13/2022 23:23:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.87 on epoch=15
06/13/2022 23:23:48 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.47963997949040527 on epoch=15
06/13/2022 23:23:48 - INFO - __main__ - Saving model with best Classification-F1: 0.3804637992462222 -> 0.47963997949040527 on epoch=15, global_step=250
06/13/2022 23:23:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=16
06/13/2022 23:23:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.85 on epoch=16
06/13/2022 23:23:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.85 on epoch=17
06/13/2022 23:23:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.80 on epoch=18
06/13/2022 23:24:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=18
06/13/2022 23:24:04 - INFO - __main__ - Global step 300 Train loss 0.80 Classification-F1 0.4967377511500989 on epoch=18
06/13/2022 23:24:04 - INFO - __main__ - Saving model with best Classification-F1: 0.47963997949040527 -> 0.4967377511500989 on epoch=18, global_step=300
06/13/2022 23:24:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=19
06/13/2022 23:24:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=19
06/13/2022 23:24:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.72 on epoch=20
06/13/2022 23:24:14 - INFO - __main__ - Step 340 Global step 340 Train loss 0.71 on epoch=21
06/13/2022 23:24:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.73 on epoch=21
06/13/2022 23:24:20 - INFO - __main__ - Global step 350 Train loss 0.72 Classification-F1 0.3765775133307322 on epoch=21
06/13/2022 23:24:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.76 on epoch=22
06/13/2022 23:24:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=23
06/13/2022 23:24:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=23
06/13/2022 23:24:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=24
06/13/2022 23:24:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.65 on epoch=24
06/13/2022 23:24:36 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.5167464324160935 on epoch=24
06/13/2022 23:24:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4967377511500989 -> 0.5167464324160935 on epoch=24, global_step=400
06/13/2022 23:24:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.71 on epoch=25
06/13/2022 23:24:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=26
06/13/2022 23:24:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=26
06/13/2022 23:24:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=27
06/13/2022 23:24:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.60 on epoch=28
06/13/2022 23:24:52 - INFO - __main__ - Global step 450 Train loss 0.62 Classification-F1 0.5482456140350878 on epoch=28
06/13/2022 23:24:52 - INFO - __main__ - Saving model with best Classification-F1: 0.5167464324160935 -> 0.5482456140350878 on epoch=28, global_step=450
06/13/2022 23:24:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=28
06/13/2022 23:24:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.61 on epoch=29
06/13/2022 23:24:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=29
06/13/2022 23:25:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=30
06/13/2022 23:25:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=31
06/13/2022 23:25:07 - INFO - __main__ - Global step 500 Train loss 0.60 Classification-F1 0.664348463496045 on epoch=31
06/13/2022 23:25:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5482456140350878 -> 0.664348463496045 on epoch=31, global_step=500
06/13/2022 23:25:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.57 on epoch=31
06/13/2022 23:25:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.63 on epoch=32
06/13/2022 23:25:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=33
06/13/2022 23:25:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=33
06/13/2022 23:25:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=34
06/13/2022 23:25:23 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6481286990065525 on epoch=34
06/13/2022 23:25:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=34
06/13/2022 23:25:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.58 on epoch=35
06/13/2022 23:25:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=36
06/13/2022 23:25:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=36
06/13/2022 23:25:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.64 on epoch=37
06/13/2022 23:25:39 - INFO - __main__ - Global step 600 Train loss 0.58 Classification-F1 0.6591426335390317 on epoch=37
06/13/2022 23:25:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=38
06/13/2022 23:25:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=38
06/13/2022 23:25:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.63 on epoch=39
06/13/2022 23:25:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=39
06/13/2022 23:25:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=40
06/13/2022 23:25:55 - INFO - __main__ - Global step 650 Train loss 0.53 Classification-F1 0.6488939525226238 on epoch=40
06/13/2022 23:25:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=41
06/13/2022 23:26:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
06/13/2022 23:26:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.55 on epoch=42
06/13/2022 23:26:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.41 on epoch=43
06/13/2022 23:26:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=43
06/13/2022 23:26:11 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.6829032586007837 on epoch=43
06/13/2022 23:26:11 - INFO - __main__ - Saving model with best Classification-F1: 0.664348463496045 -> 0.6829032586007837 on epoch=43, global_step=700
06/13/2022 23:26:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.58 on epoch=44
06/13/2022 23:26:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=44
06/13/2022 23:26:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=45
06/13/2022 23:26:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=46
06/13/2022 23:26:23 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=46
06/13/2022 23:26:27 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.6733592801609815 on epoch=46
06/13/2022 23:26:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=47
06/13/2022 23:26:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.38 on epoch=48
06/13/2022 23:26:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=48
06/13/2022 23:26:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.49 on epoch=49
06/13/2022 23:26:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=49
06/13/2022 23:26:43 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.6692714184909762 on epoch=49
06/13/2022 23:26:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=50
06/13/2022 23:26:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=51
06/13/2022 23:26:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.40 on epoch=51
06/13/2022 23:26:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=52
06/13/2022 23:26:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.33 on epoch=53
06/13/2022 23:26:59 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.662051744029839 on epoch=53
06/13/2022 23:27:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=53
06/13/2022 23:27:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=54
06/13/2022 23:27:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.33 on epoch=54
06/13/2022 23:27:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=55
06/13/2022 23:27:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=56
06/13/2022 23:27:15 - INFO - __main__ - Global step 900 Train loss 0.41 Classification-F1 0.6414677453985953 on epoch=56
06/13/2022 23:27:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.50 on epoch=56
06/13/2022 23:27:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=57
06/13/2022 23:27:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
06/13/2022 23:27:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=58
06/13/2022 23:27:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=59
06/13/2022 23:27:30 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.6574567833619825 on epoch=59
06/13/2022 23:27:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=59
06/13/2022 23:27:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
06/13/2022 23:27:38 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=61
06/13/2022 23:27:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=61
06/13/2022 23:27:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=62
06/13/2022 23:27:46 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.6603130929791271 on epoch=62
06/13/2022 23:27:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
06/13/2022 23:27:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=63
06/13/2022 23:27:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=64
06/13/2022 23:27:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=64
06/13/2022 23:27:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=65
06/13/2022 23:28:02 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.7066966779121251 on epoch=65
06/13/2022 23:28:02 - INFO - __main__ - Saving model with best Classification-F1: 0.6829032586007837 -> 0.7066966779121251 on epoch=65, global_step=1050
06/13/2022 23:28:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
06/13/2022 23:28:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.33 on epoch=66
06/13/2022 23:28:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=67
06/13/2022 23:28:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.28 on epoch=68
06/13/2022 23:28:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.37 on epoch=68
06/13/2022 23:28:18 - INFO - __main__ - Global step 1100 Train loss 0.34 Classification-F1 0.6584927037443903 on epoch=68
06/13/2022 23:28:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/13/2022 23:28:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=69
06/13/2022 23:28:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/13/2022 23:28:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=71
06/13/2022 23:28:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
06/13/2022 23:28:34 - INFO - __main__ - Global step 1150 Train loss 0.29 Classification-F1 0.7148272972057276 on epoch=71
06/13/2022 23:28:34 - INFO - __main__ - Saving model with best Classification-F1: 0.7066966779121251 -> 0.7148272972057276 on epoch=71, global_step=1150
06/13/2022 23:28:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/13/2022 23:28:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=73
06/13/2022 23:28:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
06/13/2022 23:28:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
06/13/2022 23:28:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.30 on epoch=74
06/13/2022 23:28:50 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.7197397047397047 on epoch=74
06/13/2022 23:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.7148272972057276 -> 0.7197397047397047 on epoch=74, global_step=1200
06/13/2022 23:28:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
06/13/2022 23:28:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=76
06/13/2022 23:28:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
06/13/2022 23:29:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=77
06/13/2022 23:29:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=78
06/13/2022 23:29:05 - INFO - __main__ - Global step 1250 Train loss 0.30 Classification-F1 0.6830150228455314 on epoch=78
06/13/2022 23:29:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.31 on epoch=78
06/13/2022 23:29:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=79
06/13/2022 23:29:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=79
06/13/2022 23:29:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.37 on epoch=80
06/13/2022 23:29:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=81
06/13/2022 23:29:21 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.736049640016627 on epoch=81
06/13/2022 23:29:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7197397047397047 -> 0.736049640016627 on epoch=81, global_step=1300
06/13/2022 23:29:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=81
06/13/2022 23:29:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/13/2022 23:29:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=83
06/13/2022 23:29:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=83
06/13/2022 23:29:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
06/13/2022 23:29:37 - INFO - __main__ - Global step 1350 Train loss 0.31 Classification-F1 0.6891640543364681 on epoch=84
06/13/2022 23:29:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=84
06/13/2022 23:29:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=85
06/13/2022 23:29:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=86
06/13/2022 23:29:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=86
06/13/2022 23:29:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=87
06/13/2022 23:29:53 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.777265867934882 on epoch=87
06/13/2022 23:29:53 - INFO - __main__ - Saving model with best Classification-F1: 0.736049640016627 -> 0.777265867934882 on epoch=87, global_step=1400
06/13/2022 23:29:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=88
06/13/2022 23:29:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=88
06/13/2022 23:30:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=89
06/13/2022 23:30:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
06/13/2022 23:30:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=90
06/13/2022 23:30:09 - INFO - __main__ - Global step 1450 Train loss 0.24 Classification-F1 0.6845239223397577 on epoch=90
06/13/2022 23:30:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
06/13/2022 23:30:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=91
06/13/2022 23:30:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=92
06/13/2022 23:30:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
06/13/2022 23:30:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=93
06/13/2022 23:30:25 - INFO - __main__ - Global step 1500 Train loss 0.26 Classification-F1 0.6866620490297447 on epoch=93
06/13/2022 23:30:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
06/13/2022 23:30:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
06/13/2022 23:30:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=95
06/13/2022 23:30:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=96
06/13/2022 23:30:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.29 on epoch=96
06/13/2022 23:30:40 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.6883232059257919 on epoch=96
06/13/2022 23:30:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.30 on epoch=97
06/13/2022 23:30:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=98
06/13/2022 23:30:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=98
06/13/2022 23:30:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=99
06/13/2022 23:30:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=99
06/13/2022 23:30:56 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.7194742790733315 on epoch=99
06/13/2022 23:30:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=100
06/13/2022 23:31:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=101
06/13/2022 23:31:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
06/13/2022 23:31:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=102
06/13/2022 23:31:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=103
06/13/2022 23:31:12 - INFO - __main__ - Global step 1650 Train loss 0.22 Classification-F1 0.6795226213004324 on epoch=103
06/13/2022 23:31:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=103
06/13/2022 23:31:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
06/13/2022 23:31:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
06/13/2022 23:31:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
06/13/2022 23:31:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=106
06/13/2022 23:31:28 - INFO - __main__ - Global step 1700 Train loss 0.23 Classification-F1 0.7269419306184011 on epoch=106
06/13/2022 23:31:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=106
06/13/2022 23:31:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
06/13/2022 23:31:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=108
06/13/2022 23:31:38 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
06/13/2022 23:31:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=109
06/13/2022 23:31:44 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.7349629389343328 on epoch=109
06/13/2022 23:31:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=109
06/13/2022 23:31:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=110
06/13/2022 23:31:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=111
06/13/2022 23:31:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=111
06/13/2022 23:31:56 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=112
06/13/2022 23:32:00 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.7239152239031934 on epoch=112
06/13/2022 23:32:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=113
06/13/2022 23:32:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=113
06/13/2022 23:32:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=114
06/13/2022 23:32:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.17 on epoch=114
06/13/2022 23:32:12 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
06/13/2022 23:32:15 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.6957036009553995 on epoch=115
06/13/2022 23:32:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=116
06/13/2022 23:32:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
06/13/2022 23:32:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
06/13/2022 23:32:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
06/13/2022 23:32:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
06/13/2022 23:32:31 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.7608161483040374 on epoch=118
06/13/2022 23:32:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=119
06/13/2022 23:32:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=119
06/13/2022 23:32:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.23 on epoch=120
06/13/2022 23:32:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
06/13/2022 23:32:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
06/13/2022 23:32:47 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.7133970314699943 on epoch=121
06/13/2022 23:32:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.23 on epoch=122
06/13/2022 23:32:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=123
06/13/2022 23:32:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=123
06/13/2022 23:32:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.26 on epoch=124
06/13/2022 23:32:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=124
06/13/2022 23:33:03 - INFO - __main__ - Global step 2000 Train loss 0.22 Classification-F1 0.7147542936658615 on epoch=124
06/13/2022 23:33:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
06/13/2022 23:33:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.20 on epoch=126
06/13/2022 23:33:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=126
06/13/2022 23:33:13 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.26 on epoch=127
06/13/2022 23:33:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=128
06/13/2022 23:33:19 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.6929931256217887 on epoch=128
06/13/2022 23:33:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.21 on epoch=128
06/13/2022 23:33:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.26 on epoch=129
06/13/2022 23:33:26 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.18 on epoch=129
06/13/2022 23:33:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=130
06/13/2022 23:33:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=131
06/13/2022 23:33:34 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.7041410706664943 on epoch=131
06/13/2022 23:33:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.20 on epoch=131
06/13/2022 23:33:39 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.25 on epoch=132
06/13/2022 23:33:42 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.12 on epoch=133
06/13/2022 23:33:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
06/13/2022 23:33:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=134
06/13/2022 23:33:50 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.7030460540974351 on epoch=134
06/13/2022 23:33:53 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
06/13/2022 23:33:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=135
06/13/2022 23:33:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=136
06/13/2022 23:34:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
06/13/2022 23:34:03 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
06/13/2022 23:34:06 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7352722632904061 on epoch=137
06/13/2022 23:34:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.18 on epoch=138
06/13/2022 23:34:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
06/13/2022 23:34:14 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=139
06/13/2022 23:34:16 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
06/13/2022 23:34:19 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=140
06/13/2022 23:34:22 - INFO - __main__ - Global step 2250 Train loss 0.17 Classification-F1 0.7304676848308924 on epoch=140
06/13/2022 23:34:25 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.21 on epoch=141
06/13/2022 23:34:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.22 on epoch=141
06/13/2022 23:34:30 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.21 on epoch=142
06/13/2022 23:34:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=143
06/13/2022 23:34:35 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
06/13/2022 23:34:38 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7585526089107868 on epoch=143
06/13/2022 23:34:40 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.19 on epoch=144
06/13/2022 23:34:43 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.16 on epoch=144
06/13/2022 23:34:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.20 on epoch=145
06/13/2022 23:34:48 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=146
06/13/2022 23:34:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=146
06/13/2022 23:34:54 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.7080464278450855 on epoch=146
06/13/2022 23:34:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.19 on epoch=147
06/13/2022 23:34:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=148
06/13/2022 23:35:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.25 on epoch=148
06/13/2022 23:35:04 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=149
06/13/2022 23:35:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
06/13/2022 23:35:10 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.7554297590458942 on epoch=149
06/13/2022 23:35:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
06/13/2022 23:35:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=151
06/13/2022 23:35:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
06/13/2022 23:35:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.25 on epoch=152
06/13/2022 23:35:22 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=153
06/13/2022 23:35:26 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.7374489868467828 on epoch=153
06/13/2022 23:35:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=153
06/13/2022 23:35:31 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=154
06/13/2022 23:35:33 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
06/13/2022 23:35:35 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
06/13/2022 23:35:38 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=156
06/13/2022 23:35:41 - INFO - __main__ - Global step 2500 Train loss 0.14 Classification-F1 0.734300246273849 on epoch=156
06/13/2022 23:35:44 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
06/13/2022 23:35:46 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
06/13/2022 23:35:49 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=158
06/13/2022 23:35:51 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=158
06/13/2022 23:35:54 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.22 on epoch=159
06/13/2022 23:35:57 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.75062641723356 on epoch=159
06/13/2022 23:36:00 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.15 on epoch=159
06/13/2022 23:36:02 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=160
06/13/2022 23:36:05 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.09 on epoch=161
06/13/2022 23:36:07 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
06/13/2022 23:36:10 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.22 on epoch=162
06/13/2022 23:36:13 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7393020499368503 on epoch=162
06/13/2022 23:36:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
06/13/2022 23:36:18 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.13 on epoch=163
06/13/2022 23:36:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=164
06/13/2022 23:36:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=164
06/13/2022 23:36:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.24 on epoch=165
06/13/2022 23:36:29 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.7618339208977214 on epoch=165
06/13/2022 23:36:31 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
06/13/2022 23:36:34 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
06/13/2022 23:36:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=167
06/13/2022 23:36:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
06/13/2022 23:36:41 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.21 on epoch=168
06/13/2022 23:36:45 - INFO - __main__ - Global step 2700 Train loss 0.14 Classification-F1 0.762954849551896 on epoch=168
06/13/2022 23:36:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
06/13/2022 23:36:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
06/13/2022 23:36:52 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
06/13/2022 23:36:55 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
06/13/2022 23:36:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=171
06/13/2022 23:37:01 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.7552813787988989 on epoch=171
06/13/2022 23:37:03 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.31 on epoch=172
06/13/2022 23:37:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
06/13/2022 23:37:08 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=173
06/13/2022 23:37:11 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=174
06/13/2022 23:37:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
06/13/2022 23:37:17 - INFO - __main__ - Global step 2800 Train loss 0.17 Classification-F1 0.7277315445218111 on epoch=174
06/13/2022 23:37:19 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
06/13/2022 23:37:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.13 on epoch=176
06/13/2022 23:37:24 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.22 on epoch=176
06/13/2022 23:37:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.13 on epoch=177
06/13/2022 23:37:29 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
06/13/2022 23:37:32 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.7587034455467793 on epoch=178
06/13/2022 23:37:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=178
06/13/2022 23:37:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.14 on epoch=179
06/13/2022 23:37:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=179
06/13/2022 23:37:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.26 on epoch=180
06/13/2022 23:37:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
06/13/2022 23:37:48 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.7390661801375646 on epoch=181
06/13/2022 23:37:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.15 on epoch=181
06/13/2022 23:37:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.26 on epoch=182
06/13/2022 23:37:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
06/13/2022 23:37:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.11 on epoch=183
06/13/2022 23:38:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.20 on epoch=184
06/13/2022 23:38:04 - INFO - __main__ - Global step 2950 Train loss 0.17 Classification-F1 0.7505907221597489 on epoch=184
06/13/2022 23:38:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.12 on epoch=184
06/13/2022 23:38:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
06/13/2022 23:38:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=186
06/13/2022 23:38:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
06/13/2022 23:38:16 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.13 on epoch=187
06/13/2022 23:38:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:38:18 - INFO - __main__ - Printing 3 examples
06/13/2022 23:38:18 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:38:18 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:38:18 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:38:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:38:18 - INFO - __main__ - Printing 3 examples
06/13/2022 23:38:18 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/13/2022 23:38:18 - INFO - __main__ - ['sad']
06/13/2022 23:38:18 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:38:18 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:38:19 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:38:20 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.7164563443747707 on epoch=187
06/13/2022 23:38:20 - INFO - __main__ - save last model!
06/13/2022 23:38:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 23:38:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 23:38:20 - INFO - __main__ - Printing 3 examples
06/13/2022 23:38:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 23:38:20 - INFO - __main__ - ['others']
06/13/2022 23:38:20 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 23:38:20 - INFO - __main__ - ['others']
06/13/2022 23:38:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 23:38:20 - INFO - __main__ - ['others']
06/13/2022 23:38:20 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:38:22 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:38:27 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 23:38:34 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:38:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:38:34 - INFO - __main__ - Starting training!
06/13/2022 23:39:40 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_13_0.2_8_predictions.txt
06/13/2022 23:39:40 - INFO - __main__ - Classification-F1 on test data: 0.3971
06/13/2022 23:39:40 - INFO - __main__ - prefix=emo_64_13, lr=0.2, bsz=8, dev_performance=0.777265867934882, test_performance=0.39714116358234375
06/13/2022 23:39:40 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.5, bsz=8 ...
06/13/2022 23:39:41 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:39:41 - INFO - __main__ - Printing 3 examples
06/13/2022 23:39:41 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/13/2022 23:39:41 - INFO - __main__ - ['sad']
06/13/2022 23:39:41 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/13/2022 23:39:41 - INFO - __main__ - ['sad']
06/13/2022 23:39:41 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/13/2022 23:39:41 - INFO - __main__ - ['sad']
06/13/2022 23:39:41 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:39:41 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:39:42 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:39:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:39:42 - INFO - __main__ - Printing 3 examples
06/13/2022 23:39:42 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/13/2022 23:39:42 - INFO - __main__ - ['sad']
06/13/2022 23:39:42 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/13/2022 23:39:42 - INFO - __main__ - ['sad']
06/13/2022 23:39:42 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/13/2022 23:39:42 - INFO - __main__ - ['sad']
06/13/2022 23:39:42 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:39:42 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:39:42 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:39:57 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:39:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:39:58 - INFO - __main__ - Starting training!
06/13/2022 23:40:01 - INFO - __main__ - Step 10 Global step 10 Train loss 2.59 on epoch=0
06/13/2022 23:40:04 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=1
06/13/2022 23:40:06 - INFO - __main__ - Step 30 Global step 30 Train loss 1.11 on epoch=1
06/13/2022 23:40:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=2
06/13/2022 23:40:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.96 on epoch=3
06/13/2022 23:40:15 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.31629663891458526 on epoch=3
06/13/2022 23:40:15 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.31629663891458526 on epoch=3, global_step=50
06/13/2022 23:40:17 - INFO - __main__ - Step 60 Global step 60 Train loss 0.89 on epoch=3
06/13/2022 23:40:20 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=4
06/13/2022 23:40:22 - INFO - __main__ - Step 80 Global step 80 Train loss 0.91 on epoch=4
06/13/2022 23:40:25 - INFO - __main__ - Step 90 Global step 90 Train loss 0.84 on epoch=5
06/13/2022 23:40:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=6
06/13/2022 23:40:30 - INFO - __main__ - Global step 100 Train loss 0.89 Classification-F1 0.1581196581196581 on epoch=6
06/13/2022 23:40:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.87 on epoch=6
06/13/2022 23:40:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=7
06/13/2022 23:40:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.84 on epoch=8
06/13/2022 23:40:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=8
06/13/2022 23:40:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.84 on epoch=9
06/13/2022 23:40:46 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.5080230717974347 on epoch=9
06/13/2022 23:40:46 - INFO - __main__ - Saving model with best Classification-F1: 0.31629663891458526 -> 0.5080230717974347 on epoch=9, global_step=150
06/13/2022 23:40:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
06/13/2022 23:40:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.67 on epoch=10
06/13/2022 23:40:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.73 on epoch=11
06/13/2022 23:40:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.74 on epoch=11
06/13/2022 23:40:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.64 on epoch=12
06/13/2022 23:41:02 - INFO - __main__ - Global step 200 Train loss 0.72 Classification-F1 0.4894241494997307 on epoch=12
06/13/2022 23:41:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=13
06/13/2022 23:41:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.59 on epoch=13
06/13/2022 23:41:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.59 on epoch=14
06/13/2022 23:41:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.67 on epoch=14
06/13/2022 23:41:15 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=15
06/13/2022 23:41:18 - INFO - __main__ - Global step 250 Train loss 0.64 Classification-F1 0.5467481659665625 on epoch=15
06/13/2022 23:41:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5080230717974347 -> 0.5467481659665625 on epoch=15, global_step=250
06/13/2022 23:41:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=16
06/13/2022 23:41:23 - INFO - __main__ - Step 270 Global step 270 Train loss 0.65 on epoch=16
06/13/2022 23:41:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=17
06/13/2022 23:41:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=18
06/13/2022 23:41:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=18
06/13/2022 23:41:34 - INFO - __main__ - Global step 300 Train loss 0.58 Classification-F1 0.5927893261226594 on epoch=18
06/13/2022 23:41:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5467481659665625 -> 0.5927893261226594 on epoch=18, global_step=300
06/13/2022 23:41:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.57 on epoch=19
06/13/2022 23:41:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=19
06/13/2022 23:41:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=20
06/13/2022 23:41:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=21
06/13/2022 23:41:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=21
06/13/2022 23:41:50 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.5530632400910838 on epoch=21
06/13/2022 23:41:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=22
06/13/2022 23:41:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=23
06/13/2022 23:41:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=23
06/13/2022 23:42:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=24
06/13/2022 23:42:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=24
06/13/2022 23:42:06 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.5902980118577932 on epoch=24
06/13/2022 23:42:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=25
06/13/2022 23:42:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
06/13/2022 23:42:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=26
06/13/2022 23:42:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=27
06/13/2022 23:42:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=28
06/13/2022 23:42:21 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.673810148035094 on epoch=28
06/13/2022 23:42:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5927893261226594 -> 0.673810148035094 on epoch=28, global_step=450
06/13/2022 23:42:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.45 on epoch=28
06/13/2022 23:42:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.64 on epoch=29
06/13/2022 23:42:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=29
06/13/2022 23:42:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=30
06/13/2022 23:42:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=31
06/13/2022 23:42:37 - INFO - __main__ - Global step 500 Train loss 0.48 Classification-F1 0.6870495288997909 on epoch=31
06/13/2022 23:42:37 - INFO - __main__ - Saving model with best Classification-F1: 0.673810148035094 -> 0.6870495288997909 on epoch=31, global_step=500
06/13/2022 23:42:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=31
06/13/2022 23:42:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=32
06/13/2022 23:42:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=33
06/13/2022 23:42:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=33
06/13/2022 23:42:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=34
06/13/2022 23:42:53 - INFO - __main__ - Global step 550 Train loss 0.39 Classification-F1 0.6792481203007518 on epoch=34
06/13/2022 23:42:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=34
06/13/2022 23:42:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=35
06/13/2022 23:43:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.43 on epoch=36
06/13/2022 23:43:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=36
06/13/2022 23:43:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=37
06/13/2022 23:43:09 - INFO - __main__ - Global step 600 Train loss 0.43 Classification-F1 0.6718785570749543 on epoch=37
06/13/2022 23:43:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.37 on epoch=38
06/13/2022 23:43:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=38
06/13/2022 23:43:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=39
06/13/2022 23:43:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=39
06/13/2022 23:43:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=40
06/13/2022 23:43:25 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.7090674744067696 on epoch=40
06/13/2022 23:43:25 - INFO - __main__ - Saving model with best Classification-F1: 0.6870495288997909 -> 0.7090674744067696 on epoch=40, global_step=650
06/13/2022 23:43:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=41
06/13/2022 23:43:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/13/2022 23:43:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/13/2022 23:43:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
06/13/2022 23:43:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=43
06/13/2022 23:43:40 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.6468156758236663 on epoch=43
06/13/2022 23:43:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=44
06/13/2022 23:43:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.35 on epoch=44
06/13/2022 23:43:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=45
06/13/2022 23:43:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.30 on epoch=46
06/13/2022 23:43:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.58 on epoch=46
06/13/2022 23:43:56 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.6959931923454826 on epoch=46
06/13/2022 23:43:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
06/13/2022 23:44:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=48
06/13/2022 23:44:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=48
06/13/2022 23:44:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=49
06/13/2022 23:44:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=49
06/13/2022 23:44:12 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.6631443139869379 on epoch=49
06/13/2022 23:44:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=50
06/13/2022 23:44:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=51
06/13/2022 23:44:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/13/2022 23:44:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=52
06/13/2022 23:44:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=53
06/13/2022 23:44:28 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.7089864650840261 on epoch=53
06/13/2022 23:44:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/13/2022 23:44:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=54
06/13/2022 23:44:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=54
06/13/2022 23:44:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=55
06/13/2022 23:44:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
06/13/2022 23:44:44 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.6958040619344312 on epoch=56
06/13/2022 23:44:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=56
06/13/2022 23:44:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.29 on epoch=57
06/13/2022 23:44:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
06/13/2022 23:44:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.32 on epoch=58
06/13/2022 23:44:56 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=59
06/13/2022 23:45:00 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.6746731395260261 on epoch=59
06/13/2022 23:45:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
06/13/2022 23:45:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=60
06/13/2022 23:45:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
06/13/2022 23:45:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=61
06/13/2022 23:45:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=62
06/13/2022 23:45:15 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.6903235088084941 on epoch=62
06/13/2022 23:45:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
06/13/2022 23:45:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=63
06/13/2022 23:45:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=64
06/13/2022 23:45:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=64
06/13/2022 23:45:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
06/13/2022 23:45:31 - INFO - __main__ - Global step 1050 Train loss 0.32 Classification-F1 0.6829692795674824 on epoch=65
06/13/2022 23:45:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=66
06/13/2022 23:45:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=66
06/13/2022 23:45:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=67
06/13/2022 23:45:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=68
06/13/2022 23:45:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=68
06/13/2022 23:45:47 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.7034419936334476 on epoch=68
06/13/2022 23:45:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=69
06/13/2022 23:45:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=69
06/13/2022 23:45:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=70
06/13/2022 23:45:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=71
06/13/2022 23:46:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=71
06/13/2022 23:46:03 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.6868124477781595 on epoch=71
06/13/2022 23:46:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=72
06/13/2022 23:46:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=73
06/13/2022 23:46:11 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
06/13/2022 23:46:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=74
06/13/2022 23:46:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=74
06/13/2022 23:46:19 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.6361491724323604 on epoch=74
06/13/2022 23:46:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
06/13/2022 23:46:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=76
06/13/2022 23:46:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=76
06/13/2022 23:46:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=77
06/13/2022 23:46:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=78
06/13/2022 23:46:35 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.6884733391995236 on epoch=78
06/13/2022 23:46:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=78
06/13/2022 23:46:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
06/13/2022 23:46:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=79
06/13/2022 23:46:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
06/13/2022 23:46:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=81
06/13/2022 23:46:51 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.6458028001401435 on epoch=81
06/13/2022 23:46:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=81
06/13/2022 23:46:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=82
06/13/2022 23:46:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=83
06/13/2022 23:47:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
06/13/2022 23:47:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=84
06/13/2022 23:47:07 - INFO - __main__ - Global step 1350 Train loss 0.23 Classification-F1 0.609648817131052 on epoch=84
06/13/2022 23:47:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=84
06/13/2022 23:47:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=85
06/13/2022 23:47:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=86
06/13/2022 23:47:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=86
06/13/2022 23:47:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
06/13/2022 23:47:23 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.6419032305275104 on epoch=87
06/13/2022 23:47:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=88
06/13/2022 23:47:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=88
06/13/2022 23:47:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=89
06/13/2022 23:47:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=89
06/13/2022 23:47:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=90
06/13/2022 23:47:39 - INFO - __main__ - Global step 1450 Train loss 0.25 Classification-F1 0.6961526251526251 on epoch=90
06/13/2022 23:47:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
06/13/2022 23:47:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=91
06/13/2022 23:47:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
06/13/2022 23:47:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=93
06/13/2022 23:47:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=93
06/13/2022 23:47:54 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.6830967235379 on epoch=93
06/13/2022 23:47:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=94
06/13/2022 23:47:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=94
06/13/2022 23:48:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=95
06/13/2022 23:48:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=96
06/13/2022 23:48:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=96
06/13/2022 23:48:10 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.6404633426275217 on epoch=96
06/13/2022 23:48:13 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
06/13/2022 23:48:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
06/13/2022 23:48:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=98
06/13/2022 23:48:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=99
06/13/2022 23:48:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
06/13/2022 23:48:26 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.6539353212910797 on epoch=99
06/13/2022 23:48:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=100
06/13/2022 23:48:31 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=101
06/13/2022 23:48:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
06/13/2022 23:48:36 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
06/13/2022 23:48:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=103
06/13/2022 23:48:42 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.7398740319266635 on epoch=103
06/13/2022 23:48:42 - INFO - __main__ - Saving model with best Classification-F1: 0.7090674744067696 -> 0.7398740319266635 on epoch=103, global_step=1650
06/13/2022 23:48:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=103
06/13/2022 23:48:47 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=104
06/13/2022 23:48:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=104
06/13/2022 23:48:52 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
06/13/2022 23:48:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=106
06/13/2022 23:48:58 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.6853003889802655 on epoch=106
06/13/2022 23:49:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=106
06/13/2022 23:49:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=107
06/13/2022 23:49:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.29 on epoch=108
06/13/2022 23:49:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
06/13/2022 23:49:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=109
06/13/2022 23:49:14 - INFO - __main__ - Global step 1750 Train loss 0.19 Classification-F1 0.6334418623836391 on epoch=109
06/13/2022 23:49:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
06/13/2022 23:49:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.24 on epoch=110
06/13/2022 23:49:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=111
06/13/2022 23:49:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.15 on epoch=111
06/13/2022 23:49:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=112
06/13/2022 23:49:30 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.7059240811019748 on epoch=112
06/13/2022 23:49:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=113
06/13/2022 23:49:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=113
06/13/2022 23:49:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
06/13/2022 23:49:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.26 on epoch=114
06/13/2022 23:49:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=115
06/13/2022 23:49:45 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.6674267344497608 on epoch=115
06/13/2022 23:49:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
06/13/2022 23:49:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=116
06/13/2022 23:49:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=117
06/13/2022 23:49:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=118
06/13/2022 23:49:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=118
06/13/2022 23:50:01 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.6942844371776247 on epoch=118
06/13/2022 23:50:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=119
06/13/2022 23:50:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=119
06/13/2022 23:50:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
06/13/2022 23:50:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
06/13/2022 23:50:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=121
06/13/2022 23:50:17 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.6565097833848659 on epoch=121
06/13/2022 23:50:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=122
06/13/2022 23:50:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=123
06/13/2022 23:50:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=123
06/13/2022 23:50:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=124
06/13/2022 23:50:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=124
06/13/2022 23:50:33 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.6418139614787471 on epoch=124
06/13/2022 23:50:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
06/13/2022 23:50:38 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.15 on epoch=126
06/13/2022 23:50:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.19 on epoch=126
06/13/2022 23:50:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=127
06/13/2022 23:50:45 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=128
06/13/2022 23:50:49 - INFO - __main__ - Global step 2050 Train loss 0.16 Classification-F1 0.6749800022277319 on epoch=128
06/13/2022 23:50:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
06/13/2022 23:50:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=129
06/13/2022 23:50:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=129
06/13/2022 23:50:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=130
06/13/2022 23:51:01 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=131
06/13/2022 23:51:05 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.6190823464530362 on epoch=131
06/13/2022 23:51:07 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.18 on epoch=131
06/13/2022 23:51:10 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=132
06/13/2022 23:51:12 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=133
06/13/2022 23:51:15 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
06/13/2022 23:51:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=134
06/13/2022 23:51:21 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.6245161992016727 on epoch=134
06/13/2022 23:51:23 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
06/13/2022 23:51:26 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=135
06/13/2022 23:51:28 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=136
06/13/2022 23:51:31 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=136
06/13/2022 23:51:33 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.07 on epoch=137
06/13/2022 23:51:37 - INFO - __main__ - Global step 2200 Train loss 0.14 Classification-F1 0.6988966129753333 on epoch=137
06/13/2022 23:51:39 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.16 on epoch=138
06/13/2022 23:51:42 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.19 on epoch=138
06/13/2022 23:51:44 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=139
06/13/2022 23:51:47 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=139
06/13/2022 23:51:49 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=140
06/13/2022 23:51:52 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.6613328731245703 on epoch=140
06/13/2022 23:51:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
06/13/2022 23:51:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
06/13/2022 23:52:00 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
06/13/2022 23:52:02 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=143
06/13/2022 23:52:05 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.15 on epoch=143
06/13/2022 23:52:08 - INFO - __main__ - Global step 2300 Train loss 0.16 Classification-F1 0.6414652851210434 on epoch=143
06/13/2022 23:52:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.11 on epoch=144
06/13/2022 23:52:13 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=144
06/13/2022 23:52:16 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=145
06/13/2022 23:52:18 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=146
06/13/2022 23:52:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.19 on epoch=146
06/13/2022 23:52:24 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.6491994851382541 on epoch=146
06/13/2022 23:52:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
06/13/2022 23:52:29 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.10 on epoch=148
06/13/2022 23:52:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=148
06/13/2022 23:52:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=149
06/13/2022 23:52:36 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.17 on epoch=149
06/13/2022 23:52:40 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.6319625263898663 on epoch=149
06/13/2022 23:52:42 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=150
06/13/2022 23:52:45 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=151
06/13/2022 23:52:47 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=151
06/13/2022 23:52:50 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=152
06/13/2022 23:52:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.06 on epoch=153
06/13/2022 23:52:56 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.6823933430910175 on epoch=153
06/13/2022 23:52:58 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.12 on epoch=153
06/13/2022 23:53:01 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=154
06/13/2022 23:53:03 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=154
06/13/2022 23:53:06 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=155
06/13/2022 23:53:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
06/13/2022 23:53:12 - INFO - __main__ - Global step 2500 Train loss 0.11 Classification-F1 0.6864317957652208 on epoch=156
06/13/2022 23:53:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
06/13/2022 23:53:17 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=157
06/13/2022 23:53:19 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
06/13/2022 23:53:22 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.12 on epoch=158
06/13/2022 23:53:24 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=159
06/13/2022 23:53:28 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.7128032550787631 on epoch=159
06/13/2022 23:53:30 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
06/13/2022 23:53:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
06/13/2022 23:53:35 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
06/13/2022 23:53:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.17 on epoch=161
06/13/2022 23:53:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
06/13/2022 23:53:44 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.6977776109861542 on epoch=162
06/13/2022 23:53:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.10 on epoch=163
06/13/2022 23:53:49 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
06/13/2022 23:53:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
06/13/2022 23:53:53 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
06/13/2022 23:53:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=165
06/13/2022 23:53:59 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.6785863936798702 on epoch=165
06/13/2022 23:54:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=166
06/13/2022 23:54:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.07 on epoch=166
06/13/2022 23:54:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
06/13/2022 23:54:09 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=168
06/13/2022 23:54:12 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
06/13/2022 23:54:15 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.6804995365263222 on epoch=168
06/13/2022 23:54:18 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=169
06/13/2022 23:54:20 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=169
06/13/2022 23:54:23 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=170
06/13/2022 23:54:25 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=171
06/13/2022 23:54:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
06/13/2022 23:54:31 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.6433954856511106 on epoch=171
06/13/2022 23:54:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.08 on epoch=172
06/13/2022 23:54:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=173
06/13/2022 23:54:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=173
06/13/2022 23:54:41 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.09 on epoch=174
06/13/2022 23:54:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=174
06/13/2022 23:54:47 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.6640539359856831 on epoch=174
06/13/2022 23:54:49 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
06/13/2022 23:54:52 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
06/13/2022 23:54:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.08 on epoch=176
06/13/2022 23:54:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.07 on epoch=177
06/13/2022 23:54:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=178
06/13/2022 23:55:03 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.6561660933446841 on epoch=178
06/13/2022 23:55:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=178
06/13/2022 23:55:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
06/13/2022 23:55:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=179
06/13/2022 23:55:13 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=180
06/13/2022 23:55:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=181
06/13/2022 23:55:19 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.6541559144235418 on epoch=181
06/13/2022 23:55:21 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=181
06/13/2022 23:55:24 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.10 on epoch=182
06/13/2022 23:55:26 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
06/13/2022 23:55:29 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.07 on epoch=183
06/13/2022 23:55:31 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.10 on epoch=184
06/13/2022 23:55:35 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.656588778938725 on epoch=184
06/13/2022 23:55:37 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=184
06/13/2022 23:55:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
06/13/2022 23:55:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
06/13/2022 23:55:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.14 on epoch=186
06/13/2022 23:55:47 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
06/13/2022 23:55:48 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:55:48 - INFO - __main__ - Printing 3 examples
06/13/2022 23:55:48 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/13/2022 23:55:48 - INFO - __main__ - ['sad']
06/13/2022 23:55:48 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/13/2022 23:55:48 - INFO - __main__ - ['sad']
06/13/2022 23:55:48 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/13/2022 23:55:48 - INFO - __main__ - ['sad']
06/13/2022 23:55:48 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:55:49 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:55:49 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:55:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:55:49 - INFO - __main__ - Printing 3 examples
06/13/2022 23:55:49 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/13/2022 23:55:49 - INFO - __main__ - ['sad']
06/13/2022 23:55:49 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/13/2022 23:55:49 - INFO - __main__ - ['sad']
06/13/2022 23:55:49 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/13/2022 23:55:49 - INFO - __main__ - ['sad']
06/13/2022 23:55:49 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:55:49 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:55:49 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:55:50 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.6963981957012038 on epoch=187
06/13/2022 23:55:50 - INFO - __main__ - save last model!
06/13/2022 23:55:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/13/2022 23:55:51 - INFO - __main__ - Start tokenizing ... 5509 instances
06/13/2022 23:55:51 - INFO - __main__ - Printing 3 examples
06/13/2022 23:55:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/13/2022 23:55:51 - INFO - __main__ - ['others']
06/13/2022 23:55:51 - INFO - __main__ -  [emo] what you like very little things ok
06/13/2022 23:55:51 - INFO - __main__ - ['others']
06/13/2022 23:55:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/13/2022 23:55:51 - INFO - __main__ - ['others']
06/13/2022 23:55:51 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:55:53 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:55:58 - INFO - __main__ - Loaded 5509 examples from test data
06/13/2022 23:56:04 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:56:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:56:05 - INFO - __main__ - Starting training!
06/13/2022 23:57:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_21_0.5_8_predictions.txt
06/13/2022 23:57:11 - INFO - __main__ - Classification-F1 on test data: 0.3061
06/13/2022 23:57:11 - INFO - __main__ - prefix=emo_64_21, lr=0.5, bsz=8, dev_performance=0.7398740319266635, test_performance=0.3060669054997072
06/13/2022 23:57:11 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.4, bsz=8 ...
06/13/2022 23:57:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:57:12 - INFO - __main__ - Printing 3 examples
06/13/2022 23:57:12 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:57:12 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:57:12 - INFO - __main__ - Loaded 256 examples from train data
06/13/2022 23:57:12 - INFO - __main__ - Start tokenizing ... 256 instances
06/13/2022 23:57:12 - INFO - __main__ - Printing 3 examples
06/13/2022 23:57:12 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/13/2022 23:57:12 - INFO - __main__ - ['sad']
06/13/2022 23:57:12 - INFO - __main__ - Tokenizing Input ...
06/13/2022 23:57:13 - INFO - __main__ - Tokenizing Output ...
06/13/2022 23:57:13 - INFO - __main__ - Loaded 256 examples from dev data
06/13/2022 23:57:28 - INFO - __main__ - load prompt embedding from ckpt
06/13/2022 23:57:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/13/2022 23:57:29 - INFO - __main__ - Starting training!
06/13/2022 23:57:32 - INFO - __main__ - Step 10 Global step 10 Train loss 2.72 on epoch=0
06/13/2022 23:57:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.42 on epoch=1
06/13/2022 23:57:37 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=1
06/13/2022 23:57:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.90 on epoch=2
06/13/2022 23:57:42 - INFO - __main__ - Step 50 Global step 50 Train loss 0.99 on epoch=3
06/13/2022 23:57:45 - INFO - __main__ - Global step 50 Train loss 1.42 Classification-F1 0.30727650457183286 on epoch=3
06/13/2022 23:57:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.30727650457183286 on epoch=3, global_step=50
06/13/2022 23:57:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=3
06/13/2022 23:57:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=4
06/13/2022 23:57:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
06/13/2022 23:57:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=5
06/13/2022 23:58:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.88 on epoch=6
06/13/2022 23:58:03 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.25516595634766887 on epoch=6
06/13/2022 23:58:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=6
06/13/2022 23:58:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.72 on epoch=7
06/13/2022 23:58:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=8
06/13/2022 23:58:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.86 on epoch=8
06/13/2022 23:58:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.77 on epoch=9
06/13/2022 23:58:19 - INFO - __main__ - Global step 150 Train loss 0.81 Classification-F1 0.37372048467805374 on epoch=9
06/13/2022 23:58:19 - INFO - __main__ - Saving model with best Classification-F1: 0.30727650457183286 -> 0.37372048467805374 on epoch=9, global_step=150
06/13/2022 23:58:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.65 on epoch=9
06/13/2022 23:58:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.70 on epoch=10
06/13/2022 23:58:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.69 on epoch=11
06/13/2022 23:58:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.67 on epoch=11
06/13/2022 23:58:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=12
06/13/2022 23:58:35 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.5334370574886081 on epoch=12
06/13/2022 23:58:35 - INFO - __main__ - Saving model with best Classification-F1: 0.37372048467805374 -> 0.5334370574886081 on epoch=12, global_step=200
06/13/2022 23:58:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=13
06/13/2022 23:58:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.67 on epoch=13
06/13/2022 23:58:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=14
06/13/2022 23:58:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=14
06/13/2022 23:58:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.58 on epoch=15
06/13/2022 23:58:51 - INFO - __main__ - Global step 250 Train loss 0.65 Classification-F1 0.43542833078746224 on epoch=15
06/13/2022 23:58:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.58 on epoch=16
06/13/2022 23:58:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=16
06/13/2022 23:58:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=17
06/13/2022 23:59:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=18
06/13/2022 23:59:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.52 on epoch=18
06/13/2022 23:59:07 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.6248363761470782 on epoch=18
06/13/2022 23:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5334370574886081 -> 0.6248363761470782 on epoch=18, global_step=300
06/13/2022 23:59:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.53 on epoch=19
06/13/2022 23:59:11 - INFO - __main__ - Step 320 Global step 320 Train loss 0.63 on epoch=19
06/13/2022 23:59:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.55 on epoch=20
06/13/2022 23:59:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=21
06/13/2022 23:59:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.57 on epoch=21
06/13/2022 23:59:22 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.5328726509760993 on epoch=21
06/13/2022 23:59:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=22
06/13/2022 23:59:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=23
06/13/2022 23:59:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=23
06/13/2022 23:59:32 - INFO - __main__ - Step 390 Global step 390 Train loss 0.39 on epoch=24
06/13/2022 23:59:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=24
06/13/2022 23:59:38 - INFO - __main__ - Global step 400 Train loss 0.48 Classification-F1 0.5585093322599101 on epoch=24
06/13/2022 23:59:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=25
06/13/2022 23:59:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=26
06/13/2022 23:59:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=26
06/13/2022 23:59:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=27
06/13/2022 23:59:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=28
06/13/2022 23:59:54 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.7153799292522052 on epoch=28
06/13/2022 23:59:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6248363761470782 -> 0.7153799292522052 on epoch=28, global_step=450
06/13/2022 23:59:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=28
06/13/2022 23:59:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.41 on epoch=29
06/14/2022 00:00:01 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=29
06/14/2022 00:00:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=30
06/14/2022 00:00:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=31
06/14/2022 00:00:10 - INFO - __main__ - Global step 500 Train loss 0.42 Classification-F1 0.6125914042815451 on epoch=31
06/14/2022 00:00:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=31
06/14/2022 00:00:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=32
06/14/2022 00:00:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=33
06/14/2022 00:00:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=33
06/14/2022 00:00:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=34
06/14/2022 00:00:26 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.686080961631361 on epoch=34
06/14/2022 00:00:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=34
06/14/2022 00:00:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=35
06/14/2022 00:00:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=36
06/14/2022 00:00:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=36
06/14/2022 00:00:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=37
06/14/2022 00:00:41 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.7047189177808191 on epoch=37
06/14/2022 00:00:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=38
06/14/2022 00:00:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=38
06/14/2022 00:00:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=39
06/14/2022 00:00:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=39
06/14/2022 00:00:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
06/14/2022 00:00:57 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.6707625405035673 on epoch=40
06/14/2022 00:01:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=41
06/14/2022 00:01:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=41
06/14/2022 00:01:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=42
06/14/2022 00:01:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=43
06/14/2022 00:01:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=43
06/14/2022 00:01:13 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.7091232561628158 on epoch=43
06/14/2022 00:01:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.32 on epoch=44
06/14/2022 00:01:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=44
06/14/2022 00:01:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
06/14/2022 00:01:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=46
06/14/2022 00:01:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=46
06/14/2022 00:01:29 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.7221699430051975 on epoch=46
06/14/2022 00:01:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7153799292522052 -> 0.7221699430051975 on epoch=46, global_step=750
06/14/2022 00:01:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=47
06/14/2022 00:01:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=48
06/14/2022 00:01:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=48
06/14/2022 00:01:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=49
06/14/2022 00:01:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/14/2022 00:01:45 - INFO - __main__ - Global step 800 Train loss 0.31 Classification-F1 0.6898112771089346 on epoch=49
06/14/2022 00:01:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=50
06/14/2022 00:01:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=51
06/14/2022 00:01:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.37 on epoch=51
06/14/2022 00:01:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=52
06/14/2022 00:01:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=53
06/14/2022 00:02:01 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.6799678687268951 on epoch=53
06/14/2022 00:02:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.36 on epoch=53
06/14/2022 00:02:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=54
06/14/2022 00:02:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=54
06/14/2022 00:02:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=55
06/14/2022 00:02:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.27 on epoch=56
06/14/2022 00:02:17 - INFO - __main__ - Global step 900 Train loss 0.29 Classification-F1 0.7188834227787717 on epoch=56
06/14/2022 00:02:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=56
06/14/2022 00:02:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=57
06/14/2022 00:02:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=58
06/14/2022 00:02:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.37 on epoch=58
06/14/2022 00:02:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=59
06/14/2022 00:02:32 - INFO - __main__ - Global step 950 Train loss 0.28 Classification-F1 0.6755086163301824 on epoch=59
06/14/2022 00:02:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.39 on epoch=59
06/14/2022 00:02:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
06/14/2022 00:02:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=61
06/14/2022 00:02:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=61
06/14/2022 00:02:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=62
06/14/2022 00:02:48 - INFO - __main__ - Global step 1000 Train loss 0.29 Classification-F1 0.6826957957736619 on epoch=62
06/14/2022 00:02:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=63
06/14/2022 00:02:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=63
06/14/2022 00:02:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
06/14/2022 00:02:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=64
06/14/2022 00:03:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
06/14/2022 00:03:04 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.694469696969697 on epoch=65
06/14/2022 00:03:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=66
06/14/2022 00:03:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=66
06/14/2022 00:03:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=67
06/14/2022 00:03:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=68
06/14/2022 00:03:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=68
06/14/2022 00:03:20 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.7046178797764241 on epoch=68
06/14/2022 00:03:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=69
06/14/2022 00:03:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=69
06/14/2022 00:03:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
06/14/2022 00:03:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=71
06/14/2022 00:03:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=71
06/14/2022 00:03:35 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.6629221103733848 on epoch=71
06/14/2022 00:03:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=72
06/14/2022 00:03:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=73
06/14/2022 00:03:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
06/14/2022 00:03:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=74
06/14/2022 00:03:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=74
06/14/2022 00:03:51 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.6644157887106562 on epoch=74
06/14/2022 00:03:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=75
06/14/2022 00:03:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
06/14/2022 00:03:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=76
06/14/2022 00:04:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=77
06/14/2022 00:04:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=78
06/14/2022 00:04:07 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.7014370437956204 on epoch=78
06/14/2022 00:04:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=78
06/14/2022 00:04:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=79
06/14/2022 00:04:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.36 on epoch=79
06/14/2022 00:04:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=80
06/14/2022 00:04:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=81
06/14/2022 00:04:23 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.6773713759007877 on epoch=81
06/14/2022 00:04:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=81
06/14/2022 00:04:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=82
06/14/2022 00:04:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=83
06/14/2022 00:04:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=83
06/14/2022 00:04:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=84
06/14/2022 00:04:39 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.6602740657130381 on epoch=84
06/14/2022 00:04:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
06/14/2022 00:04:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=85
06/14/2022 00:04:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=86
06/14/2022 00:04:49 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=86
06/14/2022 00:04:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=87
06/14/2022 00:04:55 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.7092134649433792 on epoch=87
06/14/2022 00:04:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=88
06/14/2022 00:05:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=88
06/14/2022 00:05:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=89
06/14/2022 00:05:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=89
06/14/2022 00:05:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=90
06/14/2022 00:05:10 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.6848172242715937 on epoch=90
06/14/2022 00:05:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=91
06/14/2022 00:05:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=91
06/14/2022 00:05:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=92
06/14/2022 00:05:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=93
06/14/2022 00:05:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=93
06/14/2022 00:05:26 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.6924844490187018 on epoch=93
06/14/2022 00:05:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=94
06/14/2022 00:05:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=94
06/14/2022 00:05:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=95
06/14/2022 00:05:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=96
06/14/2022 00:05:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=96
06/14/2022 00:05:42 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.6456710496265905 on epoch=96
06/14/2022 00:05:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
06/14/2022 00:05:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=98
06/14/2022 00:05:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=98
06/14/2022 00:05:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=99
06/14/2022 00:05:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=99
06/14/2022 00:05:58 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.6319624540722597 on epoch=99
06/14/2022 00:06:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=100
06/14/2022 00:06:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=101
06/14/2022 00:06:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=101
06/14/2022 00:06:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=102
06/14/2022 00:06:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=103
06/14/2022 00:06:14 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.6653440873600028 on epoch=103
06/14/2022 00:06:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=103
06/14/2022 00:06:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=104
06/14/2022 00:06:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
06/14/2022 00:06:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
06/14/2022 00:06:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=106
06/14/2022 00:06:30 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.6561972778266096 on epoch=106
06/14/2022 00:06:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=106
06/14/2022 00:06:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
06/14/2022 00:06:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=108
06/14/2022 00:06:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=108
06/14/2022 00:06:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=109
06/14/2022 00:06:45 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.6325861978692168 on epoch=109
06/14/2022 00:06:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=109
06/14/2022 00:06:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
06/14/2022 00:06:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=111
06/14/2022 00:06:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=111
06/14/2022 00:06:58 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=112
06/14/2022 00:07:01 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.6945462744755753 on epoch=112
06/14/2022 00:07:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=113
06/14/2022 00:07:06 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=113
06/14/2022 00:07:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=114
06/14/2022 00:07:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=114
06/14/2022 00:07:14 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=115
06/14/2022 00:07:17 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.6906153990749214 on epoch=115
06/14/2022 00:07:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=116
06/14/2022 00:07:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=116
06/14/2022 00:07:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=117
06/14/2022 00:07:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=118
06/14/2022 00:07:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=118
06/14/2022 00:07:33 - INFO - __main__ - Global step 1900 Train loss 0.13 Classification-F1 0.6811419408621343 on epoch=118
06/14/2022 00:07:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=119
06/14/2022 00:07:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=119
06/14/2022 00:07:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=120
06/14/2022 00:07:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=121
06/14/2022 00:07:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=121
06/14/2022 00:07:49 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.6751944166264852 on epoch=121
06/14/2022 00:07:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
06/14/2022 00:07:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
06/14/2022 00:07:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
06/14/2022 00:07:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=124
06/14/2022 00:08:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=124
06/14/2022 00:08:05 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.6473084635027018 on epoch=124
06/14/2022 00:08:08 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=125
06/14/2022 00:08:10 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
06/14/2022 00:08:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.06 on epoch=126
06/14/2022 00:08:15 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=127
06/14/2022 00:08:17 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=128
06/14/2022 00:08:21 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.6622022397116798 on epoch=128
06/14/2022 00:08:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=128
06/14/2022 00:08:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=129
06/14/2022 00:08:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.11 on epoch=129
06/14/2022 00:08:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=130
06/14/2022 00:08:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=131
06/14/2022 00:08:37 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.6887160017243127 on epoch=131
06/14/2022 00:08:39 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.17 on epoch=131
06/14/2022 00:08:42 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
06/14/2022 00:08:44 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=133
06/14/2022 00:08:47 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=133
06/14/2022 00:08:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=134
06/14/2022 00:08:53 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.7042347872447422 on epoch=134
06/14/2022 00:08:55 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=134
06/14/2022 00:08:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.22 on epoch=135
06/14/2022 00:09:00 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=136
06/14/2022 00:09:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
06/14/2022 00:09:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=137
06/14/2022 00:09:09 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.672330593071051 on epoch=137
06/14/2022 00:09:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.13 on epoch=138
06/14/2022 00:09:14 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.14 on epoch=138
06/14/2022 00:09:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=139
06/14/2022 00:09:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
06/14/2022 00:09:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=140
06/14/2022 00:09:25 - INFO - __main__ - Global step 2250 Train loss 0.12 Classification-F1 0.6474079743482082 on epoch=140
06/14/2022 00:09:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=141
06/14/2022 00:09:30 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=141
06/14/2022 00:09:33 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=142
06/14/2022 00:09:35 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=143
06/14/2022 00:09:38 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=143
06/14/2022 00:09:41 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.6741246328835984 on epoch=143
06/14/2022 00:09:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=144
06/14/2022 00:09:46 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=144
06/14/2022 00:09:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=145
06/14/2022 00:09:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
06/14/2022 00:09:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.04 on epoch=146
06/14/2022 00:09:57 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.6582363143312973 on epoch=146
06/14/2022 00:09:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=147
06/14/2022 00:10:02 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.23 on epoch=148
06/14/2022 00:10:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
06/14/2022 00:10:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
06/14/2022 00:10:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=149
06/14/2022 00:10:12 - INFO - __main__ - Global step 2400 Train loss 0.13 Classification-F1 0.6675718294703633 on epoch=149
06/14/2022 00:10:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
06/14/2022 00:10:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
06/14/2022 00:10:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
06/14/2022 00:10:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=152
06/14/2022 00:10:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=153
06/14/2022 00:10:28 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.6747411606610606 on epoch=153
06/14/2022 00:10:31 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=153
06/14/2022 00:10:33 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
06/14/2022 00:10:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=154
06/14/2022 00:10:38 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=155
06/14/2022 00:10:41 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
06/14/2022 00:10:44 - INFO - __main__ - Global step 2500 Train loss 0.09 Classification-F1 0.6776361670422588 on epoch=156
06/14/2022 00:10:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=156
06/14/2022 00:10:49 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=157
06/14/2022 00:10:52 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.13 on epoch=158
06/14/2022 00:10:54 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=158
06/14/2022 00:10:57 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=159
06/14/2022 00:11:00 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.690327745739861 on epoch=159
06/14/2022 00:11:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
06/14/2022 00:11:05 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=160
06/14/2022 00:11:07 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.15 on epoch=161
06/14/2022 00:11:10 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
06/14/2022 00:11:12 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=162
06/14/2022 00:11:16 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.682536995535902 on epoch=162
06/14/2022 00:11:18 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=163
06/14/2022 00:11:21 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
06/14/2022 00:11:23 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
06/14/2022 00:11:26 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=164
06/14/2022 00:11:28 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=165
06/14/2022 00:11:32 - INFO - __main__ - Global step 2650 Train loss 0.09 Classification-F1 0.7058619793256286 on epoch=165
06/14/2022 00:11:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=166
06/14/2022 00:11:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/14/2022 00:11:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=167
06/14/2022 00:11:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=168
06/14/2022 00:11:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=168
06/14/2022 00:11:47 - INFO - __main__ - Global step 2700 Train loss 0.07 Classification-F1 0.6953597725090525 on epoch=168
06/14/2022 00:11:50 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=169
06/14/2022 00:11:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=169
06/14/2022 00:11:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=170
06/14/2022 00:11:57 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=171
06/14/2022 00:12:00 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=171
06/14/2022 00:12:03 - INFO - __main__ - Global step 2750 Train loss 0.06 Classification-F1 0.6752046861753732 on epoch=171
06/14/2022 00:12:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
06/14/2022 00:12:08 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=173
06/14/2022 00:12:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=173
06/14/2022 00:12:13 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
06/14/2022 00:12:16 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=174
06/14/2022 00:12:19 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.739205706735404 on epoch=174
06/14/2022 00:12:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7221699430051975 -> 0.739205706735404 on epoch=174, global_step=2800
06/14/2022 00:12:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=175
06/14/2022 00:12:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
06/14/2022 00:12:27 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
06/14/2022 00:12:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.09 on epoch=177
06/14/2022 00:12:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=178
06/14/2022 00:12:35 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.6804842112687955 on epoch=178
06/14/2022 00:12:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=178
06/14/2022 00:12:40 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
06/14/2022 00:12:43 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=179
06/14/2022 00:12:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=180
06/14/2022 00:12:48 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
06/14/2022 00:12:51 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.6853917460111714 on epoch=181
06/14/2022 00:12:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/14/2022 00:12:56 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=182
06/14/2022 00:12:58 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=183
06/14/2022 00:13:01 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
06/14/2022 00:13:03 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
06/14/2022 00:13:07 - INFO - __main__ - Global step 2950 Train loss 0.06 Classification-F1 0.6526516074489886 on epoch=184
06/14/2022 00:13:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
06/14/2022 00:13:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.16 on epoch=185
06/14/2022 00:13:14 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
06/14/2022 00:13:17 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
06/14/2022 00:13:19 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
06/14/2022 00:13:21 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:13:21 - INFO - __main__ - Printing 3 examples
06/14/2022 00:13:21 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:13:21 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:13:21 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:13:21 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:13:21 - INFO - __main__ - Printing 3 examples
06/14/2022 00:13:21 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/14/2022 00:13:21 - INFO - __main__ - ['sad']
06/14/2022 00:13:21 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:13:21 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:13:21 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:13:23 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.6515525017281262 on epoch=187
06/14/2022 00:13:23 - INFO - __main__ - save last model!
06/14/2022 00:13:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 00:13:23 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 00:13:23 - INFO - __main__ - Printing 3 examples
06/14/2022 00:13:23 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 00:13:23 - INFO - __main__ - ['others']
06/14/2022 00:13:23 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 00:13:23 - INFO - __main__ - ['others']
06/14/2022 00:13:23 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 00:13:23 - INFO - __main__ - ['others']
06/14/2022 00:13:23 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:13:25 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:13:30 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 00:13:36 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:13:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:13:37 - INFO - __main__ - Starting training!
06/14/2022 00:14:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_21_0.4_8_predictions.txt
06/14/2022 00:14:43 - INFO - __main__ - Classification-F1 on test data: 0.1881
06/14/2022 00:14:43 - INFO - __main__ - prefix=emo_64_21, lr=0.4, bsz=8, dev_performance=0.739205706735404, test_performance=0.18808680174655754
06/14/2022 00:14:43 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.3, bsz=8 ...
06/14/2022 00:14:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:14:44 - INFO - __main__ - Printing 3 examples
06/14/2022 00:14:44 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/14/2022 00:14:44 - INFO - __main__ - ['sad']
06/14/2022 00:14:44 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/14/2022 00:14:44 - INFO - __main__ - ['sad']
06/14/2022 00:14:44 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/14/2022 00:14:44 - INFO - __main__ - ['sad']
06/14/2022 00:14:44 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:14:44 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:14:45 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:14:45 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:14:45 - INFO - __main__ - Printing 3 examples
06/14/2022 00:14:45 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/14/2022 00:14:45 - INFO - __main__ - ['sad']
06/14/2022 00:14:45 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/14/2022 00:14:45 - INFO - __main__ - ['sad']
06/14/2022 00:14:45 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/14/2022 00:14:45 - INFO - __main__ - ['sad']
06/14/2022 00:14:45 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:14:45 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:14:45 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:15:04 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:15:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:15:05 - INFO - __main__ - Starting training!
06/14/2022 00:15:08 - INFO - __main__ - Step 10 Global step 10 Train loss 2.97 on epoch=0
06/14/2022 00:15:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.59 on epoch=1
06/14/2022 00:15:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.08 on epoch=1
06/14/2022 00:15:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.06 on epoch=2
06/14/2022 00:15:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.04 on epoch=3
06/14/2022 00:15:22 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.10800578731613214 on epoch=3
06/14/2022 00:15:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10800578731613214 on epoch=3, global_step=50
06/14/2022 00:15:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.92 on epoch=3
06/14/2022 00:15:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=4
06/14/2022 00:15:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=4
06/14/2022 00:15:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.94 on epoch=5
06/14/2022 00:15:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.91 on epoch=6
06/14/2022 00:15:38 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.2552242089055086 on epoch=6
06/14/2022 00:15:38 - INFO - __main__ - Saving model with best Classification-F1: 0.10800578731613214 -> 0.2552242089055086 on epoch=6, global_step=100
06/14/2022 00:15:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.85 on epoch=6
06/14/2022 00:15:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=7
06/14/2022 00:15:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=8
06/14/2022 00:15:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.78 on epoch=8
06/14/2022 00:15:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.82 on epoch=9
06/14/2022 00:15:54 - INFO - __main__ - Global step 150 Train loss 0.84 Classification-F1 0.43148344235868197 on epoch=9
06/14/2022 00:15:54 - INFO - __main__ - Saving model with best Classification-F1: 0.2552242089055086 -> 0.43148344235868197 on epoch=9, global_step=150
06/14/2022 00:15:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=9
06/14/2022 00:15:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=10
06/14/2022 00:16:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=11
06/14/2022 00:16:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=11
06/14/2022 00:16:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=12
06/14/2022 00:16:10 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.3979681723432873 on epoch=12
06/14/2022 00:16:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.73 on epoch=13
06/14/2022 00:16:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=13
06/14/2022 00:16:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.67 on epoch=14
06/14/2022 00:16:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.69 on epoch=14
06/14/2022 00:16:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.72 on epoch=15
06/14/2022 00:16:26 - INFO - __main__ - Global step 250 Train loss 0.71 Classification-F1 0.5048308237515582 on epoch=15
06/14/2022 00:16:26 - INFO - __main__ - Saving model with best Classification-F1: 0.43148344235868197 -> 0.5048308237515582 on epoch=15, global_step=250
06/14/2022 00:16:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.68 on epoch=16
06/14/2022 00:16:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.62 on epoch=16
06/14/2022 00:16:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=17
06/14/2022 00:16:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=18
06/14/2022 00:16:38 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=18
06/14/2022 00:16:42 - INFO - __main__ - Global step 300 Train loss 0.62 Classification-F1 0.6616353071389043 on epoch=18
06/14/2022 00:16:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5048308237515582 -> 0.6616353071389043 on epoch=18, global_step=300
06/14/2022 00:16:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.50 on epoch=19
06/14/2022 00:16:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.62 on epoch=19
06/14/2022 00:16:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=20
06/14/2022 00:16:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=21
06/14/2022 00:16:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.60 on epoch=21
06/14/2022 00:16:58 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.5195802005012532 on epoch=21
06/14/2022 00:17:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=22
06/14/2022 00:17:03 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=23
06/14/2022 00:17:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.58 on epoch=23
06/14/2022 00:17:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=24
06/14/2022 00:17:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=24
06/14/2022 00:17:14 - INFO - __main__ - Global step 400 Train loss 0.53 Classification-F1 0.5480850649579101 on epoch=24
06/14/2022 00:17:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=25
06/14/2022 00:17:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=26
06/14/2022 00:17:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.57 on epoch=26
06/14/2022 00:17:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=27
06/14/2022 00:17:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=28
06/14/2022 00:17:30 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.637793083430143 on epoch=28
06/14/2022 00:17:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=28
06/14/2022 00:17:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=29
06/14/2022 00:17:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=29
06/14/2022 00:17:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=30
06/14/2022 00:17:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=31
06/14/2022 00:17:46 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.6648742830511681 on epoch=31
06/14/2022 00:17:46 - INFO - __main__ - Saving model with best Classification-F1: 0.6616353071389043 -> 0.6648742830511681 on epoch=31, global_step=500
06/14/2022 00:17:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=31
06/14/2022 00:17:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=32
06/14/2022 00:17:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=33
06/14/2022 00:17:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=33
06/14/2022 00:17:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=34
06/14/2022 00:18:02 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.5561812407220482 on epoch=34
06/14/2022 00:18:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=34
06/14/2022 00:18:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=35
06/14/2022 00:18:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.36 on epoch=36
06/14/2022 00:18:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=36
06/14/2022 00:18:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.49 on epoch=37
06/14/2022 00:18:18 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.6629011947892338 on epoch=37
06/14/2022 00:18:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=38
06/14/2022 00:18:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=38
06/14/2022 00:18:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
06/14/2022 00:18:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=39
06/14/2022 00:18:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=40
06/14/2022 00:18:34 - INFO - __main__ - Global step 650 Train loss 0.46 Classification-F1 0.647937418869987 on epoch=40
06/14/2022 00:18:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.39 on epoch=41
06/14/2022 00:18:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=41
06/14/2022 00:18:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=42
06/14/2022 00:18:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=43
06/14/2022 00:18:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
06/14/2022 00:18:50 - INFO - __main__ - Global step 700 Train loss 0.39 Classification-F1 0.6594070512820512 on epoch=43
06/14/2022 00:18:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
06/14/2022 00:18:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=44
06/14/2022 00:18:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=45
06/14/2022 00:19:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
06/14/2022 00:19:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=46
06/14/2022 00:19:06 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.6077785500818175 on epoch=46
06/14/2022 00:19:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.38 on epoch=47
06/14/2022 00:19:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/14/2022 00:19:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=48
06/14/2022 00:19:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=49
06/14/2022 00:19:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=49
06/14/2022 00:19:22 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.6514989342587535 on epoch=49
06/14/2022 00:19:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=50
06/14/2022 00:19:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=51
06/14/2022 00:19:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
06/14/2022 00:19:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=52
06/14/2022 00:19:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
06/14/2022 00:19:38 - INFO - __main__ - Global step 850 Train loss 0.34 Classification-F1 0.6620779761403752 on epoch=53
06/14/2022 00:19:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=53
06/14/2022 00:19:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/14/2022 00:19:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/14/2022 00:19:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=55
06/14/2022 00:19:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
06/14/2022 00:19:54 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.6280118236507689 on epoch=56
06/14/2022 00:19:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=56
06/14/2022 00:19:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=57
06/14/2022 00:20:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=58
06/14/2022 00:20:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.40 on epoch=58
06/14/2022 00:20:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=59
06/14/2022 00:20:10 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.6680448141009303 on epoch=59
06/14/2022 00:20:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6648742830511681 -> 0.6680448141009303 on epoch=59, global_step=950
06/14/2022 00:20:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=59
06/14/2022 00:20:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=60
06/14/2022 00:20:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=61
06/14/2022 00:20:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=61
06/14/2022 00:20:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=62
06/14/2022 00:20:26 - INFO - __main__ - Global step 1000 Train loss 0.32 Classification-F1 0.6680087025350911 on epoch=62
06/14/2022 00:20:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=63
06/14/2022 00:20:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=63
06/14/2022 00:20:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=64
06/14/2022 00:20:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
06/14/2022 00:20:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=65
06/14/2022 00:20:42 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.6840157154673284 on epoch=65
06/14/2022 00:20:42 - INFO - __main__ - Saving model with best Classification-F1: 0.6680448141009303 -> 0.6840157154673284 on epoch=65, global_step=1050
06/14/2022 00:20:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=66
06/14/2022 00:20:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=66
06/14/2022 00:20:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=67
06/14/2022 00:20:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
06/14/2022 00:20:54 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=68
06/14/2022 00:20:58 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.6583649643432252 on epoch=68
06/14/2022 00:21:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
06/14/2022 00:21:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.31 on epoch=69
06/14/2022 00:21:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=70
06/14/2022 00:21:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=71
06/14/2022 00:21:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=71
06/14/2022 00:21:14 - INFO - __main__ - Global step 1150 Train loss 0.30 Classification-F1 0.6207663095314722 on epoch=71
06/14/2022 00:21:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=72
06/14/2022 00:21:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=73
06/14/2022 00:21:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=73
06/14/2022 00:21:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=74
06/14/2022 00:21:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=74
06/14/2022 00:21:30 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.6617965367965368 on epoch=74
06/14/2022 00:21:33 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=75
06/14/2022 00:21:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=76
06/14/2022 00:21:38 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=76
06/14/2022 00:21:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=77
06/14/2022 00:21:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
06/14/2022 00:21:46 - INFO - __main__ - Global step 1250 Train loss 0.23 Classification-F1 0.6793458362222027 on epoch=78
06/14/2022 00:21:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=78
06/14/2022 00:21:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
06/14/2022 00:21:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=79
06/14/2022 00:21:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=80
06/14/2022 00:21:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=81
06/14/2022 00:22:02 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.6298547856805284 on epoch=81
06/14/2022 00:22:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=81
06/14/2022 00:22:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=82
06/14/2022 00:22:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=83
06/14/2022 00:22:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=83
06/14/2022 00:22:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=84
06/14/2022 00:22:18 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.6823910358979632 on epoch=84
06/14/2022 00:22:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=84
06/14/2022 00:22:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=85
06/14/2022 00:22:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=86
06/14/2022 00:22:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=86
06/14/2022 00:22:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=87
06/14/2022 00:22:34 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.7279282990279488 on epoch=87
06/14/2022 00:22:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6840157154673284 -> 0.7279282990279488 on epoch=87, global_step=1400
06/14/2022 00:22:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=88
06/14/2022 00:22:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=88
06/14/2022 00:22:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=89
06/14/2022 00:22:44 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=89
06/14/2022 00:22:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.29 on epoch=90
06/14/2022 00:22:50 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.6924091243121094 on epoch=90
06/14/2022 00:22:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=91
06/14/2022 00:22:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
06/14/2022 00:22:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=92
06/14/2022 00:23:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=93
06/14/2022 00:23:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=93
06/14/2022 00:23:06 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.7127103397686816 on epoch=93
06/14/2022 00:23:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=94
06/14/2022 00:23:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=94
06/14/2022 00:23:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=95
06/14/2022 00:23:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=96
06/14/2022 00:23:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=96
06/14/2022 00:23:22 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.6998909354347285 on epoch=96
06/14/2022 00:23:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=97
06/14/2022 00:23:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=98
06/14/2022 00:23:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
06/14/2022 00:23:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=99
06/14/2022 00:23:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=99
06/14/2022 00:23:38 - INFO - __main__ - Global step 1600 Train loss 0.20 Classification-F1 0.6538932250185834 on epoch=99
06/14/2022 00:23:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=100
06/14/2022 00:23:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=101
06/14/2022 00:23:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
06/14/2022 00:23:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=102
06/14/2022 00:23:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=103
06/14/2022 00:23:54 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.6854076657262398 on epoch=103
06/14/2022 00:23:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=103
06/14/2022 00:23:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.10 on epoch=104
06/14/2022 00:24:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
06/14/2022 00:24:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
06/14/2022 00:24:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=106
06/14/2022 00:24:10 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.69548294495103 on epoch=106
06/14/2022 00:24:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
06/14/2022 00:24:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
06/14/2022 00:24:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=108
06/14/2022 00:24:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=108
06/14/2022 00:24:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=109
06/14/2022 00:24:26 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.678126722562781 on epoch=109
06/14/2022 00:24:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=109
06/14/2022 00:24:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=110
06/14/2022 00:24:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=111
06/14/2022 00:24:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=111
06/14/2022 00:24:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=112
06/14/2022 00:24:42 - INFO - __main__ - Global step 1800 Train loss 0.15 Classification-F1 0.701970097591462 on epoch=112
06/14/2022 00:24:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=113
06/14/2022 00:24:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=113
06/14/2022 00:24:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
06/14/2022 00:24:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
06/14/2022 00:24:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=115
06/14/2022 00:24:58 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.7032035686640816 on epoch=115
06/14/2022 00:25:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=116
06/14/2022 00:25:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=116
06/14/2022 00:25:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=117
06/14/2022 00:25:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
06/14/2022 00:25:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.16 on epoch=118
06/14/2022 00:25:14 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.7037673435351988 on epoch=118
06/14/2022 00:25:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=119
06/14/2022 00:25:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=119
06/14/2022 00:25:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
06/14/2022 00:25:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=121
06/14/2022 00:25:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=121
06/14/2022 00:25:30 - INFO - __main__ - Global step 1950 Train loss 0.15 Classification-F1 0.6859061511909246 on epoch=121
06/14/2022 00:25:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=122
06/14/2022 00:25:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=123
06/14/2022 00:25:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
06/14/2022 00:25:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
06/14/2022 00:25:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=124
06/14/2022 00:25:46 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.6888215174129353 on epoch=124
06/14/2022 00:25:48 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
06/14/2022 00:25:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=126
06/14/2022 00:25:53 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=126
06/14/2022 00:25:56 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.24 on epoch=127
06/14/2022 00:25:58 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=128
06/14/2022 00:26:02 - INFO - __main__ - Global step 2050 Train loss 0.14 Classification-F1 0.7099515563801279 on epoch=128
06/14/2022 00:26:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=128
06/14/2022 00:26:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.14 on epoch=129
06/14/2022 00:26:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
06/14/2022 00:26:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=130
06/14/2022 00:26:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=131
06/14/2022 00:26:18 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.6785135623217126 on epoch=131
06/14/2022 00:26:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
06/14/2022 00:26:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=132
06/14/2022 00:26:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=133
06/14/2022 00:26:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
06/14/2022 00:26:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
06/14/2022 00:26:34 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.6523347442778362 on epoch=134
06/14/2022 00:26:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
06/14/2022 00:26:39 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
06/14/2022 00:26:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=136
06/14/2022 00:26:43 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=136
06/14/2022 00:26:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=137
06/14/2022 00:26:49 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.6811700642987559 on epoch=137
06/14/2022 00:26:52 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.25 on epoch=138
06/14/2022 00:26:54 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=138
06/14/2022 00:26:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=139
06/14/2022 00:26:59 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=139
06/14/2022 00:27:02 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=140
06/14/2022 00:27:05 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.6462769918868316 on epoch=140
06/14/2022 00:27:08 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.16 on epoch=141
06/14/2022 00:27:10 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.09 on epoch=141
06/14/2022 00:27:13 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
06/14/2022 00:27:15 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.08 on epoch=143
06/14/2022 00:27:18 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
06/14/2022 00:27:21 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.6954890289449114 on epoch=143
06/14/2022 00:27:24 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.20 on epoch=144
06/14/2022 00:27:26 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=144
06/14/2022 00:27:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.13 on epoch=145
06/14/2022 00:27:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=146
06/14/2022 00:27:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
06/14/2022 00:27:37 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.6896812076797233 on epoch=146
06/14/2022 00:27:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=147
06/14/2022 00:27:42 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=148
06/14/2022 00:27:45 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.13 on epoch=148
06/14/2022 00:27:47 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.12 on epoch=149
06/14/2022 00:27:50 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.14 on epoch=149
06/14/2022 00:27:53 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.6932929570266525 on epoch=149
06/14/2022 00:27:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=150
06/14/2022 00:27:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.07 on epoch=151
06/14/2022 00:28:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
06/14/2022 00:28:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
06/14/2022 00:28:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
06/14/2022 00:28:09 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.6870023017954859 on epoch=153
06/14/2022 00:28:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.09 on epoch=153
06/14/2022 00:28:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
06/14/2022 00:28:17 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=154
06/14/2022 00:28:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.11 on epoch=155
06/14/2022 00:28:22 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=156
06/14/2022 00:28:25 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.6412577034183919 on epoch=156
06/14/2022 00:28:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
06/14/2022 00:28:30 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.10 on epoch=157
06/14/2022 00:28:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=158
06/14/2022 00:28:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=158
06/14/2022 00:28:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
06/14/2022 00:28:41 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.711784011915745 on epoch=159
06/14/2022 00:28:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.17 on epoch=159
06/14/2022 00:28:46 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
06/14/2022 00:28:48 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=161
06/14/2022 00:28:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.13 on epoch=161
06/14/2022 00:28:53 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=162
06/14/2022 00:28:57 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.6900391915057709 on epoch=162
06/14/2022 00:28:59 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.09 on epoch=163
06/14/2022 00:29:02 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
06/14/2022 00:29:04 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=164
06/14/2022 00:29:07 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
06/14/2022 00:29:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
06/14/2022 00:29:13 - INFO - __main__ - Global step 2650 Train loss 0.11 Classification-F1 0.6713491749417405 on epoch=165
06/14/2022 00:29:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=166
06/14/2022 00:29:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.10 on epoch=166
06/14/2022 00:29:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
06/14/2022 00:29:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=168
06/14/2022 00:29:25 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=168
06/14/2022 00:29:29 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.7120773359782173 on epoch=168
06/14/2022 00:29:31 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
06/14/2022 00:29:34 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=169
06/14/2022 00:29:36 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=170
06/14/2022 00:29:39 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.08 on epoch=171
06/14/2022 00:29:41 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=171
06/14/2022 00:29:45 - INFO - __main__ - Global step 2750 Train loss 0.13 Classification-F1 0.6922409251470373 on epoch=171
06/14/2022 00:29:47 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
06/14/2022 00:29:50 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.09 on epoch=173
06/14/2022 00:29:52 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=173
06/14/2022 00:29:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=174
06/14/2022 00:29:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
06/14/2022 00:30:01 - INFO - __main__ - Global step 2800 Train loss 0.12 Classification-F1 0.6601830252725394 on epoch=174
06/14/2022 00:30:03 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=175
06/14/2022 00:30:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
06/14/2022 00:30:08 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=176
06/14/2022 00:30:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=177
06/14/2022 00:30:13 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=178
06/14/2022 00:30:17 - INFO - __main__ - Global step 2850 Train loss 0.06 Classification-F1 0.6551806500630962 on epoch=178
06/14/2022 00:30:19 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.09 on epoch=178
06/14/2022 00:30:22 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
06/14/2022 00:30:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.10 on epoch=179
06/14/2022 00:30:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/14/2022 00:30:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.07 on epoch=181
06/14/2022 00:30:33 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.6551753380433257 on epoch=181
06/14/2022 00:30:35 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
06/14/2022 00:30:38 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=182
06/14/2022 00:30:40 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
06/14/2022 00:30:42 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=183
06/14/2022 00:30:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
06/14/2022 00:30:48 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.6892863459955272 on epoch=184
06/14/2022 00:30:51 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.13 on epoch=184
06/14/2022 00:30:53 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
06/14/2022 00:30:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=186
06/14/2022 00:30:58 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=186
06/14/2022 00:31:00 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=187
06/14/2022 00:31:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:31:02 - INFO - __main__ - Printing 3 examples
06/14/2022 00:31:02 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:31:02 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:31:02 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:31:02 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:31:02 - INFO - __main__ - Printing 3 examples
06/14/2022 00:31:02 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/14/2022 00:31:02 - INFO - __main__ - ['sad']
06/14/2022 00:31:02 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:31:02 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:31:03 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:31:04 - INFO - __main__ - Global step 3000 Train loss 0.11 Classification-F1 0.6820509684356212 on epoch=187
06/14/2022 00:31:04 - INFO - __main__ - save last model!
06/14/2022 00:31:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 00:31:04 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 00:31:04 - INFO - __main__ - Printing 3 examples
06/14/2022 00:31:04 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 00:31:04 - INFO - __main__ - ['others']
06/14/2022 00:31:04 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 00:31:04 - INFO - __main__ - ['others']
06/14/2022 00:31:04 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 00:31:04 - INFO - __main__ - ['others']
06/14/2022 00:31:04 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:31:06 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:31:11 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 00:31:18 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:31:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:31:18 - INFO - __main__ - Starting training!
06/14/2022 00:32:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_21_0.3_8_predictions.txt
06/14/2022 00:32:24 - INFO - __main__ - Classification-F1 on test data: 0.3453
06/14/2022 00:32:25 - INFO - __main__ - prefix=emo_64_21, lr=0.3, bsz=8, dev_performance=0.7279282990279488, test_performance=0.34526238541692067
06/14/2022 00:32:25 - INFO - __main__ - Running ... prefix=emo_64_21, lr=0.2, bsz=8 ...
06/14/2022 00:32:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:32:26 - INFO - __main__ - Printing 3 examples
06/14/2022 00:32:26 - INFO - __main__ -  [emo] yes buts its real it's me and u she cheated on me
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ -  [emo] i missed you so much i missed you so much more  don't be sad
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ -  [emo] m not okay i disagree  my promotion got hold
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:32:26 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:32:26 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:32:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:32:26 - INFO - __main__ - Printing 3 examples
06/14/2022 00:32:26 - INFO - __main__ -  [emo] nothing well hmmm  good i am just concerned my boyfriend is not talking to me
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ -  [emo] yes i have a bored some matter yes you are boring me no not you i upset some matters
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ -  [emo] i like that you're so positive always helps man cuz i feel down in the dumps right now
06/14/2022 00:32:26 - INFO - __main__ - ['sad']
06/14/2022 00:32:26 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:32:26 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:32:26 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:32:45 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:32:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:32:46 - INFO - __main__ - Starting training!
06/14/2022 00:32:49 - INFO - __main__ - Step 10 Global step 10 Train loss 3.20 on epoch=0
06/14/2022 00:32:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.96 on epoch=1
06/14/2022 00:32:54 - INFO - __main__ - Step 30 Global step 30 Train loss 1.55 on epoch=1
06/14/2022 00:32:56 - INFO - __main__ - Step 40 Global step 40 Train loss 1.17 on epoch=2
06/14/2022 00:32:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.97 on epoch=3
06/14/2022 00:33:02 - INFO - __main__ - Global step 50 Train loss 1.77 Classification-F1 0.30729731766124174 on epoch=3
06/14/2022 00:33:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.30729731766124174 on epoch=3, global_step=50
06/14/2022 00:33:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.95 on epoch=3
06/14/2022 00:33:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.86 on epoch=4
06/14/2022 00:33:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.00 on epoch=4
06/14/2022 00:33:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=5
06/14/2022 00:33:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.85 on epoch=6
06/14/2022 00:33:18 - INFO - __main__ - Global step 100 Train loss 0.91 Classification-F1 0.3105447260469997 on epoch=6
06/14/2022 00:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.30729731766124174 -> 0.3105447260469997 on epoch=6, global_step=100
06/14/2022 00:33:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.91 on epoch=6
06/14/2022 00:33:23 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=7
06/14/2022 00:33:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=8
06/14/2022 00:33:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=8
06/14/2022 00:33:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=9
06/14/2022 00:33:34 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.13067758749069247 on epoch=9
06/14/2022 00:33:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.80 on epoch=9
06/14/2022 00:33:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.86 on epoch=10
06/14/2022 00:33:41 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=11
06/14/2022 00:33:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.80 on epoch=11
06/14/2022 00:33:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.79 on epoch=12
06/14/2022 00:33:49 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.5085205710212619 on epoch=12
06/14/2022 00:33:49 - INFO - __main__ - Saving model with best Classification-F1: 0.3105447260469997 -> 0.5085205710212619 on epoch=12, global_step=200
06/14/2022 00:33:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.72 on epoch=13
06/14/2022 00:33:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.72 on epoch=13
06/14/2022 00:33:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=14
06/14/2022 00:33:59 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=14
06/14/2022 00:34:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=15
06/14/2022 00:34:05 - INFO - __main__ - Global step 250 Train loss 0.74 Classification-F1 0.49052204563342244 on epoch=15
06/14/2022 00:34:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.69 on epoch=16
06/14/2022 00:34:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=16
06/14/2022 00:34:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.70 on epoch=17
06/14/2022 00:34:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.66 on epoch=18
06/14/2022 00:34:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=18
06/14/2022 00:34:21 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.5544935604257638 on epoch=18
06/14/2022 00:34:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5085205710212619 -> 0.5544935604257638 on epoch=18, global_step=300
06/14/2022 00:34:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=19
06/14/2022 00:34:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.70 on epoch=19
06/14/2022 00:34:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.66 on epoch=20
06/14/2022 00:34:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=21
06/14/2022 00:34:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=21
06/14/2022 00:34:37 - INFO - __main__ - Global step 350 Train loss 0.66 Classification-F1 0.539196573370425 on epoch=21
06/14/2022 00:34:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.61 on epoch=22
06/14/2022 00:34:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.62 on epoch=23
06/14/2022 00:34:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=23
06/14/2022 00:34:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.60 on epoch=24
06/14/2022 00:34:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=24
06/14/2022 00:34:53 - INFO - __main__ - Global step 400 Train loss 0.63 Classification-F1 0.5937312457002337 on epoch=24
06/14/2022 00:34:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5544935604257638 -> 0.5937312457002337 on epoch=24, global_step=400
06/14/2022 00:34:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.61 on epoch=25
06/14/2022 00:34:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.67 on epoch=26
06/14/2022 00:35:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.65 on epoch=26
06/14/2022 00:35:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=27
06/14/2022 00:35:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=28
06/14/2022 00:35:09 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.5936303224993702 on epoch=28
06/14/2022 00:35:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=28
06/14/2022 00:35:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.59 on epoch=29
06/14/2022 00:35:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.67 on epoch=29
06/14/2022 00:35:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=30
06/14/2022 00:35:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=31
06/14/2022 00:35:25 - INFO - __main__ - Global step 500 Train loss 0.56 Classification-F1 0.5586939102564102 on epoch=31
06/14/2022 00:35:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=31
06/14/2022 00:35:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.55 on epoch=32
06/14/2022 00:35:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.52 on epoch=33
06/14/2022 00:35:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=33
06/14/2022 00:35:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=34
06/14/2022 00:35:41 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.6244685740466462 on epoch=34
06/14/2022 00:35:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5937312457002337 -> 0.6244685740466462 on epoch=34, global_step=550
06/14/2022 00:35:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.55 on epoch=34
06/14/2022 00:35:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=35
06/14/2022 00:35:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.53 on epoch=36
06/14/2022 00:35:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=36
06/14/2022 00:35:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=37
06/14/2022 00:35:57 - INFO - __main__ - Global step 600 Train loss 0.53 Classification-F1 0.6197696736529656 on epoch=37
06/14/2022 00:35:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=38
06/14/2022 00:36:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=38
06/14/2022 00:36:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
06/14/2022 00:36:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.52 on epoch=39
06/14/2022 00:36:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=40
06/14/2022 00:36:13 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.616166353971232 on epoch=40
06/14/2022 00:36:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=41
06/14/2022 00:36:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.60 on epoch=41
06/14/2022 00:36:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=42
06/14/2022 00:36:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=43
06/14/2022 00:36:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=43
06/14/2022 00:36:29 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.6398093696312939 on epoch=43
06/14/2022 00:36:29 - INFO - __main__ - Saving model with best Classification-F1: 0.6244685740466462 -> 0.6398093696312939 on epoch=43, global_step=700
06/14/2022 00:36:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=44
06/14/2022 00:36:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=44
06/14/2022 00:36:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=45
06/14/2022 00:36:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=46
06/14/2022 00:36:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.56 on epoch=46
06/14/2022 00:36:45 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.5607064227148496 on epoch=46
06/14/2022 00:36:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=47
06/14/2022 00:36:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=48
06/14/2022 00:36:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=48
06/14/2022 00:36:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=49
06/14/2022 00:36:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=49
06/14/2022 00:37:01 - INFO - __main__ - Global step 800 Train loss 0.49 Classification-F1 0.6395395138789512 on epoch=49
06/14/2022 00:37:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=50
06/14/2022 00:37:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=51
06/14/2022 00:37:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=51
06/14/2022 00:37:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=52
06/14/2022 00:37:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=53
06/14/2022 00:37:17 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.6672913366772807 on epoch=53
06/14/2022 00:37:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6398093696312939 -> 0.6672913366772807 on epoch=53, global_step=850
06/14/2022 00:37:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=53
06/14/2022 00:37:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=54
06/14/2022 00:37:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=54
06/14/2022 00:37:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=55
06/14/2022 00:37:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=56
06/14/2022 00:37:33 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.692621193213401 on epoch=56
06/14/2022 00:37:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6672913366772807 -> 0.692621193213401 on epoch=56, global_step=900
06/14/2022 00:37:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=56
06/14/2022 00:37:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=57
06/14/2022 00:37:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/14/2022 00:37:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=58
06/14/2022 00:37:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=59
06/14/2022 00:37:49 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.6094966127676409 on epoch=59
06/14/2022 00:37:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
06/14/2022 00:37:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=60
06/14/2022 00:37:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
06/14/2022 00:37:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.46 on epoch=61
06/14/2022 00:38:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=62
06/14/2022 00:38:04 - INFO - __main__ - Global step 1000 Train loss 0.39 Classification-F1 0.6979263166767389 on epoch=62
06/14/2022 00:38:04 - INFO - __main__ - Saving model with best Classification-F1: 0.692621193213401 -> 0.6979263166767389 on epoch=62, global_step=1000
06/14/2022 00:38:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=63
06/14/2022 00:38:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=63
06/14/2022 00:38:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=64
06/14/2022 00:38:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=64
06/14/2022 00:38:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=65
06/14/2022 00:38:21 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.7064326859333736 on epoch=65
06/14/2022 00:38:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6979263166767389 -> 0.7064326859333736 on epoch=65, global_step=1050
06/14/2022 00:38:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=66
06/14/2022 00:38:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=66
06/14/2022 00:38:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=67
06/14/2022 00:38:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
06/14/2022 00:38:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=68
06/14/2022 00:38:36 - INFO - __main__ - Global step 1100 Train loss 0.39 Classification-F1 0.6872834944586166 on epoch=68
06/14/2022 00:38:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=69
06/14/2022 00:38:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=69
06/14/2022 00:38:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/14/2022 00:38:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=71
06/14/2022 00:38:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.32 on epoch=71
06/14/2022 00:38:52 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.637320465321579 on epoch=71
06/14/2022 00:38:55 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=72
06/14/2022 00:38:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
06/14/2022 00:39:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/14/2022 00:39:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=74
06/14/2022 00:39:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=74
06/14/2022 00:39:08 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.6607943433772232 on epoch=74
06/14/2022 00:39:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=75
06/14/2022 00:39:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=76
06/14/2022 00:39:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.45 on epoch=76
06/14/2022 00:39:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=77
06/14/2022 00:39:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=78
06/14/2022 00:39:24 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.674717770653394 on epoch=78
06/14/2022 00:39:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.45 on epoch=78
06/14/2022 00:39:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=79
06/14/2022 00:39:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=79
06/14/2022 00:39:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=80
06/14/2022 00:39:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
06/14/2022 00:39:40 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.7026791174920599 on epoch=81
06/14/2022 00:39:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=81
06/14/2022 00:39:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.34 on epoch=82
06/14/2022 00:39:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=83
06/14/2022 00:39:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.40 on epoch=83
06/14/2022 00:39:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=84
06/14/2022 00:39:56 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.597893896299126 on epoch=84
06/14/2022 00:39:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=84
06/14/2022 00:40:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=85
06/14/2022 00:40:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=86
06/14/2022 00:40:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=86
06/14/2022 00:40:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=87
06/14/2022 00:40:12 - INFO - __main__ - Global step 1400 Train loss 0.35 Classification-F1 0.6406447254570986 on epoch=87
06/14/2022 00:40:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=88
06/14/2022 00:40:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.28 on epoch=88
06/14/2022 00:40:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=89
06/14/2022 00:40:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.29 on epoch=89
06/14/2022 00:40:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=90
06/14/2022 00:40:28 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.699495703123068 on epoch=90
06/14/2022 00:40:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=91
06/14/2022 00:40:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.34 on epoch=91
06/14/2022 00:40:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=92
06/14/2022 00:40:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=93
06/14/2022 00:40:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.33 on epoch=93
06/14/2022 00:40:44 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.660336002620109 on epoch=93
06/14/2022 00:40:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
06/14/2022 00:40:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=94
06/14/2022 00:40:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=95
06/14/2022 00:40:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=96
06/14/2022 00:40:56 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.34 on epoch=96
06/14/2022 00:41:00 - INFO - __main__ - Global step 1550 Train loss 0.29 Classification-F1 0.6483467028607208 on epoch=96
06/14/2022 00:41:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=97
06/14/2022 00:41:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=98
06/14/2022 00:41:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=98
06/14/2022 00:41:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=99
06/14/2022 00:41:12 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
06/14/2022 00:41:16 - INFO - __main__ - Global step 1600 Train loss 0.30 Classification-F1 0.6455281936989253 on epoch=99
06/14/2022 00:41:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.32 on epoch=100
06/14/2022 00:41:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=101
06/14/2022 00:41:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=101
06/14/2022 00:41:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=102
06/14/2022 00:41:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.31 on epoch=103
06/14/2022 00:41:32 - INFO - __main__ - Global step 1650 Train loss 0.27 Classification-F1 0.6476156858649984 on epoch=103
06/14/2022 00:41:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.24 on epoch=103
06/14/2022 00:41:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=104
06/14/2022 00:41:39 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=104
06/14/2022 00:41:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=105
06/14/2022 00:41:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=106
06/14/2022 00:41:48 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.6331724641754186 on epoch=106
06/14/2022 00:41:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.46 on epoch=106
06/14/2022 00:41:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
06/14/2022 00:41:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=108
06/14/2022 00:41:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.31 on epoch=108
06/14/2022 00:42:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=109
06/14/2022 00:42:04 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.6546826285491799 on epoch=109
06/14/2022 00:42:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=109
06/14/2022 00:42:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=110
06/14/2022 00:42:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=111
06/14/2022 00:42:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.32 on epoch=111
06/14/2022 00:42:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=112
06/14/2022 00:42:20 - INFO - __main__ - Global step 1800 Train loss 0.31 Classification-F1 0.6776275625310973 on epoch=112
06/14/2022 00:42:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=113
06/14/2022 00:42:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=113
06/14/2022 00:42:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=114
06/14/2022 00:42:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=114
06/14/2022 00:42:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.28 on epoch=115
06/14/2022 00:42:36 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7065188167246027 on epoch=115
06/14/2022 00:42:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7064326859333736 -> 0.7065188167246027 on epoch=115, global_step=1850
06/14/2022 00:42:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=116
06/14/2022 00:42:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=116
06/14/2022 00:42:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=117
06/14/2022 00:42:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=118
06/14/2022 00:42:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=118
06/14/2022 00:42:51 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.6839090618258281 on epoch=118
06/14/2022 00:42:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=119
06/14/2022 00:42:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.27 on epoch=119
06/14/2022 00:42:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=120
06/14/2022 00:43:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=121
06/14/2022 00:43:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
06/14/2022 00:43:07 - INFO - __main__ - Global step 1950 Train loss 0.25 Classification-F1 0.6625128536597769 on epoch=121
06/14/2022 00:43:10 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=122
06/14/2022 00:43:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=123
06/14/2022 00:43:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=123
06/14/2022 00:43:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
06/14/2022 00:43:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=124
06/14/2022 00:43:23 - INFO - __main__ - Global step 2000 Train loss 0.23 Classification-F1 0.6752295295957154 on epoch=124
06/14/2022 00:43:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
06/14/2022 00:43:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.16 on epoch=126
06/14/2022 00:43:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.27 on epoch=126
06/14/2022 00:43:33 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
06/14/2022 00:43:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.23 on epoch=128
06/14/2022 00:43:39 - INFO - __main__ - Global step 2050 Train loss 0.22 Classification-F1 0.6565035075414747 on epoch=128
06/14/2022 00:43:42 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.22 on epoch=128
06/14/2022 00:43:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=129
06/14/2022 00:43:47 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=129
06/14/2022 00:43:49 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=130
06/14/2022 00:43:52 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.23 on epoch=131
06/14/2022 00:43:55 - INFO - __main__ - Global step 2100 Train loss 0.24 Classification-F1 0.7138357565779563 on epoch=131
06/14/2022 00:43:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7065188167246027 -> 0.7138357565779563 on epoch=131, global_step=2100
06/14/2022 00:43:58 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=131
06/14/2022 00:44:00 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=132
06/14/2022 00:44:03 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=133
06/14/2022 00:44:05 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.22 on epoch=133
06/14/2022 00:44:08 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.12 on epoch=134
06/14/2022 00:44:11 - INFO - __main__ - Global step 2150 Train loss 0.17 Classification-F1 0.7067128830981995 on epoch=134
06/14/2022 00:44:14 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
06/14/2022 00:44:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.27 on epoch=135
06/14/2022 00:44:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.16 on epoch=136
06/14/2022 00:44:22 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.23 on epoch=136
06/14/2022 00:44:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
06/14/2022 00:44:28 - INFO - __main__ - Global step 2200 Train loss 0.21 Classification-F1 0.6798751435552945 on epoch=137
06/14/2022 00:44:30 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=138
06/14/2022 00:44:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.35 on epoch=138
06/14/2022 00:44:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
06/14/2022 00:44:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
06/14/2022 00:44:41 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.28 on epoch=140
06/14/2022 00:44:44 - INFO - __main__ - Global step 2250 Train loss 0.22 Classification-F1 0.6946052844311892 on epoch=140
06/14/2022 00:44:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.12 on epoch=141
06/14/2022 00:44:49 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.31 on epoch=141
06/14/2022 00:44:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.23 on epoch=142
06/14/2022 00:44:54 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=143
06/14/2022 00:44:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.24 on epoch=143
06/14/2022 00:45:00 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.6713932690344968 on epoch=143
06/14/2022 00:45:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=144
06/14/2022 00:45:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=144
06/14/2022 00:45:07 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.21 on epoch=145
06/14/2022 00:45:10 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=146
06/14/2022 00:45:12 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.20 on epoch=146
06/14/2022 00:45:16 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.6938576726335388 on epoch=146
06/14/2022 00:45:18 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
06/14/2022 00:45:21 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
06/14/2022 00:45:23 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.19 on epoch=148
06/14/2022 00:45:26 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.15 on epoch=149
06/14/2022 00:45:28 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=149
06/14/2022 00:45:32 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.6974936659013738 on epoch=149
06/14/2022 00:45:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=150
06/14/2022 00:45:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.14 on epoch=151
06/14/2022 00:45:39 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.12 on epoch=151
06/14/2022 00:45:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
06/14/2022 00:45:44 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
06/14/2022 00:45:48 - INFO - __main__ - Global step 2450 Train loss 0.15 Classification-F1 0.6275629657976435 on epoch=153
06/14/2022 00:45:50 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.22 on epoch=153
06/14/2022 00:45:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.16 on epoch=154
06/14/2022 00:45:55 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.14 on epoch=154
06/14/2022 00:45:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.19 on epoch=155
06/14/2022 00:46:00 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=156
06/14/2022 00:46:04 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.6488416256428521 on epoch=156
06/14/2022 00:46:06 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
06/14/2022 00:46:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.17 on epoch=157
06/14/2022 00:46:11 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
06/14/2022 00:46:14 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
06/14/2022 00:46:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
06/14/2022 00:46:20 - INFO - __main__ - Global step 2550 Train loss 0.13 Classification-F1 0.6614676902009031 on epoch=159
06/14/2022 00:46:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
06/14/2022 00:46:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.19 on epoch=160
06/14/2022 00:46:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
06/14/2022 00:46:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.22 on epoch=161
06/14/2022 00:46:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.17 on epoch=162
06/14/2022 00:46:36 - INFO - __main__ - Global step 2600 Train loss 0.16 Classification-F1 0.6807155738518343 on epoch=162
06/14/2022 00:46:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.22 on epoch=163
06/14/2022 00:46:41 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.21 on epoch=163
06/14/2022 00:46:43 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.15 on epoch=164
06/14/2022 00:46:46 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.17 on epoch=164
06/14/2022 00:46:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=165
06/14/2022 00:46:52 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.7059134091755015 on epoch=165
06/14/2022 00:46:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.12 on epoch=166
06/14/2022 00:46:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.29 on epoch=166
06/14/2022 00:46:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=167
06/14/2022 00:47:02 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
06/14/2022 00:47:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.14 on epoch=168
06/14/2022 00:47:08 - INFO - __main__ - Global step 2700 Train loss 0.16 Classification-F1 0.6507817903250596 on epoch=168
06/14/2022 00:47:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.14 on epoch=169
06/14/2022 00:47:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=169
06/14/2022 00:47:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
06/14/2022 00:47:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=171
06/14/2022 00:47:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=171
06/14/2022 00:47:24 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.6670719053321699 on epoch=171
06/14/2022 00:47:26 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
06/14/2022 00:47:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.12 on epoch=173
06/14/2022 00:47:31 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.18 on epoch=173
06/14/2022 00:47:34 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.13 on epoch=174
06/14/2022 00:47:37 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.11 on epoch=174
06/14/2022 00:47:40 - INFO - __main__ - Global step 2800 Train loss 0.14 Classification-F1 0.6287717805553537 on epoch=174
06/14/2022 00:47:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.19 on epoch=175
06/14/2022 00:47:45 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.17 on epoch=176
06/14/2022 00:47:48 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
06/14/2022 00:47:50 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
06/14/2022 00:47:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.10 on epoch=178
06/14/2022 00:47:56 - INFO - __main__ - Global step 2850 Train loss 0.14 Classification-F1 0.6517635746751067 on epoch=178
06/14/2022 00:47:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.14 on epoch=178
06/14/2022 00:48:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=179
06/14/2022 00:48:04 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=179
06/14/2022 00:48:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/14/2022 00:48:09 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.12 on epoch=181
06/14/2022 00:48:12 - INFO - __main__ - Global step 2900 Train loss 0.12 Classification-F1 0.6826692221538655 on epoch=181
06/14/2022 00:48:15 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.24 on epoch=181
06/14/2022 00:48:17 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.15 on epoch=182
06/14/2022 00:48:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.10 on epoch=183
06/14/2022 00:48:22 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.08 on epoch=183
06/14/2022 00:48:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.15 on epoch=184
06/14/2022 00:48:28 - INFO - __main__ - Global step 2950 Train loss 0.14 Classification-F1 0.6742204645747952 on epoch=184
06/14/2022 00:48:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=184
06/14/2022 00:48:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.12 on epoch=185
06/14/2022 00:48:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.13 on epoch=186
06/14/2022 00:48:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.11 on epoch=186
06/14/2022 00:48:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.11 on epoch=187
06/14/2022 00:48:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:48:42 - INFO - __main__ - Printing 3 examples
06/14/2022 00:48:42 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 00:48:42 - INFO - __main__ - ['happy']
06/14/2022 00:48:42 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 00:48:42 - INFO - __main__ - ['happy']
06/14/2022 00:48:42 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 00:48:42 - INFO - __main__ - ['happy']
06/14/2022 00:48:42 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:48:42 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:48:43 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:48:43 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:48:43 - INFO - __main__ - Printing 3 examples
06/14/2022 00:48:43 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 00:48:43 - INFO - __main__ - ['happy']
06/14/2022 00:48:43 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 00:48:43 - INFO - __main__ - ['happy']
06/14/2022 00:48:43 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 00:48:43 - INFO - __main__ - ['happy']
06/14/2022 00:48:43 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:48:43 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:48:43 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:48:44 - INFO - __main__ - Global step 3000 Train loss 0.13 Classification-F1 0.6712389041934497 on epoch=187
06/14/2022 00:48:44 - INFO - __main__ - save last model!
06/14/2022 00:48:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 00:48:44 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 00:48:44 - INFO - __main__ - Printing 3 examples
06/14/2022 00:48:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 00:48:44 - INFO - __main__ - ['others']
06/14/2022 00:48:44 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 00:48:44 - INFO - __main__ - ['others']
06/14/2022 00:48:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 00:48:44 - INFO - __main__ - ['others']
06/14/2022 00:48:44 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:48:47 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:48:52 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 00:49:01 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:49:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:49:02 - INFO - __main__ - Starting training!
06/14/2022 00:50:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_21_0.2_8_predictions.txt
06/14/2022 00:50:08 - INFO - __main__ - Classification-F1 on test data: 0.2634
06/14/2022 00:50:09 - INFO - __main__ - prefix=emo_64_21, lr=0.2, bsz=8, dev_performance=0.7138357565779563, test_performance=0.2634384976713943
06/14/2022 00:50:09 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.5, bsz=8 ...
06/14/2022 00:50:09 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:50:09 - INFO - __main__ - Printing 3 examples
06/14/2022 00:50:09 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 00:50:09 - INFO - __main__ - ['happy']
06/14/2022 00:50:09 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 00:50:09 - INFO - __main__ - ['happy']
06/14/2022 00:50:09 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 00:50:09 - INFO - __main__ - ['happy']
06/14/2022 00:50:09 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:50:10 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:50:10 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 00:50:10 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 00:50:10 - INFO - __main__ - Printing 3 examples
06/14/2022 00:50:10 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 00:50:10 - INFO - __main__ - ['happy']
06/14/2022 00:50:10 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 00:50:10 - INFO - __main__ - ['happy']
06/14/2022 00:50:10 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 00:50:10 - INFO - __main__ - ['happy']
06/14/2022 00:50:10 - INFO - __main__ - Tokenizing Input ...
06/14/2022 00:50:10 - INFO - __main__ - Tokenizing Output ...
06/14/2022 00:50:10 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 00:50:25 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 00:50:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 00:50:26 - INFO - __main__ - Starting training!
06/14/2022 00:50:29 - INFO - __main__ - Step 10 Global step 10 Train loss 2.33 on epoch=0
06/14/2022 00:50:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.25 on epoch=1
06/14/2022 00:50:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.04 on epoch=1
06/14/2022 00:50:37 - INFO - __main__ - Step 40 Global step 40 Train loss 1.81 on epoch=2
06/14/2022 00:50:39 - INFO - __main__ - Step 50 Global step 50 Train loss 1.35 on epoch=3
06/14/2022 00:50:43 - INFO - __main__ - Global step 50 Train loss 1.55 Classification-F1 0.1 on epoch=3
06/14/2022 00:50:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=3, global_step=50
06/14/2022 00:50:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.93 on epoch=3
06/14/2022 00:50:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.93 on epoch=4
06/14/2022 00:50:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.92 on epoch=4
06/14/2022 00:50:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.97 on epoch=5
06/14/2022 00:50:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.95 on epoch=6
06/14/2022 00:50:59 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.15552617808256908 on epoch=6
06/14/2022 00:50:59 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.15552617808256908 on epoch=6, global_step=100
06/14/2022 00:51:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.95 on epoch=6
06/14/2022 00:51:04 - INFO - __main__ - Step 120 Global step 120 Train loss 0.80 on epoch=7
06/14/2022 00:51:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=8
06/14/2022 00:51:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=8
06/14/2022 00:51:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.89 on epoch=9
06/14/2022 00:51:15 - INFO - __main__ - Global step 150 Train loss 0.87 Classification-F1 0.45844979296066246 on epoch=9
06/14/2022 00:51:15 - INFO - __main__ - Saving model with best Classification-F1: 0.15552617808256908 -> 0.45844979296066246 on epoch=9, global_step=150
06/14/2022 00:51:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=9
06/14/2022 00:51:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.78 on epoch=10
06/14/2022 00:51:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=11
06/14/2022 00:51:25 - INFO - __main__ - Step 190 Global step 190 Train loss 0.85 on epoch=11
06/14/2022 00:51:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=12
06/14/2022 00:51:31 - INFO - __main__ - Global step 200 Train loss 0.87 Classification-F1 0.32561584436892066 on epoch=12
06/14/2022 00:51:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.88 on epoch=13
06/14/2022 00:51:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=13
06/14/2022 00:51:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.85 on epoch=14
06/14/2022 00:51:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.78 on epoch=14
06/14/2022 00:51:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=15
06/14/2022 00:51:47 - INFO - __main__ - Global step 250 Train loss 0.84 Classification-F1 0.3915781930487813 on epoch=15
06/14/2022 00:51:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=16
06/14/2022 00:51:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.78 on epoch=16
06/14/2022 00:51:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=17
06/14/2022 00:51:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=18
06/14/2022 00:52:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.75 on epoch=18
06/14/2022 00:52:03 - INFO - __main__ - Global step 300 Train loss 0.76 Classification-F1 0.34032073728791634 on epoch=18
06/14/2022 00:52:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.77 on epoch=19
06/14/2022 00:52:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.74 on epoch=19
06/14/2022 00:52:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.74 on epoch=20
06/14/2022 00:52:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.68 on epoch=21
06/14/2022 00:52:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=21
06/14/2022 00:52:19 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.4877380197302874 on epoch=21
06/14/2022 00:52:19 - INFO - __main__ - Saving model with best Classification-F1: 0.45844979296066246 -> 0.4877380197302874 on epoch=21, global_step=350
06/14/2022 00:52:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=22
06/14/2022 00:52:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.70 on epoch=23
06/14/2022 00:52:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.70 on epoch=23
06/14/2022 00:52:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.61 on epoch=24
06/14/2022 00:52:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.69 on epoch=24
06/14/2022 00:52:35 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.5974058598225984 on epoch=24
06/14/2022 00:52:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4877380197302874 -> 0.5974058598225984 on epoch=24, global_step=400
06/14/2022 00:52:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=25
06/14/2022 00:52:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.70 on epoch=26
06/14/2022 00:52:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=26
06/14/2022 00:52:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.58 on epoch=27
06/14/2022 00:52:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=28
06/14/2022 00:52:51 - INFO - __main__ - Global step 450 Train loss 0.64 Classification-F1 0.6992793522267207 on epoch=28
06/14/2022 00:52:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5974058598225984 -> 0.6992793522267207 on epoch=28, global_step=450
06/14/2022 00:52:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.68 on epoch=28
06/14/2022 00:52:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=29
06/14/2022 00:52:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.70 on epoch=29
06/14/2022 00:53:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.54 on epoch=30
06/14/2022 00:53:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=31
06/14/2022 00:53:07 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.7302633451754685 on epoch=31
06/14/2022 00:53:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6992793522267207 -> 0.7302633451754685 on epoch=31, global_step=500
06/14/2022 00:53:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.61 on epoch=31
06/14/2022 00:53:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.61 on epoch=32
06/14/2022 00:53:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=33
06/14/2022 00:53:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.66 on epoch=33
06/14/2022 00:53:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=34
06/14/2022 00:53:23 - INFO - __main__ - Global step 550 Train loss 0.62 Classification-F1 0.7096857807871351 on epoch=34
06/14/2022 00:53:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=34
06/14/2022 00:53:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=35
06/14/2022 00:53:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.56 on epoch=36
06/14/2022 00:53:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.55 on epoch=36
06/14/2022 00:53:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.53 on epoch=37
06/14/2022 00:53:39 - INFO - __main__ - Global step 600 Train loss 0.57 Classification-F1 0.6920303383109 on epoch=37
06/14/2022 00:53:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=38
06/14/2022 00:53:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=38
06/14/2022 00:53:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=39
06/14/2022 00:53:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=39
06/14/2022 00:53:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=40
06/14/2022 00:53:55 - INFO - __main__ - Global step 650 Train loss 0.55 Classification-F1 0.6512303670425634 on epoch=40
06/14/2022 00:53:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.59 on epoch=41
06/14/2022 00:54:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=41
06/14/2022 00:54:03 - INFO - __main__ - Step 680 Global step 680 Train loss 1.30 on epoch=42
06/14/2022 00:54:05 - INFO - __main__ - Step 690 Global step 690 Train loss 1.07 on epoch=43
06/14/2022 00:54:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.70 on epoch=43
06/14/2022 00:54:11 - INFO - __main__ - Global step 700 Train loss 0.84 Classification-F1 0.5673451375398137 on epoch=43
06/14/2022 00:54:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.74 on epoch=44
06/14/2022 00:54:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.63 on epoch=44
06/14/2022 00:54:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.70 on epoch=45
06/14/2022 00:54:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.66 on epoch=46
06/14/2022 00:54:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.63 on epoch=46
06/14/2022 00:54:27 - INFO - __main__ - Global step 750 Train loss 0.67 Classification-F1 0.6084404466984008 on epoch=46
06/14/2022 00:54:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.57 on epoch=47
06/14/2022 00:54:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.58 on epoch=48
06/14/2022 00:54:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=48
06/14/2022 00:54:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=49
06/14/2022 00:54:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=49
06/14/2022 00:54:43 - INFO - __main__ - Global step 800 Train loss 0.58 Classification-F1 0.6629306099097464 on epoch=49
06/14/2022 00:54:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.59 on epoch=50
06/14/2022 00:54:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.54 on epoch=51
06/14/2022 00:54:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.70 on epoch=51
06/14/2022 00:54:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=52
06/14/2022 00:54:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.52 on epoch=53
06/14/2022 00:54:59 - INFO - __main__ - Global step 850 Train loss 0.58 Classification-F1 0.6261072261072261 on epoch=53
06/14/2022 00:55:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.50 on epoch=53
06/14/2022 00:55:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=54
06/14/2022 00:55:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.61 on epoch=54
06/14/2022 00:55:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=55
06/14/2022 00:55:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=56
06/14/2022 00:55:15 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.6926188946004768 on epoch=56
06/14/2022 00:55:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=56
06/14/2022 00:55:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=57
06/14/2022 00:55:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=58
06/14/2022 00:55:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=58
06/14/2022 00:55:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=59
06/14/2022 00:55:31 - INFO - __main__ - Global step 950 Train loss 0.50 Classification-F1 0.6953997421018698 on epoch=59
06/14/2022 00:55:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.54 on epoch=59
06/14/2022 00:55:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=60
06/14/2022 00:55:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=61
06/14/2022 00:55:41 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=61
06/14/2022 00:55:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=62
06/14/2022 00:55:47 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.7522353115318428 on epoch=62
06/14/2022 00:55:47 - INFO - __main__ - Saving model with best Classification-F1: 0.7302633451754685 -> 0.7522353115318428 on epoch=62, global_step=1000
06/14/2022 00:55:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=63
06/14/2022 00:55:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=63
06/14/2022 00:55:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=64
06/14/2022 00:55:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.53 on epoch=64
06/14/2022 00:56:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.52 on epoch=65
06/14/2022 00:56:03 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.741988747100006 on epoch=65
06/14/2022 00:56:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=66
06/14/2022 00:56:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.55 on epoch=66
06/14/2022 00:56:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=67
06/14/2022 00:56:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=68
06/14/2022 00:56:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.48 on epoch=68
06/14/2022 00:56:18 - INFO - __main__ - Global step 1100 Train loss 0.50 Classification-F1 0.6977120099106354 on epoch=68
06/14/2022 00:56:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=69
06/14/2022 00:56:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=69
06/14/2022 00:56:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=70
06/14/2022 00:56:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=71
06/14/2022 00:56:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=71
06/14/2022 00:56:34 - INFO - __main__ - Global step 1150 Train loss 0.49 Classification-F1 0.7285705055883697 on epoch=71
06/14/2022 00:56:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=72
06/14/2022 00:56:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.54 on epoch=73
06/14/2022 00:56:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=73
06/14/2022 00:56:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=74
06/14/2022 00:56:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=74
06/14/2022 00:56:49 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.7507885867273724 on epoch=74
06/14/2022 00:56:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=75
06/14/2022 00:56:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.47 on epoch=76
06/14/2022 00:56:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=76
06/14/2022 00:56:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=77
06/14/2022 00:57:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.44 on epoch=78
06/14/2022 00:57:04 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.7282046731736173 on epoch=78
06/14/2022 00:57:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=78
06/14/2022 00:57:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=79
06/14/2022 00:57:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=79
06/14/2022 00:57:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=80
06/14/2022 00:57:16 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=81
06/14/2022 00:57:19 - INFO - __main__ - Global step 1300 Train loss 0.46 Classification-F1 0.7209250999300605 on epoch=81
06/14/2022 00:57:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=81
06/14/2022 00:57:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.35 on epoch=82
06/14/2022 00:57:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=83
06/14/2022 00:57:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=83
06/14/2022 00:57:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=84
06/14/2022 00:57:34 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.7369517187115199 on epoch=84
06/14/2022 00:57:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=84
06/14/2022 00:57:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=85
06/14/2022 00:57:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=86
06/14/2022 00:57:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=86
06/14/2022 00:57:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=87
06/14/2022 00:57:50 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.7069792659527881 on epoch=87
06/14/2022 00:57:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=88
06/14/2022 00:57:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=88
06/14/2022 00:57:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=89
06/14/2022 00:57:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=89
06/14/2022 00:58:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.47 on epoch=90
06/14/2022 00:58:05 - INFO - __main__ - Global step 1450 Train loss 0.44 Classification-F1 0.7191849956687583 on epoch=90
06/14/2022 00:58:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=91
06/14/2022 00:58:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=91
06/14/2022 00:58:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=92
06/14/2022 00:58:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=93
06/14/2022 00:58:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.38 on epoch=93
06/14/2022 00:58:20 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.6869309464090478 on epoch=93
06/14/2022 00:58:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=94
06/14/2022 00:58:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=94
06/14/2022 00:58:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.33 on epoch=95
06/14/2022 00:58:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.37 on epoch=96
06/14/2022 00:58:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.44 on epoch=96
06/14/2022 00:58:35 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.7239797254329855 on epoch=96
06/14/2022 00:58:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=97
06/14/2022 00:58:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=98
06/14/2022 00:58:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=98
06/14/2022 00:58:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=99
06/14/2022 00:58:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=99
06/14/2022 00:58:51 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.7422029718365925 on epoch=99
06/14/2022 00:58:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=100
06/14/2022 00:58:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=101
06/14/2022 00:58:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=101
06/14/2022 00:59:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=102
06/14/2022 00:59:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=103
06/14/2022 00:59:06 - INFO - __main__ - Global step 1650 Train loss 0.39 Classification-F1 0.7229126643837587 on epoch=103
06/14/2022 00:59:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.42 on epoch=103
06/14/2022 00:59:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=104
06/14/2022 00:59:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=104
06/14/2022 00:59:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=105
06/14/2022 00:59:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.44 on epoch=106
06/14/2022 00:59:21 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.7315499128624553 on epoch=106
06/14/2022 00:59:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=106
06/14/2022 00:59:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=107
06/14/2022 00:59:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=108
06/14/2022 00:59:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=108
06/14/2022 00:59:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=109
06/14/2022 00:59:36 - INFO - __main__ - Global step 1750 Train loss 0.38 Classification-F1 0.7428526541440427 on epoch=109
06/14/2022 00:59:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=109
06/14/2022 00:59:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.38 on epoch=110
06/14/2022 00:59:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=111
06/14/2022 00:59:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.35 on epoch=111
06/14/2022 00:59:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=112
06/14/2022 00:59:51 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.7376917465822334 on epoch=112
06/14/2022 00:59:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=113
06/14/2022 00:59:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.33 on epoch=113
06/14/2022 00:59:58 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=114
06/14/2022 01:00:01 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/14/2022 01:00:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.36 on epoch=115
06/14/2022 01:00:07 - INFO - __main__ - Global step 1850 Train loss 0.36 Classification-F1 0.7557206970915661 on epoch=115
06/14/2022 01:00:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7522353115318428 -> 0.7557206970915661 on epoch=115, global_step=1850
06/14/2022 01:00:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=116
06/14/2022 01:00:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=116
06/14/2022 01:00:14 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=117
06/14/2022 01:00:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=118
06/14/2022 01:00:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.30 on epoch=118
06/14/2022 01:00:22 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.6612561344647377 on epoch=118
06/14/2022 01:00:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.34 on epoch=119
06/14/2022 01:00:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=119
06/14/2022 01:00:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
06/14/2022 01:00:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=121
06/14/2022 01:00:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=121
06/14/2022 01:00:37 - INFO - __main__ - Global step 1950 Train loss 0.33 Classification-F1 0.7036525333814624 on epoch=121
06/14/2022 01:00:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=122
06/14/2022 01:00:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=123
06/14/2022 01:00:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.44 on epoch=123
06/14/2022 01:00:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=124
06/14/2022 01:00:49 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=124
06/14/2022 01:00:52 - INFO - __main__ - Global step 2000 Train loss 0.35 Classification-F1 0.7578648123139762 on epoch=124
06/14/2022 01:00:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7557206970915661 -> 0.7578648123139762 on epoch=124, global_step=2000
06/14/2022 01:00:55 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.33 on epoch=125
06/14/2022 01:00:57 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.31 on epoch=126
06/14/2022 01:00:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=126
06/14/2022 01:01:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.31 on epoch=127
06/14/2022 01:01:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.33 on epoch=128
06/14/2022 01:01:08 - INFO - __main__ - Global step 2050 Train loss 0.33 Classification-F1 0.7743345849476138 on epoch=128
06/14/2022 01:01:08 - INFO - __main__ - Saving model with best Classification-F1: 0.7578648123139762 -> 0.7743345849476138 on epoch=128, global_step=2050
06/14/2022 01:01:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.32 on epoch=128
06/14/2022 01:01:12 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.31 on epoch=129
06/14/2022 01:01:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.27 on epoch=129
06/14/2022 01:01:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=130
06/14/2022 01:01:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.31 on epoch=131
06/14/2022 01:01:23 - INFO - __main__ - Global step 2100 Train loss 0.31 Classification-F1 0.7486098326989898 on epoch=131
06/14/2022 01:01:25 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.36 on epoch=131
06/14/2022 01:01:28 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.31 on epoch=132
06/14/2022 01:01:30 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=133
06/14/2022 01:01:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.33 on epoch=133
06/14/2022 01:01:35 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.30 on epoch=134
06/14/2022 01:01:38 - INFO - __main__ - Global step 2150 Train loss 0.35 Classification-F1 0.7452995755551713 on epoch=134
06/14/2022 01:01:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.34 on epoch=134
06/14/2022 01:01:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
06/14/2022 01:01:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.35 on epoch=136
06/14/2022 01:01:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.29 on epoch=136
06/14/2022 01:01:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.22 on epoch=137
06/14/2022 01:01:53 - INFO - __main__ - Global step 2200 Train loss 0.29 Classification-F1 0.6851469541757738 on epoch=137
06/14/2022 01:01:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.28 on epoch=138
06/14/2022 01:01:58 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.29 on epoch=138
06/14/2022 01:02:00 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.28 on epoch=139
06/14/2022 01:02:03 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.35 on epoch=139
06/14/2022 01:02:05 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.30 on epoch=140
06/14/2022 01:02:08 - INFO - __main__ - Global step 2250 Train loss 0.30 Classification-F1 0.7448039026245964 on epoch=140
06/14/2022 01:02:11 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.35 on epoch=141
06/14/2022 01:02:13 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.27 on epoch=141
06/14/2022 01:02:16 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.22 on epoch=142
06/14/2022 01:02:18 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.27 on epoch=143
06/14/2022 01:02:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.49 on epoch=143
06/14/2022 01:02:24 - INFO - __main__ - Global step 2300 Train loss 0.32 Classification-F1 0.6546392877772256 on epoch=143
06/14/2022 01:02:26 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.25 on epoch=144
06/14/2022 01:02:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.28 on epoch=144
06/14/2022 01:02:31 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.27 on epoch=145
06/14/2022 01:02:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=146
06/14/2022 01:02:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.27 on epoch=146
06/14/2022 01:02:39 - INFO - __main__ - Global step 2350 Train loss 0.27 Classification-F1 0.683213049761102 on epoch=146
06/14/2022 01:02:41 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.23 on epoch=147
06/14/2022 01:02:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.33 on epoch=148
06/14/2022 01:02:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.29 on epoch=148
06/14/2022 01:02:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.24 on epoch=149
06/14/2022 01:02:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.28 on epoch=149
06/14/2022 01:02:54 - INFO - __main__ - Global step 2400 Train loss 0.27 Classification-F1 0.747395722446952 on epoch=149
06/14/2022 01:02:56 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.28 on epoch=150
06/14/2022 01:02:59 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.26 on epoch=151
06/14/2022 01:03:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.27 on epoch=151
06/14/2022 01:03:04 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=152
06/14/2022 01:03:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=153
06/14/2022 01:03:09 - INFO - __main__ - Global step 2450 Train loss 0.27 Classification-F1 0.7221107399534366 on epoch=153
06/14/2022 01:03:12 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.32 on epoch=153
06/14/2022 01:03:14 - INFO - __main__ - Step 2470 Global step 2470 Train loss 1.61 on epoch=154
06/14/2022 01:03:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 1.00 on epoch=154
06/14/2022 01:03:19 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=155
06/14/2022 01:03:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.34 on epoch=156
06/14/2022 01:03:25 - INFO - __main__ - Global step 2500 Train loss 0.74 Classification-F1 0.7359812014471239 on epoch=156
06/14/2022 01:03:27 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.33 on epoch=156
06/14/2022 01:03:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=157
06/14/2022 01:03:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=158
06/14/2022 01:03:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.34 on epoch=158
06/14/2022 01:03:36 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.21 on epoch=159
06/14/2022 01:03:40 - INFO - __main__ - Global step 2550 Train loss 0.28 Classification-F1 0.7308852660207283 on epoch=159
06/14/2022 01:03:42 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.27 on epoch=159
06/14/2022 01:03:44 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.23 on epoch=160
06/14/2022 01:03:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.27 on epoch=161
06/14/2022 01:03:49 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=161
06/14/2022 01:03:52 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.24 on epoch=162
06/14/2022 01:03:55 - INFO - __main__ - Global step 2600 Train loss 0.24 Classification-F1 0.7269868172443761 on epoch=162
06/14/2022 01:03:57 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=163
06/14/2022 01:04:00 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=163
06/14/2022 01:04:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=164
06/14/2022 01:04:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.32 on epoch=164
06/14/2022 01:04:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.30 on epoch=165
06/14/2022 01:04:10 - INFO - __main__ - Global step 2650 Train loss 0.29 Classification-F1 0.734327658540475 on epoch=165
06/14/2022 01:04:13 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.28 on epoch=166
06/14/2022 01:04:15 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.30 on epoch=166
06/14/2022 01:04:17 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.24 on epoch=167
06/14/2022 01:04:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.34 on epoch=168
06/14/2022 01:04:22 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.29 on epoch=168
06/14/2022 01:04:25 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.7212383988833894 on epoch=168
06/14/2022 01:04:28 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.21 on epoch=169
06/14/2022 01:04:30 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.28 on epoch=169
06/14/2022 01:04:33 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=170
06/14/2022 01:04:35 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.24 on epoch=171
06/14/2022 01:04:37 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.26 on epoch=171
06/14/2022 01:04:41 - INFO - __main__ - Global step 2750 Train loss 0.25 Classification-F1 0.7315018009730417 on epoch=171
06/14/2022 01:04:43 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=172
06/14/2022 01:04:45 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.24 on epoch=173
06/14/2022 01:04:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.23 on epoch=173
06/14/2022 01:04:50 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.24 on epoch=174
06/14/2022 01:04:53 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.25 on epoch=174
06/14/2022 01:04:56 - INFO - __main__ - Global step 2800 Train loss 0.23 Classification-F1 0.7254748929859293 on epoch=174
06/14/2022 01:04:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=175
06/14/2022 01:05:01 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.25 on epoch=176
06/14/2022 01:05:03 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.24 on epoch=176
06/14/2022 01:05:05 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=177
06/14/2022 01:05:08 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.26 on epoch=178
06/14/2022 01:05:11 - INFO - __main__ - Global step 2850 Train loss 0.23 Classification-F1 0.7284412385307961 on epoch=178
06/14/2022 01:05:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.29 on epoch=178
06/14/2022 01:05:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=179
06/14/2022 01:05:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.31 on epoch=179
06/14/2022 01:05:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=180
06/14/2022 01:05:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=181
06/14/2022 01:05:26 - INFO - __main__ - Global step 2900 Train loss 0.25 Classification-F1 0.7243615047566415 on epoch=181
06/14/2022 01:05:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=181
06/14/2022 01:05:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=182
06/14/2022 01:05:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.27 on epoch=183
06/14/2022 01:05:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=183
06/14/2022 01:05:38 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.14 on epoch=184
06/14/2022 01:05:42 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.720523319379109 on epoch=184
06/14/2022 01:05:44 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.25 on epoch=184
06/14/2022 01:05:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=185
06/14/2022 01:05:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.16 on epoch=186
06/14/2022 01:05:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.27 on epoch=186
06/14/2022 01:05:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
06/14/2022 01:05:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:05:55 - INFO - __main__ - Printing 3 examples
06/14/2022 01:05:55 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:05:55 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:05:55 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:05:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:05:55 - INFO - __main__ - Printing 3 examples
06/14/2022 01:05:55 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:05:55 - INFO - __main__ - ['happy']
06/14/2022 01:05:55 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:05:55 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:05:56 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:05:57 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.7201044729917969 on epoch=187
06/14/2022 01:05:57 - INFO - __main__ - save last model!
06/14/2022 01:05:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 01:05:57 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 01:05:57 - INFO - __main__ - Printing 3 examples
06/14/2022 01:05:57 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 01:05:57 - INFO - __main__ - ['others']
06/14/2022 01:05:57 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 01:05:57 - INFO - __main__ - ['others']
06/14/2022 01:05:57 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 01:05:57 - INFO - __main__ - ['others']
06/14/2022 01:05:57 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:05:59 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:06:05 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 01:06:14 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:06:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:06:15 - INFO - __main__ - Starting training!
06/14/2022 01:07:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_42_0.5_8_predictions.txt
06/14/2022 01:07:17 - INFO - __main__ - Classification-F1 on test data: 0.3026
06/14/2022 01:07:17 - INFO - __main__ - prefix=emo_64_42, lr=0.5, bsz=8, dev_performance=0.7743345849476138, test_performance=0.302638286083325
06/14/2022 01:07:17 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.4, bsz=8 ...
06/14/2022 01:07:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:07:18 - INFO - __main__ - Printing 3 examples
06/14/2022 01:07:18 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:07:18 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:07:18 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:07:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:07:18 - INFO - __main__ - Printing 3 examples
06/14/2022 01:07:18 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:07:18 - INFO - __main__ - ['happy']
06/14/2022 01:07:18 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:07:18 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:07:19 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:07:37 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:07:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:07:38 - INFO - __main__ - Starting training!
06/14/2022 01:07:42 - INFO - __main__ - Step 10 Global step 10 Train loss 2.52 on epoch=0
06/14/2022 01:07:44 - INFO - __main__ - Step 20 Global step 20 Train loss 1.30 on epoch=1
06/14/2022 01:07:47 - INFO - __main__ - Step 30 Global step 30 Train loss 1.12 on epoch=1
06/14/2022 01:07:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.98 on epoch=2
06/14/2022 01:07:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.90 on epoch=3
06/14/2022 01:07:55 - INFO - __main__ - Global step 50 Train loss 1.36 Classification-F1 0.1 on epoch=3
06/14/2022 01:07:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1 on epoch=3, global_step=50
06/14/2022 01:07:58 - INFO - __main__ - Step 60 Global step 60 Train loss 0.88 on epoch=3
06/14/2022 01:08:00 - INFO - __main__ - Step 70 Global step 70 Train loss 0.88 on epoch=4
06/14/2022 01:08:03 - INFO - __main__ - Step 80 Global step 80 Train loss 0.89 on epoch=4
06/14/2022 01:08:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=5
06/14/2022 01:08:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=6
06/14/2022 01:08:11 - INFO - __main__ - Global step 100 Train loss 0.87 Classification-F1 0.4971015751168709 on epoch=6
06/14/2022 01:08:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1 -> 0.4971015751168709 on epoch=6, global_step=100
06/14/2022 01:08:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.83 on epoch=6
06/14/2022 01:08:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.79 on epoch=7
06/14/2022 01:08:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.81 on epoch=8
06/14/2022 01:08:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.88 on epoch=8
06/14/2022 01:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.77 on epoch=9
06/14/2022 01:08:27 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.43673469387755104 on epoch=9
06/14/2022 01:08:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=9
06/14/2022 01:08:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.76 on epoch=10
06/14/2022 01:08:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.79 on epoch=11
06/14/2022 01:08:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.71 on epoch=11
06/14/2022 01:08:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=12
06/14/2022 01:08:42 - INFO - __main__ - Global step 200 Train loss 0.75 Classification-F1 0.4539900888585099 on epoch=12
06/14/2022 01:08:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.70 on epoch=13
06/14/2022 01:08:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.73 on epoch=13
06/14/2022 01:08:50 - INFO - __main__ - Step 230 Global step 230 Train loss 0.64 on epoch=14
06/14/2022 01:08:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=14
06/14/2022 01:08:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=15
06/14/2022 01:08:58 - INFO - __main__ - Global step 250 Train loss 0.68 Classification-F1 0.6826818974666806 on epoch=15
06/14/2022 01:08:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4971015751168709 -> 0.6826818974666806 on epoch=15, global_step=250
06/14/2022 01:09:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.66 on epoch=16
06/14/2022 01:09:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.58 on epoch=16
06/14/2022 01:09:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.63 on epoch=17
06/14/2022 01:09:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=18
06/14/2022 01:09:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.59 on epoch=18
06/14/2022 01:09:14 - INFO - __main__ - Global step 300 Train loss 0.61 Classification-F1 0.3800551768906199 on epoch=18
06/14/2022 01:09:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=19
06/14/2022 01:09:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=19
06/14/2022 01:09:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=20
06/14/2022 01:09:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=21
06/14/2022 01:09:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.63 on epoch=21
06/14/2022 01:09:29 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.6566209151946691 on epoch=21
06/14/2022 01:09:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=22
06/14/2022 01:09:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=23
06/14/2022 01:09:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=23
06/14/2022 01:09:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.55 on epoch=24
06/14/2022 01:09:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=24
06/14/2022 01:09:45 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.7558637070000707 on epoch=24
06/14/2022 01:09:45 - INFO - __main__ - Saving model with best Classification-F1: 0.6826818974666806 -> 0.7558637070000707 on epoch=24, global_step=400
06/14/2022 01:09:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=25
06/14/2022 01:09:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=26
06/14/2022 01:09:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.51 on epoch=26
06/14/2022 01:09:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=27
06/14/2022 01:09:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=28
06/14/2022 01:10:01 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.7239747669275507 on epoch=28
06/14/2022 01:10:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=28
06/14/2022 01:10:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=29
06/14/2022 01:10:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=29
06/14/2022 01:10:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.48 on epoch=30
06/14/2022 01:10:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=31
06/14/2022 01:10:17 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.7253252565781954 on epoch=31
06/14/2022 01:10:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=31
06/14/2022 01:10:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=32
06/14/2022 01:10:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=33
06/14/2022 01:10:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=33
06/14/2022 01:10:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=34
06/14/2022 01:10:32 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.7439399913251874 on epoch=34
06/14/2022 01:10:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=34
06/14/2022 01:10:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.40 on epoch=35
06/14/2022 01:10:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=36
06/14/2022 01:10:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=36
06/14/2022 01:10:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=37
06/14/2022 01:10:48 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.7023145053475935 on epoch=37
06/14/2022 01:10:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
06/14/2022 01:10:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=38
06/14/2022 01:10:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=39
06/14/2022 01:10:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=39
06/14/2022 01:11:00 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=40
06/14/2022 01:11:04 - INFO - __main__ - Global step 650 Train loss 0.40 Classification-F1 0.7531463179571314 on epoch=40
06/14/2022 01:11:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=41
06/14/2022 01:11:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/14/2022 01:11:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=42
06/14/2022 01:11:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=43
06/14/2022 01:11:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=43
06/14/2022 01:11:19 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.6418564302754283 on epoch=43
06/14/2022 01:11:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
06/14/2022 01:11:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=44
06/14/2022 01:11:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.37 on epoch=45
06/14/2022 01:11:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=46
06/14/2022 01:11:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.39 on epoch=46
06/14/2022 01:11:35 - INFO - __main__ - Global step 750 Train loss 0.36 Classification-F1 0.7355326063977463 on epoch=46
06/14/2022 01:11:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.36 on epoch=47
06/14/2022 01:11:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=48
06/14/2022 01:11:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=48
06/14/2022 01:11:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.31 on epoch=49
06/14/2022 01:11:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=49
06/14/2022 01:11:51 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.7573793847552985 on epoch=49
06/14/2022 01:11:51 - INFO - __main__ - Saving model with best Classification-F1: 0.7558637070000707 -> 0.7573793847552985 on epoch=49, global_step=800
06/14/2022 01:11:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=50
06/14/2022 01:11:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=51
06/14/2022 01:11:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=51
06/14/2022 01:12:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=52
06/14/2022 01:12:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=53
06/14/2022 01:12:06 - INFO - __main__ - Global step 850 Train loss 0.33 Classification-F1 0.7190570059219856 on epoch=53
06/14/2022 01:12:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=53
06/14/2022 01:12:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=54
06/14/2022 01:12:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=54
06/14/2022 01:12:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=55
06/14/2022 01:12:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=56
06/14/2022 01:12:22 - INFO - __main__ - Global step 900 Train loss 0.32 Classification-F1 0.8113591238940527 on epoch=56
06/14/2022 01:12:22 - INFO - __main__ - Saving model with best Classification-F1: 0.7573793847552985 -> 0.8113591238940527 on epoch=56, global_step=900
06/14/2022 01:12:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.34 on epoch=56
06/14/2022 01:12:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.28 on epoch=57
06/14/2022 01:12:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=58
06/14/2022 01:12:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=58
06/14/2022 01:12:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=59
06/14/2022 01:12:38 - INFO - __main__ - Global step 950 Train loss 0.33 Classification-F1 0.7222480595291552 on epoch=59
06/14/2022 01:12:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=59
06/14/2022 01:12:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=60
06/14/2022 01:12:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=61
06/14/2022 01:12:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=61
06/14/2022 01:12:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=62
06/14/2022 01:12:53 - INFO - __main__ - Global step 1000 Train loss 0.30 Classification-F1 0.7346364895498241 on epoch=62
06/14/2022 01:12:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=63
06/14/2022 01:12:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=63
06/14/2022 01:13:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=64
06/14/2022 01:13:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=64
06/14/2022 01:13:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=65
06/14/2022 01:13:09 - INFO - __main__ - Global step 1050 Train loss 0.29 Classification-F1 0.7634695612787179 on epoch=65
06/14/2022 01:13:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=66
06/14/2022 01:13:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=66
06/14/2022 01:13:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=67
06/14/2022 01:13:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=68
06/14/2022 01:13:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=68
06/14/2022 01:13:25 - INFO - __main__ - Global step 1100 Train loss 0.27 Classification-F1 0.6758089536101086 on epoch=68
06/14/2022 01:13:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=69
06/14/2022 01:13:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=69
06/14/2022 01:13:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=70
06/14/2022 01:13:35 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=71
06/14/2022 01:13:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=71
06/14/2022 01:13:40 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.7317856823478004 on epoch=71
06/14/2022 01:13:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=72
06/14/2022 01:13:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=73
06/14/2022 01:13:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.29 on epoch=73
06/14/2022 01:13:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=74
06/14/2022 01:13:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=74
06/14/2022 01:13:56 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.7737251422628376 on epoch=74
06/14/2022 01:13:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=75
06/14/2022 01:14:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=76
06/14/2022 01:14:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=76
06/14/2022 01:14:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=77
06/14/2022 01:14:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
06/14/2022 01:14:12 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.7915094122678495 on epoch=78
06/14/2022 01:14:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=78
06/14/2022 01:14:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=79
06/14/2022 01:14:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=79
06/14/2022 01:14:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=80
06/14/2022 01:14:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=81
06/14/2022 01:14:27 - INFO - __main__ - Global step 1300 Train loss 0.23 Classification-F1 0.7257535269553088 on epoch=81
06/14/2022 01:14:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.27 on epoch=81
06/14/2022 01:14:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=82
06/14/2022 01:14:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=83
06/14/2022 01:14:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=83
06/14/2022 01:14:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=84
06/14/2022 01:14:43 - INFO - __main__ - Global step 1350 Train loss 0.25 Classification-F1 0.7507149156800386 on epoch=84
06/14/2022 01:14:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=84
06/14/2022 01:14:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=85
06/14/2022 01:14:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=86
06/14/2022 01:14:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=86
06/14/2022 01:14:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=87
06/14/2022 01:14:59 - INFO - __main__ - Global step 1400 Train loss 0.25 Classification-F1 0.7080234600991626 on epoch=87
06/14/2022 01:15:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=88
06/14/2022 01:15:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=88
06/14/2022 01:15:06 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=89
06/14/2022 01:15:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=89
06/14/2022 01:15:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=90
06/14/2022 01:15:15 - INFO - __main__ - Global step 1450 Train loss 0.23 Classification-F1 0.7517526836209071 on epoch=90
06/14/2022 01:15:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
06/14/2022 01:15:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=91
06/14/2022 01:15:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=92
06/14/2022 01:15:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=93
06/14/2022 01:15:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=93
06/14/2022 01:15:31 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.7151960784313725 on epoch=93
06/14/2022 01:15:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=94
06/14/2022 01:15:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=94
06/14/2022 01:15:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=95
06/14/2022 01:15:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
06/14/2022 01:15:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=96
06/14/2022 01:15:46 - INFO - __main__ - Global step 1550 Train loss 0.23 Classification-F1 0.7048911388864085 on epoch=96
06/14/2022 01:15:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=97
06/14/2022 01:15:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=98
06/14/2022 01:15:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
06/14/2022 01:15:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=99
06/14/2022 01:15:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=99
06/14/2022 01:16:02 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.8001865528726595 on epoch=99
06/14/2022 01:16:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
06/14/2022 01:16:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
06/14/2022 01:16:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=101
06/14/2022 01:16:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=102
06/14/2022 01:16:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
06/14/2022 01:16:18 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.7720850801460297 on epoch=103
06/14/2022 01:16:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=103
06/14/2022 01:16:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=104
06/14/2022 01:16:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
06/14/2022 01:16:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=105
06/14/2022 01:16:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=106
06/14/2022 01:16:34 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.7662907800303015 on epoch=106
06/14/2022 01:16:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=106
06/14/2022 01:16:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=107
06/14/2022 01:16:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=108
06/14/2022 01:16:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
06/14/2022 01:16:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=109
06/14/2022 01:16:49 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.7424793740848772 on epoch=109
06/14/2022 01:16:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
06/14/2022 01:16:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=110
06/14/2022 01:16:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=111
06/14/2022 01:16:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=111
06/14/2022 01:17:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=112
06/14/2022 01:17:05 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.74436545387056 on epoch=112
06/14/2022 01:17:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=113
06/14/2022 01:17:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=113
06/14/2022 01:17:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=114
06/14/2022 01:17:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=114
06/14/2022 01:17:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=115
06/14/2022 01:17:21 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.7550287091070247 on epoch=115
06/14/2022 01:17:23 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=116
06/14/2022 01:17:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.27 on epoch=116
06/14/2022 01:17:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=117
06/14/2022 01:17:31 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.16 on epoch=118
06/14/2022 01:17:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
06/14/2022 01:17:37 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.728492951907131 on epoch=118
06/14/2022 01:17:39 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=119
06/14/2022 01:17:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=119
06/14/2022 01:17:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=120
06/14/2022 01:17:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=121
06/14/2022 01:17:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=121
06/14/2022 01:17:52 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.7582971726106201 on epoch=121
06/14/2022 01:17:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=122
06/14/2022 01:17:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=123
06/14/2022 01:18:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
06/14/2022 01:18:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=124
06/14/2022 01:18:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=124
06/14/2022 01:18:08 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.765196165003234 on epoch=124
06/14/2022 01:18:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.22 on epoch=125
06/14/2022 01:18:13 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
06/14/2022 01:18:16 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=126
06/14/2022 01:18:18 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=127
06/14/2022 01:18:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=128
06/14/2022 01:18:24 - INFO - __main__ - Global step 2050 Train loss 0.17 Classification-F1 0.7750984343184266 on epoch=128
06/14/2022 01:18:27 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.26 on epoch=128
06/14/2022 01:18:29 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=129
06/14/2022 01:18:32 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
06/14/2022 01:18:34 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.14 on epoch=130
06/14/2022 01:18:37 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.18 on epoch=131
06/14/2022 01:18:40 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.7373081515340111 on epoch=131
06/14/2022 01:18:43 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
06/14/2022 01:18:45 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=132
06/14/2022 01:18:48 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=133
06/14/2022 01:18:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
06/14/2022 01:18:53 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
06/14/2022 01:18:56 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.7517849131162534 on epoch=134
06/14/2022 01:18:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=134
06/14/2022 01:19:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.23 on epoch=135
06/14/2022 01:19:04 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
06/14/2022 01:19:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.20 on epoch=136
06/14/2022 01:19:09 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=137
06/14/2022 01:19:12 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.7173329824908878 on epoch=137
06/14/2022 01:19:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.17 on epoch=138
06/14/2022 01:19:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.15 on epoch=138
06/14/2022 01:19:20 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=139
06/14/2022 01:19:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.15 on epoch=139
06/14/2022 01:19:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.15 on epoch=140
06/14/2022 01:19:28 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.7302252081498595 on epoch=140
06/14/2022 01:19:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.15 on epoch=141
06/14/2022 01:19:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
06/14/2022 01:19:36 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
06/14/2022 01:19:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=143
06/14/2022 01:19:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.19 on epoch=143
06/14/2022 01:19:44 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.6862233954792938 on epoch=143
06/14/2022 01:19:47 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
06/14/2022 01:19:49 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=144
06/14/2022 01:19:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.14 on epoch=145
06/14/2022 01:19:54 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
06/14/2022 01:19:57 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=146
06/14/2022 01:20:00 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.7696046744069827 on epoch=146
06/14/2022 01:20:03 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.08 on epoch=147
06/14/2022 01:20:05 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.14 on epoch=148
06/14/2022 01:20:08 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=148
06/14/2022 01:20:10 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
06/14/2022 01:20:13 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
06/14/2022 01:20:16 - INFO - __main__ - Global step 2400 Train loss 0.11 Classification-F1 0.792556785524682 on epoch=149
06/14/2022 01:20:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
06/14/2022 01:20:21 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.13 on epoch=151
06/14/2022 01:20:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=151
06/14/2022 01:20:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=152
06/14/2022 01:20:29 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.14 on epoch=153
06/14/2022 01:20:32 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7527019791253242 on epoch=153
06/14/2022 01:20:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=153
06/14/2022 01:20:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
06/14/2022 01:20:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=154
06/14/2022 01:20:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
06/14/2022 01:20:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
06/14/2022 01:20:48 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.7807123069550824 on epoch=156
06/14/2022 01:20:51 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
06/14/2022 01:20:53 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=157
06/14/2022 01:20:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=158
06/14/2022 01:20:58 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=158
06/14/2022 01:21:01 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=159
06/14/2022 01:21:04 - INFO - __main__ - Global step 2550 Train loss 0.12 Classification-F1 0.7679767600820233 on epoch=159
06/14/2022 01:21:06 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=159
06/14/2022 01:21:09 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=160
06/14/2022 01:21:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=161
06/14/2022 01:21:14 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.18 on epoch=161
06/14/2022 01:21:16 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=162
06/14/2022 01:21:20 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.7104731691725514 on epoch=162
06/14/2022 01:21:22 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.14 on epoch=163
06/14/2022 01:21:25 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.11 on epoch=163
06/14/2022 01:21:27 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=164
06/14/2022 01:21:30 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.13 on epoch=164
06/14/2022 01:21:32 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=165
06/14/2022 01:21:36 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.7961692789968653 on epoch=165
06/14/2022 01:21:38 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/14/2022 01:21:41 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.17 on epoch=166
06/14/2022 01:21:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=167
06/14/2022 01:21:46 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.09 on epoch=168
06/14/2022 01:21:48 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.13 on epoch=168
06/14/2022 01:21:52 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.7677584102054911 on epoch=168
06/14/2022 01:21:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=169
06/14/2022 01:21:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.09 on epoch=169
06/14/2022 01:21:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=170
06/14/2022 01:22:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.10 on epoch=171
06/14/2022 01:22:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.09 on epoch=171
06/14/2022 01:22:08 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.7631227355072464 on epoch=171
06/14/2022 01:22:10 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.19 on epoch=172
06/14/2022 01:22:13 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.32 on epoch=173
06/14/2022 01:22:15 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.12 on epoch=173
06/14/2022 01:22:18 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=174
06/14/2022 01:22:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.13 on epoch=174
06/14/2022 01:22:24 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.7573902078002028 on epoch=174
06/14/2022 01:22:26 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.14 on epoch=175
06/14/2022 01:22:29 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.10 on epoch=176
06/14/2022 01:22:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.12 on epoch=176
06/14/2022 01:22:34 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
06/14/2022 01:22:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.13 on epoch=178
06/14/2022 01:22:40 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.7956582291868869 on epoch=178
06/14/2022 01:22:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.12 on epoch=178
06/14/2022 01:22:45 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.03 on epoch=179
06/14/2022 01:22:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.12 on epoch=179
06/14/2022 01:22:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.09 on epoch=180
06/14/2022 01:22:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.11 on epoch=181
06/14/2022 01:22:56 - INFO - __main__ - Global step 2900 Train loss 0.09 Classification-F1 0.7808159722222222 on epoch=181
06/14/2022 01:22:58 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.17 on epoch=181
06/14/2022 01:23:01 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=182
06/14/2022 01:23:03 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.14 on epoch=183
06/14/2022 01:23:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
06/14/2022 01:23:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
06/14/2022 01:23:12 - INFO - __main__ - Global step 2950 Train loss 0.11 Classification-F1 0.7637890056754155 on epoch=184
06/14/2022 01:23:14 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.10 on epoch=184
06/14/2022 01:23:17 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.15 on epoch=185
06/14/2022 01:23:19 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
06/14/2022 01:23:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.08 on epoch=186
06/14/2022 01:23:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.14 on epoch=187
06/14/2022 01:23:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:23:26 - INFO - __main__ - Printing 3 examples
06/14/2022 01:23:26 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:23:26 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:23:26 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:23:26 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:23:26 - INFO - __main__ - Printing 3 examples
06/14/2022 01:23:26 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:23:26 - INFO - __main__ - ['happy']
06/14/2022 01:23:26 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:23:26 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:23:26 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:23:28 - INFO - __main__ - Global step 3000 Train loss 0.10 Classification-F1 0.8020752239079336 on epoch=187
06/14/2022 01:23:28 - INFO - __main__ - save last model!
06/14/2022 01:23:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 01:23:28 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 01:23:28 - INFO - __main__ - Printing 3 examples
06/14/2022 01:23:28 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 01:23:28 - INFO - __main__ - ['others']
06/14/2022 01:23:28 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 01:23:28 - INFO - __main__ - ['others']
06/14/2022 01:23:28 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 01:23:28 - INFO - __main__ - ['others']
06/14/2022 01:23:28 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:23:30 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:23:35 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 01:23:45 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:23:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:23:46 - INFO - __main__ - Starting training!
06/14/2022 01:24:47 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_42_0.4_8_predictions.txt
06/14/2022 01:24:48 - INFO - __main__ - Classification-F1 on test data: 0.4631
06/14/2022 01:24:48 - INFO - __main__ - prefix=emo_64_42, lr=0.4, bsz=8, dev_performance=0.8113591238940527, test_performance=0.4631298600162911
06/14/2022 01:24:48 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.3, bsz=8 ...
06/14/2022 01:24:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:24:49 - INFO - __main__ - Printing 3 examples
06/14/2022 01:24:49 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:24:49 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:24:49 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:24:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:24:49 - INFO - __main__ - Printing 3 examples
06/14/2022 01:24:49 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:24:49 - INFO - __main__ - ['happy']
06/14/2022 01:24:49 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:24:49 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:24:50 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:25:05 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:25:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:25:06 - INFO - __main__ - Starting training!
06/14/2022 01:25:09 - INFO - __main__ - Step 10 Global step 10 Train loss 2.73 on epoch=0
06/14/2022 01:25:11 - INFO - __main__ - Step 20 Global step 20 Train loss 1.51 on epoch=1
06/14/2022 01:25:14 - INFO - __main__ - Step 30 Global step 30 Train loss 1.07 on epoch=1
06/14/2022 01:25:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.93 on epoch=2
06/14/2022 01:25:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.95 on epoch=3
06/14/2022 01:25:22 - INFO - __main__ - Global step 50 Train loss 1.44 Classification-F1 0.15117521367521367 on epoch=3
06/14/2022 01:25:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.15117521367521367 on epoch=3, global_step=50
06/14/2022 01:25:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=3
06/14/2022 01:25:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.89 on epoch=4
06/14/2022 01:25:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.86 on epoch=4
06/14/2022 01:25:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.87 on epoch=5
06/14/2022 01:25:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.92 on epoch=6
06/14/2022 01:25:38 - INFO - __main__ - Global step 100 Train loss 0.90 Classification-F1 0.3285405071119357 on epoch=6
06/14/2022 01:25:38 - INFO - __main__ - Saving model with best Classification-F1: 0.15117521367521367 -> 0.3285405071119357 on epoch=6, global_step=100
06/14/2022 01:25:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.86 on epoch=6
06/14/2022 01:25:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=7
06/14/2022 01:25:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=8
06/14/2022 01:25:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=8
06/14/2022 01:25:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.85 on epoch=9
06/14/2022 01:25:54 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.43718495823758985 on epoch=9
06/14/2022 01:25:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3285405071119357 -> 0.43718495823758985 on epoch=9, global_step=150
06/14/2022 01:25:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
06/14/2022 01:25:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.83 on epoch=10
06/14/2022 01:26:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.77 on epoch=11
06/14/2022 01:26:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=11
06/14/2022 01:26:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.78 on epoch=12
06/14/2022 01:26:09 - INFO - __main__ - Global step 200 Train loss 0.80 Classification-F1 0.4262581294797855 on epoch=12
06/14/2022 01:26:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.83 on epoch=13
06/14/2022 01:26:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.85 on epoch=13
06/14/2022 01:26:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=14
06/14/2022 01:26:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.70 on epoch=14
06/14/2022 01:26:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.74 on epoch=15
06/14/2022 01:26:25 - INFO - __main__ - Global step 250 Train loss 0.78 Classification-F1 0.461367044714207 on epoch=15
06/14/2022 01:26:25 - INFO - __main__ - Saving model with best Classification-F1: 0.43718495823758985 -> 0.461367044714207 on epoch=15, global_step=250
06/14/2022 01:26:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.76 on epoch=16
06/14/2022 01:26:30 - INFO - __main__ - Step 270 Global step 270 Train loss 0.76 on epoch=16
06/14/2022 01:26:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.64 on epoch=17
06/14/2022 01:26:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.67 on epoch=18
06/14/2022 01:26:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.70 on epoch=18
06/14/2022 01:26:41 - INFO - __main__ - Global step 300 Train loss 0.70 Classification-F1 0.3697818369500996 on epoch=18
06/14/2022 01:26:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=19
06/14/2022 01:26:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=19
06/14/2022 01:26:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.71 on epoch=20
06/14/2022 01:26:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.70 on epoch=21
06/14/2022 01:26:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.65 on epoch=21
06/14/2022 01:26:56 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.4719129554655871 on epoch=21
06/14/2022 01:26:56 - INFO - __main__ - Saving model with best Classification-F1: 0.461367044714207 -> 0.4719129554655871 on epoch=21, global_step=350
06/14/2022 01:26:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.68 on epoch=22
06/14/2022 01:27:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.66 on epoch=23
06/14/2022 01:27:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.68 on epoch=23
06/14/2022 01:27:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=24
06/14/2022 01:27:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.68 on epoch=24
06/14/2022 01:27:12 - INFO - __main__ - Global step 400 Train loss 0.66 Classification-F1 0.7114599631634305 on epoch=24
06/14/2022 01:27:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4719129554655871 -> 0.7114599631634305 on epoch=24, global_step=400
06/14/2022 01:27:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=25
06/14/2022 01:27:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
06/14/2022 01:27:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.59 on epoch=26
06/14/2022 01:27:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=27
06/14/2022 01:27:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.58 on epoch=28
06/14/2022 01:27:28 - INFO - __main__ - Global step 450 Train loss 0.59 Classification-F1 0.670436846708033 on epoch=28
06/14/2022 01:27:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.69 on epoch=28
06/14/2022 01:27:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=29
06/14/2022 01:27:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.55 on epoch=29
06/14/2022 01:27:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=30
06/14/2022 01:27:40 - INFO - __main__ - Step 500 Global step 500 Train loss 0.54 on epoch=31
06/14/2022 01:27:43 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.6494217801872597 on epoch=31
06/14/2022 01:27:46 - INFO - __main__ - Step 510 Global step 510 Train loss 0.66 on epoch=31
06/14/2022 01:27:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=32
06/14/2022 01:27:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.51 on epoch=33
06/14/2022 01:27:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=33
06/14/2022 01:27:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=34
06/14/2022 01:27:59 - INFO - __main__ - Global step 550 Train loss 0.55 Classification-F1 0.7016240709109084 on epoch=34
06/14/2022 01:28:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=34
06/14/2022 01:28:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=35
06/14/2022 01:28:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=36
06/14/2022 01:28:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=36
06/14/2022 01:28:11 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=37
06/14/2022 01:28:15 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.5527030856988046 on epoch=37
06/14/2022 01:28:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=38
06/14/2022 01:28:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=38
06/14/2022 01:28:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=39
06/14/2022 01:28:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.51 on epoch=39
06/14/2022 01:28:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=40
06/14/2022 01:28:31 - INFO - __main__ - Global step 650 Train loss 0.52 Classification-F1 0.7169240157344083 on epoch=40
06/14/2022 01:28:31 - INFO - __main__ - Saving model with best Classification-F1: 0.7114599631634305 -> 0.7169240157344083 on epoch=40, global_step=650
06/14/2022 01:28:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=41
06/14/2022 01:28:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.47 on epoch=41
06/14/2022 01:28:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.38 on epoch=42
06/14/2022 01:28:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=43
06/14/2022 01:28:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.49 on epoch=43
06/14/2022 01:28:47 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.6461110331412563 on epoch=43
06/14/2022 01:28:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=44
06/14/2022 01:28:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.49 on epoch=44
06/14/2022 01:28:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.49 on epoch=45
06/14/2022 01:28:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.50 on epoch=46
06/14/2022 01:28:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=46
06/14/2022 01:29:02 - INFO - __main__ - Global step 750 Train loss 0.47 Classification-F1 0.6637146166895901 on epoch=46
06/14/2022 01:29:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.39 on epoch=47
06/14/2022 01:29:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=48
06/14/2022 01:29:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=48
06/14/2022 01:29:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=49
06/14/2022 01:29:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=49
06/14/2022 01:29:18 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.7438465061700354 on epoch=49
06/14/2022 01:29:18 - INFO - __main__ - Saving model with best Classification-F1: 0.7169240157344083 -> 0.7438465061700354 on epoch=49, global_step=800
06/14/2022 01:29:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=50
06/14/2022 01:29:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
06/14/2022 01:29:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=51
06/14/2022 01:29:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=52
06/14/2022 01:29:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.59 on epoch=53
06/14/2022 01:29:33 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.6855841526894159 on epoch=53
06/14/2022 01:29:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=53
06/14/2022 01:29:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=54
06/14/2022 01:29:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=54
06/14/2022 01:29:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=55
06/14/2022 01:29:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.32 on epoch=56
06/14/2022 01:29:48 - INFO - __main__ - Global step 900 Train loss 0.40 Classification-F1 0.6953325170997584 on epoch=56
06/14/2022 01:29:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.48 on epoch=56
06/14/2022 01:29:53 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=57
06/14/2022 01:29:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=58
06/14/2022 01:29:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=58
06/14/2022 01:30:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=59
06/14/2022 01:30:04 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.6830558388072541 on epoch=59
06/14/2022 01:30:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=59
06/14/2022 01:30:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
06/14/2022 01:30:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
06/14/2022 01:30:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=61
06/14/2022 01:30:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.33 on epoch=62
06/14/2022 01:30:19 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.6107913870246084 on epoch=62
06/14/2022 01:30:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=63
06/14/2022 01:30:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/14/2022 01:30:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.37 on epoch=64
06/14/2022 01:30:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.46 on epoch=64
06/14/2022 01:30:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=65
06/14/2022 01:30:35 - INFO - __main__ - Global step 1050 Train loss 0.38 Classification-F1 0.7253447801221937 on epoch=65
06/14/2022 01:30:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.39 on epoch=66
06/14/2022 01:30:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=66
06/14/2022 01:30:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=67
06/14/2022 01:30:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.37 on epoch=68
06/14/2022 01:30:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.39 on epoch=68
06/14/2022 01:30:51 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.6219580148512759 on epoch=68
06/14/2022 01:30:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.32 on epoch=69
06/14/2022 01:30:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.38 on epoch=69
06/14/2022 01:30:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=70
06/14/2022 01:31:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.30 on epoch=71
06/14/2022 01:31:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=71
06/14/2022 01:31:06 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.7218262871268702 on epoch=71
06/14/2022 01:31:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=72
06/14/2022 01:31:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.33 on epoch=73
06/14/2022 01:31:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
06/14/2022 01:31:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=74
06/14/2022 01:31:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
06/14/2022 01:31:22 - INFO - __main__ - Global step 1200 Train loss 0.32 Classification-F1 0.7269210681517868 on epoch=74
06/14/2022 01:31:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.29 on epoch=75
06/14/2022 01:31:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=76
06/14/2022 01:31:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.33 on epoch=76
06/14/2022 01:31:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=77
06/14/2022 01:31:34 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
06/14/2022 01:31:38 - INFO - __main__ - Global step 1250 Train loss 0.31 Classification-F1 0.7283068052121258 on epoch=78
06/14/2022 01:31:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=78
06/14/2022 01:31:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=79
06/14/2022 01:31:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
06/14/2022 01:31:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=80
06/14/2022 01:31:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.31 on epoch=81
06/14/2022 01:31:54 - INFO - __main__ - Global step 1300 Train loss 0.31 Classification-F1 0.7047712644121313 on epoch=81
06/14/2022 01:31:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=81
06/14/2022 01:31:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=82
06/14/2022 01:32:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=83
06/14/2022 01:32:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=83
06/14/2022 01:32:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=84
06/14/2022 01:32:10 - INFO - __main__ - Global step 1350 Train loss 0.34 Classification-F1 0.6849875085169203 on epoch=84
06/14/2022 01:32:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=84
06/14/2022 01:32:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=85
06/14/2022 01:32:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=86
06/14/2022 01:32:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=86
06/14/2022 01:32:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=87
06/14/2022 01:32:25 - INFO - __main__ - Global step 1400 Train loss 0.31 Classification-F1 0.7079634908164224 on epoch=87
06/14/2022 01:32:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=88
06/14/2022 01:32:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=88
06/14/2022 01:32:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=89
06/14/2022 01:32:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.26 on epoch=89
06/14/2022 01:32:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=90
06/14/2022 01:32:41 - INFO - __main__ - Global step 1450 Train loss 0.26 Classification-F1 0.712625007496082 on epoch=90
06/14/2022 01:32:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=91
06/14/2022 01:32:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
06/14/2022 01:32:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
06/14/2022 01:32:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=93
06/14/2022 01:32:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=93
06/14/2022 01:32:57 - INFO - __main__ - Global step 1500 Train loss 0.28 Classification-F1 0.6842512640287794 on epoch=93
06/14/2022 01:32:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=94
06/14/2022 01:33:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.29 on epoch=94
06/14/2022 01:33:04 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=95
06/14/2022 01:33:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=96
06/14/2022 01:33:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=96
06/14/2022 01:33:13 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.7327984234234234 on epoch=96
06/14/2022 01:33:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=97
06/14/2022 01:33:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.32 on epoch=98
06/14/2022 01:33:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=98
06/14/2022 01:33:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=99
06/14/2022 01:33:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=99
06/14/2022 01:33:29 - INFO - __main__ - Global step 1600 Train loss 0.29 Classification-F1 0.77539315245659 on epoch=99
06/14/2022 01:33:29 - INFO - __main__ - Saving model with best Classification-F1: 0.7438465061700354 -> 0.77539315245659 on epoch=99, global_step=1600
06/14/2022 01:33:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=100
06/14/2022 01:33:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=101
06/14/2022 01:33:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
06/14/2022 01:33:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=102
06/14/2022 01:33:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=103
06/14/2022 01:33:44 - INFO - __main__ - Global step 1650 Train loss 0.25 Classification-F1 0.7459808263530305 on epoch=103
06/14/2022 01:33:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=103
06/14/2022 01:33:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=104
06/14/2022 01:33:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=104
06/14/2022 01:33:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=105
06/14/2022 01:33:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=106
06/14/2022 01:34:00 - INFO - __main__ - Global step 1700 Train loss 0.24 Classification-F1 0.7148876890222288 on epoch=106
06/14/2022 01:34:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=106
06/14/2022 01:34:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=107
06/14/2022 01:34:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=108
06/14/2022 01:34:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=108
06/14/2022 01:34:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.19 on epoch=109
06/14/2022 01:34:16 - INFO - __main__ - Global step 1750 Train loss 0.24 Classification-F1 0.7748467450595109 on epoch=109
06/14/2022 01:34:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=109
06/14/2022 01:34:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.19 on epoch=110
06/14/2022 01:34:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=111
06/14/2022 01:34:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=111
06/14/2022 01:34:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=112
06/14/2022 01:34:32 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.739063975367512 on epoch=112
06/14/2022 01:34:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=113
06/14/2022 01:34:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=113
06/14/2022 01:34:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=114
06/14/2022 01:34:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=114
06/14/2022 01:34:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=115
06/14/2022 01:34:48 - INFO - __main__ - Global step 1850 Train loss 0.29 Classification-F1 0.7751994118868328 on epoch=115
06/14/2022 01:34:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=116
06/14/2022 01:34:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=116
06/14/2022 01:34:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
06/14/2022 01:34:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=118
06/14/2022 01:35:00 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=118
06/14/2022 01:35:03 - INFO - __main__ - Global step 1900 Train loss 0.23 Classification-F1 0.7061644468574201 on epoch=118
06/14/2022 01:35:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=119
06/14/2022 01:35:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=119
06/14/2022 01:35:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=120
06/14/2022 01:35:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=121
06/14/2022 01:35:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=121
06/14/2022 01:35:19 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.7515497471155366 on epoch=121
06/14/2022 01:35:22 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=122
06/14/2022 01:35:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=123
06/14/2022 01:35:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.17 on epoch=123
06/14/2022 01:35:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
06/14/2022 01:35:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=124
06/14/2022 01:35:35 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.7885119681811809 on epoch=124
06/14/2022 01:35:35 - INFO - __main__ - Saving model with best Classification-F1: 0.77539315245659 -> 0.7885119681811809 on epoch=124, global_step=2000
06/14/2022 01:35:38 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=125
06/14/2022 01:35:40 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.14 on epoch=126
06/14/2022 01:35:42 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.22 on epoch=126
06/14/2022 01:35:45 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=127
06/14/2022 01:35:47 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=128
06/14/2022 01:35:51 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.7611481403185894 on epoch=128
06/14/2022 01:35:53 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=128
06/14/2022 01:35:56 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=129
06/14/2022 01:35:58 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=129
06/14/2022 01:36:01 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.25 on epoch=130
06/14/2022 01:36:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=131
06/14/2022 01:36:07 - INFO - __main__ - Global step 2100 Train loss 0.22 Classification-F1 0.7726830408320685 on epoch=131
06/14/2022 01:36:09 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=131
06/14/2022 01:36:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=132
06/14/2022 01:36:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=133
06/14/2022 01:36:16 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=133
06/14/2022 01:36:19 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
06/14/2022 01:36:22 - INFO - __main__ - Global step 2150 Train loss 0.16 Classification-F1 0.7533916399560241 on epoch=134
06/14/2022 01:36:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.21 on epoch=134
06/14/2022 01:36:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=135
06/14/2022 01:36:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=136
06/14/2022 01:36:32 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
06/14/2022 01:36:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.18 on epoch=137
06/14/2022 01:36:38 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.7425538361791022 on epoch=137
06/14/2022 01:36:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.23 on epoch=138
06/14/2022 01:36:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=138
06/14/2022 01:36:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.13 on epoch=139
06/14/2022 01:36:48 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=139
06/14/2022 01:36:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
06/14/2022 01:36:54 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.7595222974863364 on epoch=140
06/14/2022 01:36:56 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.24 on epoch=141
06/14/2022 01:36:59 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
06/14/2022 01:37:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=142
06/14/2022 01:37:04 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.21 on epoch=143
06/14/2022 01:37:06 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.17 on epoch=143
06/14/2022 01:37:10 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.6996232679097458 on epoch=143
06/14/2022 01:37:12 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
06/14/2022 01:37:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.14 on epoch=144
06/14/2022 01:37:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
06/14/2022 01:37:20 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=146
06/14/2022 01:37:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=146
06/14/2022 01:37:26 - INFO - __main__ - Global step 2350 Train loss 0.15 Classification-F1 0.7569940112797255 on epoch=146
06/14/2022 01:37:28 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
06/14/2022 01:37:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
06/14/2022 01:37:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.26 on epoch=148
06/14/2022 01:37:35 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=149
06/14/2022 01:37:38 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.26 on epoch=149
06/14/2022 01:37:42 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.7793165162228064 on epoch=149
06/14/2022 01:37:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=150
06/14/2022 01:37:46 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=151
06/14/2022 01:37:49 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.23 on epoch=151
06/14/2022 01:37:51 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.15 on epoch=152
06/14/2022 01:37:54 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.25 on epoch=153
06/14/2022 01:37:57 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.7910139731846302 on epoch=153
06/14/2022 01:37:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7885119681811809 -> 0.7910139731846302 on epoch=153, global_step=2450
06/14/2022 01:38:00 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.20 on epoch=153
06/14/2022 01:38:02 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.14 on epoch=154
06/14/2022 01:38:05 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.19 on epoch=154
06/14/2022 01:38:07 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.16 on epoch=155
06/14/2022 01:38:10 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
06/14/2022 01:38:13 - INFO - __main__ - Global step 2500 Train loss 0.17 Classification-F1 0.7329404755326616 on epoch=156
06/14/2022 01:38:16 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=156
06/14/2022 01:38:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.12 on epoch=157
06/14/2022 01:38:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.33 on epoch=158
06/14/2022 01:38:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.16 on epoch=158
06/14/2022 01:38:26 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=159
06/14/2022 01:38:29 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7395668127253495 on epoch=159
06/14/2022 01:38:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
06/14/2022 01:38:34 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.12 on epoch=160
06/14/2022 01:38:36 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=161
06/14/2022 01:38:39 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
06/14/2022 01:38:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
06/14/2022 01:38:45 - INFO - __main__ - Global step 2600 Train loss 0.13 Classification-F1 0.7266010318296231 on epoch=162
06/14/2022 01:38:47 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.15 on epoch=163
06/14/2022 01:38:50 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.16 on epoch=163
06/14/2022 01:38:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=164
06/14/2022 01:38:55 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.14 on epoch=164
06/14/2022 01:38:57 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.14 on epoch=165
06/14/2022 01:39:01 - INFO - __main__ - Global step 2650 Train loss 0.14 Classification-F1 0.7436690676270508 on epoch=165
06/14/2022 01:39:03 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=166
06/14/2022 01:39:06 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=166
06/14/2022 01:39:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=167
06/14/2022 01:39:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.11 on epoch=168
06/14/2022 01:39:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=168
06/14/2022 01:39:17 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.7591054778554779 on epoch=168
06/14/2022 01:39:19 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
06/14/2022 01:39:21 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.16 on epoch=169
06/14/2022 01:39:24 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=170
06/14/2022 01:39:26 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
06/14/2022 01:39:29 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
06/14/2022 01:39:32 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.7459637197615236 on epoch=171
06/14/2022 01:39:35 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=172
06/14/2022 01:39:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=173
06/14/2022 01:39:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=173
06/14/2022 01:39:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=174
06/14/2022 01:39:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.14 on epoch=174
06/14/2022 01:39:48 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.7430423586438624 on epoch=174
06/14/2022 01:39:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.18 on epoch=175
06/14/2022 01:39:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
06/14/2022 01:39:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.09 on epoch=176
06/14/2022 01:39:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.10 on epoch=177
06/14/2022 01:40:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.12 on epoch=178
06/14/2022 01:40:04 - INFO - __main__ - Global step 2850 Train loss 0.12 Classification-F1 0.7572147533619078 on epoch=178
06/14/2022 01:40:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=178
06/14/2022 01:40:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.07 on epoch=179
06/14/2022 01:40:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.14 on epoch=179
06/14/2022 01:40:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.15 on epoch=180
06/14/2022 01:40:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.10 on epoch=181
06/14/2022 01:40:20 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.7448439451108677 on epoch=181
06/14/2022 01:40:22 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=181
06/14/2022 01:40:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.12 on epoch=182
06/14/2022 01:40:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.13 on epoch=183
06/14/2022 01:40:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=183
06/14/2022 01:40:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=184
06/14/2022 01:40:35 - INFO - __main__ - Global step 2950 Train loss 0.12 Classification-F1 0.796426914013871 on epoch=184
06/14/2022 01:40:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7910139731846302 -> 0.796426914013871 on epoch=184, global_step=2950
06/14/2022 01:40:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.09 on epoch=184
06/14/2022 01:40:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.13 on epoch=185
06/14/2022 01:40:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
06/14/2022 01:40:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.23 on epoch=186
06/14/2022 01:40:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=187
06/14/2022 01:40:49 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:40:49 - INFO - __main__ - Printing 3 examples
06/14/2022 01:40:49 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:40:49 - INFO - __main__ - ['happy']
06/14/2022 01:40:49 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:40:49 - INFO - __main__ - ['happy']
06/14/2022 01:40:49 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:40:49 - INFO - __main__ - ['happy']
06/14/2022 01:40:49 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:40:49 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:40:50 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:40:50 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:40:50 - INFO - __main__ - Printing 3 examples
06/14/2022 01:40:50 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:40:50 - INFO - __main__ - ['happy']
06/14/2022 01:40:50 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:40:50 - INFO - __main__ - ['happy']
06/14/2022 01:40:50 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:40:50 - INFO - __main__ - ['happy']
06/14/2022 01:40:50 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:40:50 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:40:50 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:40:51 - INFO - __main__ - Global step 3000 Train loss 0.12 Classification-F1 0.7476907350914435 on epoch=187
06/14/2022 01:40:51 - INFO - __main__ - save last model!
06/14/2022 01:40:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 01:40:51 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 01:40:51 - INFO - __main__ - Printing 3 examples
06/14/2022 01:40:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 01:40:51 - INFO - __main__ - ['others']
06/14/2022 01:40:51 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 01:40:51 - INFO - __main__ - ['others']
06/14/2022 01:40:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 01:40:51 - INFO - __main__ - ['others']
06/14/2022 01:40:51 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:40:54 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:40:59 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 01:41:08 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:41:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:41:09 - INFO - __main__ - Starting training!
06/14/2022 01:42:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_42_0.3_8_predictions.txt
06/14/2022 01:42:16 - INFO - __main__ - Classification-F1 on test data: 0.2416
06/14/2022 01:42:16 - INFO - __main__ - prefix=emo_64_42, lr=0.3, bsz=8, dev_performance=0.796426914013871, test_performance=0.24163697996533592
06/14/2022 01:42:16 - INFO - __main__ - Running ... prefix=emo_64_42, lr=0.2, bsz=8 ...
06/14/2022 01:42:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:42:17 - INFO - __main__ - Printing 3 examples
06/14/2022 01:42:17 - INFO - __main__ -  [emo] hahah i loved it yay glad you loved it x3 grinningfacewithsweat you always make us happy
06/14/2022 01:42:17 - INFO - __main__ - ['happy']
06/14/2022 01:42:17 - INFO - __main__ -  [emo] your right i'm always right i am impressed
06/14/2022 01:42:17 - INFO - __main__ - ['happy']
06/14/2022 01:42:17 - INFO - __main__ -  [emo] okay lol well that made me rolling on floor laughing funny
06/14/2022 01:42:17 - INFO - __main__ - ['happy']
06/14/2022 01:42:17 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:42:17 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:42:18 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:42:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:42:18 - INFO - __main__ - Printing 3 examples
06/14/2022 01:42:18 - INFO - __main__ -  [emo] how r u today i am doing fairly well how are you i am also feel fantastic
06/14/2022 01:42:18 - INFO - __main__ - ['happy']
06/14/2022 01:42:18 - INFO - __main__ -  [emo] alright u a funny man u are funny girl
06/14/2022 01:42:18 - INFO - __main__ - ['happy']
06/14/2022 01:42:18 - INFO - __main__ -  [emo] best film youve ever seen eternal sunshine of the spotless mind wow
06/14/2022 01:42:18 - INFO - __main__ - ['happy']
06/14/2022 01:42:18 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:42:18 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:42:18 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:42:37 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:42:37 - INFO - __main__ - Starting training!
06/14/2022 01:42:40 - INFO - __main__ - Step 10 Global step 10 Train loss 3.12 on epoch=0
06/14/2022 01:42:43 - INFO - __main__ - Step 20 Global step 20 Train loss 1.81 on epoch=1
06/14/2022 01:42:46 - INFO - __main__ - Step 30 Global step 30 Train loss 1.41 on epoch=1
06/14/2022 01:42:48 - INFO - __main__ - Step 40 Global step 40 Train loss 1.07 on epoch=2
06/14/2022 01:42:51 - INFO - __main__ - Step 50 Global step 50 Train loss 1.02 on epoch=3
06/14/2022 01:42:54 - INFO - __main__ - Global step 50 Train loss 1.69 Classification-F1 0.10800578731613214 on epoch=3
06/14/2022 01:42:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10800578731613214 on epoch=3, global_step=50
06/14/2022 01:42:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.01 on epoch=3
06/14/2022 01:42:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.02 on epoch=4
06/14/2022 01:43:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.90 on epoch=4
06/14/2022 01:43:04 - INFO - __main__ - Step 90 Global step 90 Train loss 1.00 on epoch=5
06/14/2022 01:43:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.84 on epoch=6
06/14/2022 01:43:10 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.2986642514756078 on epoch=6
06/14/2022 01:43:10 - INFO - __main__ - Saving model with best Classification-F1: 0.10800578731613214 -> 0.2986642514756078 on epoch=6, global_step=100
06/14/2022 01:43:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.97 on epoch=6
06/14/2022 01:43:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.91 on epoch=7
06/14/2022 01:43:17 - INFO - __main__ - Step 130 Global step 130 Train loss 0.83 on epoch=8
06/14/2022 01:43:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.92 on epoch=8
06/14/2022 01:43:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=9
06/14/2022 01:43:26 - INFO - __main__ - Global step 150 Train loss 0.90 Classification-F1 0.3366694191077549 on epoch=9
06/14/2022 01:43:26 - INFO - __main__ - Saving model with best Classification-F1: 0.2986642514756078 -> 0.3366694191077549 on epoch=9, global_step=150
06/14/2022 01:43:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=9
06/14/2022 01:43:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.82 on epoch=10
06/14/2022 01:43:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.89 on epoch=11
06/14/2022 01:43:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=11
06/14/2022 01:43:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=12
06/14/2022 01:43:42 - INFO - __main__ - Global step 200 Train loss 0.84 Classification-F1 0.3315916375686244 on epoch=12
06/14/2022 01:43:44 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=13
06/14/2022 01:43:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.67 on epoch=13
06/14/2022 01:43:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=14
06/14/2022 01:43:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=14
06/14/2022 01:43:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.75 on epoch=15
06/14/2022 01:43:57 - INFO - __main__ - Global step 250 Train loss 0.76 Classification-F1 0.41135151253799584 on epoch=15
06/14/2022 01:43:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3366694191077549 -> 0.41135151253799584 on epoch=15, global_step=250
06/14/2022 01:44:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.75 on epoch=16
06/14/2022 01:44:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.72 on epoch=16
06/14/2022 01:44:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.82 on epoch=17
06/14/2022 01:44:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=18
06/14/2022 01:44:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.79 on epoch=18
06/14/2022 01:44:13 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.3170476398726422 on epoch=18
06/14/2022 01:44:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.74 on epoch=19
06/14/2022 01:44:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=19
06/14/2022 01:44:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.73 on epoch=20
06/14/2022 01:44:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.70 on epoch=21
06/14/2022 01:44:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.70 on epoch=21
06/14/2022 01:44:29 - INFO - __main__ - Global step 350 Train loss 0.71 Classification-F1 0.42679841897233206 on epoch=21
06/14/2022 01:44:29 - INFO - __main__ - Saving model with best Classification-F1: 0.41135151253799584 -> 0.42679841897233206 on epoch=21, global_step=350
06/14/2022 01:44:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.69 on epoch=22
06/14/2022 01:44:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=23
06/14/2022 01:44:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.71 on epoch=23
06/14/2022 01:44:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.62 on epoch=24
06/14/2022 01:44:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=24
06/14/2022 01:44:45 - INFO - __main__ - Global step 400 Train loss 0.65 Classification-F1 0.5656634205721242 on epoch=24
06/14/2022 01:44:45 - INFO - __main__ - Saving model with best Classification-F1: 0.42679841897233206 -> 0.5656634205721242 on epoch=24, global_step=400
06/14/2022 01:44:48 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=25
06/14/2022 01:44:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.57 on epoch=26
06/14/2022 01:44:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.72 on epoch=26
06/14/2022 01:44:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=27
06/14/2022 01:44:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.61 on epoch=28
06/14/2022 01:45:01 - INFO - __main__ - Global step 450 Train loss 0.63 Classification-F1 0.6129138200944452 on epoch=28
06/14/2022 01:45:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5656634205721242 -> 0.6129138200944452 on epoch=28, global_step=450
06/14/2022 01:45:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.60 on epoch=28
06/14/2022 01:45:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=29
06/14/2022 01:45:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=29
06/14/2022 01:45:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=30
06/14/2022 01:45:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=31
06/14/2022 01:45:17 - INFO - __main__ - Global step 500 Train loss 0.57 Classification-F1 0.6431195312145804 on epoch=31
06/14/2022 01:45:17 - INFO - __main__ - Saving model with best Classification-F1: 0.6129138200944452 -> 0.6431195312145804 on epoch=31, global_step=500
06/14/2022 01:45:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=31
06/14/2022 01:45:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=32
06/14/2022 01:45:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.56 on epoch=33
06/14/2022 01:45:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.69 on epoch=33
06/14/2022 01:45:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.61 on epoch=34
06/14/2022 01:45:33 - INFO - __main__ - Global step 550 Train loss 0.60 Classification-F1 0.4992371296550863 on epoch=34
06/14/2022 01:45:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.67 on epoch=34
06/14/2022 01:45:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=35
06/14/2022 01:45:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.68 on epoch=36
06/14/2022 01:45:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.61 on epoch=36
06/14/2022 01:45:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.63 on epoch=37
06/14/2022 01:45:49 - INFO - __main__ - Global step 600 Train loss 0.64 Classification-F1 0.6860505936586916 on epoch=37
06/14/2022 01:45:49 - INFO - __main__ - Saving model with best Classification-F1: 0.6431195312145804 -> 0.6860505936586916 on epoch=37, global_step=600
06/14/2022 01:45:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=38
06/14/2022 01:45:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.65 on epoch=38
06/14/2022 01:45:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.62 on epoch=39
06/14/2022 01:45:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.56 on epoch=39
06/14/2022 01:46:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=40
06/14/2022 01:46:04 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.7458595608218342 on epoch=40
06/14/2022 01:46:04 - INFO - __main__ - Saving model with best Classification-F1: 0.6860505936586916 -> 0.7458595608218342 on epoch=40, global_step=650
06/14/2022 01:46:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=41
06/14/2022 01:46:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=41
06/14/2022 01:46:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.50 on epoch=42
06/14/2022 01:46:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.52 on epoch=43
06/14/2022 01:46:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.62 on epoch=43
06/14/2022 01:46:20 - INFO - __main__ - Global step 700 Train loss 0.57 Classification-F1 0.5295091875974228 on epoch=43
06/14/2022 01:46:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=44
06/14/2022 01:46:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=44
06/14/2022 01:46:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=45
06/14/2022 01:46:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.58 on epoch=46
06/14/2022 01:46:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=46
06/14/2022 01:46:36 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.6483262108262108 on epoch=46
06/14/2022 01:46:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=47
06/14/2022 01:46:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=48
06/14/2022 01:46:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=48
06/14/2022 01:46:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.54 on epoch=49
06/14/2022 01:46:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=49
06/14/2022 01:46:52 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.7463611187953852 on epoch=49
06/14/2022 01:46:52 - INFO - __main__ - Saving model with best Classification-F1: 0.7458595608218342 -> 0.7463611187953852 on epoch=49, global_step=800
06/14/2022 01:46:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=50
06/14/2022 01:46:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=51
06/14/2022 01:47:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.52 on epoch=51
06/14/2022 01:47:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=52
06/14/2022 01:47:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.52 on epoch=53
06/14/2022 01:47:08 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.7095383564757173 on epoch=53
06/14/2022 01:47:11 - INFO - __main__ - Step 860 Global step 860 Train loss 0.57 on epoch=53
06/14/2022 01:47:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=54
06/14/2022 01:47:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=54
06/14/2022 01:47:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=55
06/14/2022 01:47:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=56
06/14/2022 01:47:24 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.7154919979994999 on epoch=56
06/14/2022 01:47:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=56
06/14/2022 01:47:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=57
06/14/2022 01:47:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=58
06/14/2022 01:47:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=58
06/14/2022 01:47:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
06/14/2022 01:47:40 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.6355966224802863 on epoch=59
06/14/2022 01:47:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=59
06/14/2022 01:47:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=60
06/14/2022 01:47:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=61
06/14/2022 01:47:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=61
06/14/2022 01:47:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=62
06/14/2022 01:47:56 - INFO - __main__ - Global step 1000 Train loss 0.46 Classification-F1 0.7246935423483205 on epoch=62
06/14/2022 01:47:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=63
06/14/2022 01:48:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.53 on epoch=63
06/14/2022 01:48:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=64
06/14/2022 01:48:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=64
06/14/2022 01:48:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=65
06/14/2022 01:48:12 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.7751789863726751 on epoch=65
06/14/2022 01:48:12 - INFO - __main__ - Saving model with best Classification-F1: 0.7463611187953852 -> 0.7751789863726751 on epoch=65, global_step=1050
06/14/2022 01:48:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=66
06/14/2022 01:48:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=66
06/14/2022 01:48:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=67
06/14/2022 01:48:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=68
06/14/2022 01:48:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=68
06/14/2022 01:48:28 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.6923517780296048 on epoch=68
06/14/2022 01:48:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=69
06/14/2022 01:48:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=69
06/14/2022 01:48:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.35 on epoch=70
06/14/2022 01:48:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=71
06/14/2022 01:48:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=71
06/14/2022 01:48:44 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.7148038335046578 on epoch=71
06/14/2022 01:48:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.46 on epoch=72
06/14/2022 01:48:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.41 on epoch=73
06/14/2022 01:48:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=73
06/14/2022 01:48:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.43 on epoch=74
06/14/2022 01:48:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=74
06/14/2022 01:49:00 - INFO - __main__ - Global step 1200 Train loss 0.39 Classification-F1 0.7110243055555556 on epoch=74
06/14/2022 01:49:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=75
06/14/2022 01:49:05 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.38 on epoch=76
06/14/2022 01:49:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=76
06/14/2022 01:49:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.34 on epoch=77
06/14/2022 01:49:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=78
06/14/2022 01:49:16 - INFO - __main__ - Global step 1250 Train loss 0.39 Classification-F1 0.7142087438423645 on epoch=78
06/14/2022 01:49:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=78
06/14/2022 01:49:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.51 on epoch=79
06/14/2022 01:49:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=79
06/14/2022 01:49:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.35 on epoch=80
06/14/2022 01:49:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.29 on epoch=81
06/14/2022 01:49:32 - INFO - __main__ - Global step 1300 Train loss 0.39 Classification-F1 0.7327026445152757 on epoch=81
06/14/2022 01:49:34 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.40 on epoch=81
06/14/2022 01:49:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/14/2022 01:49:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=83
06/14/2022 01:49:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=83
06/14/2022 01:49:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=84
06/14/2022 01:49:48 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.7086011505681096 on epoch=84
06/14/2022 01:49:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.39 on epoch=84
06/14/2022 01:49:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=85
06/14/2022 01:49:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=86
06/14/2022 01:49:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=86
06/14/2022 01:50:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=87
06/14/2022 01:50:04 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.6925977582667724 on epoch=87
06/14/2022 01:50:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=88
06/14/2022 01:50:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.30 on epoch=88
06/14/2022 01:50:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.30 on epoch=89
06/14/2022 01:50:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.34 on epoch=89
06/14/2022 01:50:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.33 on epoch=90
06/14/2022 01:50:20 - INFO - __main__ - Global step 1450 Train loss 0.34 Classification-F1 0.7396195142307609 on epoch=90
06/14/2022 01:50:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=91
06/14/2022 01:50:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=91
06/14/2022 01:50:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=92
06/14/2022 01:50:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=93
06/14/2022 01:50:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
06/14/2022 01:50:35 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.7511504975124379 on epoch=93
06/14/2022 01:50:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.32 on epoch=94
06/14/2022 01:50:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=94
06/14/2022 01:50:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=95
06/14/2022 01:50:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.33 on epoch=96
06/14/2022 01:50:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=96
06/14/2022 01:50:51 - INFO - __main__ - Global step 1550 Train loss 0.34 Classification-F1 0.7258682190324863 on epoch=96
06/14/2022 01:50:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=97
06/14/2022 01:50:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=98
06/14/2022 01:50:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=98
06/14/2022 01:51:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=99
06/14/2022 01:51:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.31 on epoch=99
06/14/2022 01:51:07 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.7498699085844107 on epoch=99
06/14/2022 01:51:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=100
06/14/2022 01:51:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
06/14/2022 01:51:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=101
06/14/2022 01:51:17 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=102
06/14/2022 01:51:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=103
06/14/2022 01:51:23 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.7855889788093179 on epoch=103
06/14/2022 01:51:23 - INFO - __main__ - Saving model with best Classification-F1: 0.7751789863726751 -> 0.7855889788093179 on epoch=103, global_step=1650
06/14/2022 01:51:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.29 on epoch=103
06/14/2022 01:51:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.34 on epoch=104
06/14/2022 01:51:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=104
06/14/2022 01:51:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=105
06/14/2022 01:51:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
06/14/2022 01:51:39 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.7905998879744547 on epoch=106
06/14/2022 01:51:39 - INFO - __main__ - Saving model with best Classification-F1: 0.7855889788093179 -> 0.7905998879744547 on epoch=106, global_step=1700
06/14/2022 01:51:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=106
06/14/2022 01:51:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=107
06/14/2022 01:51:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.34 on epoch=108
06/14/2022 01:51:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=108
06/14/2022 01:51:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=109
06/14/2022 01:51:55 - INFO - __main__ - Global step 1750 Train loss 0.30 Classification-F1 0.7862641560944849 on epoch=109
06/14/2022 01:51:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.31 on epoch=109
06/14/2022 01:52:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=110
06/14/2022 01:52:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=111
06/14/2022 01:52:05 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=111
06/14/2022 01:52:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=112
06/14/2022 01:52:11 - INFO - __main__ - Global step 1800 Train loss 0.27 Classification-F1 0.7335203873627097 on epoch=112
06/14/2022 01:52:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=113
06/14/2022 01:52:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=113
06/14/2022 01:52:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=114
06/14/2022 01:52:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=114
06/14/2022 01:52:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.31 on epoch=115
06/14/2022 01:52:27 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.752822801661826 on epoch=115
06/14/2022 01:52:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=116
06/14/2022 01:52:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=116
06/14/2022 01:52:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=117
06/14/2022 01:52:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=118
06/14/2022 01:52:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=118
06/14/2022 01:52:43 - INFO - __main__ - Global step 1900 Train loss 0.27 Classification-F1 0.7291533851842407 on epoch=118
06/14/2022 01:52:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=119
06/14/2022 01:52:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=119
06/14/2022 01:52:50 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=120
06/14/2022 01:52:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=121
06/14/2022 01:52:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=121
06/14/2022 01:52:59 - INFO - __main__ - Global step 1950 Train loss 0.29 Classification-F1 0.7081341203053383 on epoch=121
06/14/2022 01:53:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
06/14/2022 01:53:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=123
06/14/2022 01:53:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=123
06/14/2022 01:53:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=124
06/14/2022 01:53:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.25 on epoch=124
06/14/2022 01:53:15 - INFO - __main__ - Global step 2000 Train loss 0.24 Classification-F1 0.7359164246734983 on epoch=124
06/14/2022 01:53:17 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.23 on epoch=125
06/14/2022 01:53:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.25 on epoch=126
06/14/2022 01:53:22 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.31 on epoch=126
06/14/2022 01:53:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.22 on epoch=127
06/14/2022 01:53:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.29 on epoch=128
06/14/2022 01:53:31 - INFO - __main__ - Global step 2050 Train loss 0.26 Classification-F1 0.784965034965035 on epoch=128
06/14/2022 01:53:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.27 on epoch=128
06/14/2022 01:53:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.24 on epoch=129
06/14/2022 01:53:38 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.30 on epoch=129
06/14/2022 01:53:41 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=130
06/14/2022 01:53:43 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.25 on epoch=131
06/14/2022 01:53:46 - INFO - __main__ - Global step 2100 Train loss 0.25 Classification-F1 0.7316119507331894 on epoch=131
06/14/2022 01:53:49 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.23 on epoch=131
06/14/2022 01:53:52 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=132
06/14/2022 01:53:54 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.27 on epoch=133
06/14/2022 01:53:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.24 on epoch=133
06/14/2022 01:53:59 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.21 on epoch=134
06/14/2022 01:54:02 - INFO - __main__ - Global step 2150 Train loss 0.23 Classification-F1 0.7033942364824717 on epoch=134
06/14/2022 01:54:05 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
06/14/2022 01:54:07 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.25 on epoch=135
06/14/2022 01:54:10 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.39 on epoch=136
06/14/2022 01:54:12 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.26 on epoch=136
06/14/2022 01:54:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.13 on epoch=137
06/14/2022 01:54:18 - INFO - __main__ - Global step 2200 Train loss 0.25 Classification-F1 0.7483186617373331 on epoch=137
06/14/2022 01:54:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.21 on epoch=138
06/14/2022 01:54:23 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.27 on epoch=138
06/14/2022 01:54:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.20 on epoch=139
06/14/2022 01:54:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=139
06/14/2022 01:54:31 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.23 on epoch=140
06/14/2022 01:54:34 - INFO - __main__ - Global step 2250 Train loss 0.23 Classification-F1 0.7585135558266842 on epoch=140
06/14/2022 01:54:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.30 on epoch=141
06/14/2022 01:54:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.26 on epoch=141
06/14/2022 01:54:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.25 on epoch=142
06/14/2022 01:54:44 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.23 on epoch=143
06/14/2022 01:54:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.26 on epoch=143
06/14/2022 01:54:50 - INFO - __main__ - Global step 2300 Train loss 0.26 Classification-F1 0.7304060736894709 on epoch=143
06/14/2022 01:54:53 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
06/14/2022 01:54:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.23 on epoch=144
06/14/2022 01:54:58 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.26 on epoch=145
06/14/2022 01:55:00 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.17 on epoch=146
06/14/2022 01:55:03 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.25 on epoch=146
06/14/2022 01:55:06 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.7541499523921404 on epoch=146
06/14/2022 01:55:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=147
06/14/2022 01:55:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=148
06/14/2022 01:55:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=148
06/14/2022 01:55:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.21 on epoch=149
06/14/2022 01:55:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.22 on epoch=149
06/14/2022 01:55:22 - INFO - __main__ - Global step 2400 Train loss 0.23 Classification-F1 0.7903695638530498 on epoch=149
06/14/2022 01:55:25 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.14 on epoch=150
06/14/2022 01:55:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
06/14/2022 01:55:30 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=151
06/14/2022 01:55:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.18 on epoch=152
06/14/2022 01:55:35 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=153
06/14/2022 01:55:38 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.7677084861824408 on epoch=153
06/14/2022 01:55:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.19 on epoch=153
06/14/2022 01:55:43 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=154
06/14/2022 01:55:46 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.23 on epoch=154
06/14/2022 01:55:48 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.25 on epoch=155
06/14/2022 01:55:51 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=156
06/14/2022 01:55:54 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.743013402525057 on epoch=156
06/14/2022 01:55:57 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=156
06/14/2022 01:55:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=157
06/14/2022 01:56:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=158
06/14/2022 01:56:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.15 on epoch=158
06/14/2022 01:56:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.20 on epoch=159
06/14/2022 01:56:10 - INFO - __main__ - Global step 2550 Train loss 0.17 Classification-F1 0.7334955883474363 on epoch=159
06/14/2022 01:56:13 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.28 on epoch=159
06/14/2022 01:56:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.20 on epoch=160
06/14/2022 01:56:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.28 on epoch=161
06/14/2022 01:56:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
06/14/2022 01:56:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.15 on epoch=162
06/14/2022 01:56:26 - INFO - __main__ - Global step 2600 Train loss 0.22 Classification-F1 0.7265985247414244 on epoch=162
06/14/2022 01:56:29 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.21 on epoch=163
06/14/2022 01:56:31 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=163
06/14/2022 01:56:34 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
06/14/2022 01:56:36 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=164
06/14/2022 01:56:39 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.18 on epoch=165
06/14/2022 01:56:42 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.761899741242807 on epoch=165
06/14/2022 01:56:45 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=166
06/14/2022 01:56:47 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
06/14/2022 01:56:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.13 on epoch=167
06/14/2022 01:56:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=168
06/14/2022 01:56:55 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.20 on epoch=168
06/14/2022 01:56:58 - INFO - __main__ - Global step 2700 Train loss 0.17 Classification-F1 0.7537166537166538 on epoch=168
06/14/2022 01:57:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.13 on epoch=169
06/14/2022 01:57:03 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=169
06/14/2022 01:57:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.25 on epoch=170
06/14/2022 01:57:08 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.21 on epoch=171
06/14/2022 01:57:11 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.17 on epoch=171
06/14/2022 01:57:14 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.7777218947766216 on epoch=171
06/14/2022 01:57:17 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.16 on epoch=172
06/14/2022 01:57:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=173
06/14/2022 01:57:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.20 on epoch=173
06/14/2022 01:57:24 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.10 on epoch=174
06/14/2022 01:57:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.15 on epoch=174
06/14/2022 01:57:30 - INFO - __main__ - Global step 2800 Train loss 0.16 Classification-F1 0.7952597699873014 on epoch=174
06/14/2022 01:57:30 - INFO - __main__ - Saving model with best Classification-F1: 0.7905998879744547 -> 0.7952597699873014 on epoch=174, global_step=2800
06/14/2022 01:57:33 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.16 on epoch=175
06/14/2022 01:57:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=176
06/14/2022 01:57:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.14 on epoch=176
06/14/2022 01:57:40 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.20 on epoch=177
06/14/2022 01:57:42 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.22 on epoch=178
06/14/2022 01:57:46 - INFO - __main__ - Global step 2850 Train loss 0.17 Classification-F1 0.7936522335107241 on epoch=178
06/14/2022 01:57:48 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.28 on epoch=178
06/14/2022 01:57:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.18 on epoch=179
06/14/2022 01:57:53 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.19 on epoch=179
06/14/2022 01:57:56 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.14 on epoch=180
06/14/2022 01:57:58 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.13 on epoch=181
06/14/2022 01:58:02 - INFO - __main__ - Global step 2900 Train loss 0.18 Classification-F1 0.7700643820082028 on epoch=181
06/14/2022 01:58:04 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.13 on epoch=181
06/14/2022 01:58:07 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
06/14/2022 01:58:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.23 on epoch=183
06/14/2022 01:58:12 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=183
06/14/2022 01:58:14 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=184
06/14/2022 01:58:18 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.7671845437960202 on epoch=184
06/14/2022 01:58:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=184
06/14/2022 01:58:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.21 on epoch=185
06/14/2022 01:58:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.12 on epoch=186
06/14/2022 01:58:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.18 on epoch=186
06/14/2022 01:58:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=187
06/14/2022 01:58:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:58:32 - INFO - __main__ - Printing 3 examples
06/14/2022 01:58:32 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:58:32 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:58:32 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:58:32 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:58:32 - INFO - __main__ - Printing 3 examples
06/14/2022 01:58:32 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 01:58:32 - INFO - __main__ - ['others']
06/14/2022 01:58:32 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:58:32 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:58:32 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 01:58:34 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.7773776782440149 on epoch=187
06/14/2022 01:58:34 - INFO - __main__ - save last model!
06/14/2022 01:58:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 01:58:34 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 01:58:34 - INFO - __main__ - Printing 3 examples
06/14/2022 01:58:34 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 01:58:34 - INFO - __main__ - ['others']
06/14/2022 01:58:34 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 01:58:34 - INFO - __main__ - ['others']
06/14/2022 01:58:34 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 01:58:34 - INFO - __main__ - ['others']
06/14/2022 01:58:34 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:58:36 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:58:41 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 01:58:50 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 01:58:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 01:58:51 - INFO - __main__ - Starting training!
06/14/2022 01:59:53 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_42_0.2_8_predictions.txt
06/14/2022 01:59:53 - INFO - __main__ - Classification-F1 on test data: 0.3405
06/14/2022 01:59:53 - INFO - __main__ - prefix=emo_64_42, lr=0.2, bsz=8, dev_performance=0.7952597699873014, test_performance=0.3404668954964817
06/14/2022 01:59:53 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.5, bsz=8 ...
06/14/2022 01:59:54 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:59:54 - INFO - __main__ - Printing 3 examples
06/14/2022 01:59:54 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 01:59:54 - INFO - __main__ - ['others']
06/14/2022 01:59:54 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 01:59:54 - INFO - __main__ - ['others']
06/14/2022 01:59:54 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 01:59:54 - INFO - __main__ - ['others']
06/14/2022 01:59:54 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:59:54 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:59:55 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 01:59:55 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 01:59:55 - INFO - __main__ - Printing 3 examples
06/14/2022 01:59:55 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 01:59:55 - INFO - __main__ - ['others']
06/14/2022 01:59:55 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 01:59:55 - INFO - __main__ - ['others']
06/14/2022 01:59:55 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 01:59:55 - INFO - __main__ - ['others']
06/14/2022 01:59:55 - INFO - __main__ - Tokenizing Input ...
06/14/2022 01:59:55 - INFO - __main__ - Tokenizing Output ...
06/14/2022 01:59:55 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:00:14 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:00:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:00:14 - INFO - __main__ - Starting training!
06/14/2022 02:00:18 - INFO - __main__ - Step 10 Global step 10 Train loss 2.42 on epoch=0
06/14/2022 02:00:20 - INFO - __main__ - Step 20 Global step 20 Train loss 1.21 on epoch=1
06/14/2022 02:00:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.99 on epoch=1
06/14/2022 02:00:25 - INFO - __main__ - Step 40 Global step 40 Train loss 1.05 on epoch=2
06/14/2022 02:00:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.87 on epoch=3
06/14/2022 02:00:31 - INFO - __main__ - Global step 50 Train loss 1.31 Classification-F1 0.2402816953105368 on epoch=3
06/14/2022 02:00:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2402816953105368 on epoch=3, global_step=50
06/14/2022 02:00:34 - INFO - __main__ - Step 60 Global step 60 Train loss 0.87 on epoch=3
06/14/2022 02:00:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.90 on epoch=4
06/14/2022 02:00:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.88 on epoch=4
06/14/2022 02:00:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.83 on epoch=5
06/14/2022 02:00:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.80 on epoch=6
06/14/2022 02:00:47 - INFO - __main__ - Global step 100 Train loss 0.86 Classification-F1 0.21273733308944576 on epoch=6
06/14/2022 02:00:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=6
06/14/2022 02:00:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.85 on epoch=7
06/14/2022 02:00:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.85 on epoch=8
06/14/2022 02:00:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.76 on epoch=8
06/14/2022 02:01:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.75 on epoch=9
06/14/2022 02:01:03 - INFO - __main__ - Global step 150 Train loss 0.82 Classification-F1 0.517270832244616 on epoch=9
06/14/2022 02:01:03 - INFO - __main__ - Saving model with best Classification-F1: 0.2402816953105368 -> 0.517270832244616 on epoch=9, global_step=150
06/14/2022 02:01:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.72 on epoch=9
06/14/2022 02:01:08 - INFO - __main__ - Step 170 Global step 170 Train loss 0.79 on epoch=10
06/14/2022 02:01:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=11
06/14/2022 02:01:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.63 on epoch=11
06/14/2022 02:01:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.80 on epoch=12
06/14/2022 02:01:19 - INFO - __main__ - Global step 200 Train loss 0.73 Classification-F1 0.5459537729982493 on epoch=12
06/14/2022 02:01:19 - INFO - __main__ - Saving model with best Classification-F1: 0.517270832244616 -> 0.5459537729982493 on epoch=12, global_step=200
06/14/2022 02:01:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.66 on epoch=13
06/14/2022 02:01:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.60 on epoch=13
06/14/2022 02:01:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=14
06/14/2022 02:01:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=14
06/14/2022 02:01:32 - INFO - __main__ - Step 250 Global step 250 Train loss 0.54 on epoch=15
06/14/2022 02:01:35 - INFO - __main__ - Global step 250 Train loss 0.62 Classification-F1 0.5931105474216726 on epoch=15
06/14/2022 02:01:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5459537729982493 -> 0.5931105474216726 on epoch=15, global_step=250
06/14/2022 02:01:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.70 on epoch=16
06/14/2022 02:01:40 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=16
06/14/2022 02:01:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.65 on epoch=17
06/14/2022 02:01:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.60 on epoch=18
06/14/2022 02:01:48 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=18
06/14/2022 02:01:51 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.6962934483620905 on epoch=18
06/14/2022 02:01:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5931105474216726 -> 0.6962934483620905 on epoch=18, global_step=300
06/14/2022 02:01:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=19
06/14/2022 02:01:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.59 on epoch=19
06/14/2022 02:01:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.57 on epoch=20
06/14/2022 02:02:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.61 on epoch=21
06/14/2022 02:02:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.51 on epoch=21
06/14/2022 02:02:07 - INFO - __main__ - Global step 350 Train loss 0.60 Classification-F1 0.5445983071424326 on epoch=21
06/14/2022 02:02:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=22
06/14/2022 02:02:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.66 on epoch=23
06/14/2022 02:02:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=23
06/14/2022 02:02:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=24
06/14/2022 02:02:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=24
06/14/2022 02:02:23 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.7470347222222222 on epoch=24
06/14/2022 02:02:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6962934483620905 -> 0.7470347222222222 on epoch=24, global_step=400
06/14/2022 02:02:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=25
06/14/2022 02:02:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
06/14/2022 02:02:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.52 on epoch=26
06/14/2022 02:02:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=27
06/14/2022 02:02:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.57 on epoch=28
06/14/2022 02:02:39 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.6163640725779453 on epoch=28
06/14/2022 02:02:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=28
06/14/2022 02:02:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=29
06/14/2022 02:02:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=29
06/14/2022 02:02:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=30
06/14/2022 02:02:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=31
06/14/2022 02:02:54 - INFO - __main__ - Global step 500 Train loss 0.49 Classification-F1 0.7637576391634722 on epoch=31
06/14/2022 02:02:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7470347222222222 -> 0.7637576391634722 on epoch=31, global_step=500
06/14/2022 02:02:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=31
06/14/2022 02:02:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.42 on epoch=32
06/14/2022 02:03:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=33
06/14/2022 02:03:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=33
06/14/2022 02:03:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=34
06/14/2022 02:03:10 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.7607376136295225 on epoch=34
06/14/2022 02:03:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=34
06/14/2022 02:03:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=35
06/14/2022 02:03:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
06/14/2022 02:03:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=36
06/14/2022 02:03:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.44 on epoch=37
06/14/2022 02:03:26 - INFO - __main__ - Global step 600 Train loss 0.41 Classification-F1 0.7082860149852748 on epoch=37
06/14/2022 02:03:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=38
06/14/2022 02:03:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=38
06/14/2022 02:03:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=39
06/14/2022 02:03:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=39
06/14/2022 02:03:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=40
06/14/2022 02:03:42 - INFO - __main__ - Global step 650 Train loss 0.39 Classification-F1 0.709345534812259 on epoch=40
06/14/2022 02:03:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=41
06/14/2022 02:03:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=41
06/14/2022 02:03:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=42
06/14/2022 02:03:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=43
06/14/2022 02:03:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=43
06/14/2022 02:03:58 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.726370952648325 on epoch=43
06/14/2022 02:04:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=44
06/14/2022 02:04:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=44
06/14/2022 02:04:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=45
06/14/2022 02:04:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.29 on epoch=46
06/14/2022 02:04:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.30 on epoch=46
06/14/2022 02:04:14 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.7466023698834804 on epoch=46
06/14/2022 02:04:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=47
06/14/2022 02:04:19 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=48
06/14/2022 02:04:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=48
06/14/2022 02:04:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=49
06/14/2022 02:04:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=49
06/14/2022 02:04:29 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.7305626439836967 on epoch=49
06/14/2022 02:04:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.35 on epoch=50
06/14/2022 02:04:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=51
06/14/2022 02:04:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=51
06/14/2022 02:04:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=52
06/14/2022 02:04:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=53
06/14/2022 02:04:45 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.7244289915316071 on epoch=53
06/14/2022 02:04:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=53
06/14/2022 02:04:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=54
06/14/2022 02:04:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=54
06/14/2022 02:04:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.33 on epoch=55
06/14/2022 02:04:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=56
06/14/2022 02:05:01 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.6794782464569864 on epoch=56
06/14/2022 02:05:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=56
06/14/2022 02:05:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.32 on epoch=57
06/14/2022 02:05:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=58
06/14/2022 02:05:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.27 on epoch=58
06/14/2022 02:05:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.29 on epoch=59
06/14/2022 02:05:17 - INFO - __main__ - Global step 950 Train loss 0.29 Classification-F1 0.7389838278922787 on epoch=59
06/14/2022 02:05:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=59
06/14/2022 02:05:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.32 on epoch=60
06/14/2022 02:05:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=61
06/14/2022 02:05:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=61
06/14/2022 02:05:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.27 on epoch=62
06/14/2022 02:05:33 - INFO - __main__ - Global step 1000 Train loss 0.26 Classification-F1 0.7620942099350736 on epoch=62
06/14/2022 02:05:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=63
06/14/2022 02:05:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=63
06/14/2022 02:05:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.28 on epoch=64
06/14/2022 02:05:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=64
06/14/2022 02:05:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.33 on epoch=65
06/14/2022 02:05:49 - INFO - __main__ - Global step 1050 Train loss 0.25 Classification-F1 0.7644812344913151 on epoch=65
06/14/2022 02:05:49 - INFO - __main__ - Saving model with best Classification-F1: 0.7637576391634722 -> 0.7644812344913151 on epoch=65, global_step=1050
06/14/2022 02:05:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=66
06/14/2022 02:05:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=66
06/14/2022 02:05:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=67
06/14/2022 02:05:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=68
06/14/2022 02:06:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=68
06/14/2022 02:06:05 - INFO - __main__ - Global step 1100 Train loss 0.25 Classification-F1 0.7133234287484571 on epoch=68
06/14/2022 02:06:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=69
06/14/2022 02:06:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=69
06/14/2022 02:06:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=70
06/14/2022 02:06:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=71
06/14/2022 02:06:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=71
06/14/2022 02:06:20 - INFO - __main__ - Global step 1150 Train loss 0.25 Classification-F1 0.7026310975609755 on epoch=71
06/14/2022 02:06:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=72
06/14/2022 02:06:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=73
06/14/2022 02:06:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=73
06/14/2022 02:06:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=74
06/14/2022 02:06:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.57 on epoch=74
06/14/2022 02:06:45 - INFO - __main__ - Global step 1200 Train loss 0.53 Classification-F1 0.46407255408623405 on epoch=74
06/14/2022 02:06:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.27 on epoch=75
06/14/2022 02:06:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.62 on epoch=76
06/14/2022 02:06:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=76
06/14/2022 02:06:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.54 on epoch=77
06/14/2022 02:06:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.32 on epoch=78
06/14/2022 02:07:01 - INFO - __main__ - Global step 1250 Train loss 0.59 Classification-F1 0.7151620295183466 on epoch=78
06/14/2022 02:07:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=78
06/14/2022 02:07:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=79
06/14/2022 02:07:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=79
06/14/2022 02:07:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=80
06/14/2022 02:07:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=81
06/14/2022 02:07:17 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.7315766951161689 on epoch=81
06/14/2022 02:07:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=81
06/14/2022 02:07:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=82
06/14/2022 02:07:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=83
06/14/2022 02:07:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=83
06/14/2022 02:07:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=84
06/14/2022 02:07:33 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.7089470259796609 on epoch=84
06/14/2022 02:07:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=84
06/14/2022 02:07:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=85
06/14/2022 02:07:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=86
06/14/2022 02:07:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=86
06/14/2022 02:07:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=87
06/14/2022 02:07:49 - INFO - __main__ - Global step 1400 Train loss 0.23 Classification-F1 0.7513295415859432 on epoch=87
06/14/2022 02:07:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=88
06/14/2022 02:07:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=88
06/14/2022 02:07:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
06/14/2022 02:07:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=89
06/14/2022 02:08:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=90
06/14/2022 02:08:05 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.7621577259691861 on epoch=90
06/14/2022 02:08:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=91
06/14/2022 02:08:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=91
06/14/2022 02:08:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=92
06/14/2022 02:08:15 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=93
06/14/2022 02:08:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=93
06/14/2022 02:08:21 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.7753367003367004 on epoch=93
06/14/2022 02:08:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7644812344913151 -> 0.7753367003367004 on epoch=93, global_step=1500
06/14/2022 02:08:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=94
06/14/2022 02:08:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=94
06/14/2022 02:08:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=95
06/14/2022 02:08:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=96
06/14/2022 02:08:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=96
06/14/2022 02:08:37 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.7406400966183575 on epoch=96
06/14/2022 02:08:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=97
06/14/2022 02:08:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=98
06/14/2022 02:08:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=98
06/14/2022 02:08:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=99
06/14/2022 02:08:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=99
06/14/2022 02:08:53 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.7543196990316696 on epoch=99
06/14/2022 02:08:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=100
06/14/2022 02:08:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=101
06/14/2022 02:09:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=101
06/14/2022 02:09:03 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=102
06/14/2022 02:09:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=103
06/14/2022 02:09:09 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.7311232807711681 on epoch=103
06/14/2022 02:09:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=103
06/14/2022 02:09:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=104
06/14/2022 02:09:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=104
06/14/2022 02:09:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=105
06/14/2022 02:09:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=106
06/14/2022 02:09:25 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.7406355734616603 on epoch=106
06/14/2022 02:09:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=106
06/14/2022 02:09:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=107
06/14/2022 02:09:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=108
06/14/2022 02:09:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=108
06/14/2022 02:09:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=109
06/14/2022 02:09:41 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.7623396557376645 on epoch=109
06/14/2022 02:09:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=109
06/14/2022 02:09:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=110
06/14/2022 02:09:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=111
06/14/2022 02:09:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=111
06/14/2022 02:09:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=112
06/14/2022 02:09:57 - INFO - __main__ - Global step 1800 Train loss 0.23 Classification-F1 0.7718929774227921 on epoch=112
06/14/2022 02:09:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=113
06/14/2022 02:10:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=113
06/14/2022 02:10:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=114
06/14/2022 02:10:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=114
06/14/2022 02:10:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=115
06/14/2022 02:10:13 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.7559900907153266 on epoch=115
06/14/2022 02:10:15 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=116
06/14/2022 02:10:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=116
06/14/2022 02:10:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.15 on epoch=117
06/14/2022 02:10:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=118
06/14/2022 02:10:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
06/14/2022 02:10:28 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.7383343005955897 on epoch=118
06/14/2022 02:10:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.12 on epoch=119
06/14/2022 02:10:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=119
06/14/2022 02:10:36 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=120
06/14/2022 02:10:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=121
06/14/2022 02:10:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
06/14/2022 02:10:44 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.7528409047083119 on epoch=121
06/14/2022 02:10:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=122
06/14/2022 02:10:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=123
06/14/2022 02:10:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=123
06/14/2022 02:10:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.23 on epoch=124
06/14/2022 02:10:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=124
06/14/2022 02:11:00 - INFO - __main__ - Global step 2000 Train loss 0.13 Classification-F1 0.7687263893606727 on epoch=124
06/14/2022 02:11:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=125
06/14/2022 02:11:05 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=126
06/14/2022 02:11:08 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=126
06/14/2022 02:11:10 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.14 on epoch=127
06/14/2022 02:11:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=128
06/14/2022 02:11:16 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.7175132502522549 on epoch=128
06/14/2022 02:11:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
06/14/2022 02:11:21 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=129
06/14/2022 02:11:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.12 on epoch=129
06/14/2022 02:11:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=130
06/14/2022 02:11:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=131
06/14/2022 02:11:32 - INFO - __main__ - Global step 2100 Train loss 0.13 Classification-F1 0.7411862419386358 on epoch=131
06/14/2022 02:11:35 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=131
06/14/2022 02:11:37 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.13 on epoch=132
06/14/2022 02:11:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=133
06/14/2022 02:11:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=133
06/14/2022 02:11:45 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
06/14/2022 02:11:48 - INFO - __main__ - Global step 2150 Train loss 0.14 Classification-F1 0.7279953523660069 on epoch=134
06/14/2022 02:11:51 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=134
06/14/2022 02:11:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=135
06/14/2022 02:11:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=136
06/14/2022 02:11:58 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.14 on epoch=136
06/14/2022 02:12:01 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=137
06/14/2022 02:12:04 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.7721741884133415 on epoch=137
06/14/2022 02:12:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.05 on epoch=138
06/14/2022 02:12:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=138
06/14/2022 02:12:12 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=139
06/14/2022 02:12:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=139
06/14/2022 02:12:17 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=140
06/14/2022 02:12:20 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.7572615976286121 on epoch=140
06/14/2022 02:12:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
06/14/2022 02:12:25 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=141
06/14/2022 02:12:28 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=142
06/14/2022 02:12:30 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.12 on epoch=143
06/14/2022 02:12:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.23 on epoch=143
06/14/2022 02:12:36 - INFO - __main__ - Global step 2300 Train loss 0.15 Classification-F1 0.75868594682154 on epoch=143
06/14/2022 02:12:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=144
06/14/2022 02:12:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.10 on epoch=144
06/14/2022 02:12:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=145
06/14/2022 02:12:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=146
06/14/2022 02:12:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.10 on epoch=146
06/14/2022 02:12:52 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.7542067665294101 on epoch=146
06/14/2022 02:12:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=147
06/14/2022 02:12:57 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=148
06/14/2022 02:13:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=148
06/14/2022 02:13:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=149
06/14/2022 02:13:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.13 on epoch=149
06/14/2022 02:13:08 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.7456084519974402 on epoch=149
06/14/2022 02:13:10 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.11 on epoch=150
06/14/2022 02:13:13 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=151
06/14/2022 02:13:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.17 on epoch=151
06/14/2022 02:13:18 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.16 on epoch=152
06/14/2022 02:13:21 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.11 on epoch=153
06/14/2022 02:13:24 - INFO - __main__ - Global step 2450 Train loss 0.13 Classification-F1 0.7544253555881463 on epoch=153
06/14/2022 02:13:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
06/14/2022 02:13:29 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=154
06/14/2022 02:13:32 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
06/14/2022 02:13:34 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.18 on epoch=155
06/14/2022 02:13:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=156
06/14/2022 02:13:40 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7329309446171023 on epoch=156
06/14/2022 02:13:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=156
06/14/2022 02:13:45 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=157
06/14/2022 02:13:48 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.10 on epoch=158
06/14/2022 02:13:50 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=158
06/14/2022 02:13:53 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=159
06/14/2022 02:13:56 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.7800270854301107 on epoch=159
06/14/2022 02:13:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7753367003367004 -> 0.7800270854301107 on epoch=159, global_step=2550
06/14/2022 02:13:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=159
06/14/2022 02:14:01 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.15 on epoch=160
06/14/2022 02:14:04 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=161
06/14/2022 02:14:06 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.08 on epoch=161
06/14/2022 02:14:09 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.12 on epoch=162
06/14/2022 02:14:12 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7478780913429637 on epoch=162
06/14/2022 02:14:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=163
06/14/2022 02:14:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=163
06/14/2022 02:14:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.12 on epoch=164
06/14/2022 02:14:22 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
06/14/2022 02:14:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.05 on epoch=165
06/14/2022 02:14:28 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.7526732102054914 on epoch=165
06/14/2022 02:14:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.10 on epoch=166
06/14/2022 02:14:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=166
06/14/2022 02:14:35 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=167
06/14/2022 02:14:38 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.13 on epoch=168
06/14/2022 02:14:40 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.10 on epoch=168
06/14/2022 02:14:44 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.7465834818775996 on epoch=168
06/14/2022 02:14:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=169
06/14/2022 02:14:49 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.08 on epoch=169
06/14/2022 02:14:51 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.15 on epoch=170
06/14/2022 02:14:54 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.13 on epoch=171
06/14/2022 02:14:56 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.16 on epoch=171
06/14/2022 02:15:00 - INFO - __main__ - Global step 2750 Train loss 0.11 Classification-F1 0.6992459191606921 on epoch=171
06/14/2022 02:15:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.06 on epoch=172
06/14/2022 02:15:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=173
06/14/2022 02:15:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
06/14/2022 02:15:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=174
06/14/2022 02:15:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=174
06/14/2022 02:15:16 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.7398261326054274 on epoch=174
06/14/2022 02:15:18 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=175
06/14/2022 02:15:21 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=176
06/14/2022 02:15:23 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.07 on epoch=176
06/14/2022 02:15:26 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
06/14/2022 02:15:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=178
06/14/2022 02:15:32 - INFO - __main__ - Global step 2850 Train loss 0.07 Classification-F1 0.701461203722313 on epoch=178
06/14/2022 02:15:35 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=178
06/14/2022 02:15:37 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=179
06/14/2022 02:15:40 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
06/14/2022 02:15:42 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=180
06/14/2022 02:15:45 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
06/14/2022 02:15:48 - INFO - __main__ - Global step 2900 Train loss 0.06 Classification-F1 0.7830926442450212 on epoch=181
06/14/2022 02:15:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7800270854301107 -> 0.7830926442450212 on epoch=181, global_step=2900
06/14/2022 02:15:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.07 on epoch=181
06/14/2022 02:15:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=182
06/14/2022 02:15:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=183
06/14/2022 02:15:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=183
06/14/2022 02:16:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=184
06/14/2022 02:16:04 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.7348702476753776 on epoch=184
06/14/2022 02:16:07 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
06/14/2022 02:16:09 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=185
06/14/2022 02:16:12 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=186
06/14/2022 02:16:14 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=186
06/14/2022 02:16:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=187
06/14/2022 02:16:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:16:18 - INFO - __main__ - Printing 3 examples
06/14/2022 02:16:18 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:16:18 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:16:18 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:16:18 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:16:18 - INFO - __main__ - Printing 3 examples
06/14/2022 02:16:18 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:16:18 - INFO - __main__ - ['others']
06/14/2022 02:16:18 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:16:19 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:16:19 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:16:20 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7827307804394709 on epoch=187
06/14/2022 02:16:20 - INFO - __main__ - save last model!
06/14/2022 02:16:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 02:16:20 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 02:16:20 - INFO - __main__ - Printing 3 examples
06/14/2022 02:16:20 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 02:16:20 - INFO - __main__ - ['others']
06/14/2022 02:16:20 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 02:16:20 - INFO - __main__ - ['others']
06/14/2022 02:16:20 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 02:16:20 - INFO - __main__ - ['others']
06/14/2022 02:16:20 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:16:22 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:16:28 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 02:16:37 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:16:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:16:38 - INFO - __main__ - Starting training!
06/14/2022 02:17:40 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_87_0.5_8_predictions.txt
06/14/2022 02:17:40 - INFO - __main__ - Classification-F1 on test data: 0.5465
06/14/2022 02:17:41 - INFO - __main__ - prefix=emo_64_87, lr=0.5, bsz=8, dev_performance=0.7830926442450212, test_performance=0.5465179972937738
06/14/2022 02:17:41 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.4, bsz=8 ...
06/14/2022 02:17:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:17:42 - INFO - __main__ - Printing 3 examples
06/14/2022 02:17:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:17:42 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:17:42 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:17:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:17:42 - INFO - __main__ - Printing 3 examples
06/14/2022 02:17:42 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:17:42 - INFO - __main__ - ['others']
06/14/2022 02:17:42 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:17:42 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:17:43 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:18:01 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:18:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:18:02 - INFO - __main__ - Starting training!
06/14/2022 02:18:05 - INFO - __main__ - Step 10 Global step 10 Train loss 2.75 on epoch=0
06/14/2022 02:18:08 - INFO - __main__ - Step 20 Global step 20 Train loss 1.40 on epoch=1
06/14/2022 02:18:10 - INFO - __main__ - Step 30 Global step 30 Train loss 1.04 on epoch=1
06/14/2022 02:18:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.02 on epoch=2
06/14/2022 02:18:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.92 on epoch=3
06/14/2022 02:18:19 - INFO - __main__ - Global step 50 Train loss 1.43 Classification-F1 0.10820468839336764 on epoch=3
06/14/2022 02:18:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.10820468839336764 on epoch=3, global_step=50
06/14/2022 02:18:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.91 on epoch=3
06/14/2022 02:18:24 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=4
06/14/2022 02:18:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.96 on epoch=4
06/14/2022 02:18:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.86 on epoch=5
06/14/2022 02:18:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.87 on epoch=6
06/14/2022 02:18:35 - INFO - __main__ - Global step 100 Train loss 0.92 Classification-F1 0.25960469196365565 on epoch=6
06/14/2022 02:18:35 - INFO - __main__ - Saving model with best Classification-F1: 0.10820468839336764 -> 0.25960469196365565 on epoch=6, global_step=100
06/14/2022 02:18:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.79 on epoch=6
06/14/2022 02:18:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.90 on epoch=7
06/14/2022 02:18:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=8
06/14/2022 02:18:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.79 on epoch=8
06/14/2022 02:18:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.79 on epoch=9
06/14/2022 02:18:50 - INFO - __main__ - Global step 150 Train loss 0.81 Classification-F1 0.3823151730066624 on epoch=9
06/14/2022 02:18:50 - INFO - __main__ - Saving model with best Classification-F1: 0.25960469196365565 -> 0.3823151730066624 on epoch=9, global_step=150
06/14/2022 02:18:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.76 on epoch=9
06/14/2022 02:18:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.68 on epoch=10
06/14/2022 02:18:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.76 on epoch=11
06/14/2022 02:19:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.66 on epoch=11
06/14/2022 02:19:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.84 on epoch=12
06/14/2022 02:19:07 - INFO - __main__ - Global step 200 Train loss 0.74 Classification-F1 0.48286047644883856 on epoch=12
06/14/2022 02:19:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3823151730066624 -> 0.48286047644883856 on epoch=12, global_step=200
06/14/2022 02:19:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.78 on epoch=13
06/14/2022 02:19:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.76 on epoch=13
06/14/2022 02:19:14 - INFO - __main__ - Step 230 Global step 230 Train loss 0.70 on epoch=14
06/14/2022 02:19:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.60 on epoch=14
06/14/2022 02:19:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.73 on epoch=15
06/14/2022 02:19:22 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.6477657135399564 on epoch=15
06/14/2022 02:19:23 - INFO - __main__ - Saving model with best Classification-F1: 0.48286047644883856 -> 0.6477657135399564 on epoch=15, global_step=250
06/14/2022 02:19:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.62 on epoch=16
06/14/2022 02:19:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=16
06/14/2022 02:19:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.69 on epoch=17
06/14/2022 02:19:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.56 on epoch=18
06/14/2022 02:19:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=18
06/14/2022 02:19:39 - INFO - __main__ - Global step 300 Train loss 0.60 Classification-F1 0.5936526923369029 on epoch=18
06/14/2022 02:19:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.68 on epoch=19
06/14/2022 02:19:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=19
06/14/2022 02:19:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.61 on epoch=20
06/14/2022 02:19:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.60 on epoch=21
06/14/2022 02:19:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.50 on epoch=21
06/14/2022 02:19:55 - INFO - __main__ - Global step 350 Train loss 0.58 Classification-F1 0.6072137215510361 on epoch=21
06/14/2022 02:19:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=22
06/14/2022 02:20:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=23
06/14/2022 02:20:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=23
06/14/2022 02:20:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=24
06/14/2022 02:20:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.59 on epoch=24
06/14/2022 02:20:12 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.7170009256882542 on epoch=24
06/14/2022 02:20:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6477657135399564 -> 0.7170009256882542 on epoch=24, global_step=400
06/14/2022 02:20:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=25
06/14/2022 02:20:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.59 on epoch=26
06/14/2022 02:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=26
06/14/2022 02:20:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.56 on epoch=27
06/14/2022 02:20:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=28
06/14/2022 02:20:28 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.5561396076101959 on epoch=28
06/14/2022 02:20:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=28
06/14/2022 02:20:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.56 on epoch=29
06/14/2022 02:20:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=29
06/14/2022 02:20:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=30
06/14/2022 02:20:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=31
06/14/2022 02:20:44 - INFO - __main__ - Global step 500 Train loss 0.52 Classification-F1 0.7016895592561139 on epoch=31
06/14/2022 02:20:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.42 on epoch=31
06/14/2022 02:20:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=32
06/14/2022 02:20:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.49 on epoch=33
06/14/2022 02:20:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=33
06/14/2022 02:20:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=34
06/14/2022 02:21:01 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.7440722291765924 on epoch=34
06/14/2022 02:21:01 - INFO - __main__ - Saving model with best Classification-F1: 0.7170009256882542 -> 0.7440722291765924 on epoch=34, global_step=550
06/14/2022 02:21:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=34
06/14/2022 02:21:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.48 on epoch=35
06/14/2022 02:21:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.41 on epoch=36
06/14/2022 02:21:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=36
06/14/2022 02:21:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=37
06/14/2022 02:21:17 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.7560223564688244 on epoch=37
06/14/2022 02:21:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7440722291765924 -> 0.7560223564688244 on epoch=37, global_step=600
06/14/2022 02:21:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=38
06/14/2022 02:21:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=38
06/14/2022 02:21:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.40 on epoch=39
06/14/2022 02:21:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.40 on epoch=39
06/14/2022 02:21:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=40
06/14/2022 02:21:34 - INFO - __main__ - Global step 650 Train loss 0.42 Classification-F1 0.7329417715375426 on epoch=40
06/14/2022 02:21:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=41
06/14/2022 02:21:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.40 on epoch=41
06/14/2022 02:21:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=42
06/14/2022 02:21:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=43
06/14/2022 02:21:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=43
06/14/2022 02:21:50 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.7207888125352895 on epoch=43
06/14/2022 02:21:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=44
06/14/2022 02:21:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=44
06/14/2022 02:21:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=45
06/14/2022 02:22:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=46
06/14/2022 02:22:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.36 on epoch=46
06/14/2022 02:22:07 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.7608176948961006 on epoch=46
06/14/2022 02:22:07 - INFO - __main__ - Saving model with best Classification-F1: 0.7560223564688244 -> 0.7608176948961006 on epoch=46, global_step=750
06/14/2022 02:22:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=47
06/14/2022 02:22:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.49 on epoch=48
06/14/2022 02:22:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.30 on epoch=48
06/14/2022 02:22:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=49
06/14/2022 02:22:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=49
06/14/2022 02:22:23 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.7566727351030896 on epoch=49
06/14/2022 02:22:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=50
06/14/2022 02:22:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=51
06/14/2022 02:22:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=51
06/14/2022 02:22:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=52
06/14/2022 02:22:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=53
06/14/2022 02:22:40 - INFO - __main__ - Global step 850 Train loss 0.35 Classification-F1 0.7441206286941566 on epoch=53
06/14/2022 02:22:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.33 on epoch=53
06/14/2022 02:22:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=54
06/14/2022 02:22:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=54
06/14/2022 02:22:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=55
06/14/2022 02:22:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=56
06/14/2022 02:22:56 - INFO - __main__ - Global step 900 Train loss 0.30 Classification-F1 0.7863897547610492 on epoch=56
06/14/2022 02:22:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7608176948961006 -> 0.7863897547610492 on epoch=56, global_step=900
06/14/2022 02:22:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=56
06/14/2022 02:23:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=57
06/14/2022 02:23:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=58
06/14/2022 02:23:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=58
06/14/2022 02:23:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=59
06/14/2022 02:23:12 - INFO - __main__ - Global step 950 Train loss 0.32 Classification-F1 0.7740851777330651 on epoch=59
06/14/2022 02:23:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.28 on epoch=59
06/14/2022 02:23:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.38 on epoch=60
06/14/2022 02:23:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.32 on epoch=61
06/14/2022 02:23:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=61
06/14/2022 02:23:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=62
06/14/2022 02:23:29 - INFO - __main__ - Global step 1000 Train loss 0.33 Classification-F1 0.7427006266009676 on epoch=62
06/14/2022 02:23:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=63
06/14/2022 02:23:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=63
06/14/2022 02:23:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.35 on epoch=64
06/14/2022 02:23:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=64
06/14/2022 02:23:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=65
06/14/2022 02:23:46 - INFO - __main__ - Global step 1050 Train loss 0.28 Classification-F1 0.7583578016424461 on epoch=65
06/14/2022 02:23:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=66
06/14/2022 02:23:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=66
06/14/2022 02:23:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=67
06/14/2022 02:23:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=68
06/14/2022 02:23:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
06/14/2022 02:24:02 - INFO - __main__ - Global step 1100 Train loss 0.26 Classification-F1 0.7874020917794529 on epoch=68
06/14/2022 02:24:02 - INFO - __main__ - Saving model with best Classification-F1: 0.7863897547610492 -> 0.7874020917794529 on epoch=68, global_step=1100
06/14/2022 02:24:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=69
06/14/2022 02:24:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
06/14/2022 02:24:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=70
06/14/2022 02:24:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=71
06/14/2022 02:24:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=71
06/14/2022 02:24:18 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.7238942205592601 on epoch=71
06/14/2022 02:24:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=72
06/14/2022 02:24:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=73
06/14/2022 02:24:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=73
06/14/2022 02:24:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=74
06/14/2022 02:24:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=74
06/14/2022 02:24:34 - INFO - __main__ - Global step 1200 Train loss 0.25 Classification-F1 0.7568230380730381 on epoch=74
06/14/2022 02:24:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=75
06/14/2022 02:24:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=76
06/14/2022 02:24:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=76
06/14/2022 02:24:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=77
06/14/2022 02:24:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=78
06/14/2022 02:24:51 - INFO - __main__ - Global step 1250 Train loss 0.26 Classification-F1 0.7233180154117919 on epoch=78
06/14/2022 02:24:53 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=78
06/14/2022 02:24:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=79
06/14/2022 02:24:58 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=79
06/14/2022 02:25:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=80
06/14/2022 02:25:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
06/14/2022 02:25:07 - INFO - __main__ - Global step 1300 Train loss 0.24 Classification-F1 0.7579717423301728 on epoch=81
06/14/2022 02:25:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=81
06/14/2022 02:25:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=82
06/14/2022 02:25:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=83
06/14/2022 02:25:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=83
06/14/2022 02:25:19 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=84
06/14/2022 02:25:23 - INFO - __main__ - Global step 1350 Train loss 0.28 Classification-F1 0.7834564647785791 on epoch=84
06/14/2022 02:25:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=84
06/14/2022 02:25:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=85
06/14/2022 02:25:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=86
06/14/2022 02:25:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=86
06/14/2022 02:25:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.30 on epoch=87
06/14/2022 02:25:39 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.7765503444369751 on epoch=87
06/14/2022 02:25:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=88
06/14/2022 02:25:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=88
06/14/2022 02:25:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.22 on epoch=89
06/14/2022 02:25:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=89
06/14/2022 02:25:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=90
06/14/2022 02:25:56 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.7915320749336157 on epoch=90
06/14/2022 02:25:56 - INFO - __main__ - Saving model with best Classification-F1: 0.7874020917794529 -> 0.7915320749336157 on epoch=90, global_step=1450
06/14/2022 02:25:58 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=91
06/14/2022 02:26:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=91
06/14/2022 02:26:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=92
06/14/2022 02:26:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=93
06/14/2022 02:26:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=93
06/14/2022 02:26:12 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.7366588965429643 on epoch=93
06/14/2022 02:26:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=94
06/14/2022 02:26:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=94
06/14/2022 02:26:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=95
06/14/2022 02:26:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
06/14/2022 02:26:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=96
06/14/2022 02:26:28 - INFO - __main__ - Global step 1550 Train loss 0.22 Classification-F1 0.7594799077641323 on epoch=96
06/14/2022 02:26:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=97
06/14/2022 02:26:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
06/14/2022 02:26:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=98
06/14/2022 02:26:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=99
06/14/2022 02:26:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.27 on epoch=99
06/14/2022 02:26:44 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.7545495771215196 on epoch=99
06/14/2022 02:26:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=100
06/14/2022 02:26:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
06/14/2022 02:26:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=101
06/14/2022 02:26:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=102
06/14/2022 02:26:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.30 on epoch=103
06/14/2022 02:27:00 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.7350406777130916 on epoch=103
06/14/2022 02:27:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=103
06/14/2022 02:27:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=104
06/14/2022 02:27:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=104
06/14/2022 02:27:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=105
06/14/2022 02:27:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
06/14/2022 02:27:16 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.7671119764524621 on epoch=106
06/14/2022 02:27:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=106
06/14/2022 02:27:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=107
06/14/2022 02:27:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=108
06/14/2022 02:27:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.18 on epoch=108
06/14/2022 02:27:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=109
06/14/2022 02:27:32 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.7689899730985571 on epoch=109
06/14/2022 02:27:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=109
06/14/2022 02:27:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=110
06/14/2022 02:27:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=111
06/14/2022 02:27:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=111
06/14/2022 02:27:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=112
06/14/2022 02:27:49 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.7901425360482408 on epoch=112
06/14/2022 02:27:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
06/14/2022 02:27:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=113
06/14/2022 02:27:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=114
06/14/2022 02:27:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=114
06/14/2022 02:28:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=115
06/14/2022 02:28:05 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.7727595059711848 on epoch=115
06/14/2022 02:28:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=116
06/14/2022 02:28:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=116
06/14/2022 02:28:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=117
06/14/2022 02:28:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=118
06/14/2022 02:28:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=118
06/14/2022 02:28:21 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.7535541255815938 on epoch=118
06/14/2022 02:28:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=119
06/14/2022 02:28:26 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
06/14/2022 02:28:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=120
06/14/2022 02:28:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=121
06/14/2022 02:28:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=121
06/14/2022 02:28:37 - INFO - __main__ - Global step 1950 Train loss 0.17 Classification-F1 0.7299594241419052 on epoch=121
06/14/2022 02:28:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=122
06/14/2022 02:28:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=123
06/14/2022 02:28:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=123
06/14/2022 02:28:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=124
06/14/2022 02:28:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.14 on epoch=124
06/14/2022 02:28:53 - INFO - __main__ - Global step 2000 Train loss 0.14 Classification-F1 0.7688299213315919 on epoch=124
06/14/2022 02:28:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=125
06/14/2022 02:28:59 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.17 on epoch=126
06/14/2022 02:29:01 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=126
06/14/2022 02:29:04 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=127
06/14/2022 02:29:06 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=128
06/14/2022 02:29:10 - INFO - __main__ - Global step 2050 Train loss 0.12 Classification-F1 0.7558992014720354 on epoch=128
06/14/2022 02:29:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.11 on epoch=128
06/14/2022 02:29:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=129
06/14/2022 02:29:17 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=129
06/14/2022 02:29:20 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=130
06/14/2022 02:29:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.12 on epoch=131
06/14/2022 02:29:26 - INFO - __main__ - Global step 2100 Train loss 0.12 Classification-F1 0.74725972926777 on epoch=131
06/14/2022 02:29:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=131
06/14/2022 02:29:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.11 on epoch=132
06/14/2022 02:29:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.23 on epoch=133
06/14/2022 02:29:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=133
06/14/2022 02:29:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.08 on epoch=134
06/14/2022 02:29:42 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.7771882916695206 on epoch=134
06/14/2022 02:29:45 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.11 on epoch=134
06/14/2022 02:29:47 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.21 on epoch=135
06/14/2022 02:29:50 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.23 on epoch=136
06/14/2022 02:29:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=136
06/14/2022 02:29:55 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=137
06/14/2022 02:29:58 - INFO - __main__ - Global step 2200 Train loss 0.16 Classification-F1 0.7877561480207258 on epoch=137
06/14/2022 02:30:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=138
06/14/2022 02:30:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=138
06/14/2022 02:30:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.17 on epoch=139
06/14/2022 02:30:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.17 on epoch=139
06/14/2022 02:30:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=140
06/14/2022 02:30:15 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.8000093189537223 on epoch=140
06/14/2022 02:30:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7915320749336157 -> 0.8000093189537223 on epoch=140, global_step=2250
06/14/2022 02:30:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=141
06/14/2022 02:30:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=141
06/14/2022 02:30:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=142
06/14/2022 02:30:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=143
06/14/2022 02:30:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=143
06/14/2022 02:30:31 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.7861481952391043 on epoch=143
06/14/2022 02:30:33 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
06/14/2022 02:30:36 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=144
06/14/2022 02:30:39 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=145
06/14/2022 02:30:41 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=146
06/14/2022 02:30:44 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=146
06/14/2022 02:30:47 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.7685981963207819 on epoch=146
06/14/2022 02:30:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=147
06/14/2022 02:30:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
06/14/2022 02:30:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.10 on epoch=148
06/14/2022 02:30:57 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.14 on epoch=149
06/14/2022 02:31:00 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.10 on epoch=149
06/14/2022 02:31:03 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7472911369775941 on epoch=149
06/14/2022 02:31:06 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=150
06/14/2022 02:31:08 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.19 on epoch=151
06/14/2022 02:31:11 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=151
06/14/2022 02:31:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=152
06/14/2022 02:31:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=153
06/14/2022 02:31:20 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.7668787857038313 on epoch=153
06/14/2022 02:31:22 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=153
06/14/2022 02:31:25 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.04 on epoch=154
06/14/2022 02:31:27 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.05 on epoch=154
06/14/2022 02:31:30 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=155
06/14/2022 02:31:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.08 on epoch=156
06/14/2022 02:31:36 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.7844079880132626 on epoch=156
06/14/2022 02:31:38 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=156
06/14/2022 02:31:41 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=157
06/14/2022 02:31:43 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
06/14/2022 02:31:46 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=158
06/14/2022 02:31:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=159
06/14/2022 02:31:52 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.7763199614006064 on epoch=159
06/14/2022 02:31:55 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=159
06/14/2022 02:31:57 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.10 on epoch=160
06/14/2022 02:32:00 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=161
06/14/2022 02:32:02 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=161
06/14/2022 02:32:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=162
06/14/2022 02:32:08 - INFO - __main__ - Global step 2600 Train loss 0.11 Classification-F1 0.7958596348852672 on epoch=162
06/14/2022 02:32:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
06/14/2022 02:32:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.03 on epoch=163
06/14/2022 02:32:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.05 on epoch=164
06/14/2022 02:32:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=164
06/14/2022 02:32:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.15 on epoch=165
06/14/2022 02:32:25 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.8086775766007295 on epoch=165
06/14/2022 02:32:25 - INFO - __main__ - Saving model with best Classification-F1: 0.8000093189537223 -> 0.8086775766007295 on epoch=165, global_step=2650
06/14/2022 02:32:27 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=166
06/14/2022 02:32:30 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=166
06/14/2022 02:32:32 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
06/14/2022 02:32:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
06/14/2022 02:32:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=168
06/14/2022 02:32:41 - INFO - __main__ - Global step 2700 Train loss 0.09 Classification-F1 0.7782986746847427 on epoch=168
06/14/2022 02:32:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.10 on epoch=169
06/14/2022 02:32:46 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=169
06/14/2022 02:32:49 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=170
06/14/2022 02:32:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
06/14/2022 02:32:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=171
06/14/2022 02:32:57 - INFO - __main__ - Global step 2750 Train loss 0.09 Classification-F1 0.7805244218926107 on epoch=171
06/14/2022 02:33:00 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=172
06/14/2022 02:33:02 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=173
06/14/2022 02:33:05 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
06/14/2022 02:33:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.16 on epoch=174
06/14/2022 02:33:10 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=174
06/14/2022 02:33:14 - INFO - __main__ - Global step 2800 Train loss 0.09 Classification-F1 0.7730580884836203 on epoch=174
06/14/2022 02:33:16 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=175
06/14/2022 02:33:19 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=176
06/14/2022 02:33:21 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=176
06/14/2022 02:33:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=177
06/14/2022 02:33:26 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.06 on epoch=178
06/14/2022 02:33:30 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.7479656190376507 on epoch=178
06/14/2022 02:33:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=178
06/14/2022 02:33:35 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.08 on epoch=179
06/14/2022 02:33:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=179
06/14/2022 02:33:40 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
06/14/2022 02:33:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.08 on epoch=181
06/14/2022 02:33:46 - INFO - __main__ - Global step 2900 Train loss 0.08 Classification-F1 0.797028057673288 on epoch=181
06/14/2022 02:33:49 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.09 on epoch=181
06/14/2022 02:33:51 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=182
06/14/2022 02:33:54 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=183
06/14/2022 02:33:56 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=183
06/14/2022 02:33:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.05 on epoch=184
06/14/2022 02:34:02 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.7605080563406665 on epoch=184
06/14/2022 02:34:05 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=184
06/14/2022 02:34:07 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.02 on epoch=185
06/14/2022 02:34:10 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=186
06/14/2022 02:34:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=186
06/14/2022 02:34:15 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=187
06/14/2022 02:34:16 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:34:16 - INFO - __main__ - Printing 3 examples
06/14/2022 02:34:16 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:34:16 - INFO - __main__ - ['others']
06/14/2022 02:34:16 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:34:16 - INFO - __main__ - ['others']
06/14/2022 02:34:16 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:34:16 - INFO - __main__ - ['others']
06/14/2022 02:34:16 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:34:17 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:34:17 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:34:17 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:34:17 - INFO - __main__ - Printing 3 examples
06/14/2022 02:34:17 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:34:17 - INFO - __main__ - ['others']
06/14/2022 02:34:17 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:34:17 - INFO - __main__ - ['others']
06/14/2022 02:34:17 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:34:17 - INFO - __main__ - ['others']
06/14/2022 02:34:17 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:34:17 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:34:17 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:34:19 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.8046892230576441 on epoch=187
06/14/2022 02:34:19 - INFO - __main__ - save last model!
06/14/2022 02:34:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 02:34:19 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 02:34:19 - INFO - __main__ - Printing 3 examples
06/14/2022 02:34:19 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 02:34:19 - INFO - __main__ - ['others']
06/14/2022 02:34:19 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 02:34:19 - INFO - __main__ - ['others']
06/14/2022 02:34:19 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 02:34:19 - INFO - __main__ - ['others']
06/14/2022 02:34:19 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:34:21 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:34:26 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 02:34:35 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:34:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:34:36 - INFO - __main__ - Starting training!
06/14/2022 02:35:43 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_87_0.4_8_predictions.txt
06/14/2022 02:35:43 - INFO - __main__ - Classification-F1 on test data: 0.3311
06/14/2022 02:35:43 - INFO - __main__ - prefix=emo_64_87, lr=0.4, bsz=8, dev_performance=0.8086775766007295, test_performance=0.3310947985825232
06/14/2022 02:35:43 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.3, bsz=8 ...
06/14/2022 02:35:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:35:44 - INFO - __main__ - Printing 3 examples
06/14/2022 02:35:44 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:35:44 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:35:44 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:35:44 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:35:44 - INFO - __main__ - Printing 3 examples
06/14/2022 02:35:44 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:35:44 - INFO - __main__ - ['others']
06/14/2022 02:35:44 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:35:45 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:35:45 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:36:03 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:36:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:36:04 - INFO - __main__ - Starting training!
06/14/2022 02:36:07 - INFO - __main__ - Step 10 Global step 10 Train loss 2.89 on epoch=0
06/14/2022 02:36:10 - INFO - __main__ - Step 20 Global step 20 Train loss 1.70 on epoch=1
06/14/2022 02:36:12 - INFO - __main__ - Step 30 Global step 30 Train loss 1.23 on epoch=1
06/14/2022 02:36:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.11 on epoch=2
06/14/2022 02:36:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.89 on epoch=3
06/14/2022 02:36:21 - INFO - __main__ - Global step 50 Train loss 1.56 Classification-F1 0.3026165857351082 on epoch=3
06/14/2022 02:36:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3026165857351082 on epoch=3, global_step=50
06/14/2022 02:36:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.97 on epoch=3
06/14/2022 02:36:26 - INFO - __main__ - Step 70 Global step 70 Train loss 1.00 on epoch=4
06/14/2022 02:36:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.85 on epoch=4
06/14/2022 02:36:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.90 on epoch=5
06/14/2022 02:36:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.96 on epoch=6
06/14/2022 02:36:36 - INFO - __main__ - Global step 100 Train loss 0.94 Classification-F1 0.33524983293247235 on epoch=6
06/14/2022 02:36:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3026165857351082 -> 0.33524983293247235 on epoch=6, global_step=100
06/14/2022 02:36:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=6
06/14/2022 02:36:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=7
06/14/2022 02:36:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.82 on epoch=8
06/14/2022 02:36:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.73 on epoch=8
06/14/2022 02:36:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.93 on epoch=9
06/14/2022 02:36:52 - INFO - __main__ - Global step 150 Train loss 0.86 Classification-F1 0.2336272160539204 on epoch=9
06/14/2022 02:36:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.82 on epoch=9
06/14/2022 02:36:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.77 on epoch=10
06/14/2022 02:37:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.80 on epoch=11
06/14/2022 02:37:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=11
06/14/2022 02:37:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.86 on epoch=12
06/14/2022 02:37:08 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.44647091635533415 on epoch=12
06/14/2022 02:37:08 - INFO - __main__ - Saving model with best Classification-F1: 0.33524983293247235 -> 0.44647091635533415 on epoch=12, global_step=200
06/14/2022 02:37:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.80 on epoch=13
06/14/2022 02:37:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.69 on epoch=13
06/14/2022 02:37:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=14
06/14/2022 02:37:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.79 on epoch=14
06/14/2022 02:37:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.65 on epoch=15
06/14/2022 02:37:24 - INFO - __main__ - Global step 250 Train loss 0.72 Classification-F1 0.3192673621172607 on epoch=15
06/14/2022 02:37:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.79 on epoch=16
06/14/2022 02:37:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=16
06/14/2022 02:37:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=17
06/14/2022 02:37:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.68 on epoch=18
06/14/2022 02:37:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.68 on epoch=18
06/14/2022 02:37:39 - INFO - __main__ - Global step 300 Train loss 0.71 Classification-F1 0.5964986115584944 on epoch=18
06/14/2022 02:37:39 - INFO - __main__ - Saving model with best Classification-F1: 0.44647091635533415 -> 0.5964986115584944 on epoch=18, global_step=300
06/14/2022 02:37:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=19
06/14/2022 02:37:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.67 on epoch=19
06/14/2022 02:37:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.70 on epoch=20
06/14/2022 02:37:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.75 on epoch=21
06/14/2022 02:37:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.63 on epoch=21
06/14/2022 02:37:54 - INFO - __main__ - Global step 350 Train loss 0.67 Classification-F1 0.6024900096328668 on epoch=21
06/14/2022 02:37:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5964986115584944 -> 0.6024900096328668 on epoch=21, global_step=350
06/14/2022 02:37:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=22
06/14/2022 02:37:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.67 on epoch=23
06/14/2022 02:38:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.64 on epoch=23
06/14/2022 02:38:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.63 on epoch=24
06/14/2022 02:38:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.58 on epoch=24
06/14/2022 02:38:10 - INFO - __main__ - Global step 400 Train loss 0.63 Classification-F1 0.6750870347456994 on epoch=24
06/14/2022 02:38:10 - INFO - __main__ - Saving model with best Classification-F1: 0.6024900096328668 -> 0.6750870347456994 on epoch=24, global_step=400
06/14/2022 02:38:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.63 on epoch=25
06/14/2022 02:38:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=26
06/14/2022 02:38:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.61 on epoch=26
06/14/2022 02:38:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.62 on epoch=27
06/14/2022 02:38:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=28
06/14/2022 02:38:26 - INFO - __main__ - Global step 450 Train loss 0.62 Classification-F1 0.5260621271642659 on epoch=28
06/14/2022 02:38:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=28
06/14/2022 02:38:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.62 on epoch=29
06/14/2022 02:38:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=29
06/14/2022 02:38:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.61 on epoch=30
06/14/2022 02:38:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.60 on epoch=31
06/14/2022 02:38:41 - INFO - __main__ - Global step 500 Train loss 0.59 Classification-F1 0.5280505293343862 on epoch=31
06/14/2022 02:38:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.59 on epoch=31
06/14/2022 02:38:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=32
06/14/2022 02:38:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.64 on epoch=33
06/14/2022 02:38:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=33
06/14/2022 02:38:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.56 on epoch=34
06/14/2022 02:38:57 - INFO - __main__ - Global step 550 Train loss 0.58 Classification-F1 0.6953456333299071 on epoch=34
06/14/2022 02:38:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6750870347456994 -> 0.6953456333299071 on epoch=34, global_step=550
06/14/2022 02:38:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=34
06/14/2022 02:39:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=35
06/14/2022 02:39:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.55 on epoch=36
06/14/2022 02:39:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=36
06/14/2022 02:39:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=37
06/14/2022 02:39:12 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.7249272733849684 on epoch=37
06/14/2022 02:39:12 - INFO - __main__ - Saving model with best Classification-F1: 0.6953456333299071 -> 0.7249272733849684 on epoch=37, global_step=600
06/14/2022 02:39:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=38
06/14/2022 02:39:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.53 on epoch=38
06/14/2022 02:39:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.57 on epoch=39
06/14/2022 02:39:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=39
06/14/2022 02:39:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.49 on epoch=40
06/14/2022 02:39:28 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.655507099540402 on epoch=40
06/14/2022 02:39:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=41
06/14/2022 02:39:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=41
06/14/2022 02:39:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.54 on epoch=42
06/14/2022 02:39:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=43
06/14/2022 02:39:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=43
06/14/2022 02:39:43 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.6720825064234588 on epoch=43
06/14/2022 02:39:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=44
06/14/2022 02:39:48 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=44
06/14/2022 02:39:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=45
06/14/2022 02:39:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=46
06/14/2022 02:39:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.41 on epoch=46
06/14/2022 02:39:59 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.5965670548184555 on epoch=46
06/14/2022 02:40:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=47
06/14/2022 02:40:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=48
06/14/2022 02:40:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=48
06/14/2022 02:40:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=49
06/14/2022 02:40:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=49
06/14/2022 02:40:15 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.7577948537250864 on epoch=49
06/14/2022 02:40:15 - INFO - __main__ - Saving model with best Classification-F1: 0.7249272733849684 -> 0.7577948537250864 on epoch=49, global_step=800
06/14/2022 02:40:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=50
06/14/2022 02:40:20 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=51
06/14/2022 02:40:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=51
06/14/2022 02:40:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=52
06/14/2022 02:40:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=53
06/14/2022 02:40:31 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.6852809169264864 on epoch=53
06/14/2022 02:40:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
06/14/2022 02:40:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=54
06/14/2022 02:40:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=54
06/14/2022 02:40:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=55
06/14/2022 02:40:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=56
06/14/2022 02:40:46 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.7156528791565289 on epoch=56
06/14/2022 02:40:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=56
06/14/2022 02:40:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=57
06/14/2022 02:40:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=58
06/14/2022 02:40:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=58
06/14/2022 02:40:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=59
06/14/2022 02:41:02 - INFO - __main__ - Global step 950 Train loss 0.39 Classification-F1 0.7550009750226864 on epoch=59
06/14/2022 02:41:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=59
06/14/2022 02:41:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=60
06/14/2022 02:41:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=61
06/14/2022 02:41:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=61
06/14/2022 02:41:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=62
06/14/2022 02:41:18 - INFO - __main__ - Global step 1000 Train loss 0.40 Classification-F1 0.705636461072695 on epoch=62
06/14/2022 02:41:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.41 on epoch=63
06/14/2022 02:41:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=63
06/14/2022 02:41:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=64
06/14/2022 02:41:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=64
06/14/2022 02:41:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=65
06/14/2022 02:41:33 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.7598354989455203 on epoch=65
06/14/2022 02:41:33 - INFO - __main__ - Saving model with best Classification-F1: 0.7577948537250864 -> 0.7598354989455203 on epoch=65, global_step=1050
06/14/2022 02:41:36 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=66
06/14/2022 02:41:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=66
06/14/2022 02:41:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=67
06/14/2022 02:41:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=68
06/14/2022 02:41:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=68
06/14/2022 02:41:49 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.7227193780202927 on epoch=68
06/14/2022 02:41:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=69
06/14/2022 02:41:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=69
06/14/2022 02:41:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=70
06/14/2022 02:41:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=71
06/14/2022 02:42:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=71
06/14/2022 02:42:05 - INFO - __main__ - Global step 1150 Train loss 0.38 Classification-F1 0.7216666666666667 on epoch=71
06/14/2022 02:42:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.37 on epoch=72
06/14/2022 02:42:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=73
06/14/2022 02:42:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=73
06/14/2022 02:42:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.35 on epoch=74
06/14/2022 02:42:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=74
06/14/2022 02:42:20 - INFO - __main__ - Global step 1200 Train loss 0.33 Classification-F1 0.5905648558338215 on epoch=74
06/14/2022 02:42:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=75
06/14/2022 02:42:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=76
06/14/2022 02:42:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=76
06/14/2022 02:42:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=77
06/14/2022 02:42:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=78
06/14/2022 02:42:36 - INFO - __main__ - Global step 1250 Train loss 0.32 Classification-F1 0.6908979183488988 on epoch=78
06/14/2022 02:42:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=78
06/14/2022 02:42:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=79
06/14/2022 02:42:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=79
06/14/2022 02:42:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=80
06/14/2022 02:42:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=81
06/14/2022 02:42:51 - INFO - __main__ - Global step 1300 Train loss 0.28 Classification-F1 0.7088258958357863 on epoch=81
06/14/2022 02:42:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=81
06/14/2022 02:42:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=82
06/14/2022 02:42:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.36 on epoch=83
06/14/2022 02:43:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.29 on epoch=83
06/14/2022 02:43:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.32 on epoch=84
06/14/2022 02:43:07 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.7410201047785402 on epoch=84
06/14/2022 02:43:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.32 on epoch=84
06/14/2022 02:43:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=85
06/14/2022 02:43:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=86
06/14/2022 02:43:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.33 on epoch=86
06/14/2022 02:43:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=87
06/14/2022 02:43:23 - INFO - __main__ - Global step 1400 Train loss 0.32 Classification-F1 0.7508395332162426 on epoch=87
06/14/2022 02:43:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=88
06/14/2022 02:43:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=88
06/14/2022 02:43:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=89
06/14/2022 02:43:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=89
06/14/2022 02:43:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=90
06/14/2022 02:43:38 - INFO - __main__ - Global step 1450 Train loss 0.27 Classification-F1 0.7350644742515509 on epoch=90
06/14/2022 02:43:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=91
06/14/2022 02:43:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.28 on epoch=91
06/14/2022 02:43:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=92
06/14/2022 02:43:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.28 on epoch=93
06/14/2022 02:43:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=93
06/14/2022 02:43:54 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.6681586485803354 on epoch=93
06/14/2022 02:43:56 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=94
06/14/2022 02:43:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=94
06/14/2022 02:44:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=95
06/14/2022 02:44:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=96
06/14/2022 02:44:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.28 on epoch=96
06/14/2022 02:44:10 - INFO - __main__ - Global step 1550 Train loss 0.26 Classification-F1 0.5836106405386094 on epoch=96
06/14/2022 02:44:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=97
06/14/2022 02:44:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=98
06/14/2022 02:44:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=98
06/14/2022 02:44:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.31 on epoch=99
06/14/2022 02:44:22 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=99
06/14/2022 02:44:25 - INFO - __main__ - Global step 1600 Train loss 0.25 Classification-F1 0.5592433714178278 on epoch=99
06/14/2022 02:44:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=100
06/14/2022 02:44:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=101
06/14/2022 02:44:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=101
06/14/2022 02:44:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=102
06/14/2022 02:44:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=103
06/14/2022 02:44:41 - INFO - __main__ - Global step 1650 Train loss 0.23 Classification-F1 0.5738954131769404 on epoch=103
06/14/2022 02:44:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=103
06/14/2022 02:44:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=104
06/14/2022 02:44:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.28 on epoch=104
06/14/2022 02:44:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=105
06/14/2022 02:44:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.26 on epoch=106
06/14/2022 02:44:57 - INFO - __main__ - Global step 1700 Train loss 0.26 Classification-F1 0.7240862398471093 on epoch=106
06/14/2022 02:44:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.30 on epoch=106
06/14/2022 02:45:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=107
06/14/2022 02:45:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
06/14/2022 02:45:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=108
06/14/2022 02:45:09 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=109
06/14/2022 02:45:12 - INFO - __main__ - Global step 1750 Train loss 0.25 Classification-F1 0.7047045532803189 on epoch=109
06/14/2022 02:45:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=109
06/14/2022 02:45:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=110
06/14/2022 02:45:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=111
06/14/2022 02:45:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=111
06/14/2022 02:45:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=112
06/14/2022 02:45:28 - INFO - __main__ - Global step 1800 Train loss 0.25 Classification-F1 0.7393067226890757 on epoch=112
06/14/2022 02:45:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=113
06/14/2022 02:45:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=113
06/14/2022 02:45:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=114
06/14/2022 02:45:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.27 on epoch=114
06/14/2022 02:45:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=115
06/14/2022 02:45:44 - INFO - __main__ - Global step 1850 Train loss 0.25 Classification-F1 0.7786064425770308 on epoch=115
06/14/2022 02:45:44 - INFO - __main__ - Saving model with best Classification-F1: 0.7598354989455203 -> 0.7786064425770308 on epoch=115, global_step=1850
06/14/2022 02:45:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.22 on epoch=116
06/14/2022 02:45:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.23 on epoch=116
06/14/2022 02:45:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=117
06/14/2022 02:45:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=118
06/14/2022 02:45:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=118
06/14/2022 02:46:00 - INFO - __main__ - Global step 1900 Train loss 0.22 Classification-F1 0.7135152378258997 on epoch=118
06/14/2022 02:46:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
06/14/2022 02:46:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.23 on epoch=119
06/14/2022 02:46:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=120
06/14/2022 02:46:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=121
06/14/2022 02:46:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=121
06/14/2022 02:46:15 - INFO - __main__ - Global step 1950 Train loss 0.20 Classification-F1 0.5840422735159577 on epoch=121
06/14/2022 02:46:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=122
06/14/2022 02:46:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
06/14/2022 02:46:23 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=123
06/14/2022 02:46:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=124
06/14/2022 02:46:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=124
06/14/2022 02:46:31 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.6954066102499892 on epoch=124
06/14/2022 02:46:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=125
06/14/2022 02:46:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.23 on epoch=126
06/14/2022 02:46:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.23 on epoch=126
06/14/2022 02:46:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=127
06/14/2022 02:46:43 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.26 on epoch=128
06/14/2022 02:46:47 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.6918769284455257 on epoch=128
06/14/2022 02:46:49 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.12 on epoch=128
06/14/2022 02:46:51 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=129
06/14/2022 02:46:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.23 on epoch=129
06/14/2022 02:46:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.16 on epoch=130
06/14/2022 02:46:59 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.22 on epoch=131
06/14/2022 02:47:02 - INFO - __main__ - Global step 2100 Train loss 0.18 Classification-F1 0.7085892049127344 on epoch=131
06/14/2022 02:47:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
06/14/2022 02:47:07 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.18 on epoch=132
06/14/2022 02:47:10 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.19 on epoch=133
06/14/2022 02:47:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=133
06/14/2022 02:47:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.22 on epoch=134
06/14/2022 02:47:18 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.734841142082665 on epoch=134
06/14/2022 02:47:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.18 on epoch=134
06/14/2022 02:47:23 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.15 on epoch=135
06/14/2022 02:47:25 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=136
06/14/2022 02:47:28 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.18 on epoch=136
06/14/2022 02:47:30 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=137
06/14/2022 02:47:34 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7287217406607946 on epoch=137
06/14/2022 02:47:36 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.26 on epoch=138
06/14/2022 02:47:38 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.17 on epoch=138
06/14/2022 02:47:41 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.22 on epoch=139
06/14/2022 02:47:43 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.25 on epoch=139
06/14/2022 02:47:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.14 on epoch=140
06/14/2022 02:47:49 - INFO - __main__ - Global step 2250 Train loss 0.21 Classification-F1 0.7316207988923884 on epoch=140
06/14/2022 02:47:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.22 on epoch=141
06/14/2022 02:47:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.17 on epoch=141
06/14/2022 02:47:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=142
06/14/2022 02:47:59 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.24 on epoch=143
06/14/2022 02:48:02 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.12 on epoch=143
06/14/2022 02:48:05 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.7275460854441148 on epoch=143
06/14/2022 02:48:07 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=144
06/14/2022 02:48:10 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.24 on epoch=144
06/14/2022 02:48:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=145
06/14/2022 02:48:15 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.15 on epoch=146
06/14/2022 02:48:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=146
06/14/2022 02:48:20 - INFO - __main__ - Global step 2350 Train loss 0.17 Classification-F1 0.7128058901991086 on epoch=146
06/14/2022 02:48:23 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.15 on epoch=147
06/14/2022 02:48:25 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.19 on epoch=148
06/14/2022 02:48:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.12 on epoch=148
06/14/2022 02:48:30 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.23 on epoch=149
06/14/2022 02:48:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=149
06/14/2022 02:48:36 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.771168432849335 on epoch=149
06/14/2022 02:48:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.17 on epoch=150
06/14/2022 02:48:41 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.21 on epoch=151
06/14/2022 02:48:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=151
06/14/2022 02:48:46 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.12 on epoch=152
06/14/2022 02:48:48 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.15 on epoch=153
06/14/2022 02:48:52 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.7104133648421727 on epoch=153
06/14/2022 02:48:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.05 on epoch=153
06/14/2022 02:48:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.25 on epoch=154
06/14/2022 02:48:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.13 on epoch=154
06/14/2022 02:49:02 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=155
06/14/2022 02:49:04 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.14 on epoch=156
06/14/2022 02:49:07 - INFO - __main__ - Global step 2500 Train loss 0.13 Classification-F1 0.7230655470423436 on epoch=156
06/14/2022 02:49:10 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.19 on epoch=156
06/14/2022 02:49:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.20 on epoch=157
06/14/2022 02:49:15 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.15 on epoch=158
06/14/2022 02:49:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=158
06/14/2022 02:49:20 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.11 on epoch=159
06/14/2022 02:49:23 - INFO - __main__ - Global step 2550 Train loss 0.16 Classification-F1 0.7121950506358254 on epoch=159
06/14/2022 02:49:26 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.14 on epoch=159
06/14/2022 02:49:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.16 on epoch=160
06/14/2022 02:49:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.16 on epoch=161
06/14/2022 02:49:33 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.19 on epoch=161
06/14/2022 02:49:36 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.11 on epoch=162
06/14/2022 02:49:39 - INFO - __main__ - Global step 2600 Train loss 0.15 Classification-F1 0.7287047829083961 on epoch=162
06/14/2022 02:49:41 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=163
06/14/2022 02:49:44 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.08 on epoch=163
06/14/2022 02:49:46 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.21 on epoch=164
06/14/2022 02:49:49 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=164
06/14/2022 02:49:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.23 on epoch=165
06/14/2022 02:49:55 - INFO - __main__ - Global step 2650 Train loss 0.17 Classification-F1 0.735679695028215 on epoch=165
06/14/2022 02:49:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=166
06/14/2022 02:49:59 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.13 on epoch=166
06/14/2022 02:50:02 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.15 on epoch=167
06/14/2022 02:50:04 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.16 on epoch=168
06/14/2022 02:50:07 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=168
06/14/2022 02:50:10 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.7218048621434862 on epoch=168
06/14/2022 02:50:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.15 on epoch=169
06/14/2022 02:50:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.15 on epoch=169
06/14/2022 02:50:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.13 on epoch=170
06/14/2022 02:50:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.09 on epoch=171
06/14/2022 02:50:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.15 on epoch=171
06/14/2022 02:50:26 - INFO - __main__ - Global step 2750 Train loss 0.14 Classification-F1 0.7587964272247361 on epoch=171
06/14/2022 02:50:28 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=172
06/14/2022 02:50:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.16 on epoch=173
06/14/2022 02:50:33 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=173
06/14/2022 02:50:36 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=174
06/14/2022 02:50:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=174
06/14/2022 02:50:42 - INFO - __main__ - Global step 2800 Train loss 0.13 Classification-F1 0.742238707257364 on epoch=174
06/14/2022 02:50:44 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.11 on epoch=175
06/14/2022 02:50:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.09 on epoch=176
06/14/2022 02:50:49 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.17 on epoch=176
06/14/2022 02:50:52 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.16 on epoch=177
06/14/2022 02:50:54 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.19 on epoch=178
06/14/2022 02:50:57 - INFO - __main__ - Global step 2850 Train loss 0.15 Classification-F1 0.7496826750023682 on epoch=178
06/14/2022 02:51:00 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.16 on epoch=178
06/14/2022 02:51:02 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=179
06/14/2022 02:51:05 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.15 on epoch=179
06/14/2022 02:51:07 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.12 on epoch=180
06/14/2022 02:51:10 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.14 on epoch=181
06/14/2022 02:51:13 - INFO - __main__ - Global step 2900 Train loss 0.14 Classification-F1 0.7372711333769095 on epoch=181
06/14/2022 02:51:16 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/14/2022 02:51:18 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.07 on epoch=182
06/14/2022 02:51:20 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.11 on epoch=183
06/14/2022 02:51:23 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.14 on epoch=183
06/14/2022 02:51:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.09 on epoch=184
06/14/2022 02:51:29 - INFO - __main__ - Global step 2950 Train loss 0.10 Classification-F1 0.7215686964732387 on epoch=184
06/14/2022 02:51:31 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.18 on epoch=184
06/14/2022 02:51:34 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.14 on epoch=185
06/14/2022 02:51:36 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.19 on epoch=186
06/14/2022 02:51:38 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.13 on epoch=186
06/14/2022 02:51:41 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=187
06/14/2022 02:51:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:51:42 - INFO - __main__ - Printing 3 examples
06/14/2022 02:51:42 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:51:42 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:51:42 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:51:42 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:51:42 - INFO - __main__ - Printing 3 examples
06/14/2022 02:51:42 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:51:42 - INFO - __main__ - ['others']
06/14/2022 02:51:42 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:51:42 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:51:43 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:51:44 - INFO - __main__ - Global step 3000 Train loss 0.15 Classification-F1 0.7422764516434592 on epoch=187
06/14/2022 02:51:44 - INFO - __main__ - save last model!
06/14/2022 02:51:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 02:51:44 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 02:51:44 - INFO - __main__ - Printing 3 examples
06/14/2022 02:51:44 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 02:51:44 - INFO - __main__ - ['others']
06/14/2022 02:51:44 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 02:51:44 - INFO - __main__ - ['others']
06/14/2022 02:51:44 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 02:51:44 - INFO - __main__ - ['others']
06/14/2022 02:51:44 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:51:46 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:51:52 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 02:51:58 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:51:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:51:59 - INFO - __main__ - Starting training!
06/14/2022 02:53:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_87_0.3_8_predictions.txt
06/14/2022 02:53:04 - INFO - __main__ - Classification-F1 on test data: 0.4645
06/14/2022 02:53:04 - INFO - __main__ - prefix=emo_64_87, lr=0.3, bsz=8, dev_performance=0.7786064425770308, test_performance=0.4644660181291144
06/14/2022 02:53:04 - INFO - __main__ - Running ... prefix=emo_64_87, lr=0.2, bsz=8 ...
06/14/2022 02:53:05 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:53:05 - INFO - __main__ - Printing 3 examples
06/14/2022 02:53:05 - INFO - __main__ -  [emo] cool i agree cool info  whats the information u gave
06/14/2022 02:53:05 - INFO - __main__ - ['others']
06/14/2022 02:53:05 - INFO - __main__ -  [emo] will still love her will you oh btw who are you loving again grinningsquintingface my baby
06/14/2022 02:53:05 - INFO - __main__ - ['others']
06/14/2022 02:53:05 - INFO - __main__ -  [emo] nayis thenks bro what  you're doing
06/14/2022 02:53:05 - INFO - __main__ - ['others']
06/14/2022 02:53:05 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:53:05 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:53:06 - INFO - __main__ - Loaded 256 examples from train data
06/14/2022 02:53:06 - INFO - __main__ - Start tokenizing ... 256 instances
06/14/2022 02:53:06 - INFO - __main__ - Printing 3 examples
06/14/2022 02:53:06 - INFO - __main__ -  [emo] wt abt u still half asleep d useropenreflink
06/14/2022 02:53:06 - INFO - __main__ - ['others']
06/14/2022 02:53:06 - INFO - __main__ -  [emo] waiting for going out for icecream my cake was a giant chocolate chip lava cookie topped with vanilla ice cream why there are quotes around cake
06/14/2022 02:53:06 - INFO - __main__ - ['others']
06/14/2022 02:53:06 - INFO - __main__ -  [emo] how do i know you were thinking about me i asked in dinner very
06/14/2022 02:53:06 - INFO - __main__ - ['others']
06/14/2022 02:53:06 - INFO - __main__ - Tokenizing Input ...
06/14/2022 02:53:06 - INFO - __main__ - Tokenizing Output ...
06/14/2022 02:53:06 - INFO - __main__ - Loaded 256 examples from dev data
06/14/2022 02:53:21 - INFO - __main__ - load prompt embedding from ckpt
06/14/2022 02:53:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/14/2022 02:53:22 - INFO - __main__ - Starting training!
06/14/2022 02:53:25 - INFO - __main__ - Step 10 Global step 10 Train loss 3.20 on epoch=0
06/14/2022 02:53:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.82 on epoch=1
06/14/2022 02:53:30 - INFO - __main__ - Step 30 Global step 30 Train loss 1.50 on epoch=1
06/14/2022 02:53:32 - INFO - __main__ - Step 40 Global step 40 Train loss 1.19 on epoch=2
06/14/2022 02:53:35 - INFO - __main__ - Step 50 Global step 50 Train loss 1.09 on epoch=3
06/14/2022 02:53:38 - INFO - __main__ - Global step 50 Train loss 1.76 Classification-F1 0.19260444952058228 on epoch=3
06/14/2022 02:53:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.19260444952058228 on epoch=3, global_step=50
06/14/2022 02:53:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.98 on epoch=3
06/14/2022 02:53:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.01 on epoch=4
06/14/2022 02:53:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.93 on epoch=4
06/14/2022 02:53:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.92 on epoch=5
06/14/2022 02:53:50 - INFO - __main__ - Step 100 Global step 100 Train loss 0.94 on epoch=6
06/14/2022 02:53:54 - INFO - __main__ - Global step 100 Train loss 0.95 Classification-F1 0.2514847177044409 on epoch=6
06/14/2022 02:53:54 - INFO - __main__ - Saving model with best Classification-F1: 0.19260444952058228 -> 0.2514847177044409 on epoch=6, global_step=100
06/14/2022 02:53:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.89 on epoch=6
06/14/2022 02:53:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.88 on epoch=7
06/14/2022 02:54:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.92 on epoch=8
06/14/2022 02:54:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.84 on epoch=8
06/14/2022 02:54:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.88 on epoch=9
06/14/2022 02:54:09 - INFO - __main__ - Global step 150 Train loss 0.88 Classification-F1 0.11578044596912522 on epoch=9
06/14/2022 02:54:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.86 on epoch=9
06/14/2022 02:54:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.76 on epoch=10
06/14/2022 02:54:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.81 on epoch=11
06/14/2022 02:54:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.87 on epoch=11
06/14/2022 02:54:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.77 on epoch=12
06/14/2022 02:54:25 - INFO - __main__ - Global step 200 Train loss 0.81 Classification-F1 0.25101488190051424 on epoch=12
06/14/2022 02:54:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.91 on epoch=13
06/14/2022 02:54:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.81 on epoch=13
06/14/2022 02:54:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=14
06/14/2022 02:54:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.76 on epoch=14
06/14/2022 02:54:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.80 on epoch=15
06/14/2022 02:54:40 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.5011363542949598 on epoch=15
06/14/2022 02:54:40 - INFO - __main__ - Saving model with best Classification-F1: 0.2514847177044409 -> 0.5011363542949598 on epoch=15, global_step=250
06/14/2022 02:54:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.83 on epoch=16
06/14/2022 02:54:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.74 on epoch=16
06/14/2022 02:54:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.77 on epoch=17
06/14/2022 02:54:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.75 on epoch=18
06/14/2022 02:54:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=18
06/14/2022 02:54:56 - INFO - __main__ - Global step 300 Train loss 0.75 Classification-F1 0.48143326938887365 on epoch=18
06/14/2022 02:54:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.80 on epoch=19
06/14/2022 02:55:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.75 on epoch=19
06/14/2022 02:55:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.73 on epoch=20
06/14/2022 02:55:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.81 on epoch=21
06/14/2022 02:55:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.70 on epoch=21
06/14/2022 02:55:11 - INFO - __main__ - Global step 350 Train loss 0.76 Classification-F1 0.53963056635754 on epoch=21
06/14/2022 02:55:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5011363542949598 -> 0.53963056635754 on epoch=21, global_step=350
06/14/2022 02:55:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.70 on epoch=22
06/14/2022 02:55:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.65 on epoch=23
06/14/2022 02:55:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.57 on epoch=23
06/14/2022 02:55:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=24
06/14/2022 02:55:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.60 on epoch=24
06/14/2022 02:55:27 - INFO - __main__ - Global step 400 Train loss 0.64 Classification-F1 0.6766052330367907 on epoch=24
06/14/2022 02:55:27 - INFO - __main__ - Saving model with best Classification-F1: 0.53963056635754 -> 0.6766052330367907 on epoch=24, global_step=400
06/14/2022 02:55:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=25
06/14/2022 02:55:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.61 on epoch=26
06/14/2022 02:55:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.62 on epoch=26
06/14/2022 02:55:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.59 on epoch=27
06/14/2022 02:55:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.68 on epoch=28
06/14/2022 02:55:42 - INFO - __main__ - Global step 450 Train loss 0.60 Classification-F1 0.5826302489254467 on epoch=28
06/14/2022 02:55:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.62 on epoch=28
06/14/2022 02:55:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=29
06/14/2022 02:55:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.59 on epoch=29
06/14/2022 02:55:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.57 on epoch=30
06/14/2022 02:55:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.65 on epoch=31
06/14/2022 02:55:58 - INFO - __main__ - Global step 500 Train loss 0.62 Classification-F1 0.6946483989279164 on epoch=31
06/14/2022 02:55:58 - INFO - __main__ - Saving model with best Classification-F1: 0.6766052330367907 -> 0.6946483989279164 on epoch=31, global_step=500
06/14/2022 02:56:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.56 on epoch=31
06/14/2022 02:56:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.62 on epoch=32
06/14/2022 02:56:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.62 on epoch=33
06/14/2022 02:56:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=33
06/14/2022 02:56:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=34
06/14/2022 02:56:13 - INFO - __main__ - Global step 550 Train loss 0.56 Classification-F1 0.658498131121221 on epoch=34
06/14/2022 02:56:16 - INFO - __main__ - Step 560 Global step 560 Train loss 0.53 on epoch=34
06/14/2022 02:56:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=35
06/14/2022 02:56:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=36
06/14/2022 02:56:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.60 on epoch=36
06/14/2022 02:56:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.56 on epoch=37
06/14/2022 02:56:29 - INFO - __main__ - Global step 600 Train loss 0.56 Classification-F1 0.5943360471808035 on epoch=37
06/14/2022 02:56:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=38
06/14/2022 02:56:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.55 on epoch=38
06/14/2022 02:56:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.57 on epoch=39
06/14/2022 02:56:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.53 on epoch=39
06/14/2022 02:56:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.55 on epoch=40
06/14/2022 02:56:44 - INFO - __main__ - Global step 650 Train loss 0.57 Classification-F1 0.68528985621575 on epoch=40
06/14/2022 02:56:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=41
06/14/2022 02:56:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=41
06/14/2022 02:56:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.61 on epoch=42
06/14/2022 02:56:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=43
06/14/2022 02:56:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=43
06/14/2022 02:57:00 - INFO - __main__ - Global step 700 Train loss 0.53 Classification-F1 0.5905210803855275 on epoch=43
06/14/2022 02:57:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=44
06/14/2022 02:57:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=44
06/14/2022 02:57:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=45
06/14/2022 02:57:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=46
06/14/2022 02:57:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.57 on epoch=46
06/14/2022 02:57:15 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.5617141610119896 on epoch=46
06/14/2022 02:57:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=47
06/14/2022 02:57:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.61 on epoch=48
06/14/2022 02:57:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.58 on epoch=48
06/14/2022 02:57:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.51 on epoch=49
06/14/2022 02:57:27 - INFO - __main__ - Step 800 Global step 800 Train loss 0.46 on epoch=49
06/14/2022 02:57:31 - INFO - __main__ - Global step 800 Train loss 0.54 Classification-F1 0.7193262442897659 on epoch=49
06/14/2022 02:57:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6946483989279164 -> 0.7193262442897659 on epoch=49, global_step=800
06/14/2022 02:57:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=50
06/14/2022 02:57:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.60 on epoch=51
06/14/2022 02:57:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=51
06/14/2022 02:57:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=52
06/14/2022 02:57:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=53
06/14/2022 02:57:46 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.691373461646652 on epoch=53
06/14/2022 02:57:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=53
06/14/2022 02:57:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=54
06/14/2022 02:57:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.51 on epoch=54
06/14/2022 02:57:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=55
06/14/2022 02:57:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=56
06/14/2022 02:58:02 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.6945538467543619 on epoch=56
06/14/2022 02:58:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=56
06/14/2022 02:58:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=57
06/14/2022 02:58:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=58
06/14/2022 02:58:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.42 on epoch=58
06/14/2022 02:58:14 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=59
06/14/2022 02:58:17 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.7406047897085759 on epoch=59
06/14/2022 02:58:17 - INFO - __main__ - Saving model with best Classification-F1: 0.7193262442897659 -> 0.7406047897085759 on epoch=59, global_step=950
06/14/2022 02:58:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=59
06/14/2022 02:58:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=60
06/14/2022 02:58:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=61
06/14/2022 02:58:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=61
06/14/2022 02:58:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=62
06/14/2022 02:58:32 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.7462413137293182 on epoch=62
06/14/2022 02:58:32 - INFO - __main__ - Saving model with best Classification-F1: 0.7406047897085759 -> 0.7462413137293182 on epoch=62, global_step=1000
06/14/2022 02:58:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=63
06/14/2022 02:58:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=63
06/14/2022 02:58:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.47 on epoch=64
06/14/2022 02:58:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.36 on epoch=64
06/14/2022 02:58:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=65
06/14/2022 02:58:48 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.7411391026676557 on epoch=65
06/14/2022 02:58:50 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=66
06/14/2022 02:58:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=66
06/14/2022 02:58:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=67
06/14/2022 02:58:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=68
06/14/2022 02:59:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=68
06/14/2022 02:59:03 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.735720897541378 on epoch=68
06/14/2022 02:59:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=69
06/14/2022 02:59:08 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=69
06/14/2022 02:59:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=70
06/14/2022 02:59:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=71
06/14/2022 02:59:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.47 on epoch=71
06/14/2022 02:59:19 - INFO - __main__ - Global step 1150 Train loss 0.39 Classification-F1 0.7489034678127376 on epoch=71
06/14/2022 02:59:19 - INFO - __main__ - Saving model with best Classification-F1: 0.7462413137293182 -> 0.7489034678127376 on epoch=71, global_step=1150
06/14/2022 02:59:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=72
06/14/2022 02:59:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=73
06/14/2022 02:59:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.33 on epoch=73
06/14/2022 02:59:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=74
06/14/2022 02:59:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=74
06/14/2022 02:59:34 - INFO - __main__ - Global step 1200 Train loss 0.38 Classification-F1 0.7399127630779008 on epoch=74
06/14/2022 02:59:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=75
06/14/2022 02:59:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=76
06/14/2022 02:59:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=76
06/14/2022 02:59:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.29 on epoch=77
06/14/2022 02:59:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=78
06/14/2022 02:59:50 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.7461590784679528 on epoch=78
06/14/2022 02:59:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=78
06/14/2022 02:59:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=79
06/14/2022 02:59:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.30 on epoch=79
06/14/2022 02:59:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.33 on epoch=80
06/14/2022 03:00:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=81
06/14/2022 03:00:05 - INFO - __main__ - Global step 1300 Train loss 0.29 Classification-F1 0.7082455472776309 on epoch=81
06/14/2022 03:00:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=81
06/14/2022 03:00:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.36 on epoch=82
06/14/2022 03:00:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=83
06/14/2022 03:00:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=83
06/14/2022 03:00:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=84
06/14/2022 03:00:21 - INFO - __main__ - Global step 1350 Train loss 0.32 Classification-F1 0.7523512118551627 on epoch=84
06/14/2022 03:00:21 - INFO - __main__ - Saving model with best Classification-F1: 0.7489034678127376 -> 0.7523512118551627 on epoch=84, global_step=1350
06/14/2022 03:00:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=84
06/14/2022 03:00:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.27 on epoch=85
06/14/2022 03:00:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=86
06/14/2022 03:00:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=86
06/14/2022 03:00:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=87
06/14/2022 03:00:36 - INFO - __main__ - Global step 1400 Train loss 0.33 Classification-F1 0.7696472874104454 on epoch=87
06/14/2022 03:00:36 - INFO - __main__ - Saving model with best Classification-F1: 0.7523512118551627 -> 0.7696472874104454 on epoch=87, global_step=1400
06/14/2022 03:00:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.29 on epoch=88
06/14/2022 03:00:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=88
06/14/2022 03:00:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=89
06/14/2022 03:00:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=89
06/14/2022 03:00:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=90
06/14/2022 03:00:52 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.7585088535914506 on epoch=90
06/14/2022 03:00:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=91
06/14/2022 03:00:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=91
06/14/2022 03:00:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.29 on epoch=92
06/14/2022 03:01:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.29 on epoch=93
06/14/2022 03:01:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=93
06/14/2022 03:01:07 - INFO - __main__ - Global step 1500 Train loss 0.29 Classification-F1 0.7453420920781607 on epoch=93
06/14/2022 03:01:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=94
06/14/2022 03:01:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=94
06/14/2022 03:01:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=95
06/14/2022 03:01:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=96
06/14/2022 03:01:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=96
06/14/2022 03:01:22 - INFO - __main__ - Global step 1550 Train loss 0.28 Classification-F1 0.7289490884339008 on epoch=96
06/14/2022 03:01:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=97
06/14/2022 03:01:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=98
06/14/2022 03:01:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=98
06/14/2022 03:01:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=99
06/14/2022 03:01:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=99
06/14/2022 03:01:38 - INFO - __main__ - Global step 1600 Train loss 0.26 Classification-F1 0.7759526218407458 on epoch=99
06/14/2022 03:01:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7696472874104454 -> 0.7759526218407458 on epoch=99, global_step=1600
06/14/2022 03:01:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=100
06/14/2022 03:01:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=101
06/14/2022 03:01:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=101
06/14/2022 03:01:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=102
06/14/2022 03:01:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=103
06/14/2022 03:01:53 - INFO - __main__ - Global step 1650 Train loss 0.24 Classification-F1 0.7578009118194879 on epoch=103
06/14/2022 03:01:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=103
06/14/2022 03:01:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=104
06/14/2022 03:02:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=104
06/14/2022 03:02:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.26 on epoch=105
06/14/2022 03:02:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.19 on epoch=106
06/14/2022 03:02:09 - INFO - __main__ - Global step 1700 Train loss 0.25 Classification-F1 0.757905791074533 on epoch=106
06/14/2022 03:02:11 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=106
06/14/2022 03:02:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=107
06/14/2022 03:02:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=108
06/14/2022 03:02:18 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=108
06/14/2022 03:02:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=109
06/14/2022 03:02:24 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.7177951269732327 on epoch=109
06/14/2022 03:02:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=109
06/14/2022 03:02:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=110
06/14/2022 03:02:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=111
06/14/2022 03:02:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=111
06/14/2022 03:02:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=112
06/14/2022 03:02:40 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.7636458409763495 on epoch=112
06/14/2022 03:02:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=113
06/14/2022 03:02:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=113
06/14/2022 03:02:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=114
06/14/2022 03:02:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.32 on epoch=114
06/14/2022 03:02:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=115
06/14/2022 03:02:55 - INFO - __main__ - Global step 1850 Train loss 0.23 Classification-F1 0.7575135153087762 on epoch=115
06/14/2022 03:02:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=116
06/14/2022 03:03:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=116
06/14/2022 03:03:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=117
06/14/2022 03:03:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=118
06/14/2022 03:03:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=118
06/14/2022 03:03:10 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.7638463080607043 on epoch=118
06/14/2022 03:03:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=119
06/14/2022 03:03:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=119
06/14/2022 03:03:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.27 on epoch=120
06/14/2022 03:03:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=121
06/14/2022 03:03:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=121
06/14/2022 03:03:26 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.7752349382798723 on epoch=121
06/14/2022 03:03:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=122
06/14/2022 03:03:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=123
06/14/2022 03:03:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=123
06/14/2022 03:03:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=124
06/14/2022 03:03:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=124
06/14/2022 03:03:41 - INFO - __main__ - Global step 2000 Train loss 0.18 Classification-F1 0.7873856679848368 on epoch=124
06/14/2022 03:03:41 - INFO - __main__ - Saving model with best Classification-F1: 0.7759526218407458 -> 0.7873856679848368 on epoch=124, global_step=2000
06/14/2022 03:03:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.17 on epoch=125
06/14/2022 03:03:46 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.26 on epoch=126
06/14/2022 03:03:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=126
06/14/2022 03:03:51 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.19 on epoch=127
06/14/2022 03:03:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.21 on epoch=128
06/14/2022 03:03:57 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.7783407011321136 on epoch=128
06/14/2022 03:03:59 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=128
06/14/2022 03:04:02 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=129
06/14/2022 03:04:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.16 on epoch=129
06/14/2022 03:04:07 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.18 on epoch=130
06/14/2022 03:04:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.16 on epoch=131
06/14/2022 03:04:12 - INFO - __main__ - Global step 2100 Train loss 0.16 Classification-F1 0.7527145576398937 on epoch=131
06/14/2022 03:04:15 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=131
06/14/2022 03:04:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=132
06/14/2022 03:04:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.25 on epoch=133
06/14/2022 03:04:22 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=133
06/14/2022 03:04:25 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.16 on epoch=134
06/14/2022 03:04:28 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.7708490338146781 on epoch=134
06/14/2022 03:04:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.23 on epoch=134
06/14/2022 03:04:33 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=135
06/14/2022 03:04:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.15 on epoch=136
06/14/2022 03:04:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=136
06/14/2022 03:04:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.16 on epoch=137
06/14/2022 03:04:43 - INFO - __main__ - Global step 2200 Train loss 0.17 Classification-F1 0.7718006487969602 on epoch=137
06/14/2022 03:04:46 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=138
06/14/2022 03:04:48 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=138
06/14/2022 03:04:51 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=139
06/14/2022 03:04:53 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.23 on epoch=139
06/14/2022 03:04:56 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.13 on epoch=140
06/14/2022 03:04:59 - INFO - __main__ - Global step 2250 Train loss 0.15 Classification-F1 0.7846434756956339 on epoch=140
06/14/2022 03:05:01 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=141
06/14/2022 03:05:04 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.15 on epoch=141
06/14/2022 03:05:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.16 on epoch=142
06/14/2022 03:05:08 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.14 on epoch=143
06/14/2022 03:05:11 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=143
06/14/2022 03:05:14 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.7651468253968254 on epoch=143
06/14/2022 03:05:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=144
06/14/2022 03:05:19 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.15 on epoch=144
06/14/2022 03:05:22 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=145
06/14/2022 03:05:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=146
06/14/2022 03:05:26 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.15 on epoch=146
06/14/2022 03:05:30 - INFO - __main__ - Global step 2350 Train loss 0.14 Classification-F1 0.7499697849027448 on epoch=146
06/14/2022 03:05:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.14 on epoch=147
06/14/2022 03:05:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=148
06/14/2022 03:05:37 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.14 on epoch=148
06/14/2022 03:05:39 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=149
06/14/2022 03:05:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=149
06/14/2022 03:05:45 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.7676247803345936 on epoch=149
06/14/2022 03:05:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=150
06/14/2022 03:05:50 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.15 on epoch=151
06/14/2022 03:05:52 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=151
06/14/2022 03:05:55 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.10 on epoch=152
06/14/2022 03:05:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.16 on epoch=153
06/14/2022 03:06:01 - INFO - __main__ - Global step 2450 Train loss 0.10 Classification-F1 0.7711553482523877 on epoch=153
06/14/2022 03:06:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=153
06/14/2022 03:06:06 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.10 on epoch=154
06/14/2022 03:06:08 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=154
06/14/2022 03:06:10 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.14 on epoch=155
06/14/2022 03:06:13 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=156
06/14/2022 03:06:16 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.7604702879697913 on epoch=156
06/14/2022 03:06:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=156
06/14/2022 03:06:21 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.14 on epoch=157
06/14/2022 03:06:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.11 on epoch=158
06/14/2022 03:06:26 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=158
06/14/2022 03:06:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=159
06/14/2022 03:06:32 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.7683961200090232 on epoch=159
06/14/2022 03:06:34 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=159
06/14/2022 03:06:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=160
06/14/2022 03:06:39 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.12 on epoch=161
06/14/2022 03:06:41 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.15 on epoch=161
06/14/2022 03:06:44 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.10 on epoch=162
06/14/2022 03:06:47 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.7470482701098555 on epoch=162
06/14/2022 03:06:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=163
06/14/2022 03:06:52 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=163
06/14/2022 03:06:55 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=164
06/14/2022 03:06:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=164
06/14/2022 03:06:59 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.11 on epoch=165
06/14/2022 03:07:03 - INFO - __main__ - Global step 2650 Train loss 0.08 Classification-F1 0.7849965642178911 on epoch=165
06/14/2022 03:07:05 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=166
06/14/2022 03:07:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.18 on epoch=166
06/14/2022 03:07:10 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=167
06/14/2022 03:07:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=168
06/14/2022 03:07:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=168
06/14/2022 03:07:18 - INFO - __main__ - Global step 2700 Train loss 0.10 Classification-F1 0.7810091856733647 on epoch=168
06/14/2022 03:07:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=169
06/14/2022 03:07:23 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.12 on epoch=169
06/14/2022 03:07:26 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.14 on epoch=170
06/14/2022 03:07:28 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.11 on epoch=171
06/14/2022 03:07:30 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.08 on epoch=171
06/14/2022 03:07:34 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.7801403747339645 on epoch=171
06/14/2022 03:07:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=172
06/14/2022 03:07:39 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.17 on epoch=173
06/14/2022 03:07:41 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=173
06/14/2022 03:07:44 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.08 on epoch=174
06/14/2022 03:07:46 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.08 on epoch=174
06/14/2022 03:07:49 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.7679097058051568 on epoch=174
06/14/2022 03:07:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.09 on epoch=175
06/14/2022 03:07:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=176
06/14/2022 03:07:57 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=176
06/14/2022 03:07:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.08 on epoch=177
06/14/2022 03:08:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.09 on epoch=178
06/14/2022 03:08:05 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.8022253575541674 on epoch=178
06/14/2022 03:08:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7873856679848368 -> 0.8022253575541674 on epoch=178, global_step=2850
06/14/2022 03:08:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=178
06/14/2022 03:08:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=179
06/14/2022 03:08:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=179
06/14/2022 03:08:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.18 on epoch=180
06/14/2022 03:08:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=181
06/14/2022 03:08:20 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.7823540898832521 on epoch=181
06/14/2022 03:08:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=181
06/14/2022 03:08:25 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.14 on epoch=182
06/14/2022 03:08:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=183
06/14/2022 03:08:30 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=183
06/14/2022 03:08:32 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=184
06/14/2022 03:08:36 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.7653331718801857 on epoch=184
06/14/2022 03:08:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=184
06/14/2022 03:08:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=185
06/14/2022 03:08:43 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=186
06/14/2022 03:08:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.09 on epoch=186
06/14/2022 03:08:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=187
06/14/2022 03:08:51 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.7965729602093238 on epoch=187
06/14/2022 03:08:51 - INFO - __main__ - save last model!
06/14/2022 03:08:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/14/2022 03:08:51 - INFO - __main__ - Start tokenizing ... 5509 instances
06/14/2022 03:08:51 - INFO - __main__ - Printing 3 examples
06/14/2022 03:08:51 - INFO - __main__ -  [emo] hmm what does your bio mean i dont have any bio
06/14/2022 03:08:51 - INFO - __main__ - ['others']
06/14/2022 03:08:51 - INFO - __main__ -  [emo] what you like very little things ok
06/14/2022 03:08:51 - INFO - __main__ - ['others']
06/14/2022 03:08:51 - INFO - __main__ -  [emo] yes how so i want to fuck babu
06/14/2022 03:08:51 - INFO - __main__ - ['others']
06/14/2022 03:08:51 - INFO - __main__ - Tokenizing Input ...
06/14/2022 03:08:53 - INFO - __main__ - Tokenizing Output ...
06/14/2022 03:08:59 - INFO - __main__ - Loaded 5509 examples from test data
06/14/2022 03:10:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-emo/emo_64_87_0.2_8_predictions.txt
06/14/2022 03:10:11 - INFO - __main__ - Classification-F1 on test data: 0.4820
06/14/2022 03:10:11 - INFO - __main__ - prefix=emo_64_87, lr=0.2, bsz=8, dev_performance=0.8022253575541674, test_performance=0.4819974066189724
