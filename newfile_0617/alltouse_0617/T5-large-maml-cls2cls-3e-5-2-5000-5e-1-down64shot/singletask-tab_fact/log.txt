05/21/2022 21:25:19 - INFO - __main__ - Namespace(task_dir='data_64/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:25:19 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact
05/21/2022 21:25:19 - INFO - __main__ - Namespace(task_dir='data_64/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:25:19 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact
05/21/2022 21:25:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:25:21 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:25:21 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:25:21 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:21 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_64_100', 'tab_fact_64_13', 'tab_fact_64_21', 'tab_fact_64_42', 'tab_fact_64_87']
05/21/2022 21:25:21 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:25:21 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:21 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_64_100', 'tab_fact_64_13', 'tab_fact_64_21', 'tab_fact_64_42', 'tab_fact_64_87']
05/21/2022 21:25:27 - INFO - __main__ - Running ... prefix=tab_fact_64_100, lr=0.5, bsz=8 ...
06/15/2022 00:21:11 - INFO - __main__ - Namespace(task_dir='data_64/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/15/2022 00:21:11 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact
06/15/2022 00:21:11 - INFO - __main__ - Namespace(task_dir='data_64/tab_fact/', task_name='tab_fact', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/15/2022 00:21:11 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact
06/15/2022 00:21:12 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/15/2022 00:21:12 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/15/2022 00:21:12 - INFO - __main__ - args.device: cuda:0
06/15/2022 00:21:12 - INFO - __main__ - args.device: cuda:1
06/15/2022 00:21:12 - INFO - __main__ - Using 2 gpus
06/15/2022 00:21:12 - INFO - __main__ - Using 2 gpus
06/15/2022 00:21:12 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_64_100', 'tab_fact_64_13', 'tab_fact_64_21', 'tab_fact_64_42', 'tab_fact_64_87']
06/15/2022 00:21:12 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_64_100', 'tab_fact_64_13', 'tab_fact_64_21', 'tab_fact_64_42', 'tab_fact_64_87']
06/15/2022 00:21:17 - INFO - __main__ - Running ... prefix=tab_fact_64_100, lr=0.5, bsz=8 ...
06/15/2022 00:21:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:21:18 - INFO - __main__ - Printing 3 examples
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 00:21:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:21:18 - INFO - __main__ - Printing 3 examples
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:21:18 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 00:21:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:21:18 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 00:21:18 - INFO - __main__ - Printing 3 examples
06/15/2022 00:21:18 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 00:21:18 - INFO - __main__ - Printing 3 examples
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:21:18 - INFO - __main__ - ['refuted']
06/15/2022 00:21:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:21:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:21:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:21:19 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 00:21:19 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 00:21:37 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 00:21:37 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 00:21:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 00:21:38 - INFO - __main__ - Starting training!
06/15/2022 00:21:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 00:21:42 - INFO - __main__ - Starting training!
06/15/2022 00:21:48 - INFO - __main__ - Step 10 Global step 10 Train loss 3.00 on epoch=1
06/15/2022 00:21:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=2
06/15/2022 00:21:56 - INFO - __main__ - Step 30 Global step 30 Train loss 0.41 on epoch=3
06/15/2022 00:22:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=4
06/15/2022 00:22:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.27 on epoch=6
06/15/2022 00:22:10 - INFO - __main__ - Global step 50 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 00:22:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 00:22:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=7
06/15/2022 00:22:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=8
06/15/2022 00:22:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=9
06/15/2022 00:22:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=11
06/15/2022 00:22:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.97 on epoch=12
06/15/2022 00:22:39 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 00:22:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=13
06/15/2022 00:22:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 00:22:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=16
06/15/2022 00:22:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=17
06/15/2022 00:23:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=18
06/15/2022 00:23:07 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 00:23:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=19
06/15/2022 00:23:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=21
06/15/2022 00:23:20 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=22
06/15/2022 00:23:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=23
06/15/2022 00:23:29 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=24
06/15/2022 00:23:35 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 00:23:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.21 on epoch=26
06/15/2022 00:23:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=27
06/15/2022 00:23:48 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=28
06/15/2022 00:23:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=29
06/15/2022 00:23:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=31
06/15/2022 00:24:03 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 00:24:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=32
06/15/2022 00:24:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=33
06/15/2022 00:24:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.20 on epoch=34
06/15/2022 00:24:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=36
06/15/2022 00:24:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=37
06/15/2022 00:24:31 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 00:24:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=38
06/15/2022 00:24:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=39
06/15/2022 00:24:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=41
06/15/2022 00:24:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=42
06/15/2022 00:24:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 00:24:59 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 00:25:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=44
06/15/2022 00:25:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=46
06/15/2022 00:25:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=47
06/15/2022 00:25:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=48
06/15/2022 00:25:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=49
06/15/2022 00:25:27 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 00:25:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=51
06/15/2022 00:25:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=52
06/15/2022 00:25:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=53
06/15/2022 00:25:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=54
06/15/2022 00:25:49 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=56
06/15/2022 00:25:55 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 00:25:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=57
06/15/2022 00:26:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=58
06/15/2022 00:26:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 00:26:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=61
06/15/2022 00:26:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=62
06/15/2022 00:26:22 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 00:26:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=63
06/15/2022 00:26:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=64
06/15/2022 00:26:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=66
06/15/2022 00:26:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=67
06/15/2022 00:26:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 00:26:50 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 00:26:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=69
06/15/2022 00:26:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=71
06/15/2022 00:27:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=72
06/15/2022 00:27:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=73
06/15/2022 00:27:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=74
06/15/2022 00:27:18 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 00:27:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 00:27:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 00:27:31 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=78
06/15/2022 00:27:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=79
06/15/2022 00:27:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=81
06/15/2022 00:27:45 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.22105263157894736 on epoch=81
06/15/2022 00:27:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=82
06/15/2022 00:27:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 00:27:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=84
06/15/2022 00:28:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 00:28:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=87
06/15/2022 00:28:12 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.2175438596491228 on epoch=87
06/15/2022 00:28:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=88
06/15/2022 00:28:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=89
06/15/2022 00:28:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=91
06/15/2022 00:28:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=92
06/15/2022 00:28:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=93
06/15/2022 00:28:40 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.2385083713850837 on epoch=93
06/15/2022 00:28:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=94
06/15/2022 00:28:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 00:28:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 00:28:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=98
06/15/2022 00:29:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=99
06/15/2022 00:29:07 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 00:29:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=101
06/15/2022 00:29:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=102
06/15/2022 00:29:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=103
06/15/2022 00:29:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=104
06/15/2022 00:29:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=106
06/15/2022 00:29:35 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.3511520737327189 on epoch=106
06/15/2022 00:29:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3511520737327189 on epoch=106, global_step=850
06/15/2022 00:29:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 00:29:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 00:29:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=109
06/15/2022 00:29:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=111
06/15/2022 00:29:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=112
06/15/2022 00:30:02 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.35372735372735364 on epoch=112
06/15/2022 00:30:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3511520737327189 -> 0.35372735372735364 on epoch=112, global_step=900
06/15/2022 00:30:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 00:30:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=114
06/15/2022 00:30:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=116
06/15/2022 00:30:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=117
06/15/2022 00:30:24 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=118
06/15/2022 00:30:30 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.3915298184961106 on epoch=118
06/15/2022 00:30:30 - INFO - __main__ - Saving model with best Classification-F1: 0.35372735372735364 -> 0.3915298184961106 on epoch=118, global_step=950
06/15/2022 00:30:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 00:30:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=121
06/15/2022 00:30:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=122
06/15/2022 00:30:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=123
06/15/2022 00:30:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 00:30:58 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.5207817754933689 on epoch=124
06/15/2022 00:30:58 - INFO - __main__ - Saving model with best Classification-F1: 0.3915298184961106 -> 0.5207817754933689 on epoch=124, global_step=1000
06/15/2022 00:31:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=126
06/15/2022 00:31:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=127
06/15/2022 00:31:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=128
06/15/2022 00:31:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 00:31:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=131
06/15/2022 00:31:25 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.4167176766646263 on epoch=131
06/15/2022 00:31:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=132
06/15/2022 00:31:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=133
06/15/2022 00:31:38 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=134
06/15/2022 00:31:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 00:31:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=137
06/15/2022 00:31:53 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.423313662034393 on epoch=137
06/15/2022 00:31:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=138
06/15/2022 00:32:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=139
06/15/2022 00:32:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=141
06/15/2022 00:32:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=142
06/15/2022 00:32:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=143
06/15/2022 00:32:21 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.5700763358778627 on epoch=143
06/15/2022 00:32:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5207817754933689 -> 0.5700763358778627 on epoch=143, global_step=1150
06/15/2022 00:32:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=144
06/15/2022 00:32:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=146
06/15/2022 00:32:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=147
06/15/2022 00:32:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=148
06/15/2022 00:32:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=149
06/15/2022 00:32:49 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.3708141321044547 on epoch=149
06/15/2022 00:32:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=151
06/15/2022 00:32:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=152
06/15/2022 00:33:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=153
06/15/2022 00:33:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.16 on epoch=154
06/15/2022 00:33:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=156
06/15/2022 00:33:16 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.4429517502365184 on epoch=156
06/15/2022 00:33:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=157
06/15/2022 00:33:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=158
06/15/2022 00:33:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=159
06/15/2022 00:33:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=161
06/15/2022 00:33:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=162
06/15/2022 00:33:44 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.35518871580252653 on epoch=162
06/15/2022 00:33:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=163
06/15/2022 00:33:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=164
06/15/2022 00:33:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=166
06/15/2022 00:34:01 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=167
06/15/2022 00:34:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=168
06/15/2022 00:34:11 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.5148803976390183 on epoch=168
06/15/2022 00:34:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=169
06/15/2022 00:34:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=171
06/15/2022 00:34:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=172
06/15/2022 00:34:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=173
06/15/2022 00:34:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.15 on epoch=174
06/15/2022 00:34:39 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.5070603337612323 on epoch=174
06/15/2022 00:34:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=176
06/15/2022 00:34:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=177
06/15/2022 00:34:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=178
06/15/2022 00:34:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=179
06/15/2022 00:35:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=181
06/15/2022 00:35:07 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.559813313682142 on epoch=181
06/15/2022 00:35:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 00:35:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=183
06/15/2022 00:35:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=184
06/15/2022 00:35:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=186
06/15/2022 00:35:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=187
06/15/2022 00:35:35 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.49556650246305417 on epoch=187
06/15/2022 00:35:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=188
06/15/2022 00:35:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=189
06/15/2022 00:35:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=191
06/15/2022 00:35:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.09 on epoch=192
06/15/2022 00:35:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=193
06/15/2022 00:36:02 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.39636200314394787 on epoch=193
06/15/2022 00:36:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=194
06/15/2022 00:36:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=196
06/15/2022 00:36:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=197
06/15/2022 00:36:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=198
06/15/2022 00:36:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=199
06/15/2022 00:36:30 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.5075370545569221 on epoch=199
06/15/2022 00:36:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=201
06/15/2022 00:36:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=202
06/15/2022 00:36:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=203
06/15/2022 00:36:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=204
06/15/2022 00:36:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=206
06/15/2022 00:36:57 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.5388091603053435 on epoch=206
06/15/2022 00:37:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=207
06/15/2022 00:37:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=208
06/15/2022 00:37:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=209
06/15/2022 00:37:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=211
06/15/2022 00:37:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=212
06/15/2022 00:37:25 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.5464320625610948 on epoch=212
06/15/2022 00:37:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=213
06/15/2022 00:37:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.10 on epoch=214
06/15/2022 00:37:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=216
06/15/2022 00:37:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=217
06/15/2022 00:37:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=218
06/15/2022 00:37:53 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.33329075114971896 on epoch=218
06/15/2022 00:37:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=219
06/15/2022 00:38:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=221
06/15/2022 00:38:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=222
06/15/2022 00:38:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=223
06/15/2022 00:38:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=224
06/15/2022 00:38:21 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.46558704453441296 on epoch=224
06/15/2022 00:38:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=226
06/15/2022 00:38:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.08 on epoch=227
06/15/2022 00:38:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=228
06/15/2022 00:38:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=229
06/15/2022 00:38:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=231
06/15/2022 00:38:49 - INFO - __main__ - Global step 1850 Train loss 0.08 Classification-F1 0.5370264610698648 on epoch=231
06/15/2022 00:38:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=232
06/15/2022 00:38:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=233
06/15/2022 00:39:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=234
06/15/2022 00:39:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=236
06/15/2022 00:39:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=237
06/15/2022 00:39:17 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.5255744996293551 on epoch=237
06/15/2022 00:39:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=238
06/15/2022 00:39:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=239
06/15/2022 00:39:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=241
06/15/2022 00:39:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=242
06/15/2022 00:39:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=243
06/15/2022 00:39:45 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.500880503144654 on epoch=243
06/15/2022 00:39:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=244
06/15/2022 00:39:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=246
06/15/2022 00:39:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=247
06/15/2022 00:40:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=248
06/15/2022 00:40:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=249
06/15/2022 00:40:12 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.5303643724696356 on epoch=249
06/15/2022 00:40:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.06 on epoch=251
06/15/2022 00:40:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.08 on epoch=252
06/15/2022 00:40:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=253
06/15/2022 00:40:30 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.05 on epoch=254
06/15/2022 00:40:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=256
06/15/2022 00:40:40 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.4781408768738631 on epoch=256
06/15/2022 00:40:45 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=257
06/15/2022 00:40:49 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=258
06/15/2022 00:40:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.05 on epoch=259
06/15/2022 00:40:58 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=261
06/15/2022 00:41:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.06 on epoch=262
06/15/2022 00:41:08 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.36573047548657306 on epoch=262
06/15/2022 00:41:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=263
06/15/2022 00:41:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.05 on epoch=264
06/15/2022 00:41:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=266
06/15/2022 00:41:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.07 on epoch=267
06/15/2022 00:41:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=268
06/15/2022 00:41:35 - INFO - __main__ - Global step 2150 Train loss 0.06 Classification-F1 0.4396135265700484 on epoch=268
06/15/2022 00:41:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=269
06/15/2022 00:41:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=271
06/15/2022 00:41:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=272
06/15/2022 00:41:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=273
06/15/2022 00:41:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=274
06/15/2022 00:42:02 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.46803878883831385 on epoch=274
06/15/2022 00:42:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.04 on epoch=276
06/15/2022 00:42:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=277
06/15/2022 00:42:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.03 on epoch=278
06/15/2022 00:42:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.05 on epoch=279
06/15/2022 00:42:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.04 on epoch=281
06/15/2022 00:42:30 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.531135531135531 on epoch=281
06/15/2022 00:42:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.04 on epoch=282
06/15/2022 00:42:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=283
06/15/2022 00:42:43 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=284
06/15/2022 00:42:48 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=286
06/15/2022 00:42:52 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=287
06/15/2022 00:42:58 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.24677147669848398 on epoch=287
06/15/2022 00:43:02 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=288
06/15/2022 00:43:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=289
06/15/2022 00:43:11 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=291
06/15/2022 00:43:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=292
06/15/2022 00:43:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=293
06/15/2022 00:43:26 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.33395895895895894 on epoch=293
06/15/2022 00:43:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.04 on epoch=294
06/15/2022 00:43:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=296
06/15/2022 00:43:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=297
06/15/2022 00:43:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=298
06/15/2022 00:43:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=299
06/15/2022 00:43:54 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.4995112414467253 on epoch=299
06/15/2022 00:43:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=301
06/15/2022 00:44:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=302
06/15/2022 00:44:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=303
06/15/2022 00:44:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=304
06/15/2022 00:44:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=306
06/15/2022 00:44:23 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.4919083969465649 on epoch=306
06/15/2022 00:44:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.01 on epoch=307
06/15/2022 00:44:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=308
06/15/2022 00:44:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=309
06/15/2022 00:44:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.03 on epoch=311
06/15/2022 00:44:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=312
06/15/2022 00:44:51 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5302177636408123 on epoch=312
06/15/2022 00:44:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.05 on epoch=313
06/15/2022 00:45:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=314
06/15/2022 00:45:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=316
06/15/2022 00:45:10 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=317
06/15/2022 00:45:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.06 on epoch=318
06/15/2022 00:45:20 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.49672346002621226 on epoch=318
06/15/2022 00:45:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.02 on epoch=319
06/15/2022 00:45:29 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=321
06/15/2022 00:45:34 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=322
06/15/2022 00:45:38 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=323
06/15/2022 00:45:43 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=324
06/15/2022 00:45:48 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.3501538461538461 on epoch=324
06/15/2022 00:45:53 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=326
06/15/2022 00:45:57 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=327
06/15/2022 00:46:02 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=328
06/15/2022 00:46:06 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.02 on epoch=329
06/15/2022 00:46:11 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=331
06/15/2022 00:46:17 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.5263124882393527 on epoch=331
06/15/2022 00:46:21 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=332
06/15/2022 00:46:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.03 on epoch=333
06/15/2022 00:46:30 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.01 on epoch=334
06/15/2022 00:46:35 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=336
06/15/2022 00:46:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.03 on epoch=337
06/15/2022 00:46:45 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.515625 on epoch=337
06/15/2022 00:46:49 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=338
06/15/2022 00:46:54 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=339
06/15/2022 00:46:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=341
06/15/2022 00:47:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=342
06/15/2022 00:47:08 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=343
06/15/2022 00:47:13 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.5643294758339006 on epoch=343
06/15/2022 00:47:18 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=344
06/15/2022 00:47:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=346
06/15/2022 00:47:27 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=347
06/15/2022 00:47:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=348
06/15/2022 00:47:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=349
06/15/2022 00:47:42 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.48950637463223273 on epoch=349
06/15/2022 00:47:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=351
06/15/2022 00:47:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=352
06/15/2022 00:47:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=353
06/15/2022 00:48:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=354
06/15/2022 00:48:05 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=356
06/15/2022 00:48:10 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.5151515151515151 on epoch=356
06/15/2022 00:48:15 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=357
06/15/2022 00:48:19 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=358
06/15/2022 00:48:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.03 on epoch=359
06/15/2022 00:48:28 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=361
06/15/2022 00:48:33 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=362
06/15/2022 00:48:39 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.5536737235367373 on epoch=362
06/15/2022 00:48:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=363
06/15/2022 00:48:48 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=364
06/15/2022 00:48:52 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=366
06/15/2022 00:48:57 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=367
06/15/2022 00:49:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=368
06/15/2022 00:49:07 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5326443468036388 on epoch=368
06/15/2022 00:49:12 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 00:49:16 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=371
06/15/2022 00:49:21 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=372
06/15/2022 00:49:25 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=373
06/15/2022 00:49:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=374
06/15/2022 00:49:31 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:49:31 - INFO - __main__ - Printing 3 examples
06/15/2022 00:49:31 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 00:49:31 - INFO - __main__ - ['refuted']
06/15/2022 00:49:31 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 00:49:31 - INFO - __main__ - ['refuted']
06/15/2022 00:49:31 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 00:49:31 - INFO - __main__ - ['refuted']
06/15/2022 00:49:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:49:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:49:32 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 00:49:32 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:49:32 - INFO - __main__ - Printing 3 examples
06/15/2022 00:49:32 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 00:49:32 - INFO - __main__ - ['refuted']
06/15/2022 00:49:32 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 00:49:32 - INFO - __main__ - ['refuted']
06/15/2022 00:49:32 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 00:49:32 - INFO - __main__ - ['refuted']
06/15/2022 00:49:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:49:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:49:32 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 00:49:35 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.5513742851872347 on epoch=374
06/15/2022 00:49:35 - INFO - __main__ - save last model!
06/15/2022 00:49:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 00:49:35 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 00:49:35 - INFO - __main__ - Printing 3 examples
06/15/2022 00:49:35 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 00:49:35 - INFO - __main__ - ['entailed']
06/15/2022 00:49:35 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 00:49:35 - INFO - __main__ - ['entailed']
06/15/2022 00:49:35 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 00:49:35 - INFO - __main__ - ['entailed']
06/15/2022 00:49:35 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:49:50 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 00:49:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 00:49:51 - INFO - __main__ - Starting training!
06/15/2022 00:50:01 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:50:14 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 00:59:02 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_100_0.5_8_predictions.txt
06/15/2022 00:59:02 - INFO - __main__ - Classification-F1 on test data: 0.3280
06/15/2022 00:59:03 - INFO - __main__ - prefix=tab_fact_64_100, lr=0.5, bsz=8, dev_performance=0.5700763358778627, test_performance=0.3279609080844978
06/15/2022 00:59:03 - INFO - __main__ - Running ... prefix=tab_fact_64_100, lr=0.4, bsz=8 ...
06/15/2022 00:59:03 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:59:03 - INFO - __main__ - Printing 3 examples
06/15/2022 00:59:03 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 00:59:03 - INFO - __main__ - ['refuted']
06/15/2022 00:59:03 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 00:59:03 - INFO - __main__ - ['refuted']
06/15/2022 00:59:03 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 00:59:03 - INFO - __main__ - ['refuted']
06/15/2022 00:59:03 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:59:04 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:59:04 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 00:59:04 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 00:59:04 - INFO - __main__ - Printing 3 examples
06/15/2022 00:59:04 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 00:59:04 - INFO - __main__ - ['refuted']
06/15/2022 00:59:04 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 00:59:04 - INFO - __main__ - ['refuted']
06/15/2022 00:59:04 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 00:59:04 - INFO - __main__ - ['refuted']
06/15/2022 00:59:04 - INFO - __main__ - Tokenizing Input ...
06/15/2022 00:59:04 - INFO - __main__ - Tokenizing Output ...
06/15/2022 00:59:04 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 00:59:23 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 00:59:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 00:59:24 - INFO - __main__ - Starting training!
06/15/2022 00:59:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.16 on epoch=1
06/15/2022 00:59:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.57 on epoch=2
06/15/2022 00:59:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=3
06/15/2022 00:59:42 - INFO - __main__ - Step 40 Global step 40 Train loss 2.59 on epoch=4
06/15/2022 00:59:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=6
06/15/2022 00:59:52 - INFO - __main__ - Global step 50 Train loss 1.46 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 00:59:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 00:59:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=7
06/15/2022 01:00:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=8
06/15/2022 01:00:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=9
06/15/2022 01:00:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=11
06/15/2022 01:00:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=12
06/15/2022 01:00:19 - INFO - __main__ - Global step 100 Train loss 0.49 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 01:00:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=13
06/15/2022 01:00:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=14
06/15/2022 01:00:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=16
06/15/2022 01:00:37 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=17
06/15/2022 01:00:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=18
06/15/2022 01:00:47 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 01:00:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=19
06/15/2022 01:00:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=21
06/15/2022 01:01:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=22
06/15/2022 01:01:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.30 on epoch=23
06/15/2022 01:01:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=24
06/15/2022 01:01:15 - INFO - __main__ - Global step 200 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 01:01:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=26
06/15/2022 01:01:24 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=27
06/15/2022 01:01:28 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=28
06/15/2022 01:01:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=29
06/15/2022 01:01:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.30 on epoch=31
06/15/2022 01:01:42 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 01:01:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=32
06/15/2022 01:01:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 01:01:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=34
06/15/2022 01:02:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=36
06/15/2022 01:02:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=37
06/15/2022 01:02:10 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 01:02:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=38
06/15/2022 01:02:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=39
06/15/2022 01:02:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=41
06/15/2022 01:02:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=42
06/15/2022 01:02:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 01:02:38 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 01:02:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=44
06/15/2022 01:02:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=46
06/15/2022 01:02:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=47
06/15/2022 01:02:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=48
06/15/2022 01:03:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=49
06/15/2022 01:03:06 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 01:03:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=51
06/15/2022 01:03:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=52
06/15/2022 01:03:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=53
06/15/2022 01:03:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=54
06/15/2022 01:03:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=56
06/15/2022 01:03:34 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 01:03:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=57
06/15/2022 01:03:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=58
06/15/2022 01:03:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=59
06/15/2022 01:03:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=61
06/15/2022 01:03:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=62
06/15/2022 01:04:02 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 01:04:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=63
06/15/2022 01:04:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=64
06/15/2022 01:04:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 01:04:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=67
06/15/2022 01:04:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 01:04:30 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 01:04:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=69
06/15/2022 01:04:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=71
06/15/2022 01:04:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=72
06/15/2022 01:04:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=73
06/15/2022 01:04:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=74
06/15/2022 01:04:58 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.22338568935427575 on epoch=74
06/15/2022 01:05:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=76
06/15/2022 01:05:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=77
06/15/2022 01:05:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=78
06/15/2022 01:05:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=79
06/15/2022 01:05:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=81
06/15/2022 01:05:26 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 01:05:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=82
06/15/2022 01:05:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=83
06/15/2022 01:05:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=84
06/15/2022 01:05:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=86
06/15/2022 01:05:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=87
06/15/2022 01:05:54 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 01:05:58 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=88
06/15/2022 01:06:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=89
06/15/2022 01:06:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=91
06/15/2022 01:06:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=92
06/15/2022 01:06:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=93
06/15/2022 01:06:22 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 01:06:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=94
06/15/2022 01:06:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=96
06/15/2022 01:06:35 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=97
06/15/2022 01:06:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=98
06/15/2022 01:06:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 01:06:50 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 01:06:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 01:06:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 01:07:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=103
06/15/2022 01:07:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=104
06/15/2022 01:07:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=106
06/15/2022 01:07:17 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 01:07:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=107
06/15/2022 01:07:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=108
06/15/2022 01:07:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 01:07:35 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=111
06/15/2022 01:07:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=112
06/15/2022 01:07:45 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 01:07:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 01:07:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=114
06/15/2022 01:07:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=116
06/15/2022 01:08:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 01:08:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=118
06/15/2022 01:08:12 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.4748717948717949 on epoch=118
06/15/2022 01:08:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4748717948717949 on epoch=118, global_step=950
06/15/2022 01:08:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 01:08:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=121
06/15/2022 01:08:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=122
06/15/2022 01:08:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=123
06/15/2022 01:08:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=124
06/15/2022 01:08:40 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 01:08:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=126
06/15/2022 01:08:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=127
06/15/2022 01:08:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=128
06/15/2022 01:08:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=129
06/15/2022 01:09:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=131
06/15/2022 01:09:08 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.16666666666666666 on epoch=131
06/15/2022 01:09:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=132
06/15/2022 01:09:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=133
06/15/2022 01:09:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=134
06/15/2022 01:09:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=136
06/15/2022 01:09:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=137
06/15/2022 01:09:36 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.16193181818181818 on epoch=137
06/15/2022 01:09:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=138
06/15/2022 01:09:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=139
06/15/2022 01:09:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=141
06/15/2022 01:09:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 01:09:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=143
06/15/2022 01:10:04 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.24223602484472048 on epoch=143
06/15/2022 01:10:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=144
06/15/2022 01:10:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=146
06/15/2022 01:10:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=147
06/15/2022 01:10:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=148
06/15/2022 01:10:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=149
06/15/2022 01:10:32 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.414931437277806 on epoch=149
06/15/2022 01:10:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=151
06/15/2022 01:10:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=152
06/15/2022 01:10:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=153
06/15/2022 01:10:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=154
06/15/2022 01:10:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=156
06/15/2022 01:11:00 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.48950637463223273 on epoch=156
06/15/2022 01:11:00 - INFO - __main__ - Saving model with best Classification-F1: 0.4748717948717949 -> 0.48950637463223273 on epoch=156, global_step=1250
06/15/2022 01:11:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=157
06/15/2022 01:11:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=158
06/15/2022 01:11:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=159
06/15/2022 01:11:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=161
06/15/2022 01:11:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=162
06/15/2022 01:11:28 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.46843853820598 on epoch=162
06/15/2022 01:11:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=163
06/15/2022 01:11:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=164
06/15/2022 01:11:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=166
06/15/2022 01:11:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=167
06/15/2022 01:11:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=168
06/15/2022 01:11:56 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.41075141075141075 on epoch=168
06/15/2022 01:12:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=169
06/15/2022 01:12:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=171
06/15/2022 01:12:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=172
06/15/2022 01:12:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=173
06/15/2022 01:12:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=174
06/15/2022 01:12:24 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.3860677578987438 on epoch=174
06/15/2022 01:12:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=176
06/15/2022 01:12:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 01:12:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=178
06/15/2022 01:12:42 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=179
06/15/2022 01:12:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=181
06/15/2022 01:12:52 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.45718194254445965 on epoch=181
06/15/2022 01:12:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=182
06/15/2022 01:13:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=183
06/15/2022 01:13:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=184
06/15/2022 01:13:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=186
06/15/2022 01:13:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=187
06/15/2022 01:13:20 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.41075141075141075 on epoch=187
06/15/2022 01:13:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=188
06/15/2022 01:13:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=189
06/15/2022 01:13:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=191
06/15/2022 01:13:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=192
06/15/2022 01:13:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=193
06/15/2022 01:13:48 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.4055576703464027 on epoch=193
06/15/2022 01:13:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=194
06/15/2022 01:13:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=196
06/15/2022 01:14:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=197
06/15/2022 01:14:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=198
06/15/2022 01:14:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=199
06/15/2022 01:14:16 - INFO - __main__ - Global step 1600 Train loss 0.17 Classification-F1 0.3948694102146787 on epoch=199
06/15/2022 01:14:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.17 on epoch=201
06/15/2022 01:14:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=202
06/15/2022 01:14:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=203
06/15/2022 01:14:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=204
06/15/2022 01:14:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=206
06/15/2022 01:14:44 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.42329972108989483 on epoch=206
06/15/2022 01:14:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=207
06/15/2022 01:14:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=208
06/15/2022 01:14:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=209
06/15/2022 01:15:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=211
06/15/2022 01:15:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=212
06/15/2022 01:15:12 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.3245718837116687 on epoch=212
06/15/2022 01:15:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=213
06/15/2022 01:15:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=214
06/15/2022 01:15:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=216
06/15/2022 01:15:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=217
06/15/2022 01:15:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=218
06/15/2022 01:15:40 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.3511520737327189 on epoch=218
06/15/2022 01:15:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.16 on epoch=219
06/15/2022 01:15:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=221
06/15/2022 01:15:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=222
06/15/2022 01:15:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=223
06/15/2022 01:16:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=224
06/15/2022 01:16:08 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.4762748091603053 on epoch=224
06/15/2022 01:16:13 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.18 on epoch=226
06/15/2022 01:16:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=227
06/15/2022 01:16:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=228
06/15/2022 01:16:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=229
06/15/2022 01:16:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=231
06/15/2022 01:16:36 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.36657784545108485 on epoch=231
06/15/2022 01:16:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=232
06/15/2022 01:16:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=233
06/15/2022 01:16:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=234
06/15/2022 01:16:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=236
06/15/2022 01:16:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.15 on epoch=237
06/15/2022 01:17:04 - INFO - __main__ - Global step 1900 Train loss 0.15 Classification-F1 0.464039408866995 on epoch=237
06/15/2022 01:17:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=238
06/15/2022 01:17:13 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=239
06/15/2022 01:17:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=241
06/15/2022 01:17:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=242
06/15/2022 01:17:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=243
06/15/2022 01:17:32 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.46803878883831385 on epoch=243
06/15/2022 01:17:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=244
06/15/2022 01:17:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=246
06/15/2022 01:17:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=247
06/15/2022 01:17:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=248
06/15/2022 01:17:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=249
06/15/2022 01:17:59 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.3915298184961106 on epoch=249
06/15/2022 01:18:04 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.14 on epoch=251
06/15/2022 01:18:08 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=252
06/15/2022 01:18:13 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=253
06/15/2022 01:18:17 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=254
06/15/2022 01:18:22 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=256
06/15/2022 01:18:27 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.5075419847328244 on epoch=256
06/15/2022 01:18:27 - INFO - __main__ - Saving model with best Classification-F1: 0.48950637463223273 -> 0.5075419847328244 on epoch=256, global_step=2050
06/15/2022 01:18:32 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=257
06/15/2022 01:18:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=258
06/15/2022 01:18:41 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=259
06/15/2022 01:18:45 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=261
06/15/2022 01:18:50 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=262
06/15/2022 01:18:56 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.4458874458874459 on epoch=262
06/15/2022 01:19:00 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=263
06/15/2022 01:19:05 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=264
06/15/2022 01:19:09 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=266
06/15/2022 01:19:13 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=267
06/15/2022 01:19:18 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=268
06/15/2022 01:19:23 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.5102552844508562 on epoch=268
06/15/2022 01:19:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5075419847328244 -> 0.5102552844508562 on epoch=268, global_step=2150
06/15/2022 01:19:28 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.10 on epoch=269
06/15/2022 01:19:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=271
06/15/2022 01:19:37 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.13 on epoch=272
06/15/2022 01:19:41 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.11 on epoch=273
06/15/2022 01:19:46 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=274
06/15/2022 01:19:51 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.531135531135531 on epoch=274
06/15/2022 01:19:51 - INFO - __main__ - Saving model with best Classification-F1: 0.5102552844508562 -> 0.531135531135531 on epoch=274, global_step=2200
06/15/2022 01:19:56 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=276
06/15/2022 01:20:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=277
06/15/2022 01:20:05 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=278
06/15/2022 01:20:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.12 on epoch=279
06/15/2022 01:20:14 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=281
06/15/2022 01:20:19 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.28989139515455303 on epoch=281
06/15/2022 01:20:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=282
06/15/2022 01:20:28 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=283
06/15/2022 01:20:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=284
06/15/2022 01:20:37 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.09 on epoch=286
06/15/2022 01:20:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=287
06/15/2022 01:20:47 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.34775 on epoch=287
06/15/2022 01:20:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=288
06/15/2022 01:20:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.06 on epoch=289
06/15/2022 01:21:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=291
06/15/2022 01:21:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=292
06/15/2022 01:21:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=293
06/15/2022 01:21:15 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.50633608815427 on epoch=293
06/15/2022 01:21:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=294
06/15/2022 01:21:24 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=296
06/15/2022 01:21:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=297
06/15/2022 01:21:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=298
06/15/2022 01:21:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.06 on epoch=299
06/15/2022 01:21:43 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.33395895895895894 on epoch=299
06/15/2022 01:21:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=301
06/15/2022 01:21:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=302
06/15/2022 01:21:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=303
06/15/2022 01:22:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=304
06/15/2022 01:22:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=306
06/15/2022 01:22:10 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.4487674487674488 on epoch=306
06/15/2022 01:22:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=307
06/15/2022 01:22:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=308
06/15/2022 01:22:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=309
06/15/2022 01:22:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=311
06/15/2022 01:22:32 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=312
06/15/2022 01:22:38 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.2943442961332049 on epoch=312
06/15/2022 01:22:43 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=313
06/15/2022 01:22:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=314
06/15/2022 01:22:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=316
06/15/2022 01:22:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.06 on epoch=317
06/15/2022 01:23:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=318
06/15/2022 01:23:06 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.5097603162836669 on epoch=318
06/15/2022 01:23:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.07 on epoch=319
06/15/2022 01:23:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=321
06/15/2022 01:23:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=322
06/15/2022 01:23:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=323
06/15/2022 01:23:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.06 on epoch=324
06/15/2022 01:23:33 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.48511665325824616 on epoch=324
06/15/2022 01:23:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=326
06/15/2022 01:23:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=327
06/15/2022 01:23:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=328
06/15/2022 01:23:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=329
06/15/2022 01:23:55 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=331
06/15/2022 01:24:01 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.5307859583721652 on epoch=331
06/15/2022 01:24:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=332
06/15/2022 01:24:10 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=333
06/15/2022 01:24:14 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=334
06/15/2022 01:24:19 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=336
06/15/2022 01:24:23 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=337
06/15/2022 01:24:29 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.5220079583715946 on epoch=337
06/15/2022 01:24:33 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=338
06/15/2022 01:24:38 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=339
06/15/2022 01:24:42 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=341
06/15/2022 01:24:47 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=342
06/15/2022 01:24:51 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=343
06/15/2022 01:24:57 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.531135531135531 on epoch=343
06/15/2022 01:25:01 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=344
06/15/2022 01:25:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.05 on epoch=346
06/15/2022 01:25:10 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=347
06/15/2022 01:25:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.11 on epoch=348
06/15/2022 01:25:19 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=349
06/15/2022 01:25:25 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.5 on epoch=349
06/15/2022 01:25:29 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.06 on epoch=351
06/15/2022 01:25:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=352
06/15/2022 01:25:38 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=353
06/15/2022 01:25:42 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=354
06/15/2022 01:25:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=356
06/15/2022 01:25:52 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.5326443468036388 on epoch=356
06/15/2022 01:25:52 - INFO - __main__ - Saving model with best Classification-F1: 0.531135531135531 -> 0.5326443468036388 on epoch=356, global_step=2850
06/15/2022 01:25:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.03 on epoch=357
06/15/2022 01:26:01 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=358
06/15/2022 01:26:06 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=359
06/15/2022 01:26:10 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=361
06/15/2022 01:26:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=362
06/15/2022 01:26:20 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.5283714075165806 on epoch=362
06/15/2022 01:26:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=363
06/15/2022 01:26:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=364
06/15/2022 01:26:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=366
06/15/2022 01:26:38 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=367
06/15/2022 01:26:42 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=368
06/15/2022 01:26:48 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.5220079583715946 on epoch=368
06/15/2022 01:26:53 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 01:26:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=371
06/15/2022 01:27:01 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.03 on epoch=372
06/15/2022 01:27:06 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=373
06/15/2022 01:27:10 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=374
06/15/2022 01:27:12 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 01:27:12 - INFO - __main__ - Printing 3 examples
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ - Tokenizing Input ...
06/15/2022 01:27:12 - INFO - __main__ - Tokenizing Output ...
06/15/2022 01:27:12 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 01:27:12 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 01:27:12 - INFO - __main__ - Printing 3 examples
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 01:27:12 - INFO - __main__ - ['refuted']
06/15/2022 01:27:12 - INFO - __main__ - Tokenizing Input ...
06/15/2022 01:27:12 - INFO - __main__ - Tokenizing Output ...
06/15/2022 01:27:12 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 01:27:16 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5356330320359097 on epoch=374
06/15/2022 01:27:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5326443468036388 -> 0.5356330320359097 on epoch=374, global_step=3000
06/15/2022 01:27:16 - INFO - __main__ - save last model!
06/15/2022 01:27:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 01:27:16 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 01:27:16 - INFO - __main__ - Printing 3 examples
06/15/2022 01:27:16 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 01:27:16 - INFO - __main__ - ['entailed']
06/15/2022 01:27:16 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 01:27:16 - INFO - __main__ - ['entailed']
06/15/2022 01:27:16 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 01:27:16 - INFO - __main__ - ['entailed']
06/15/2022 01:27:16 - INFO - __main__ - Tokenizing Input ...
06/15/2022 01:27:31 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 01:27:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 01:27:32 - INFO - __main__ - Starting training!
06/15/2022 01:27:41 - INFO - __main__ - Tokenizing Output ...
06/15/2022 01:27:53 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 01:36:48 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_100_0.4_8_predictions.txt
06/15/2022 01:36:48 - INFO - __main__ - Classification-F1 on test data: 0.1661
06/15/2022 01:36:48 - INFO - __main__ - prefix=tab_fact_64_100, lr=0.4, bsz=8, dev_performance=0.5356330320359097, test_performance=0.1661387525246477
06/15/2022 01:36:48 - INFO - __main__ - Running ... prefix=tab_fact_64_100, lr=0.3, bsz=8 ...
06/15/2022 01:36:49 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 01:36:49 - INFO - __main__ - Printing 3 examples
06/15/2022 01:36:49 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 01:36:49 - INFO - __main__ - ['refuted']
06/15/2022 01:36:49 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 01:36:49 - INFO - __main__ - ['refuted']
06/15/2022 01:36:49 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 01:36:49 - INFO - __main__ - ['refuted']
06/15/2022 01:36:49 - INFO - __main__ - Tokenizing Input ...
06/15/2022 01:36:49 - INFO - __main__ - Tokenizing Output ...
06/15/2022 01:36:50 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 01:36:50 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 01:36:50 - INFO - __main__ - Printing 3 examples
06/15/2022 01:36:50 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 01:36:50 - INFO - __main__ - ['refuted']
06/15/2022 01:36:50 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 01:36:50 - INFO - __main__ - ['refuted']
06/15/2022 01:36:50 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 01:36:50 - INFO - __main__ - ['refuted']
06/15/2022 01:36:50 - INFO - __main__ - Tokenizing Input ...
06/15/2022 01:36:50 - INFO - __main__ - Tokenizing Output ...
06/15/2022 01:36:50 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 01:37:06 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 01:37:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 01:37:06 - INFO - __main__ - Starting training!
06/15/2022 01:37:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.21 on epoch=1
06/15/2022 01:37:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.67 on epoch=2
06/15/2022 01:37:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=3
06/15/2022 01:37:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=4
06/15/2022 01:37:29 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=6
06/15/2022 01:37:34 - INFO - __main__ - Global step 50 Train loss 1.01 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 01:37:34 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 01:37:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=7
06/15/2022 01:37:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=8
06/15/2022 01:37:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=9
06/15/2022 01:37:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=11
06/15/2022 01:37:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=12
06/15/2022 01:38:02 - INFO - __main__ - Global step 100 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 01:38:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=13
06/15/2022 01:38:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=14
06/15/2022 01:38:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=16
06/15/2022 01:38:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=17
06/15/2022 01:38:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=18
06/15/2022 01:38:30 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 01:38:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=19
06/15/2022 01:38:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=21
06/15/2022 01:38:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=22
06/15/2022 01:38:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=23
06/15/2022 01:38:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=24
06/15/2022 01:38:58 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 01:39:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=26
06/15/2022 01:39:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=27
06/15/2022 01:39:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=28
06/15/2022 01:39:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=29
06/15/2022 01:39:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=31
06/15/2022 01:39:27 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 01:39:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=32
06/15/2022 01:39:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=33
06/15/2022 01:39:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=34
06/15/2022 01:39:45 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 01:39:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=37
06/15/2022 01:39:55 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 01:40:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=38
06/15/2022 01:40:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=39
06/15/2022 01:40:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=41
06/15/2022 01:40:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=42
06/15/2022 01:40:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=43
06/15/2022 01:40:24 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.34296770117665637 on epoch=43
06/15/2022 01:40:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34296770117665637 on epoch=43, global_step=350
06/15/2022 01:40:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=44
06/15/2022 01:40:33 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=46
06/15/2022 01:40:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=47
06/15/2022 01:40:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=48
06/15/2022 01:40:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=49
06/15/2022 01:40:53 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 01:40:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.18 on epoch=51
06/15/2022 01:41:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=52
06/15/2022 01:41:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=53
06/15/2022 01:41:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=54
06/15/2022 01:41:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 01:41:21 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 01:41:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=57
06/15/2022 01:41:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=58
06/15/2022 01:41:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 01:41:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=61
06/15/2022 01:41:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=62
06/15/2022 01:41:50 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 01:41:55 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=63
06/15/2022 01:42:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=64
06/15/2022 01:42:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 01:42:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=67
06/15/2022 01:42:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=68
06/15/2022 01:42:19 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 01:42:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 01:42:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=71
06/15/2022 01:42:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=72
06/15/2022 01:42:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=73
06/15/2022 01:42:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=74
06/15/2022 01:42:48 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 01:42:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=76
06/15/2022 01:42:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=77
06/15/2022 01:43:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=78
06/15/2022 01:43:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=79
06/15/2022 01:43:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=81
06/15/2022 01:43:16 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 01:43:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=82
06/15/2022 01:43:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=83
06/15/2022 01:43:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 01:43:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=86
06/15/2022 01:43:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=87
06/15/2022 01:43:45 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.39756367663344405 on epoch=87
06/15/2022 01:43:45 - INFO - __main__ - Saving model with best Classification-F1: 0.34296770117665637 -> 0.39756367663344405 on epoch=87, global_step=700
06/15/2022 01:43:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=88
06/15/2022 01:43:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=89
06/15/2022 01:43:58 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=91
06/15/2022 01:44:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=92
06/15/2022 01:44:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=93
06/15/2022 01:44:14 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.28159148220790137 on epoch=93
06/15/2022 01:44:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=94
06/15/2022 01:44:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=96
06/15/2022 01:44:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 01:44:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=98
06/15/2022 01:44:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 01:44:43 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.396357832112022 on epoch=99
06/15/2022 01:44:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=101
06/15/2022 01:44:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=102
06/15/2022 01:44:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=103
06/15/2022 01:45:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=104
06/15/2022 01:45:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=106
06/15/2022 01:45:12 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.36374269005847953 on epoch=106
06/15/2022 01:45:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=107
06/15/2022 01:45:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 01:45:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=109
06/15/2022 01:45:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=111
06/15/2022 01:45:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 01:45:40 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 01:45:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 01:45:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=114
06/15/2022 01:45:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=116
06/15/2022 01:45:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 01:46:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 01:46:09 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.24928255265333918 on epoch=118
06/15/2022 01:46:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=119
06/15/2022 01:46:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=121
06/15/2022 01:46:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=122
06/15/2022 01:46:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=123
06/15/2022 01:46:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 01:46:38 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.3591989987484355 on epoch=124
06/15/2022 01:46:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=126
06/15/2022 01:46:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=127
06/15/2022 01:46:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 01:46:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=129
06/15/2022 01:47:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=131
06/15/2022 01:47:07 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.5440923605993613 on epoch=131
06/15/2022 01:47:07 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.5440923605993613 on epoch=131, global_step=1050
06/15/2022 01:47:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=132
06/15/2022 01:47:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=133
06/15/2022 01:47:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=134
06/15/2022 01:47:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 01:47:29 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=137
06/15/2022 01:47:35 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.45705196182396607 on epoch=137
06/15/2022 01:47:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=138
06/15/2022 01:47:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=139
06/15/2022 01:47:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=141
06/15/2022 01:47:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=142
06/15/2022 01:47:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=143
06/15/2022 01:48:04 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.49672346002621226 on epoch=143
06/15/2022 01:48:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=144
06/15/2022 01:48:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=146
06/15/2022 01:48:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=147
06/15/2022 01:48:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=148
06/15/2022 01:48:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=149
06/15/2022 01:48:33 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.523175572519084 on epoch=149
06/15/2022 01:48:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=151
06/15/2022 01:48:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=152
06/15/2022 01:48:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=153
06/15/2022 01:48:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=154
06/15/2022 01:48:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=156
06/15/2022 01:49:02 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.5572028663207315 on epoch=156
06/15/2022 01:49:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5440923605993613 -> 0.5572028663207315 on epoch=156, global_step=1250
06/15/2022 01:49:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=157
06/15/2022 01:49:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=158
06/15/2022 01:49:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=159
06/15/2022 01:49:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.15 on epoch=161
06/15/2022 01:49:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=162
06/15/2022 01:49:32 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.3712316968130922 on epoch=162
06/15/2022 01:49:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=163
06/15/2022 01:49:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=164
06/15/2022 01:49:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.15 on epoch=166
06/15/2022 01:49:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=167
06/15/2022 01:49:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=168
06/15/2022 01:50:00 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.37639965546942294 on epoch=168
06/15/2022 01:50:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=169
06/15/2022 01:50:09 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=171
06/15/2022 01:50:14 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=172
06/15/2022 01:50:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=173
06/15/2022 01:50:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=174
06/15/2022 01:50:29 - INFO - __main__ - Global step 1400 Train loss 0.14 Classification-F1 0.5195195195195195 on epoch=174
06/15/2022 01:50:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=176
06/15/2022 01:50:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=177
06/15/2022 01:50:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=178
06/15/2022 01:50:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=179
06/15/2022 01:50:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=181
06/15/2022 01:51:01 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.500880503144654 on epoch=181
06/15/2022 01:51:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 01:51:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=183
06/15/2022 01:51:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=184
06/15/2022 01:51:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=186
06/15/2022 01:51:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=187
06/15/2022 01:51:30 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.42758857929136573 on epoch=187
06/15/2022 01:51:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=188
06/15/2022 01:51:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=189
06/15/2022 01:51:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=191
06/15/2022 01:51:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=192
06/15/2022 01:51:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=193
06/15/2022 01:51:59 - INFO - __main__ - Global step 1550 Train loss 0.12 Classification-F1 0.4402206822310435 on epoch=193
06/15/2022 01:52:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=194
06/15/2022 01:52:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=196
06/15/2022 01:52:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=197
06/15/2022 01:52:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=198
06/15/2022 01:52:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=199
06/15/2022 01:52:27 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.44824036543781764 on epoch=199
06/15/2022 01:52:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=201
06/15/2022 01:52:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=202
06/15/2022 01:52:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=203
06/15/2022 01:52:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=204
06/15/2022 01:52:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=206
06/15/2022 01:52:55 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.5195195195195195 on epoch=206
06/15/2022 01:53:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=207
06/15/2022 01:53:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=208
06/15/2022 01:53:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=209
06/15/2022 01:53:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=211
06/15/2022 01:53:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=212
06/15/2022 01:53:24 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.577195987276731 on epoch=212
06/15/2022 01:53:24 - INFO - __main__ - Saving model with best Classification-F1: 0.5572028663207315 -> 0.577195987276731 on epoch=212, global_step=1700
06/15/2022 01:53:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=213
06/15/2022 01:53:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=214
06/15/2022 01:53:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=216
06/15/2022 01:53:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=217
06/15/2022 01:53:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=218
06/15/2022 01:53:52 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.4562295424472456 on epoch=218
06/15/2022 01:53:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=219
06/15/2022 01:54:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=221
06/15/2022 01:54:05 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=222
06/15/2022 01:54:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=223
06/15/2022 01:54:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=224
06/15/2022 01:54:20 - INFO - __main__ - Global step 1800 Train loss 0.12 Classification-F1 0.42840679919331603 on epoch=224
06/15/2022 01:54:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=226
06/15/2022 01:54:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=227
06/15/2022 01:54:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=228
06/15/2022 01:54:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=229
06/15/2022 01:54:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=231
06/15/2022 01:54:50 - INFO - __main__ - Global step 1850 Train loss 0.11 Classification-F1 0.5342584027135369 on epoch=231
06/15/2022 01:54:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.14 on epoch=232
06/15/2022 01:54:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=233
06/15/2022 01:55:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=234
06/15/2022 01:55:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=236
06/15/2022 01:55:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=237
06/15/2022 01:55:19 - INFO - __main__ - Global step 1900 Train loss 0.09 Classification-F1 0.5075370545569221 on epoch=237
06/15/2022 01:55:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.10 on epoch=238
06/15/2022 01:55:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=239
06/15/2022 01:55:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=241
06/15/2022 01:55:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=242
06/15/2022 01:55:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=243
06/15/2022 01:55:47 - INFO - __main__ - Global step 1950 Train loss 0.10 Classification-F1 0.500959217773377 on epoch=243
06/15/2022 01:55:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=244
06/15/2022 01:55:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=246
06/15/2022 01:56:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=247
06/15/2022 01:56:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=248
06/15/2022 01:56:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=249
06/15/2022 01:56:17 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.4167176766646263 on epoch=249
06/15/2022 01:56:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=251
06/15/2022 01:56:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.09 on epoch=252
06/15/2022 01:56:30 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=253
06/15/2022 01:56:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.08 on epoch=254
06/15/2022 01:56:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.08 on epoch=256
06/15/2022 01:56:46 - INFO - __main__ - Global step 2050 Train loss 0.08 Classification-F1 0.5234084111579076 on epoch=256
06/15/2022 01:56:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=257
06/15/2022 01:56:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.07 on epoch=258
06/15/2022 01:56:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=259
06/15/2022 01:57:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=261
06/15/2022 01:57:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.05 on epoch=262
06/15/2022 01:57:15 - INFO - __main__ - Global step 2100 Train loss 0.06 Classification-F1 0.523175572519084 on epoch=262
06/15/2022 01:57:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=263
06/15/2022 01:57:24 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.09 on epoch=264
06/15/2022 01:57:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.11 on epoch=266
06/15/2022 01:57:33 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.09 on epoch=267
06/15/2022 01:57:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=268
06/15/2022 01:57:44 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.4920634920634921 on epoch=268
06/15/2022 01:57:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=269
06/15/2022 01:57:53 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.08 on epoch=271
06/15/2022 01:57:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=272
06/15/2022 01:58:02 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.08 on epoch=273
06/15/2022 01:58:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.08 on epoch=274
06/15/2022 01:58:13 - INFO - __main__ - Global step 2200 Train loss 0.07 Classification-F1 0.546875 on epoch=274
06/15/2022 01:58:17 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=276
06/15/2022 01:58:22 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.07 on epoch=277
06/15/2022 01:58:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=278
06/15/2022 01:58:31 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=279
06/15/2022 01:58:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.06 on epoch=281
06/15/2022 01:58:42 - INFO - __main__ - Global step 2250 Train loss 0.06 Classification-F1 0.5026827012025902 on epoch=281
06/15/2022 01:58:46 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.07 on epoch=282
06/15/2022 01:58:51 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=283
06/15/2022 01:58:55 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=284
06/15/2022 01:59:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.04 on epoch=286
06/15/2022 01:59:04 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.06 on epoch=287
06/15/2022 01:59:12 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.5467643467643467 on epoch=287
06/15/2022 01:59:17 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=288
06/15/2022 01:59:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=289
06/15/2022 01:59:25 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=291
06/15/2022 01:59:30 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=292
06/15/2022 01:59:34 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.09 on epoch=293
06/15/2022 01:59:42 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.5370264610698648 on epoch=293
06/15/2022 01:59:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=294
06/15/2022 01:59:51 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=296
06/15/2022 01:59:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.02 on epoch=297
06/15/2022 02:00:00 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=298
06/15/2022 02:00:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=299
06/15/2022 02:00:11 - INFO - __main__ - Global step 2400 Train loss 0.05 Classification-F1 0.4812085482682388 on epoch=299
06/15/2022 02:00:15 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.05 on epoch=301
06/15/2022 02:00:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.11 on epoch=302
06/15/2022 02:00:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.04 on epoch=303
06/15/2022 02:00:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.03 on epoch=304
06/15/2022 02:00:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.05 on epoch=306
06/15/2022 02:00:39 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.49556650246305417 on epoch=306
06/15/2022 02:00:44 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=307
06/15/2022 02:00:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.05 on epoch=308
06/15/2022 02:00:53 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=309
06/15/2022 02:00:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.06 on epoch=311
06/15/2022 02:01:02 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=312
06/15/2022 02:01:09 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.5658341042244834 on epoch=312
06/15/2022 02:01:14 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=313
06/15/2022 02:01:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.02 on epoch=314
06/15/2022 02:01:23 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.08 on epoch=316
06/15/2022 02:01:27 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=317
06/15/2022 02:01:32 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=318
06/15/2022 02:01:38 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.46471080229042006 on epoch=318
06/15/2022 02:01:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=319
06/15/2022 02:01:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.05 on epoch=321
06/15/2022 02:01:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=322
06/15/2022 02:01:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=323
06/15/2022 02:02:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=324
06/15/2022 02:02:07 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.3575569358178054 on epoch=324
06/15/2022 02:02:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=326
06/15/2022 02:02:16 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=327
06/15/2022 02:02:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=328
06/15/2022 02:02:25 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=329
06/15/2022 02:02:30 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=331
06/15/2022 02:02:36 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.5137254901960784 on epoch=331
06/15/2022 02:02:41 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=332
06/15/2022 02:02:45 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=333
06/15/2022 02:02:50 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=334
06/15/2022 02:02:54 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=336
06/15/2022 02:02:59 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=337
06/15/2022 02:03:05 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.5210697417653193 on epoch=337
06/15/2022 02:03:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=338
06/15/2022 02:03:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=339
06/15/2022 02:03:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=341
06/15/2022 02:03:23 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=342
06/15/2022 02:03:28 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.02 on epoch=343
06/15/2022 02:03:35 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.35381844078589086 on epoch=343
06/15/2022 02:03:39 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.03 on epoch=344
06/15/2022 02:03:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.06 on epoch=346
06/15/2022 02:03:48 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.03 on epoch=347
06/15/2022 02:03:53 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.06 on epoch=348
06/15/2022 02:03:57 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=349
06/15/2022 02:04:05 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.515625 on epoch=349
06/15/2022 02:04:09 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=351
06/15/2022 02:04:14 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=352
06/15/2022 02:04:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=353
06/15/2022 02:04:23 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=354
06/15/2022 02:04:27 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=356
06/15/2022 02:04:34 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5113300492610837 on epoch=356
06/15/2022 02:04:38 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=357
06/15/2022 02:04:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=358
06/15/2022 02:04:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=359
06/15/2022 02:04:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=361
06/15/2022 02:04:57 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.04 on epoch=362
06/15/2022 02:05:03 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.531135531135531 on epoch=362
06/15/2022 02:05:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=363
06/15/2022 02:05:12 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=364
06/15/2022 02:05:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.05 on epoch=366
06/15/2022 02:05:21 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=367
06/15/2022 02:05:26 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=368
06/15/2022 02:05:33 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.5058530510585305 on epoch=368
06/15/2022 02:05:38 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.05 on epoch=369
06/15/2022 02:05:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=371
06/15/2022 02:05:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=372
06/15/2022 02:05:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=373
06/15/2022 02:05:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=374
06/15/2022 02:05:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:05:57 - INFO - __main__ - Printing 3 examples
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:05:57 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:05:57 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 02:05:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:05:57 - INFO - __main__ - Printing 3 examples
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 02:05:57 - INFO - __main__ - ['refuted']
06/15/2022 02:05:57 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:05:58 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:05:58 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 02:06:03 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.5286775260562941 on epoch=374
06/15/2022 02:06:03 - INFO - __main__ - save last model!
06/15/2022 02:06:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 02:06:04 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 02:06:04 - INFO - __main__ - Printing 3 examples
06/15/2022 02:06:04 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:06:04 - INFO - __main__ - ['entailed']
06/15/2022 02:06:04 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:06:04 - INFO - __main__ - ['entailed']
06/15/2022 02:06:04 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:06:04 - INFO - __main__ - ['entailed']
06/15/2022 02:06:04 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:06:16 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 02:06:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 02:06:17 - INFO - __main__ - Starting training!
06/15/2022 02:06:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:06:42 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 02:17:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_100_0.3_8_predictions.txt
06/15/2022 02:17:10 - INFO - __main__ - Classification-F1 on test data: 0.0816
06/15/2022 02:17:11 - INFO - __main__ - prefix=tab_fact_64_100, lr=0.3, bsz=8, dev_performance=0.577195987276731, test_performance=0.08164170605474207
06/15/2022 02:17:11 - INFO - __main__ - Running ... prefix=tab_fact_64_100, lr=0.2, bsz=8 ...
06/15/2022 02:17:11 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:17:11 - INFO - __main__ - Printing 3 examples
06/15/2022 02:17:11 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
06/15/2022 02:17:11 - INFO - __main__ - ['refuted']
06/15/2022 02:17:11 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
06/15/2022 02:17:11 - INFO - __main__ - ['refuted']
06/15/2022 02:17:11 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malmö#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#borås#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#borås#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocław#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzów#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krško#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzów#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#poznań#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
06/15/2022 02:17:11 - INFO - __main__ - ['refuted']
06/15/2022 02:17:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:17:12 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:17:12 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 02:17:12 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:17:12 - INFO - __main__ - Printing 3 examples
06/15/2022 02:17:12 - INFO - __main__ -  [tab_fact] statement: marci greer be rank 1 from 1981 - 82 [SEP] table_caption: ladies professional racquetball tour [SEP] table_text: rank#1980 - 81#1981 - 82#1982 - 83#1983 - 84#1984 - 85 [n] 1#heather mckay#lynn adams#heather mckay#heather mckay#lynn adams [n] 2#lynn adams#heather mckay#lynn adams#lynn adams#vicki panzeri [n] 3#shannon wright#shannon wright#shannon wright#shannon wright - hamilton#terri gilreath [n] 4#marci greer#marci greer#laura martino#janell marriott#caryn mckinney [n] 5#karin walton - trent#peggy gardner#vicki panzeri#vicki panzeri#marci drexler [n] 6#peggy gardner#laura martino#janell marriott#terri gilreath#janell marriott [n] 7#laura martino#terri gilreath#marci greer#laura martino#heather mckay [n] 8#linda prefontaine#rita hoff#terri gilreath#caryn mckinney#liz alvarado [n] 9#francine davis#jennifer harding#peggy gardner#marci greer#diane bullard [n] 
06/15/2022 02:17:12 - INFO - __main__ - ['refuted']
06/15/2022 02:17:12 - INFO - __main__ -  [tab_fact] statement: the crowd at junction oval and arden street oval be of the same size [SEP] table_caption: 1926 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#9.16 (70)#south melbourne#15.16 (106)#arden street oval#12000#28 august 1926 [n] footscray#9.19 (73)#hawthorn#11.14 (80)#western oval#5000#28 august 1926 [n] collingwood#8.16 (64)#geelong#14.13 (97)#victoria park#26550#28 august 1926 [n] carlton#18.18 (126)#fitzroy#15.14 (104)#princes park#25000#28 august 1926 [n] richmond#10.8 (68)#melbourne#13.16 (94)#punt road oval#15000#28 august 1926 [n] st kilda#2.11 (23)#essendon#14.11 (95)#junction oval#13000#28 august 1926 [n] 
06/15/2022 02:17:12 - INFO - __main__ - ['refuted']
06/15/2022 02:17:12 - INFO - __main__ -  [tab_fact] statement: bosko 's fox hunt have 4977 as the smallest production number [SEP] table_caption: looney tunes and merrie melodies filmography (1929 - 39) [SEP] table_text: title#series#characters#production num#release date [n] big man from the north#lt#bosko , honey#4500#1931 - 01 - xx [n] ain't nature grand!#lt#bosko#4626#1931 - 03 - xx [n] ups 'n downs#lt#bosko#4640#1931 - 03 - xx [n] dumb patrol#lt#bosko , honey#4664#1931 - 05 - xx [n] yodeling yokels#lt#bosko , honey#4680#1931 - 06 - xx [n] bosko 's holiday#lt#bosko , honey#4694#1931 - 07 - xx [n] the tree 's knees#lt#bosko#4725#1931 - 07 - xx [n] lady , play your mandolin!#mm#animals (cartoon character) , foxy , roxy#4645#1931 - 08 - xx [n] smile , darn ya , smile!#mm#foxy , radio , roxy#4825#1931 - 09 - 05 [n] bosko shipwrecked#lt#bosko#4666#1931 - 09 - 19 [n] one more time#mm#foxy , mugs , roxy#4851#1931 - 10 - 03 [n] bosko the doughboy#lt#bosko#5017#1931 - 10 - 17 [n] you don't know what you 're doin'#mm#fluffy , piggy , the car#4977#1931 - 10 - 31 [n] bosko 's soda fountain#lt#bosko#5045#1931 - 11 - 14 [n] hittin' the trail for hallelujah land#mm#banjo player , fluffy#5025#11 / 28 / 31 [n] bosko 's fox hunt#lt#bosko , bruno#5046#1931 - 12 - 12 [n] red - headed baby#mm#red - headed baby , toymaker#5038#1931 - 12 - 26 [n] 
06/15/2022 02:17:12 - INFO - __main__ - ['refuted']
06/15/2022 02:17:12 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:17:12 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:17:12 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 02:17:31 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 02:17:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 02:17:32 - INFO - __main__ - Starting training!
06/15/2022 02:17:37 - INFO - __main__ - Step 10 Global step 10 Train loss 3.92 on epoch=1
06/15/2022 02:17:41 - INFO - __main__ - Step 20 Global step 20 Train loss 1.20 on epoch=2
06/15/2022 02:17:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=3
06/15/2022 02:17:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=4
06/15/2022 02:17:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=6
06/15/2022 02:18:00 - INFO - __main__ - Global step 50 Train loss 1.31 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 02:18:00 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 02:18:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=7
06/15/2022 02:18:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=8
06/15/2022 02:18:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=9
06/15/2022 02:18:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=11
06/15/2022 02:18:22 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=12
06/15/2022 02:18:28 - INFO - __main__ - Global step 100 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 02:18:32 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=13
06/15/2022 02:18:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=14
06/15/2022 02:18:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=16
06/15/2022 02:18:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=17
06/15/2022 02:18:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=18
06/15/2022 02:18:56 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 02:19:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=19
06/15/2022 02:19:05 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=21
06/15/2022 02:19:10 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=22
06/15/2022 02:19:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 02:19:19 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=24
06/15/2022 02:19:24 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 02:19:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=26
06/15/2022 02:19:33 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=27
06/15/2022 02:19:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=28
06/15/2022 02:19:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=29
06/15/2022 02:19:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=31
06/15/2022 02:19:52 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 02:19:57 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=32
06/15/2022 02:20:01 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=33
06/15/2022 02:20:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=34
06/15/2022 02:20:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=36
06/15/2022 02:20:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=37
06/15/2022 02:20:21 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 02:20:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=38
06/15/2022 02:20:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=39
06/15/2022 02:20:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=41
06/15/2022 02:20:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=42
06/15/2022 02:20:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=43
06/15/2022 02:20:49 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 02:20:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 02:20:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=46
06/15/2022 02:21:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=47
06/15/2022 02:21:07 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=48
06/15/2022 02:21:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=49
06/15/2022 02:21:18 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 02:21:22 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=51
06/15/2022 02:21:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=52
06/15/2022 02:21:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=53
06/15/2022 02:21:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=54
06/15/2022 02:21:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 02:21:46 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.15217391304347827 on epoch=56
06/15/2022 02:21:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=57
06/15/2022 02:21:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=58
06/15/2022 02:22:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 02:22:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=61
06/15/2022 02:22:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 02:22:14 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 02:22:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=63
06/15/2022 02:22:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=64
06/15/2022 02:22:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=66
06/15/2022 02:22:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=67
06/15/2022 02:22:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 02:22:42 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 02:22:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 02:22:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 02:22:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=72
06/15/2022 02:23:01 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=73
06/15/2022 02:23:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 02:23:11 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 02:23:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=76
06/15/2022 02:23:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 02:23:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=78
06/15/2022 02:23:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=79
06/15/2022 02:23:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=81
06/15/2022 02:23:40 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 02:23:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=82
06/15/2022 02:23:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=83
06/15/2022 02:23:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=84
06/15/2022 02:23:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=86
06/15/2022 02:24:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=87
06/15/2022 02:24:08 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 02:24:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=88
06/15/2022 02:24:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=89
06/15/2022 02:24:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=91
06/15/2022 02:24:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=92
06/15/2022 02:24:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=93
06/15/2022 02:24:37 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.4882924043403769 on epoch=93
06/15/2022 02:24:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4882924043403769 on epoch=93, global_step=750
06/15/2022 02:24:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=94
06/15/2022 02:24:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 02:24:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=97
06/15/2022 02:24:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=98
06/15/2022 02:24:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=99
06/15/2022 02:25:05 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 02:25:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=101
06/15/2022 02:25:14 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 02:25:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=103
06/15/2022 02:25:23 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=104
06/15/2022 02:25:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=106
06/15/2022 02:25:33 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 02:25:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=107
06/15/2022 02:25:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 02:25:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=109
06/15/2022 02:25:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=111
06/15/2022 02:25:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 02:26:01 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.37922403003754696 on epoch=112
06/15/2022 02:26:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=113
06/15/2022 02:26:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=114
06/15/2022 02:26:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=116
06/15/2022 02:26:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 02:26:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 02:26:29 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.5026827012025902 on epoch=118
06/15/2022 02:26:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4882924043403769 -> 0.5026827012025902 on epoch=118, global_step=950
06/15/2022 02:26:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=119
06/15/2022 02:26:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=121
06/15/2022 02:26:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=122
06/15/2022 02:26:47 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=123
06/15/2022 02:26:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=124
06/15/2022 02:26:57 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 02:27:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=126
06/15/2022 02:27:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 02:27:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=128
06/15/2022 02:27:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 02:27:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=131
06/15/2022 02:27:25 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.44379029997196523 on epoch=131
06/15/2022 02:27:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=132
06/15/2022 02:27:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=133
06/15/2022 02:27:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=134
06/15/2022 02:27:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=136
06/15/2022 02:27:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=137
06/15/2022 02:27:54 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.4202898550724638 on epoch=137
06/15/2022 02:27:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=138
06/15/2022 02:28:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=139
06/15/2022 02:28:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=141
06/15/2022 02:28:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=142
06/15/2022 02:28:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=143
06/15/2022 02:28:22 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.4727272727272727 on epoch=143
06/15/2022 02:28:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=144
06/15/2022 02:28:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=146
06/15/2022 02:28:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=147
06/15/2022 02:28:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=148
06/15/2022 02:28:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=149
06/15/2022 02:28:50 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.5339508144362823 on epoch=149
06/15/2022 02:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5026827012025902 -> 0.5339508144362823 on epoch=149, global_step=1200
06/15/2022 02:28:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=151
06/15/2022 02:28:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=152
06/15/2022 02:29:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=153
06/15/2022 02:29:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=154
06/15/2022 02:29:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=156
06/15/2022 02:29:18 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.5305925497008299 on epoch=156
06/15/2022 02:29:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=157
06/15/2022 02:29:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=158
06/15/2022 02:29:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=159
06/15/2022 02:29:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=161
06/15/2022 02:29:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=162
06/15/2022 02:29:47 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.4833333333333334 on epoch=162
06/15/2022 02:29:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=163
06/15/2022 02:29:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=164
06/15/2022 02:30:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=166
06/15/2022 02:30:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=167
06/15/2022 02:30:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=168
06/15/2022 02:30:15 - INFO - __main__ - Global step 1350 Train loss 0.16 Classification-F1 0.5405128205128205 on epoch=168
06/15/2022 02:30:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5339508144362823 -> 0.5405128205128205 on epoch=168, global_step=1350
06/15/2022 02:30:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=169
06/15/2022 02:30:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=171
06/15/2022 02:30:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=172
06/15/2022 02:30:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=173
06/15/2022 02:30:37 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=174
06/15/2022 02:30:43 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.504537089916873 on epoch=174
06/15/2022 02:30:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=176
06/15/2022 02:30:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=177
06/15/2022 02:30:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=178
06/15/2022 02:31:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=179
06/15/2022 02:31:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=181
06/15/2022 02:31:11 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.4335050424435899 on epoch=181
06/15/2022 02:31:15 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=182
06/15/2022 02:31:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=183
06/15/2022 02:31:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=184
06/15/2022 02:31:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=186
06/15/2022 02:31:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=187
06/15/2022 02:31:39 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.5053671103477888 on epoch=187
06/15/2022 02:31:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=188
06/15/2022 02:31:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=189
06/15/2022 02:31:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=191
06/15/2022 02:31:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=192
06/15/2022 02:32:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=193
06/15/2022 02:32:07 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.36657784545108485 on epoch=193
06/15/2022 02:32:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=194
06/15/2022 02:32:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=196
06/15/2022 02:32:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=197
06/15/2022 02:32:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=198
06/15/2022 02:32:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=199
06/15/2022 02:32:35 - INFO - __main__ - Global step 1600 Train loss 0.15 Classification-F1 0.5370705244122965 on epoch=199
06/15/2022 02:32:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=201
06/15/2022 02:32:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=202
06/15/2022 02:32:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=203
06/15/2022 02:32:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=204
06/15/2022 02:32:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=206
06/15/2022 02:33:03 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.5097603162836669 on epoch=206
06/15/2022 02:33:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=207
06/15/2022 02:33:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=208
06/15/2022 02:33:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=209
06/15/2022 02:33:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=211
06/15/2022 02:33:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=212
06/15/2022 02:33:31 - INFO - __main__ - Global step 1700 Train loss 0.14 Classification-F1 0.500959217773377 on epoch=212
06/15/2022 02:33:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=213
06/15/2022 02:33:40 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=214
06/15/2022 02:33:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=216
06/15/2022 02:33:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=217
06/15/2022 02:33:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=218
06/15/2022 02:33:59 - INFO - __main__ - Global step 1750 Train loss 0.13 Classification-F1 0.47714452441159305 on epoch=218
06/15/2022 02:34:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=219
06/15/2022 02:34:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=221
06/15/2022 02:34:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=222
06/15/2022 02:34:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=223
06/15/2022 02:34:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=224
06/15/2022 02:34:27 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.48747093774218553 on epoch=224
06/15/2022 02:34:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=226
06/15/2022 02:34:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=227
06/15/2022 02:34:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=228
06/15/2022 02:34:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=229
06/15/2022 02:34:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=231
06/15/2022 02:34:55 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.3288987435328899 on epoch=231
06/15/2022 02:35:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=232
06/15/2022 02:35:04 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=233
06/15/2022 02:35:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=234
06/15/2022 02:35:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=236
06/15/2022 02:35:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=237
06/15/2022 02:35:23 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.48689485044711683 on epoch=237
06/15/2022 02:35:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=238
06/15/2022 02:35:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=239
06/15/2022 02:35:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=241
06/15/2022 02:35:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=242
06/15/2022 02:35:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=243
06/15/2022 02:35:51 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.5076923076923077 on epoch=243
06/15/2022 02:35:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=244
06/15/2022 02:36:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=246
06/15/2022 02:36:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=247
06/15/2022 02:36:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=248
06/15/2022 02:36:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=249
06/15/2022 02:36:19 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.504537089916873 on epoch=249
06/15/2022 02:36:23 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=251
06/15/2022 02:36:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=252
06/15/2022 02:36:32 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.08 on epoch=253
06/15/2022 02:36:37 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.12 on epoch=254
06/15/2022 02:36:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.12 on epoch=256
06/15/2022 02:36:46 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.5127004930412533 on epoch=256
06/15/2022 02:36:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.15 on epoch=257
06/15/2022 02:36:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=258
06/15/2022 02:37:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=259
06/15/2022 02:37:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.06 on epoch=261
06/15/2022 02:37:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=262
06/15/2022 02:37:15 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.3152917755381361 on epoch=262
06/15/2022 02:37:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.08 on epoch=263
06/15/2022 02:37:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=264
06/15/2022 02:37:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=266
06/15/2022 02:37:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.10 on epoch=267
06/15/2022 02:37:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=268
06/15/2022 02:37:42 - INFO - __main__ - Global step 2150 Train loss 0.09 Classification-F1 0.4832395400048935 on epoch=268
06/15/2022 02:37:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=269
06/15/2022 02:37:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.09 on epoch=271
06/15/2022 02:37:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.08 on epoch=272
06/15/2022 02:38:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=273
06/15/2022 02:38:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=274
06/15/2022 02:38:10 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.501245608431811 on epoch=274
06/15/2022 02:38:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=276
06/15/2022 02:38:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=277
06/15/2022 02:38:23 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=278
06/15/2022 02:38:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.06 on epoch=279
06/15/2022 02:38:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=281
06/15/2022 02:38:38 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5155067155067155 on epoch=281
06/15/2022 02:38:42 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=282
06/15/2022 02:38:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=283
06/15/2022 02:38:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=284
06/15/2022 02:38:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=286
06/15/2022 02:39:00 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=287
06/15/2022 02:39:06 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.4995112414467253 on epoch=287
06/15/2022 02:39:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=288
06/15/2022 02:39:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=289
06/15/2022 02:39:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.08 on epoch=291
06/15/2022 02:39:24 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.08 on epoch=292
06/15/2022 02:39:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.08 on epoch=293
06/15/2022 02:39:34 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.4988989478835331 on epoch=293
06/15/2022 02:39:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=294
06/15/2022 02:39:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=296
06/15/2022 02:39:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=297
06/15/2022 02:39:52 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.08 on epoch=298
06/15/2022 02:39:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=299
06/15/2022 02:40:03 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.455803033187385 on epoch=299
06/15/2022 02:40:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=301
06/15/2022 02:40:11 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=302
06/15/2022 02:40:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=303
06/15/2022 02:40:20 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.08 on epoch=304
06/15/2022 02:40:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=306
06/15/2022 02:40:30 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.4817813765182186 on epoch=306
06/15/2022 02:40:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=307
06/15/2022 02:40:38 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=308
06/15/2022 02:40:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.04 on epoch=309
06/15/2022 02:40:47 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.07 on epoch=311
06/15/2022 02:40:52 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.06 on epoch=312
06/15/2022 02:40:57 - INFO - __main__ - Global step 2500 Train loss 0.06 Classification-F1 0.4874874874874875 on epoch=312
06/15/2022 02:41:01 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=313
06/15/2022 02:41:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.07 on epoch=314
06/15/2022 02:41:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=316
06/15/2022 02:41:15 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=317
06/15/2022 02:41:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.04 on epoch=318
06/15/2022 02:41:24 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.4752444023967203 on epoch=318
06/15/2022 02:41:29 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.09 on epoch=319
06/15/2022 02:41:33 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=321
06/15/2022 02:41:38 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=322
06/15/2022 02:41:42 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=323
06/15/2022 02:41:46 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=324
06/15/2022 02:41:52 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.3393730227207363 on epoch=324
06/15/2022 02:41:56 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.05 on epoch=326
06/15/2022 02:42:01 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=327
06/15/2022 02:42:05 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=328
06/15/2022 02:42:10 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=329
06/15/2022 02:42:14 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=331
06/15/2022 02:42:20 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.3258678611422172 on epoch=331
06/15/2022 02:42:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.06 on epoch=332
06/15/2022 02:42:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=333
06/15/2022 02:42:33 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=334
06/15/2022 02:42:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.05 on epoch=336
06/15/2022 02:42:42 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=337
06/15/2022 02:42:47 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.4458874458874459 on epoch=337
06/15/2022 02:42:51 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=338
06/15/2022 02:42:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.06 on epoch=339
06/15/2022 02:43:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=341
06/15/2022 02:43:04 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=342
06/15/2022 02:43:09 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=343
06/15/2022 02:43:14 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.49215650369285235 on epoch=343
06/15/2022 02:43:19 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=344
06/15/2022 02:43:23 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=346
06/15/2022 02:43:28 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.05 on epoch=347
06/15/2022 02:43:32 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=348
06/15/2022 02:43:36 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=349
06/15/2022 02:43:42 - INFO - __main__ - Global step 2800 Train loss 0.06 Classification-F1 0.523175572519084 on epoch=349
06/15/2022 02:43:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=351
06/15/2022 02:43:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=352
06/15/2022 02:43:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=353
06/15/2022 02:44:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=354
06/15/2022 02:44:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=356
06/15/2022 02:44:09 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.32418799212598426 on epoch=356
06/15/2022 02:44:14 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=357
06/15/2022 02:44:18 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=358
06/15/2022 02:44:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=359
06/15/2022 02:44:27 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=361
06/15/2022 02:44:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=362
06/15/2022 02:44:37 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.5234084111579076 on epoch=362
06/15/2022 02:44:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=363
06/15/2022 02:44:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=364
06/15/2022 02:44:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.04 on epoch=366
06/15/2022 02:44:55 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=367
06/15/2022 02:44:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=368
06/15/2022 02:45:04 - INFO - __main__ - Global step 2950 Train loss 0.04 Classification-F1 0.298807594831526 on epoch=368
06/15/2022 02:45:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 02:45:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=371
06/15/2022 02:45:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=372
06/15/2022 02:45:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=373
06/15/2022 02:45:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.04 on epoch=374
06/15/2022 02:45:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:45:28 - INFO - __main__ - Printing 3 examples
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:45:28 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:45:28 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 02:45:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:45:28 - INFO - __main__ - Printing 3 examples
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 02:45:28 - INFO - __main__ - ['refuted']
06/15/2022 02:45:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:45:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:45:29 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 02:45:32 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.3352860525987785 on epoch=374
06/15/2022 02:45:32 - INFO - __main__ - save last model!
06/15/2022 02:45:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 02:45:32 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 02:45:32 - INFO - __main__ - Printing 3 examples
06/15/2022 02:45:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:45:32 - INFO - __main__ - ['entailed']
06/15/2022 02:45:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:45:32 - INFO - __main__ - ['entailed']
06/15/2022 02:45:32 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 02:45:32 - INFO - __main__ - ['entailed']
06/15/2022 02:45:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:45:45 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 02:45:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 02:45:46 - INFO - __main__ - Starting training!
06/15/2022 02:45:57 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:46:09 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 02:54:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_100_0.2_8_predictions.txt
06/15/2022 02:54:55 - INFO - __main__ - Classification-F1 on test data: 0.0699
06/15/2022 02:54:56 - INFO - __main__ - prefix=tab_fact_64_100, lr=0.2, bsz=8, dev_performance=0.5405128205128205, test_performance=0.06992339742154265
06/15/2022 02:54:56 - INFO - __main__ - Running ... prefix=tab_fact_64_13, lr=0.5, bsz=8 ...
06/15/2022 02:54:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:54:57 - INFO - __main__ - Printing 3 examples
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:54:57 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:54:57 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 02:54:57 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 02:54:57 - INFO - __main__ - Printing 3 examples
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 02:54:57 - INFO - __main__ - ['refuted']
06/15/2022 02:54:57 - INFO - __main__ - Tokenizing Input ...
06/15/2022 02:54:58 - INFO - __main__ - Tokenizing Output ...
06/15/2022 02:54:58 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 02:55:13 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 02:55:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 02:55:14 - INFO - __main__ - Starting training!
06/15/2022 02:55:19 - INFO - __main__ - Step 10 Global step 10 Train loss 2.18 on epoch=1
06/15/2022 02:55:24 - INFO - __main__ - Step 20 Global step 20 Train loss 0.52 on epoch=2
06/15/2022 02:55:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=3
06/15/2022 02:55:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=4
06/15/2022 02:55:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.25 on epoch=6
06/15/2022 02:55:42 - INFO - __main__ - Global step 50 Train loss 0.74 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 02:55:43 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 02:55:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=7
06/15/2022 02:55:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=8
06/15/2022 02:55:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=9
06/15/2022 02:56:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=11
06/15/2022 02:56:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=12
06/15/2022 02:56:10 - INFO - __main__ - Global step 100 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 02:56:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.19 on epoch=13
06/15/2022 02:56:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 02:56:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=16
06/15/2022 02:56:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=17
06/15/2022 02:56:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=18
06/15/2022 02:56:38 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 02:56:42 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=19
06/15/2022 02:56:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 02:56:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=22
06/15/2022 02:56:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=23
06/15/2022 02:57:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=24
06/15/2022 02:57:05 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.4059668508287293 on epoch=24
06/15/2022 02:57:05 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4059668508287293 on epoch=24, global_step=200
06/15/2022 02:57:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.21 on epoch=26
06/15/2022 02:57:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=27
06/15/2022 02:57:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=28
06/15/2022 02:57:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=29
06/15/2022 02:57:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=31
06/15/2022 02:57:32 - INFO - __main__ - Global step 250 Train loss 0.22 Classification-F1 0.350463149416029 on epoch=31
06/15/2022 02:57:37 - INFO - __main__ - Step 260 Global step 260 Train loss 0.18 on epoch=32
06/15/2022 02:57:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 02:57:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=34
06/15/2022 02:57:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=36
06/15/2022 02:57:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=37
06/15/2022 02:57:59 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 02:58:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=38
06/15/2022 02:58:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=39
06/15/2022 02:58:13 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=41
06/15/2022 02:58:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=42
06/15/2022 02:58:22 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 02:58:27 - INFO - __main__ - Global step 350 Train loss 0.21 Classification-F1 0.5 on epoch=43
06/15/2022 02:58:27 - INFO - __main__ - Saving model with best Classification-F1: 0.4059668508287293 -> 0.5 on epoch=43, global_step=350
06/15/2022 02:58:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 02:58:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=46
06/15/2022 02:58:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=47
06/15/2022 02:58:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=48
06/15/2022 02:58:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=49
06/15/2022 02:58:55 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 02:59:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=51
06/15/2022 02:59:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=52
06/15/2022 02:59:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=53
06/15/2022 02:59:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=54
06/15/2022 02:59:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=56
06/15/2022 02:59:23 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 02:59:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=57
06/15/2022 02:59:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=58
06/15/2022 02:59:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 02:59:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=61
06/15/2022 02:59:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 02:59:51 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 02:59:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=63
06/15/2022 03:00:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=64
06/15/2022 03:00:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=66
06/15/2022 03:00:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=67
06/15/2022 03:00:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 03:00:20 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.39636200314394787 on epoch=68
06/15/2022 03:00:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=69
06/15/2022 03:00:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=71
06/15/2022 03:00:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 03:00:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=73
06/15/2022 03:00:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 03:00:48 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.42482504604051563 on epoch=74
06/15/2022 03:00:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=76
06/15/2022 03:00:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=77
06/15/2022 03:01:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=78
06/15/2022 03:01:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=79
06/15/2022 03:01:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=81
06/15/2022 03:01:16 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.36657784545108485 on epoch=81
06/15/2022 03:01:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=82
06/15/2022 03:01:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=83
06/15/2022 03:01:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 03:01:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.18 on epoch=86
06/15/2022 03:01:38 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=87
06/15/2022 03:01:44 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3860677578987438 on epoch=87
06/15/2022 03:01:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=88
06/15/2022 03:01:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=89
06/15/2022 03:01:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=91
06/15/2022 03:02:01 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=92
06/15/2022 03:02:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.17 on epoch=93
06/15/2022 03:02:12 - INFO - __main__ - Global step 750 Train loss 0.19 Classification-F1 0.42482504604051563 on epoch=93
06/15/2022 03:02:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=94
06/15/2022 03:02:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=96
06/15/2022 03:02:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 03:02:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=98
06/15/2022 03:02:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.17 on epoch=99
06/15/2022 03:02:40 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.46843853820598 on epoch=99
06/15/2022 03:02:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=101
06/15/2022 03:02:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.16 on epoch=102
06/15/2022 03:02:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.17 on epoch=103
06/15/2022 03:02:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=104
06/15/2022 03:03:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=106
06/15/2022 03:03:08 - INFO - __main__ - Global step 850 Train loss 0.17 Classification-F1 0.49703629703629704 on epoch=106
06/15/2022 03:03:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=107
06/15/2022 03:03:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=108
06/15/2022 03:03:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=109
06/15/2022 03:03:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=111
06/15/2022 03:03:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=112
06/15/2022 03:03:36 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.5075419847328244 on epoch=112
06/15/2022 03:03:36 - INFO - __main__ - Saving model with best Classification-F1: 0.5 -> 0.5075419847328244 on epoch=112, global_step=900
06/15/2022 03:03:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=113
06/15/2022 03:03:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.16 on epoch=114
06/15/2022 03:03:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.15 on epoch=116
06/15/2022 03:03:54 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=117
06/15/2022 03:03:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=118
06/15/2022 03:04:04 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.554006968641115 on epoch=118
06/15/2022 03:04:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5075419847328244 -> 0.554006968641115 on epoch=118, global_step=950
06/15/2022 03:04:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.16 on epoch=119
06/15/2022 03:04:13 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=121
06/15/2022 03:04:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=122
06/15/2022 03:04:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=123
06/15/2022 03:04:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.14 on epoch=124
06/15/2022 03:04:32 - INFO - __main__ - Global step 1000 Train loss 0.15 Classification-F1 0.5524750046003803 on epoch=124
06/15/2022 03:04:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=126
06/15/2022 03:04:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=127
06/15/2022 03:04:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=128
06/15/2022 03:04:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=129
06/15/2022 03:04:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.12 on epoch=131
06/15/2022 03:05:00 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.5217932752179327 on epoch=131
06/15/2022 03:05:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=132
06/15/2022 03:05:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=133
06/15/2022 03:05:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=134
06/15/2022 03:05:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=136
06/15/2022 03:05:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=137
06/15/2022 03:05:28 - INFO - __main__ - Global step 1100 Train loss 0.13 Classification-F1 0.5169811320754718 on epoch=137
06/15/2022 03:05:32 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=138
06/15/2022 03:05:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=139
06/15/2022 03:05:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=141
06/15/2022 03:05:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=142
06/15/2022 03:05:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.12 on epoch=143
06/15/2022 03:05:56 - INFO - __main__ - Global step 1150 Train loss 0.12 Classification-F1 0.562753036437247 on epoch=143
06/15/2022 03:05:56 - INFO - __main__ - Saving model with best Classification-F1: 0.554006968641115 -> 0.562753036437247 on epoch=143, global_step=1150
06/15/2022 03:06:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.14 on epoch=144
06/15/2022 03:06:05 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=146
06/15/2022 03:06:09 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=147
06/15/2022 03:06:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=148
06/15/2022 03:06:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.11 on epoch=149
06/15/2022 03:06:24 - INFO - __main__ - Global step 1200 Train loss 0.11 Classification-F1 0.500880503144654 on epoch=149
06/15/2022 03:06:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=151
06/15/2022 03:06:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=152
06/15/2022 03:06:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=153
06/15/2022 03:06:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=154
06/15/2022 03:06:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=156
06/15/2022 03:06:52 - INFO - __main__ - Global step 1250 Train loss 0.11 Classification-F1 0.5169811320754718 on epoch=156
06/15/2022 03:06:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=157
06/15/2022 03:07:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.08 on epoch=158
06/15/2022 03:07:05 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=159
06/15/2022 03:07:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=161
06/15/2022 03:07:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.08 on epoch=162
06/15/2022 03:07:20 - INFO - __main__ - Global step 1300 Train loss 0.08 Classification-F1 0.5270935960591133 on epoch=162
06/15/2022 03:07:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=163
06/15/2022 03:07:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=164
06/15/2022 03:07:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=166
06/15/2022 03:07:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=167
06/15/2022 03:07:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.10 on epoch=168
06/15/2022 03:07:47 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.34461859979101356 on epoch=168
06/15/2022 03:07:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=169
06/15/2022 03:07:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.10 on epoch=171
06/15/2022 03:08:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=172
06/15/2022 03:08:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=173
06/15/2022 03:08:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=174
06/15/2022 03:08:15 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.5370705244122965 on epoch=174
06/15/2022 03:08:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=176
06/15/2022 03:08:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=177
06/15/2022 03:08:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.29 on epoch=178
06/15/2022 03:08:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=179
06/15/2022 03:08:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=181
06/15/2022 03:08:43 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.5635334234060349 on epoch=181
06/15/2022 03:08:43 - INFO - __main__ - Saving model with best Classification-F1: 0.562753036437247 -> 0.5635334234060349 on epoch=181, global_step=1450
06/15/2022 03:08:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.07 on epoch=182
06/15/2022 03:08:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=183
06/15/2022 03:08:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=184
06/15/2022 03:09:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=186
06/15/2022 03:09:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.07 on epoch=187
06/15/2022 03:09:11 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.4811812391430226 on epoch=187
06/15/2022 03:09:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=188
06/15/2022 03:09:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=189
06/15/2022 03:09:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=191
06/15/2022 03:09:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=192
06/15/2022 03:09:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=193
06/15/2022 03:09:39 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.5137254901960784 on epoch=193
06/15/2022 03:09:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=194
06/15/2022 03:09:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.07 on epoch=196
06/15/2022 03:09:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=197
06/15/2022 03:09:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=198
06/15/2022 03:10:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=199
06/15/2022 03:10:07 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.5789473684210527 on epoch=199
06/15/2022 03:10:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5635334234060349 -> 0.5789473684210527 on epoch=199, global_step=1600
06/15/2022 03:10:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.06 on epoch=201
06/15/2022 03:10:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=202
06/15/2022 03:10:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=203
06/15/2022 03:10:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=204
06/15/2022 03:10:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=206
06/15/2022 03:10:35 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.3703703703703704 on epoch=206
06/15/2022 03:10:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=207
06/15/2022 03:10:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=208
06/15/2022 03:10:49 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=209
06/15/2022 03:10:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=211
06/15/2022 03:10:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=212
06/15/2022 03:11:03 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.38086419753086415 on epoch=212
06/15/2022 03:11:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=213
06/15/2022 03:11:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=214
06/15/2022 03:11:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=216
06/15/2022 03:11:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.05 on epoch=217
06/15/2022 03:11:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=218
06/15/2022 03:11:31 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.35223541298307653 on epoch=218
06/15/2022 03:11:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=219
06/15/2022 03:11:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=221
06/15/2022 03:11:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=222
06/15/2022 03:11:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=223
06/15/2022 03:11:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=224
06/15/2022 03:11:59 - INFO - __main__ - Global step 1800 Train loss 0.03 Classification-F1 0.5326443468036388 on epoch=224
06/15/2022 03:12:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=226
06/15/2022 03:12:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=227
06/15/2022 03:12:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=228
06/15/2022 03:12:17 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=229
06/15/2022 03:12:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=231
06/15/2022 03:12:27 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.4998778998778999 on epoch=231
06/15/2022 03:12:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=232
06/15/2022 03:12:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.05 on epoch=233
06/15/2022 03:12:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=234
06/15/2022 03:12:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=236
06/15/2022 03:12:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=237
06/15/2022 03:12:55 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.5018141038327066 on epoch=237
06/15/2022 03:13:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=238
06/15/2022 03:13:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=239
06/15/2022 03:13:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=241
06/15/2022 03:13:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=242
06/15/2022 03:13:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=243
06/15/2022 03:13:23 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.537733499377335 on epoch=243
06/15/2022 03:13:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=244
06/15/2022 03:13:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=246
06/15/2022 03:13:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=247
06/15/2022 03:13:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=248
06/15/2022 03:13:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=249
06/15/2022 03:13:51 - INFO - __main__ - Global step 2000 Train loss 0.02 Classification-F1 0.36481481481481487 on epoch=249
06/15/2022 03:13:56 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.03 on epoch=251
06/15/2022 03:14:00 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.07 on epoch=252
06/15/2022 03:14:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.03 on epoch=253
06/15/2022 03:14:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.02 on epoch=254
06/15/2022 03:14:13 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=256
06/15/2022 03:14:19 - INFO - __main__ - Global step 2050 Train loss 0.04 Classification-F1 0.5383580903478208 on epoch=256
06/15/2022 03:14:24 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.02 on epoch=257
06/15/2022 03:14:28 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.01 on epoch=258
06/15/2022 03:14:33 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.03 on epoch=259
06/15/2022 03:14:37 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.01 on epoch=261
06/15/2022 03:14:42 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.02 on epoch=262
06/15/2022 03:14:48 - INFO - __main__ - Global step 2100 Train loss 0.02 Classification-F1 0.546875 on epoch=262
06/15/2022 03:14:52 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.01 on epoch=263
06/15/2022 03:14:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=264
06/15/2022 03:15:01 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.04 on epoch=266
06/15/2022 03:15:06 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=267
06/15/2022 03:15:10 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=268
06/15/2022 03:15:16 - INFO - __main__ - Global step 2150 Train loss 0.03 Classification-F1 0.5175982114340467 on epoch=268
06/15/2022 03:15:20 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=269
06/15/2022 03:15:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.03 on epoch=271
06/15/2022 03:15:29 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.01 on epoch=272
06/15/2022 03:15:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.02 on epoch=273
06/15/2022 03:15:38 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.01 on epoch=274
06/15/2022 03:15:43 - INFO - __main__ - Global step 2200 Train loss 0.02 Classification-F1 0.24596774193548387 on epoch=274
06/15/2022 03:15:48 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.03 on epoch=276
06/15/2022 03:15:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.01 on epoch=277
06/15/2022 03:15:57 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=278
06/15/2022 03:16:01 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.02 on epoch=279
06/15/2022 03:16:06 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=281
06/15/2022 03:16:11 - INFO - __main__ - Global step 2250 Train loss 0.03 Classification-F1 0.3485351728836655 on epoch=281
06/15/2022 03:16:16 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.02 on epoch=282
06/15/2022 03:16:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.01 on epoch=283
06/15/2022 03:16:24 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.02 on epoch=284
06/15/2022 03:16:29 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.01 on epoch=286
06/15/2022 03:16:33 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.03 on epoch=287
06/15/2022 03:16:39 - INFO - __main__ - Global step 2300 Train loss 0.02 Classification-F1 0.523175572519084 on epoch=287
06/15/2022 03:16:43 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.03 on epoch=288
06/15/2022 03:16:48 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.01 on epoch=289
06/15/2022 03:16:52 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.01 on epoch=291
06/15/2022 03:16:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.02 on epoch=292
06/15/2022 03:17:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.01 on epoch=293
06/15/2022 03:17:07 - INFO - __main__ - Global step 2350 Train loss 0.02 Classification-F1 0.5515515515515517 on epoch=293
06/15/2022 03:17:11 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=294
06/15/2022 03:17:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=296
06/15/2022 03:17:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=297
06/15/2022 03:17:25 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.01 on epoch=298
06/15/2022 03:17:29 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.00 on epoch=299
06/15/2022 03:17:35 - INFO - __main__ - Global step 2400 Train loss 0.02 Classification-F1 0.36481481481481487 on epoch=299
06/15/2022 03:17:39 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.02 on epoch=301
06/15/2022 03:17:44 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=302
06/15/2022 03:17:48 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.02 on epoch=303
06/15/2022 03:17:53 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.01 on epoch=304
06/15/2022 03:17:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=306
06/15/2022 03:18:03 - INFO - __main__ - Global step 2450 Train loss 0.02 Classification-F1 0.5413886829750433 on epoch=306
06/15/2022 03:18:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=307
06/15/2022 03:18:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.01 on epoch=308
06/15/2022 03:18:16 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.02 on epoch=309
06/15/2022 03:18:20 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.02 on epoch=311
06/15/2022 03:18:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=312
06/15/2022 03:18:30 - INFO - __main__ - Global step 2500 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=312
06/15/2022 03:18:35 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.02 on epoch=313
06/15/2022 03:18:39 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=314
06/15/2022 03:18:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=316
06/15/2022 03:18:48 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=317
06/15/2022 03:18:52 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.01 on epoch=318
06/15/2022 03:18:58 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5292881534016286 on epoch=318
06/15/2022 03:19:02 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=319
06/15/2022 03:19:07 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=321
06/15/2022 03:19:11 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=322
06/15/2022 03:19:16 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=323
06/15/2022 03:19:20 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=324
06/15/2022 03:19:26 - INFO - __main__ - Global step 2600 Train loss 0.02 Classification-F1 0.5702862723554905 on epoch=324
06/15/2022 03:19:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.02 on epoch=326
06/15/2022 03:19:35 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.02 on epoch=327
06/15/2022 03:19:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=328
06/15/2022 03:19:44 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.00 on epoch=329
06/15/2022 03:19:48 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=331
06/15/2022 03:19:54 - INFO - __main__ - Global step 2650 Train loss 0.02 Classification-F1 0.3623242503421675 on epoch=331
06/15/2022 03:19:59 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.01 on epoch=332
06/15/2022 03:20:03 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.02 on epoch=333
06/15/2022 03:20:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=334
06/15/2022 03:20:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.00 on epoch=336
06/15/2022 03:20:16 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=337
06/15/2022 03:20:22 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.34702003235037954 on epoch=337
06/15/2022 03:20:27 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.07 on epoch=338
06/15/2022 03:20:31 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.02 on epoch=339
06/15/2022 03:20:35 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=341
06/15/2022 03:20:40 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=342
06/15/2022 03:20:44 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.01 on epoch=343
06/15/2022 03:20:50 - INFO - __main__ - Global step 2750 Train loss 0.02 Classification-F1 0.3707809805370781 on epoch=343
06/15/2022 03:20:55 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 03:20:59 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=346
06/15/2022 03:21:03 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=347
06/15/2022 03:21:08 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=348
06/15/2022 03:21:12 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.01 on epoch=349
06/15/2022 03:21:18 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.386858619416759 on epoch=349
06/15/2022 03:21:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=351
06/15/2022 03:21:27 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=352
06/15/2022 03:21:32 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.00 on epoch=353
06/15/2022 03:21:36 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=354
06/15/2022 03:21:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=356
06/15/2022 03:21:46 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.577195987276731 on epoch=356
06/15/2022 03:21:51 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.01 on epoch=357
06/15/2022 03:21:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.00 on epoch=358
06/15/2022 03:21:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=359
06/15/2022 03:22:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.01 on epoch=361
06/15/2022 03:22:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=362
06/15/2022 03:22:14 - INFO - __main__ - Global step 2900 Train loss 0.01 Classification-F1 0.5607843137254902 on epoch=362
06/15/2022 03:22:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.01 on epoch=363
06/15/2022 03:22:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.00 on epoch=364
06/15/2022 03:22:27 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.00 on epoch=366
06/15/2022 03:22:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=367
06/15/2022 03:22:36 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=368
06/15/2022 03:22:42 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.27129905613512173 on epoch=368
06/15/2022 03:22:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=369
06/15/2022 03:22:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=371
06/15/2022 03:22:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=372
06/15/2022 03:23:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=373
06/15/2022 03:23:04 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=374
06/15/2022 03:23:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 03:23:06 - INFO - __main__ - Printing 3 examples
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 03:23:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 03:23:06 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 03:23:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 03:23:06 - INFO - __main__ - Printing 3 examples
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 03:23:06 - INFO - __main__ - ['refuted']
06/15/2022 03:23:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 03:23:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 03:23:07 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 03:23:11 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3674540682414698 on epoch=374
06/15/2022 03:23:11 - INFO - __main__ - save last model!
06/15/2022 03:23:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 03:23:11 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 03:23:11 - INFO - __main__ - Printing 3 examples
06/15/2022 03:23:11 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 03:23:11 - INFO - __main__ - ['entailed']
06/15/2022 03:23:11 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 03:23:11 - INFO - __main__ - ['entailed']
06/15/2022 03:23:11 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 03:23:11 - INFO - __main__ - ['entailed']
06/15/2022 03:23:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 03:23:22 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 03:23:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 03:23:23 - INFO - __main__ - Starting training!
06/15/2022 03:23:35 - INFO - __main__ - Tokenizing Output ...
06/15/2022 03:23:48 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 03:32:55 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_13_0.5_8_predictions.txt
06/15/2022 03:32:55 - INFO - __main__ - Classification-F1 on test data: 0.1994
06/15/2022 03:32:55 - INFO - __main__ - prefix=tab_fact_64_13, lr=0.5, bsz=8, dev_performance=0.5789473684210527, test_performance=0.19943483418503724
06/15/2022 03:32:55 - INFO - __main__ - Running ... prefix=tab_fact_64_13, lr=0.4, bsz=8 ...
06/15/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 03:32:56 - INFO - __main__ - Printing 3 examples
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
06/15/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 03:32:56 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 03:32:56 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 03:32:56 - INFO - __main__ - Printing 3 examples
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 03:32:56 - INFO - __main__ - ['refuted']
06/15/2022 03:32:56 - INFO - __main__ - Tokenizing Input ...
06/15/2022 03:32:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 03:32:57 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 03:33:12 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 03:33:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 03:33:13 - INFO - __main__ - Starting training!
06/15/2022 03:33:18 - INFO - __main__ - Step 10 Global step 10 Train loss 3.01 on epoch=1
06/15/2022 03:33:22 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=2
06/15/2022 03:33:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=3
06/15/2022 03:33:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=4
06/15/2022 03:33:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=6
06/15/2022 03:33:40 - INFO - __main__ - Global step 50 Train loss 0.93 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 03:33:40 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 03:33:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=7
06/15/2022 03:33:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=8
06/15/2022 03:33:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=9
06/15/2022 03:33:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=11
06/15/2022 03:34:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=12
06/15/2022 03:34:08 - INFO - __main__ - Global step 100 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 03:34:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=13
06/15/2022 03:34:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=14
06/15/2022 03:34:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=16
06/15/2022 03:34:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=17
06/15/2022 03:34:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=18
06/15/2022 03:34:35 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 03:34:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=19
06/15/2022 03:34:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=21
06/15/2022 03:34:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=22
06/15/2022 03:34:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=23
06/15/2022 03:34:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=24
06/15/2022 03:35:03 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 03:35:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=26
06/15/2022 03:35:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=27
06/15/2022 03:35:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=28
06/15/2022 03:35:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=29
06/15/2022 03:35:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=31
06/15/2022 03:35:30 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 03:35:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=32
06/15/2022 03:35:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=33
06/15/2022 03:35:43 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=34
06/15/2022 03:35:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 03:35:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=37
06/15/2022 03:35:58 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 03:36:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=38
06/15/2022 03:36:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=39
06/15/2022 03:36:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=41
06/15/2022 03:36:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=42
06/15/2022 03:36:20 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=43
06/15/2022 03:36:26 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 03:36:30 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 03:36:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=46
06/15/2022 03:36:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=47
06/15/2022 03:36:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=48
06/15/2022 03:36:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=49
06/15/2022 03:36:53 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 03:36:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=51
06/15/2022 03:37:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.19 on epoch=52
06/15/2022 03:37:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=53
06/15/2022 03:37:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=54
06/15/2022 03:37:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 03:37:21 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.5269402839914414 on epoch=56
06/15/2022 03:37:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5269402839914414 on epoch=56, global_step=450
06/15/2022 03:37:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=57
06/15/2022 03:37:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=58
06/15/2022 03:37:34 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 03:37:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=61
06/15/2022 03:37:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=62
06/15/2022 03:37:49 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.5675675675675675 on epoch=62
06/15/2022 03:37:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5269402839914414 -> 0.5675675675675675 on epoch=62, global_step=500
06/15/2022 03:37:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=63
06/15/2022 03:37:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=64
06/15/2022 03:38:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 03:38:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=67
06/15/2022 03:38:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=68
06/15/2022 03:38:17 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 03:38:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=69
06/15/2022 03:38:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=71
06/15/2022 03:38:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 03:38:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=73
06/15/2022 03:38:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=74
06/15/2022 03:38:45 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.42840679919331603 on epoch=74
06/15/2022 03:38:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=76
06/15/2022 03:38:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 03:38:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=78
06/15/2022 03:39:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=79
06/15/2022 03:39:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=81
06/15/2022 03:39:14 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 03:39:19 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=82
06/15/2022 03:39:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=83
06/15/2022 03:39:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 03:39:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 03:39:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=87
06/15/2022 03:39:42 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 03:39:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=88
06/15/2022 03:39:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 03:39:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=91
06/15/2022 03:40:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=92
06/15/2022 03:40:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=93
06/15/2022 03:40:15 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.5241025641025642 on epoch=93
06/15/2022 03:40:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=94
06/15/2022 03:40:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 03:40:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=97
06/15/2022 03:40:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=98
06/15/2022 03:40:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=99
06/15/2022 03:40:42 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.5079365079365079 on epoch=99
06/15/2022 03:40:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=101
06/15/2022 03:40:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=102
06/15/2022 03:40:56 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=103
06/15/2022 03:41:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=104
06/15/2022 03:41:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=106
06/15/2022 03:41:13 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.46843853820598 on epoch=106
06/15/2022 03:41:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=107
06/15/2022 03:41:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=108
06/15/2022 03:41:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=109
06/15/2022 03:41:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=111
06/15/2022 03:41:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 03:41:41 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.34673046251993617 on epoch=112
06/15/2022 03:41:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=113
06/15/2022 03:41:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=114
06/15/2022 03:41:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=116
06/15/2022 03:41:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 03:42:03 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 03:42:12 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3314900153609831 on epoch=118
06/15/2022 03:42:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 03:42:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=121
06/15/2022 03:42:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=122
06/15/2022 03:42:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 03:42:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=124
06/15/2022 03:42:41 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.43636363636363634 on epoch=124
06/15/2022 03:42:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=126
06/15/2022 03:42:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 03:42:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 03:42:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 03:43:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=131
06/15/2022 03:43:09 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.500959217773377 on epoch=131
06/15/2022 03:43:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=132
06/15/2022 03:43:18 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=133
06/15/2022 03:43:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=134
06/15/2022 03:43:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=136
06/15/2022 03:43:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=137
06/15/2022 03:43:37 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.4059668508287293 on epoch=137
06/15/2022 03:43:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=138
06/15/2022 03:43:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=139
06/15/2022 03:43:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=141
06/15/2022 03:43:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 03:43:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=143
06/15/2022 03:44:05 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.537733499377335 on epoch=143
06/15/2022 03:44:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=144
06/15/2022 03:44:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=146
06/15/2022 03:44:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=147
06/15/2022 03:44:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=148
06/15/2022 03:44:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.18 on epoch=149
06/15/2022 03:44:32 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.5515515515515517 on epoch=149
06/15/2022 03:44:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=151
06/15/2022 03:44:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=152
06/15/2022 03:44:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=153
06/15/2022 03:44:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=154
06/15/2022 03:44:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=156
06/15/2022 03:45:00 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.3871086556169429 on epoch=156
06/15/2022 03:45:05 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=157
06/15/2022 03:45:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=158
06/15/2022 03:45:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=159
06/15/2022 03:45:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=161
06/15/2022 03:45:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=162
06/15/2022 03:45:28 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.439671682626539 on epoch=162
06/15/2022 03:45:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=163
06/15/2022 03:45:37 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=164
06/15/2022 03:45:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.12 on epoch=166
06/15/2022 03:45:46 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.16 on epoch=167
06/15/2022 03:45:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=168
06/15/2022 03:45:56 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.43636363636363634 on epoch=168
06/15/2022 03:46:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=169
06/15/2022 03:46:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=171
06/15/2022 03:46:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.12 on epoch=172
06/15/2022 03:46:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=173
06/15/2022 03:46:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=174
06/15/2022 03:46:24 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.5533516988062442 on epoch=174
06/15/2022 03:46:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=176
06/15/2022 03:46:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=177
06/15/2022 03:46:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.12 on epoch=178
06/15/2022 03:46:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=179
06/15/2022 03:46:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=181
06/15/2022 03:46:52 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.5169811320754718 on epoch=181
06/15/2022 03:46:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=182
06/15/2022 03:47:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=183
06/15/2022 03:47:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=184
06/15/2022 03:47:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=186
06/15/2022 03:47:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=187
06/15/2022 03:47:20 - INFO - __main__ - Global step 1500 Train loss 0.13 Classification-F1 0.5075419847328244 on epoch=187
06/15/2022 03:47:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=188
06/15/2022 03:47:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=189
06/15/2022 03:47:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=191
06/15/2022 03:47:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=192
06/15/2022 03:47:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=193
06/15/2022 03:47:48 - INFO - __main__ - Global step 1550 Train loss 0.13 Classification-F1 0.5912552198477032 on epoch=193
06/15/2022 03:47:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5675675675675675 -> 0.5912552198477032 on epoch=193, global_step=1550
06/15/2022 03:47:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=194
06/15/2022 03:47:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=196
06/15/2022 03:48:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=197
06/15/2022 03:48:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=198
06/15/2022 03:48:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=199
06/15/2022 03:48:16 - INFO - __main__ - Global step 1600 Train loss 0.11 Classification-F1 0.5743842364532019 on epoch=199
06/15/2022 03:48:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=201
06/15/2022 03:48:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=202
06/15/2022 03:48:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=203
06/15/2022 03:48:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=204
06/15/2022 03:48:38 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=206
06/15/2022 03:48:43 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.5921568627450979 on epoch=206
06/15/2022 03:48:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5912552198477032 -> 0.5921568627450979 on epoch=206, global_step=1650
06/15/2022 03:48:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=207
06/15/2022 03:48:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=208
06/15/2022 03:48:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=209
06/15/2022 03:49:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=211
06/15/2022 03:49:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=212
06/15/2022 03:49:11 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.5666666666666667 on epoch=212
06/15/2022 03:49:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=213
06/15/2022 03:49:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=214
06/15/2022 03:49:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=216
06/15/2022 03:49:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=217
06/15/2022 03:49:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.06 on epoch=218
06/15/2022 03:49:39 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.523683380637023 on epoch=218
06/15/2022 03:49:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=219
06/15/2022 03:49:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=221
06/15/2022 03:49:52 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=222
06/15/2022 03:49:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=223
06/15/2022 03:50:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=224
06/15/2022 03:50:07 - INFO - __main__ - Global step 1800 Train loss 0.09 Classification-F1 0.4297594297594297 on epoch=224
06/15/2022 03:50:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=226
06/15/2022 03:50:16 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=227
06/15/2022 03:50:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=228
06/15/2022 03:50:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=229
06/15/2022 03:50:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=231
06/15/2022 03:50:35 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.5766086251014167 on epoch=231
06/15/2022 03:50:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=232
06/15/2022 03:50:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=233
06/15/2022 03:50:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=234
06/15/2022 03:50:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=236
06/15/2022 03:50:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=237
06/15/2022 03:51:02 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.5635334234060349 on epoch=237
06/15/2022 03:51:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=238
06/15/2022 03:51:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=239
06/15/2022 03:51:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=241
06/15/2022 03:51:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=242
06/15/2022 03:51:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=243
06/15/2022 03:51:30 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.5572028663207315 on epoch=243
06/15/2022 03:51:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=244
06/15/2022 03:51:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=246
06/15/2022 03:51:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=247
06/15/2022 03:51:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=248
06/15/2022 03:51:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=249
06/15/2022 03:51:58 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.562753036437247 on epoch=249
06/15/2022 03:52:03 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=251
06/15/2022 03:52:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.04 on epoch=252
06/15/2022 03:52:12 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=253
06/15/2022 03:52:16 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.06 on epoch=254
06/15/2022 03:52:21 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.05 on epoch=256
06/15/2022 03:52:26 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.5151515151515151 on epoch=256
06/15/2022 03:52:31 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=257
06/15/2022 03:52:35 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.06 on epoch=258
06/15/2022 03:52:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.08 on epoch=259
06/15/2022 03:52:44 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=261
06/15/2022 03:52:49 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.04 on epoch=262
06/15/2022 03:52:54 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.4119425547996977 on epoch=262
06/15/2022 03:52:59 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=263
06/15/2022 03:53:03 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.06 on epoch=264
06/15/2022 03:53:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.03 on epoch=266
06/15/2022 03:53:12 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.03 on epoch=267
06/15/2022 03:53:17 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.03 on epoch=268
06/15/2022 03:53:22 - INFO - __main__ - Global step 2150 Train loss 0.04 Classification-F1 0.40591058348067693 on epoch=268
06/15/2022 03:53:27 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.03 on epoch=269
06/15/2022 03:53:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.06 on epoch=271
06/15/2022 03:53:36 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=272
06/15/2022 03:53:40 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=273
06/15/2022 03:53:45 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.02 on epoch=274
06/15/2022 03:53:50 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.5813836477987422 on epoch=274
06/15/2022 03:53:55 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=276
06/15/2022 03:53:59 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.05 on epoch=277
06/15/2022 03:54:04 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.02 on epoch=278
06/15/2022 03:54:08 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=279
06/15/2022 03:54:13 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.03 on epoch=281
06/15/2022 03:54:18 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.41035196687370606 on epoch=281
06/15/2022 03:54:23 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.05 on epoch=282
06/15/2022 03:54:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=283
06/15/2022 03:54:32 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.03 on epoch=284
06/15/2022 03:54:36 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=286
06/15/2022 03:54:41 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=287
06/15/2022 03:54:47 - INFO - __main__ - Global step 2300 Train loss 0.04 Classification-F1 0.3764102564102563 on epoch=287
06/15/2022 03:54:51 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=288
06/15/2022 03:54:56 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=289
06/15/2022 03:55:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=291
06/15/2022 03:55:05 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.05 on epoch=292
06/15/2022 03:55:09 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.02 on epoch=293
06/15/2022 03:55:14 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.5303643724696356 on epoch=293
06/15/2022 03:55:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.03 on epoch=294
06/15/2022 03:55:23 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.02 on epoch=296
06/15/2022 03:55:28 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=297
06/15/2022 03:55:32 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.03 on epoch=298
06/15/2022 03:55:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.03 on epoch=299
06/15/2022 03:55:42 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.6078431372549019 on epoch=299
06/15/2022 03:55:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5921568627450979 -> 0.6078431372549019 on epoch=299, global_step=2400
06/15/2022 03:55:47 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=301
06/15/2022 03:55:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.02 on epoch=302
06/15/2022 03:55:56 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.06 on epoch=303
06/15/2022 03:56:00 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=304
06/15/2022 03:56:05 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.03 on epoch=306
06/15/2022 03:56:10 - INFO - __main__ - Global step 2450 Train loss 0.04 Classification-F1 0.5255744996293551 on epoch=306
06/15/2022 03:56:15 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=307
06/15/2022 03:56:19 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.06 on epoch=308
06/15/2022 03:56:24 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=309
06/15/2022 03:56:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=311
06/15/2022 03:56:33 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.07 on epoch=312
06/15/2022 03:56:38 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.531135531135531 on epoch=312
06/15/2022 03:56:42 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.01 on epoch=313
06/15/2022 03:56:47 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=314
06/15/2022 03:56:51 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.02 on epoch=316
06/15/2022 03:56:56 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=317
06/15/2022 03:57:00 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=318
06/15/2022 03:57:06 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5103416974648253 on epoch=318
06/15/2022 03:57:10 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=319
06/15/2022 03:57:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=321
06/15/2022 03:57:19 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.04 on epoch=322
06/15/2022 03:57:24 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=323
06/15/2022 03:57:28 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=324
06/15/2022 03:57:34 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.5413886829750433 on epoch=324
06/15/2022 03:57:38 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=326
06/15/2022 03:57:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=327
06/15/2022 03:57:47 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.02 on epoch=328
06/15/2022 03:57:52 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=329
06/15/2022 03:57:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.01 on epoch=331
06/15/2022 03:58:02 - INFO - __main__ - Global step 2650 Train loss 0.03 Classification-F1 0.5859122260880181 on epoch=331
06/15/2022 03:58:06 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=332
06/15/2022 03:58:11 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.01 on epoch=333
06/15/2022 03:58:15 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=334
06/15/2022 03:58:20 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=336
06/15/2022 03:58:24 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=337
06/15/2022 03:58:30 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.5440923605993613 on epoch=337
06/15/2022 03:58:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.01 on epoch=338
06/15/2022 03:58:39 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=339
06/15/2022 03:58:43 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=341
06/15/2022 03:58:48 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=342
06/15/2022 03:58:52 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=343
06/15/2022 03:58:58 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.554442748091603 on epoch=343
06/15/2022 03:59:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 03:59:07 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=346
06/15/2022 03:59:11 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=347
06/15/2022 03:59:16 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=348
06/15/2022 03:59:20 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=349
06/15/2022 03:59:26 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.3816847112860892 on epoch=349
06/15/2022 03:59:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=351
06/15/2022 03:59:35 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=352
06/15/2022 03:59:39 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.03 on epoch=353
06/15/2022 03:59:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=354
06/15/2022 03:59:48 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=356
06/15/2022 03:59:54 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.37647637795275585 on epoch=356
06/15/2022 03:59:59 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=357
06/15/2022 04:00:03 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=358
06/15/2022 04:00:07 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=359
06/15/2022 04:00:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=361
06/15/2022 04:00:17 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=362
06/15/2022 04:00:22 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.5041505257332595 on epoch=362
06/15/2022 04:00:27 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=363
06/15/2022 04:00:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=364
06/15/2022 04:00:36 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=366
06/15/2022 04:00:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=367
06/15/2022 04:00:45 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.00 on epoch=368
06/15/2022 04:00:50 - INFO - __main__ - Global step 2950 Train loss 0.01 Classification-F1 0.537733499377335 on epoch=368
06/15/2022 04:00:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 04:00:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.00 on epoch=371
06/15/2022 04:01:04 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=372
06/15/2022 04:01:08 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=373
06/15/2022 04:01:13 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=374
06/15/2022 04:01:14 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:01:14 - INFO - __main__ - Printing 3 examples
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:01:14 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:01:14 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 04:01:14 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:01:14 - INFO - __main__ - Printing 3 examples
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 04:01:14 - INFO - __main__ - ['refuted']
06/15/2022 04:01:14 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:01:15 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:01:15 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 04:01:18 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5466915191053122 on epoch=374
06/15/2022 04:01:18 - INFO - __main__ - save last model!
06/15/2022 04:01:18 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 04:01:18 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 04:01:18 - INFO - __main__ - Printing 3 examples
06/15/2022 04:01:18 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:01:18 - INFO - __main__ - ['entailed']
06/15/2022 04:01:18 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:01:18 - INFO - __main__ - ['entailed']
06/15/2022 04:01:18 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:01:18 - INFO - __main__ - ['entailed']
06/15/2022 04:01:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:01:33 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 04:01:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 04:01:34 - INFO - __main__ - Starting training!
06/15/2022 04:01:43 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:01:56 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 04:10:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_13_0.4_8_predictions.txt
06/15/2022 04:10:41 - INFO - __main__ - Classification-F1 on test data: 0.0607
06/15/2022 04:10:41 - INFO - __main__ - prefix=tab_fact_64_13, lr=0.4, bsz=8, dev_performance=0.6078431372549019, test_performance=0.060714425082576726
06/15/2022 04:10:41 - INFO - __main__ - Running ... prefix=tab_fact_64_13, lr=0.3, bsz=8 ...
06/15/2022 04:10:42 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:10:42 - INFO - __main__ - Printing 3 examples
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:10:42 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:10:42 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 04:10:42 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:10:42 - INFO - __main__ - Printing 3 examples
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 04:10:42 - INFO - __main__ - ['refuted']
06/15/2022 04:10:42 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:10:43 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:10:43 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 04:11:02 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 04:11:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 04:11:02 - INFO - __main__ - Starting training!
06/15/2022 04:11:07 - INFO - __main__ - Step 10 Global step 10 Train loss 3.75 on epoch=1
06/15/2022 04:11:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.95 on epoch=2
06/15/2022 04:11:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=3
06/15/2022 04:11:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=4
06/15/2022 04:11:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.35 on epoch=6
06/15/2022 04:11:31 - INFO - __main__ - Global step 50 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 04:11:31 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 04:11:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=7
06/15/2022 04:11:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=8
06/15/2022 04:11:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=9
06/15/2022 04:11:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.23 on epoch=11
06/15/2022 04:11:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=12
06/15/2022 04:11:58 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 04:12:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 04:12:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=14
06/15/2022 04:12:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=16
06/15/2022 04:12:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=17
06/15/2022 04:12:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=18
06/15/2022 04:12:26 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 04:12:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=19
06/15/2022 04:12:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=21
06/15/2022 04:12:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=22
06/15/2022 04:12:45 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 04:12:49 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=24
06/15/2022 04:12:54 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 04:12:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=26
06/15/2022 04:13:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=27
06/15/2022 04:13:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=28
06/15/2022 04:13:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=29
06/15/2022 04:13:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=31
06/15/2022 04:13:22 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.199546485260771 on epoch=31
06/15/2022 04:13:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=32
06/15/2022 04:13:31 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=33
06/15/2022 04:13:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=34
06/15/2022 04:13:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 04:13:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=37
06/15/2022 04:13:49 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 04:13:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=38
06/15/2022 04:13:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=39
06/15/2022 04:14:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=41
06/15/2022 04:14:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=42
06/15/2022 04:14:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 04:14:17 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.11061946902654868 on epoch=43
06/15/2022 04:14:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=44
06/15/2022 04:14:26 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=46
06/15/2022 04:14:30 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=47
06/15/2022 04:14:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=48
06/15/2022 04:14:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=49
06/15/2022 04:14:44 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.34673046251993617 on epoch=49
06/15/2022 04:14:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34673046251993617 on epoch=49, global_step=400
06/15/2022 04:14:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=51
06/15/2022 04:14:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=52
06/15/2022 04:14:58 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=53
06/15/2022 04:15:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=54
06/15/2022 04:15:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=56
06/15/2022 04:15:12 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.2198952879581152 on epoch=56
06/15/2022 04:15:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=57
06/15/2022 04:15:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=58
06/15/2022 04:15:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=59
06/15/2022 04:15:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=61
06/15/2022 04:15:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=62
06/15/2022 04:15:40 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.21631205673758866 on epoch=62
06/15/2022 04:15:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=63
06/15/2022 04:15:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=64
06/15/2022 04:15:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 04:15:57 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 04:16:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 04:16:07 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.16310160427807485 on epoch=68
06/15/2022 04:16:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=69
06/15/2022 04:16:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=71
06/15/2022 04:16:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=72
06/15/2022 04:16:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=73
06/15/2022 04:16:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 04:16:34 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.1601123595505618 on epoch=74
06/15/2022 04:16:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=76
06/15/2022 04:16:43 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 04:16:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=78
06/15/2022 04:16:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=79
06/15/2022 04:16:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=81
06/15/2022 04:17:02 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.23129027834910187 on epoch=81
06/15/2022 04:17:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=82
06/15/2022 04:17:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=83
06/15/2022 04:17:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=84
06/15/2022 04:17:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=86
06/15/2022 04:17:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=87
06/15/2022 04:17:29 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.350463149416029 on epoch=87
06/15/2022 04:17:29 - INFO - __main__ - Saving model with best Classification-F1: 0.34673046251993617 -> 0.350463149416029 on epoch=87, global_step=700
06/15/2022 04:17:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=88
06/15/2022 04:17:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.19 on epoch=89
06/15/2022 04:17:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=91
06/15/2022 04:17:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=92
06/15/2022 04:17:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=93
06/15/2022 04:17:57 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.13372352285395764 on epoch=93
06/15/2022 04:18:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=94
06/15/2022 04:18:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=96
06/15/2022 04:18:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 04:18:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=98
06/15/2022 04:18:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=99
06/15/2022 04:18:25 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.17246937459703415 on epoch=99
06/15/2022 04:18:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=101
06/15/2022 04:18:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 04:18:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=103
06/15/2022 04:18:43 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=104
06/15/2022 04:18:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=106
06/15/2022 04:18:52 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.20221718724712737 on epoch=106
06/15/2022 04:18:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 04:19:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=108
06/15/2022 04:19:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=109
06/15/2022 04:19:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=111
06/15/2022 04:19:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=112
06/15/2022 04:19:20 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.350463149416029 on epoch=112
06/15/2022 04:19:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=113
06/15/2022 04:19:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=114
06/15/2022 04:19:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 04:19:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=117
06/15/2022 04:19:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=118
06/15/2022 04:19:47 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.350463149416029 on epoch=118
06/15/2022 04:19:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=119
06/15/2022 04:19:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=121
06/15/2022 04:20:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=122
06/15/2022 04:20:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 04:20:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 04:20:15 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.2336606655755592 on epoch=124
06/15/2022 04:20:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=126
06/15/2022 04:20:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=127
06/15/2022 04:20:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 04:20:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=129
06/15/2022 04:20:38 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=131
06/15/2022 04:20:43 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.350463149416029 on epoch=131
06/15/2022 04:20:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=132
06/15/2022 04:20:52 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=133
06/15/2022 04:20:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=134
06/15/2022 04:21:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=136
06/15/2022 04:21:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=137
06/15/2022 04:21:10 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.350463149416029 on epoch=137
06/15/2022 04:21:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=138
06/15/2022 04:21:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=139
06/15/2022 04:21:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=141
06/15/2022 04:21:28 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 04:21:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=143
06/15/2022 04:21:38 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.35518871580252653 on epoch=143
06/15/2022 04:21:38 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.35518871580252653 on epoch=143, global_step=1150
06/15/2022 04:21:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=144
06/15/2022 04:21:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=146
06/15/2022 04:21:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=147
06/15/2022 04:21:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=148
06/15/2022 04:22:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=149
06/15/2022 04:22:06 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.39636200314394787 on epoch=149
06/15/2022 04:22:06 - INFO - __main__ - Saving model with best Classification-F1: 0.35518871580252653 -> 0.39636200314394787 on epoch=149, global_step=1200
06/15/2022 04:22:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=151
06/15/2022 04:22:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=152
06/15/2022 04:22:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=153
06/15/2022 04:22:24 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=154
06/15/2022 04:22:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=156
06/15/2022 04:22:34 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.42758857929136573 on epoch=156
06/15/2022 04:22:34 - INFO - __main__ - Saving model with best Classification-F1: 0.39636200314394787 -> 0.42758857929136573 on epoch=156, global_step=1250
06/15/2022 04:22:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=157
06/15/2022 04:22:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=158
06/15/2022 04:22:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=159
06/15/2022 04:22:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=161
06/15/2022 04:22:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=162
06/15/2022 04:23:02 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.35518871580252653 on epoch=162
06/15/2022 04:23:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=163
06/15/2022 04:23:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=164
06/15/2022 04:23:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=166
06/15/2022 04:23:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=167
06/15/2022 04:23:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=168
06/15/2022 04:23:30 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.3915298184961106 on epoch=168
06/15/2022 04:23:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=169
06/15/2022 04:23:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=171
06/15/2022 04:23:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=172
06/15/2022 04:23:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=173
06/15/2022 04:23:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=174
06/15/2022 04:23:58 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.5307859583721652 on epoch=174
06/15/2022 04:23:58 - INFO - __main__ - Saving model with best Classification-F1: 0.42758857929136573 -> 0.5307859583721652 on epoch=174, global_step=1400
06/15/2022 04:24:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=176
06/15/2022 04:24:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=177
06/15/2022 04:24:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=178
06/15/2022 04:24:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=179
06/15/2022 04:24:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=181
06/15/2022 04:24:25 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3671451355661882 on epoch=181
06/15/2022 04:24:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=182
06/15/2022 04:24:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=183
06/15/2022 04:24:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=184
06/15/2022 04:24:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=186
06/15/2022 04:24:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=187
06/15/2022 04:24:52 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.4258480515839641 on epoch=187
06/15/2022 04:24:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=188
06/15/2022 04:25:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=189
06/15/2022 04:25:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=191
06/15/2022 04:25:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=192
06/15/2022 04:25:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=193
06/15/2022 04:25:20 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.4341290893015031 on epoch=193
06/15/2022 04:25:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=194
06/15/2022 04:25:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=196
06/15/2022 04:25:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=197
06/15/2022 04:25:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.19 on epoch=198
06/15/2022 04:25:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=199
06/15/2022 04:25:48 - INFO - __main__ - Global step 1600 Train loss 0.18 Classification-F1 0.4584615384615385 on epoch=199
06/15/2022 04:25:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=201
06/15/2022 04:25:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=202
06/15/2022 04:26:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=203
06/15/2022 04:26:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=204
06/15/2022 04:26:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=206
06/15/2022 04:26:15 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.42758857929136573 on epoch=206
06/15/2022 04:26:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=207
06/15/2022 04:26:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=208
06/15/2022 04:26:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=209
06/15/2022 04:26:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=211
06/15/2022 04:26:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=212
06/15/2022 04:26:43 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.3591989987484355 on epoch=212
06/15/2022 04:26:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=213
06/15/2022 04:26:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=214
06/15/2022 04:26:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=216
06/15/2022 04:27:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.14 on epoch=217
06/15/2022 04:27:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=218
06/15/2022 04:27:10 - INFO - __main__ - Global step 1750 Train loss 0.16 Classification-F1 0.45744466123931915 on epoch=218
06/15/2022 04:27:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.17 on epoch=219
06/15/2022 04:27:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=221
06/15/2022 04:27:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=222
06/15/2022 04:27:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=223
06/15/2022 04:27:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=224
06/15/2022 04:27:38 - INFO - __main__ - Global step 1800 Train loss 0.19 Classification-F1 0.5148803976390183 on epoch=224
06/15/2022 04:27:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=226
06/15/2022 04:27:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=227
06/15/2022 04:27:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=228
06/15/2022 04:27:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=229
06/15/2022 04:28:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=231
06/15/2022 04:28:06 - INFO - __main__ - Global step 1850 Train loss 0.16 Classification-F1 0.5097603162836669 on epoch=231
06/15/2022 04:28:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=232
06/15/2022 04:28:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.15 on epoch=233
06/15/2022 04:28:20 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=234
06/15/2022 04:28:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=236
06/15/2022 04:28:28 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=237
06/15/2022 04:28:34 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.4740190880169671 on epoch=237
06/15/2022 04:28:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=238
06/15/2022 04:28:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=239
06/15/2022 04:28:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=241
06/15/2022 04:28:52 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=242
06/15/2022 04:28:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=243
06/15/2022 04:29:02 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.5217932752179327 on epoch=243
06/15/2022 04:29:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=244
06/15/2022 04:29:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=246
06/15/2022 04:29:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=247
06/15/2022 04:29:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=248
06/15/2022 04:29:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=249
06/15/2022 04:29:30 - INFO - __main__ - Global step 2000 Train loss 0.15 Classification-F1 0.5145583557621727 on epoch=249
06/15/2022 04:29:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.13 on epoch=251
06/15/2022 04:29:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.18 on epoch=252
06/15/2022 04:29:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.12 on epoch=253
06/15/2022 04:29:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.17 on epoch=254
06/15/2022 04:29:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=256
06/15/2022 04:29:58 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.5155067155067155 on epoch=256
06/15/2022 04:30:02 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=257
06/15/2022 04:30:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.16 on epoch=258
06/15/2022 04:30:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.13 on epoch=259
06/15/2022 04:30:15 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=261
06/15/2022 04:30:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.13 on epoch=262
06/15/2022 04:30:26 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.5110771581359816 on epoch=262
06/15/2022 04:30:30 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=263
06/15/2022 04:30:34 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.15 on epoch=264
06/15/2022 04:30:39 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=266
06/15/2022 04:30:43 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.14 on epoch=267
06/15/2022 04:30:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.11 on epoch=268
06/15/2022 04:30:53 - INFO - __main__ - Global step 2150 Train loss 0.12 Classification-F1 0.46476939399613054 on epoch=268
06/15/2022 04:30:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.13 on epoch=269
06/15/2022 04:31:02 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=271
06/15/2022 04:31:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.14 on epoch=272
06/15/2022 04:31:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.15 on epoch=273
06/15/2022 04:31:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.14 on epoch=274
06/15/2022 04:31:21 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.5227092120545265 on epoch=274
06/15/2022 04:31:25 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=276
06/15/2022 04:31:30 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.11 on epoch=277
06/15/2022 04:31:34 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.12 on epoch=278
06/15/2022 04:31:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=279
06/15/2022 04:31:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=281
06/15/2022 04:31:48 - INFO - __main__ - Global step 2250 Train loss 0.11 Classification-F1 0.4452324665090622 on epoch=281
06/15/2022 04:31:53 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.10 on epoch=282
06/15/2022 04:31:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.14 on epoch=283
06/15/2022 04:32:02 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.13 on epoch=284
06/15/2022 04:32:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.11 on epoch=286
06/15/2022 04:32:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=287
06/15/2022 04:32:16 - INFO - __main__ - Global step 2300 Train loss 0.11 Classification-F1 0.4439473513583609 on epoch=287
06/15/2022 04:32:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.06 on epoch=288
06/15/2022 04:32:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=289
06/15/2022 04:32:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=291
06/15/2022 04:32:33 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=292
06/15/2022 04:32:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=293
06/15/2022 04:32:43 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.34939226519337013 on epoch=293
06/15/2022 04:32:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=294
06/15/2022 04:32:52 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=296
06/15/2022 04:32:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=297
06/15/2022 04:33:01 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=298
06/15/2022 04:33:05 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=299
06/15/2022 04:33:11 - INFO - __main__ - Global step 2400 Train loss 0.09 Classification-F1 0.33985480497108406 on epoch=299
06/15/2022 04:33:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.07 on epoch=301
06/15/2022 04:33:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.09 on epoch=302
06/15/2022 04:33:24 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.11 on epoch=303
06/15/2022 04:33:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=304
06/15/2022 04:33:33 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=306
06/15/2022 04:33:39 - INFO - __main__ - Global step 2450 Train loss 0.08 Classification-F1 0.2593283582089552 on epoch=306
06/15/2022 04:33:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=307
06/15/2022 04:33:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=308
06/15/2022 04:33:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=309
06/15/2022 04:33:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.09 on epoch=311
06/15/2022 04:34:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.10 on epoch=312
06/15/2022 04:34:06 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.5148803976390183 on epoch=312
06/15/2022 04:34:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=313
06/15/2022 04:34:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.13 on epoch=314
06/15/2022 04:34:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=316
06/15/2022 04:34:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=317
06/15/2022 04:34:28 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=318
06/15/2022 04:34:34 - INFO - __main__ - Global step 2550 Train loss 0.09 Classification-F1 0.3152917755381361 on epoch=318
06/15/2022 04:34:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=319
06/15/2022 04:34:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=321
06/15/2022 04:34:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.13 on epoch=322
06/15/2022 04:34:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=323
06/15/2022 04:34:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=324
06/15/2022 04:35:01 - INFO - __main__ - Global step 2600 Train loss 0.08 Classification-F1 0.5155067155067155 on epoch=324
06/15/2022 04:35:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=326
06/15/2022 04:35:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=327
06/15/2022 04:35:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=328
06/15/2022 04:35:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=329
06/15/2022 04:35:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.10 on epoch=331
06/15/2022 04:35:29 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.2551730566084633 on epoch=331
06/15/2022 04:35:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=332
06/15/2022 04:35:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=333
06/15/2022 04:35:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=334
06/15/2022 04:35:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=336
06/15/2022 04:35:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=337
06/15/2022 04:35:57 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.3241048357327427 on epoch=337
06/15/2022 04:36:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.06 on epoch=338
06/15/2022 04:36:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.07 on epoch=339
06/15/2022 04:36:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=341
06/15/2022 04:36:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=342
06/15/2022 04:36:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=343
06/15/2022 04:36:25 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.33456379968007877 on epoch=343
06/15/2022 04:36:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 04:36:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=346
06/15/2022 04:36:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.06 on epoch=347
06/15/2022 04:36:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.05 on epoch=348
06/15/2022 04:36:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=349
06/15/2022 04:36:53 - INFO - __main__ - Global step 2800 Train loss 0.04 Classification-F1 0.29390775736751684 on epoch=349
06/15/2022 04:36:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=351
06/15/2022 04:37:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=352
06/15/2022 04:37:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=353
06/15/2022 04:37:10 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.06 on epoch=354
06/15/2022 04:37:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=356
06/15/2022 04:37:20 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.23616625310173697 on epoch=356
06/15/2022 04:37:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.07 on epoch=357
06/15/2022 04:37:29 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=358
06/15/2022 04:37:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=359
06/15/2022 04:37:38 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=361
06/15/2022 04:37:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=362
06/15/2022 04:37:48 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.35381844078589086 on epoch=362
06/15/2022 04:37:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=363
06/15/2022 04:37:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=364
06/15/2022 04:38:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=366
06/15/2022 04:38:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=367
06/15/2022 04:38:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=368
06/15/2022 04:38:16 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.24404761904761904 on epoch=368
06/15/2022 04:38:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.04 on epoch=369
06/15/2022 04:38:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=371
06/15/2022 04:38:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=372
06/15/2022 04:38:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=373
06/15/2022 04:38:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=374
06/15/2022 04:38:40 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:38:40 - INFO - __main__ - Printing 3 examples
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:38:40 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:38:40 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 04:38:40 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:38:40 - INFO - __main__ - Printing 3 examples
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 04:38:40 - INFO - __main__ - ['refuted']
06/15/2022 04:38:40 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:38:40 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:38:40 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 04:38:44 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.3241048357327427 on epoch=374
06/15/2022 04:38:44 - INFO - __main__ - save last model!
06/15/2022 04:38:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 04:38:44 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 04:38:44 - INFO - __main__ - Printing 3 examples
06/15/2022 04:38:44 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:38:44 - INFO - __main__ - ['entailed']
06/15/2022 04:38:44 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:38:44 - INFO - __main__ - ['entailed']
06/15/2022 04:38:44 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 04:38:44 - INFO - __main__ - ['entailed']
06/15/2022 04:38:44 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:38:56 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 04:38:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 04:38:57 - INFO - __main__ - Starting training!
06/15/2022 04:39:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:39:21 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 04:48:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_13_0.3_8_predictions.txt
06/15/2022 04:48:30 - INFO - __main__ - Classification-F1 on test data: 0.0291
06/15/2022 04:48:30 - INFO - __main__ - prefix=tab_fact_64_13, lr=0.3, bsz=8, dev_performance=0.5307859583721652, test_performance=0.02908564709057963
06/15/2022 04:48:30 - INFO - __main__ - Running ... prefix=tab_fact_64_13, lr=0.2, bsz=8 ...
06/15/2022 04:48:31 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:48:31 - INFO - __main__ - Printing 3 examples
06/15/2022 04:48:31 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 w∕kg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
06/15/2022 04:48:31 - INFO - __main__ - ['refuted']
06/15/2022 04:48:31 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
06/15/2022 04:48:31 - INFO - __main__ - ['refuted']
06/15/2022 04:48:31 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
06/15/2022 04:48:31 - INFO - __main__ - ['refuted']
06/15/2022 04:48:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:48:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:48:32 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 04:48:32 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 04:48:32 - INFO - __main__ - Printing 3 examples
06/15/2022 04:48:32 - INFO - __main__ -  [tab_fact] statement: north melbourne be the only away team on the 30th of may 1959 [SEP] table_caption: 1959 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#11.17 (83)#st kilda#16.10 (106)#punt road oval#23000#30 may 1959 [n] geelong#11.10 (76)#melbourne#19.15 (129)#kardinia park#16736#30 may 1959 [n] collingwood#16.16 (112)#footscray#10.11 (71)#victoria park#24740#30 may 1959 [n] south melbourne#8.18 (66)#north melbourne#13.18 (96)#lake oval#20700#30 may 1959 [n] fitzroy#11.9 (75)#essendon#12.22 (94)#brunswick street oval#22000#30 may 1959 [n] hawthorn#11.9 (75)#carlton#12.11 (83)#glenferrie oval#28000#30 may 1959 [n] 
06/15/2022 04:48:32 - INFO - __main__ - ['refuted']
06/15/2022 04:48:32 - INFO - __main__ -  [tab_fact] statement: veronika vadovicova be the name of the player who compete in the women 's individual class 3 in table tennis [SEP] table_caption: slovakia at the 2008 summer paralympics [SEP] table_text: medal#name#sport#event#date [n] gold#veronika vadovicova#shooting#women 's r2 - 10 m air rifle standing sh1#september 7 [n] gold#rastislav revucky jan riapos#table tennis#men 's team class 1 - 2#september 15 [n] silver#rastislav turecek#cycling#men 's time trial hc a#september 12 [n] silver#miroslav jambor richard csejtey#table tennis#men 's team class 6 - 8#september 16 [n] silver#alena kanova#table tennis#women 's individual class 3#september 10 [n] bronze#miroslav jambor#table tennis#men 's individual class 8#september 11 [n] 
06/15/2022 04:48:32 - INFO - __main__ - ['refuted']
06/15/2022 04:48:32 - INFO - __main__ -  [tab_fact] statement: all of the airport have an international civil aviation organization code except for china bay airport airport which be yet to be announce [SEP] table_caption: helitours [SEP] table_text: city#country#iata#icao#airport [n] ampara#sri lanka#goy#vccg#ampara airport [n] anuradhapura#sri lanka#acj#vcca#anuradhapura airport [n] batticaloa#sri lanka#btc#vccb#batticaloa airport [n] colombo#sri lanka#rml#vccc#ratmalana airport [n] hambantota#sri lanka#hri#vcri#mattala rajapaksa international airport [n] jaffna#sri lanka#jaf#vccj#jaffna airport [n] killinochchi#sri lanka#tba#tba#iranamadu airport [n] malã#maldives#mle#vrmm#ibrahim nasir international airport [n] trincomalee#sri lanka#trr#vcct#china bay airport [n] vavuniya#sri lanka#tba#vccv#vavuniya airport [n] hambantota#sri lanka#wrz#vccw#weerawila airport [n] koggala#sri lanka#kct#vcck#koggala airport [n] 
06/15/2022 04:48:32 - INFO - __main__ - ['refuted']
06/15/2022 04:48:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 04:48:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 04:48:32 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 04:48:48 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 04:48:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 04:48:49 - INFO - __main__ - Starting training!
06/15/2022 04:48:54 - INFO - __main__ - Step 10 Global step 10 Train loss 3.66 on epoch=1
06/15/2022 04:48:58 - INFO - __main__ - Step 20 Global step 20 Train loss 1.09 on epoch=2
06/15/2022 04:49:02 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=3
06/15/2022 04:49:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=4
06/15/2022 04:49:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=6
06/15/2022 04:49:17 - INFO - __main__ - Global step 50 Train loss 1.24 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 04:49:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 04:49:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=7
06/15/2022 04:49:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=8
06/15/2022 04:49:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=9
06/15/2022 04:49:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=11
06/15/2022 04:49:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=12
06/15/2022 04:49:44 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 04:49:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 04:49:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=14
06/15/2022 04:49:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=16
06/15/2022 04:50:01 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=17
06/15/2022 04:50:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=18
06/15/2022 04:50:11 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 04:50:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=19
06/15/2022 04:50:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=21
06/15/2022 04:50:24 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=22
06/15/2022 04:50:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 04:50:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=24
06/15/2022 04:50:39 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 04:50:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=26
06/15/2022 04:50:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=27
06/15/2022 04:50:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=28
06/15/2022 04:50:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=29
06/15/2022 04:51:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=31
06/15/2022 04:51:07 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 04:51:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=32
06/15/2022 04:51:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=33
06/15/2022 04:51:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.20 on epoch=34
06/15/2022 04:51:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=36
06/15/2022 04:51:29 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=37
06/15/2022 04:51:35 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 04:51:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=38
06/15/2022 04:51:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=39
06/15/2022 04:51:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=41
06/15/2022 04:51:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=42
06/15/2022 04:51:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=43
06/15/2022 04:52:03 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.22904233171408742 on epoch=43
06/15/2022 04:52:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 04:52:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=46
06/15/2022 04:52:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=47
06/15/2022 04:52:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=48
06/15/2022 04:52:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=49
06/15/2022 04:52:31 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3591989987484355 on epoch=49
06/15/2022 04:52:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3591989987484355 on epoch=49, global_step=400
06/15/2022 04:52:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=51
06/15/2022 04:52:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=52
06/15/2022 04:52:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=53
06/15/2022 04:52:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=54
06/15/2022 04:52:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=56
06/15/2022 04:52:58 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.2556162720097146 on epoch=56
06/15/2022 04:53:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=57
06/15/2022 04:53:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=58
06/15/2022 04:53:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 04:53:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.18 on epoch=61
06/15/2022 04:53:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=62
06/15/2022 04:53:26 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 04:53:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=63
06/15/2022 04:53:35 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=64
06/15/2022 04:53:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=66
06/15/2022 04:53:44 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=67
06/15/2022 04:53:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 04:53:54 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.4562295424472456 on epoch=68
06/15/2022 04:53:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3591989987484355 -> 0.4562295424472456 on epoch=68, global_step=550
06/15/2022 04:53:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=69
06/15/2022 04:54:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=71
06/15/2022 04:54:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 04:54:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=73
06/15/2022 04:54:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=74
06/15/2022 04:54:21 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.46173254835996635 on epoch=74
06/15/2022 04:54:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4562295424472456 -> 0.46173254835996635 on epoch=74, global_step=600
06/15/2022 04:54:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 04:54:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=77
06/15/2022 04:54:35 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=78
06/15/2022 04:54:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=79
06/15/2022 04:54:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=81
06/15/2022 04:54:49 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.46281389748882007 on epoch=81
06/15/2022 04:54:49 - INFO - __main__ - Saving model with best Classification-F1: 0.46173254835996635 -> 0.46281389748882007 on epoch=81, global_step=650
06/15/2022 04:54:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=82
06/15/2022 04:54:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=83
06/15/2022 04:55:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=84
06/15/2022 04:55:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 04:55:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=87
06/15/2022 04:55:17 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 04:55:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=88
06/15/2022 04:55:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 04:55:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=91
06/15/2022 04:55:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=92
06/15/2022 04:55:39 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=93
06/15/2022 04:55:45 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3860677578987438 on epoch=93
06/15/2022 04:55:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=94
06/15/2022 04:55:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 04:55:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=97
06/15/2022 04:56:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=98
06/15/2022 04:56:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 04:56:12 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.46843853820598 on epoch=99
06/15/2022 04:56:13 - INFO - __main__ - Saving model with best Classification-F1: 0.46281389748882007 -> 0.46843853820598 on epoch=99, global_step=800
06/15/2022 04:56:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=101
06/15/2022 04:56:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 04:56:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=103
06/15/2022 04:56:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=104
06/15/2022 04:56:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=106
06/15/2022 04:56:40 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.26935286935286934 on epoch=106
06/15/2022 04:56:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 04:56:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 04:56:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=109
06/15/2022 04:56:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=111
06/15/2022 04:57:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=112
06/15/2022 04:57:08 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3298429319371728 on epoch=112
06/15/2022 04:57:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=113
06/15/2022 04:57:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=114
06/15/2022 04:57:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=116
06/15/2022 04:57:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=117
06/15/2022 04:57:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 04:57:35 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.42482504604051563 on epoch=118
06/15/2022 04:57:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=119
06/15/2022 04:57:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 04:57:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=122
06/15/2022 04:57:53 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=123
06/15/2022 04:57:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=124
06/15/2022 04:58:03 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.4181818181818182 on epoch=124
06/15/2022 04:58:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=126
06/15/2022 04:58:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 04:58:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=128
06/15/2022 04:58:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 04:58:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=131
06/15/2022 04:58:31 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.5305925497008299 on epoch=131
06/15/2022 04:58:31 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5305925497008299 on epoch=131, global_step=1050
06/15/2022 04:58:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=132
06/15/2022 04:58:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=133
06/15/2022 04:58:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=134
06/15/2022 04:58:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 04:58:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=137
06/15/2022 04:58:58 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.4297594297594297 on epoch=137
06/15/2022 04:59:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=138
06/15/2022 04:59:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=139
06/15/2022 04:59:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=141
06/15/2022 04:59:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=142
06/15/2022 04:59:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=143
06/15/2022 04:59:26 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.4402206822310435 on epoch=143
06/15/2022 04:59:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=144
06/15/2022 04:59:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=146
06/15/2022 04:59:39 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=147
06/15/2022 04:59:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=148
06/15/2022 04:59:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=149
06/15/2022 04:59:54 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.4297594297594297 on epoch=149
06/15/2022 04:59:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 05:00:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=152
06/15/2022 05:00:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=153
06/15/2022 05:00:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=154
06/15/2022 05:00:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=156
06/15/2022 05:00:21 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.4155251141552511 on epoch=156
06/15/2022 05:00:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=157
06/15/2022 05:00:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=158
06/15/2022 05:00:34 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=159
06/15/2022 05:00:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=161
06/15/2022 05:00:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=162
06/15/2022 05:00:48 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.41075141075141075 on epoch=162
06/15/2022 05:00:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=163
06/15/2022 05:00:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=164
06/15/2022 05:01:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.17 on epoch=166
06/15/2022 05:01:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=167
06/15/2022 05:01:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=168
06/15/2022 05:01:16 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.4079058031959629 on epoch=168
06/15/2022 05:01:20 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=169
06/15/2022 05:01:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=171
06/15/2022 05:01:29 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=172
06/15/2022 05:01:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=173
06/15/2022 05:01:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=174
06/15/2022 05:01:43 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.5491823899371069 on epoch=174
06/15/2022 05:01:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5305925497008299 -> 0.5491823899371069 on epoch=174, global_step=1400
06/15/2022 05:01:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=176
06/15/2022 05:01:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=177
06/15/2022 05:01:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=178
06/15/2022 05:02:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=179
06/15/2022 05:02:06 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=181
06/15/2022 05:02:11 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.6078431372549019 on epoch=181
06/15/2022 05:02:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5491823899371069 -> 0.6078431372549019 on epoch=181, global_step=1450
06/15/2022 05:02:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=182
06/15/2022 05:02:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=183
06/15/2022 05:02:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=184
06/15/2022 05:02:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=186
06/15/2022 05:02:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=187
06/15/2022 05:02:39 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.4345381526104417 on epoch=187
06/15/2022 05:02:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=188
06/15/2022 05:02:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=189
06/15/2022 05:02:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=191
06/15/2022 05:02:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=192
06/15/2022 05:03:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=193
06/15/2022 05:03:06 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.3172071830608416 on epoch=193
06/15/2022 05:03:11 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=194
06/15/2022 05:03:15 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=196
06/15/2022 05:03:19 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=197
06/15/2022 05:03:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=198
06/15/2022 05:03:28 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=199
06/15/2022 05:03:34 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.40996830884471336 on epoch=199
06/15/2022 05:03:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=201
06/15/2022 05:03:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=202
06/15/2022 05:03:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.14 on epoch=203
06/15/2022 05:03:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=204
06/15/2022 05:03:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.17 on epoch=206
06/15/2022 05:04:01 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.4198830409356725 on epoch=206
06/15/2022 05:04:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=207
06/15/2022 05:04:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=208
06/15/2022 05:04:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=209
06/15/2022 05:04:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=211
06/15/2022 05:04:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=212
06/15/2022 05:04:28 - INFO - __main__ - Global step 1700 Train loss 0.14 Classification-F1 0.3750290630086026 on epoch=212
06/15/2022 05:04:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=213
06/15/2022 05:04:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=214
06/15/2022 05:04:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=216
06/15/2022 05:04:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=217
06/15/2022 05:04:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=218
06/15/2022 05:04:56 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.5450980392156863 on epoch=218
06/15/2022 05:05:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.13 on epoch=219
06/15/2022 05:05:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=221
06/15/2022 05:05:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=222
06/15/2022 05:05:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=223
06/15/2022 05:05:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=224
06/15/2022 05:05:24 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.5294117647058825 on epoch=224
06/15/2022 05:05:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=226
06/15/2022 05:05:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=227
06/15/2022 05:05:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=228
06/15/2022 05:05:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=229
06/15/2022 05:05:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=231
06/15/2022 05:05:51 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.4059668508287293 on epoch=231
06/15/2022 05:05:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=232
06/15/2022 05:06:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.10 on epoch=233
06/15/2022 05:06:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.16 on epoch=234
06/15/2022 05:06:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.12 on epoch=236
06/15/2022 05:06:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.11 on epoch=237
06/15/2022 05:06:19 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.5500462534690101 on epoch=237
06/15/2022 05:06:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=238
06/15/2022 05:06:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.15 on epoch=239
06/15/2022 05:06:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=241
06/15/2022 05:06:37 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=242
06/15/2022 05:06:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=243
06/15/2022 05:06:46 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.44379029997196523 on epoch=243
06/15/2022 05:06:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=244
06/15/2022 05:06:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=246
06/15/2022 05:06:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=247
06/15/2022 05:07:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=248
06/15/2022 05:07:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=249
06/15/2022 05:07:13 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.5625970798384592 on epoch=249
06/15/2022 05:07:18 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.10 on epoch=251
06/15/2022 05:07:22 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=252
06/15/2022 05:07:27 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=253
06/15/2022 05:07:31 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=254
06/15/2022 05:07:35 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.10 on epoch=256
06/15/2022 05:07:41 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.5307859583721652 on epoch=256
06/15/2022 05:07:46 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.10 on epoch=257
06/15/2022 05:07:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=258
06/15/2022 05:07:54 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=259
06/15/2022 05:07:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.09 on epoch=261
06/15/2022 05:08:03 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=262
06/15/2022 05:08:09 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.5097603162836669 on epoch=262
06/15/2022 05:08:13 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.13 on epoch=263
06/15/2022 05:08:17 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.08 on epoch=264
06/15/2022 05:08:22 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=266
06/15/2022 05:08:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.12 on epoch=267
06/15/2022 05:08:31 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=268
06/15/2022 05:08:36 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.4202898550724638 on epoch=268
06/15/2022 05:08:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=269
06/15/2022 05:08:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.11 on epoch=271
06/15/2022 05:08:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=272
06/15/2022 05:08:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=273
06/15/2022 05:08:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=274
06/15/2022 05:09:04 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.5110771581359816 on epoch=274
06/15/2022 05:09:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.11 on epoch=276
06/15/2022 05:09:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.08 on epoch=277
06/15/2022 05:09:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.10 on epoch=278
06/15/2022 05:09:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=279
06/15/2022 05:09:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=281
06/15/2022 05:09:31 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.523683380637023 on epoch=281
06/15/2022 05:09:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=282
06/15/2022 05:09:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.10 on epoch=283
06/15/2022 05:09:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.08 on epoch=284
06/15/2022 05:09:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=286
06/15/2022 05:09:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.08 on epoch=287
06/15/2022 05:09:59 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.5145583557621727 on epoch=287
06/15/2022 05:10:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.08 on epoch=288
06/15/2022 05:10:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.07 on epoch=289
06/15/2022 05:10:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=291
06/15/2022 05:10:16 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.09 on epoch=292
06/15/2022 05:10:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=293
06/15/2022 05:10:26 - INFO - __main__ - Global step 2350 Train loss 0.09 Classification-F1 0.5536737235367373 on epoch=293
06/15/2022 05:10:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.11 on epoch=294
06/15/2022 05:10:35 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.06 on epoch=296
06/15/2022 05:10:39 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=297
06/15/2022 05:10:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=298
06/15/2022 05:10:48 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.09 on epoch=299
06/15/2022 05:10:54 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.5053671103477888 on epoch=299
06/15/2022 05:10:58 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=301
06/15/2022 05:11:03 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=302
06/15/2022 05:11:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=303
06/15/2022 05:11:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=304
06/15/2022 05:11:16 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=306
06/15/2022 05:11:21 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.5035035035035035 on epoch=306
06/15/2022 05:11:26 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=307
06/15/2022 05:11:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=308
06/15/2022 05:11:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=309
06/15/2022 05:11:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=311
06/15/2022 05:11:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.04 on epoch=312
06/15/2022 05:11:49 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.5137254901960784 on epoch=312
06/15/2022 05:11:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=313
06/15/2022 05:11:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=314
06/15/2022 05:12:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.06 on epoch=316
06/15/2022 05:12:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.07 on epoch=317
06/15/2022 05:12:11 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=318
06/15/2022 05:12:17 - INFO - __main__ - Global step 2550 Train loss 0.06 Classification-F1 0.5555555555555556 on epoch=318
06/15/2022 05:12:21 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.06 on epoch=319
06/15/2022 05:12:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.08 on epoch=321
06/15/2022 05:12:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=322
06/15/2022 05:12:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=323
06/15/2022 05:12:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.08 on epoch=324
06/15/2022 05:12:45 - INFO - __main__ - Global step 2600 Train loss 0.06 Classification-F1 0.5137254901960784 on epoch=324
06/15/2022 05:12:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=326
06/15/2022 05:12:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.07 on epoch=327
06/15/2022 05:12:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=328
06/15/2022 05:13:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=329
06/15/2022 05:13:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.08 on epoch=331
06/15/2022 05:13:14 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.5623931623931624 on epoch=331
06/15/2022 05:13:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.11 on epoch=332
06/15/2022 05:13:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.05 on epoch=333
06/15/2022 05:13:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.07 on epoch=334
06/15/2022 05:13:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=336
06/15/2022 05:13:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.08 on epoch=337
06/15/2022 05:13:42 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.5696139476961395 on epoch=337
06/15/2022 05:13:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=338
06/15/2022 05:13:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=339
06/15/2022 05:13:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.06 on epoch=341
06/15/2022 05:14:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=342
06/15/2022 05:14:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=343
06/15/2022 05:14:10 - INFO - __main__ - Global step 2750 Train loss 0.05 Classification-F1 0.5460992907801419 on epoch=343
06/15/2022 05:14:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=344
06/15/2022 05:14:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.04 on epoch=346
06/15/2022 05:14:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=347
06/15/2022 05:14:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=348
06/15/2022 05:14:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.07 on epoch=349
06/15/2022 05:14:39 - INFO - __main__ - Global step 2800 Train loss 0.05 Classification-F1 0.5383580903478208 on epoch=349
06/15/2022 05:14:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=351
06/15/2022 05:14:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.05 on epoch=352
06/15/2022 05:14:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=353
06/15/2022 05:14:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=354
06/15/2022 05:15:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=356
06/15/2022 05:15:08 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.5764705882352941 on epoch=356
06/15/2022 05:15:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=357
06/15/2022 05:15:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=358
06/15/2022 05:15:21 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=359
06/15/2022 05:15:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.05 on epoch=361
06/15/2022 05:15:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=362
06/15/2022 05:15:36 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.5606315920863758 on epoch=362
06/15/2022 05:15:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=363
06/15/2022 05:15:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=364
06/15/2022 05:15:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=366
06/15/2022 05:15:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.03 on epoch=367
06/15/2022 05:15:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=368
06/15/2022 05:16:05 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.5376798285889195 on epoch=368
06/15/2022 05:16:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 05:16:14 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=371
06/15/2022 05:16:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=372
06/15/2022 05:16:23 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=373
06/15/2022 05:16:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.07 on epoch=374
06/15/2022 05:16:29 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:16:29 - INFO - __main__ - Printing 3 examples
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:16:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:16:29 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 05:16:29 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:16:29 - INFO - __main__ - Printing 3 examples
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 05:16:29 - INFO - __main__ - ['entailed']
06/15/2022 05:16:29 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:16:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:16:30 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 05:16:33 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.5383580903478208 on epoch=374
06/15/2022 05:16:33 - INFO - __main__ - save last model!
06/15/2022 05:16:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 05:16:33 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 05:16:33 - INFO - __main__ - Printing 3 examples
06/15/2022 05:16:33 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:16:33 - INFO - __main__ - ['entailed']
06/15/2022 05:16:33 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:16:33 - INFO - __main__ - ['entailed']
06/15/2022 05:16:33 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:16:33 - INFO - __main__ - ['entailed']
06/15/2022 05:16:33 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:16:45 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 05:16:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 05:16:46 - INFO - __main__ - Starting training!
06/15/2022 05:16:59 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:17:12 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 05:26:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_13_0.2_8_predictions.txt
06/15/2022 05:26:16 - INFO - __main__ - Classification-F1 on test data: 0.1668
06/15/2022 05:26:16 - INFO - __main__ - prefix=tab_fact_64_13, lr=0.2, bsz=8, dev_performance=0.6078431372549019, test_performance=0.16679952278445062
06/15/2022 05:26:16 - INFO - __main__ - Running ... prefix=tab_fact_64_21, lr=0.5, bsz=8 ...
06/15/2022 05:26:17 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:26:17 - INFO - __main__ - Printing 3 examples
06/15/2022 05:26:17 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 05:26:17 - INFO - __main__ - ['entailed']
06/15/2022 05:26:17 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 05:26:17 - INFO - __main__ - ['entailed']
06/15/2022 05:26:17 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 05:26:17 - INFO - __main__ - ['entailed']
06/15/2022 05:26:17 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:26:17 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:26:17 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 05:26:17 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:26:17 - INFO - __main__ - Printing 3 examples
06/15/2022 05:26:17 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 05:26:18 - INFO - __main__ - ['entailed']
06/15/2022 05:26:18 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 05:26:18 - INFO - __main__ - ['entailed']
06/15/2022 05:26:18 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 05:26:18 - INFO - __main__ - ['entailed']
06/15/2022 05:26:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:26:18 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:26:18 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 05:26:37 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 05:26:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 05:26:37 - INFO - __main__ - Starting training!
06/15/2022 05:26:42 - INFO - __main__ - Step 10 Global step 10 Train loss 2.04 on epoch=1
06/15/2022 05:26:47 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=2
06/15/2022 05:26:51 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=3
06/15/2022 05:26:56 - INFO - __main__ - Step 40 Global step 40 Train loss 0.30 on epoch=4
06/15/2022 05:27:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=6
06/15/2022 05:27:06 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 05:27:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 05:27:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=7
06/15/2022 05:27:15 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=8
06/15/2022 05:27:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=9
06/15/2022 05:27:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=11
06/15/2022 05:27:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=12
06/15/2022 05:27:34 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 05:27:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=13
06/15/2022 05:27:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=14
06/15/2022 05:27:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=16
06/15/2022 05:27:52 - INFO - __main__ - Step 140 Global step 140 Train loss 1.19 on epoch=17
06/15/2022 05:27:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=18
06/15/2022 05:28:02 - INFO - __main__ - Global step 150 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 05:28:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=19
06/15/2022 05:28:11 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=21
06/15/2022 05:28:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=22
06/15/2022 05:28:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.32 on epoch=23
06/15/2022 05:28:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=24
06/15/2022 05:28:30 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 05:28:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=26
06/15/2022 05:28:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=27
06/15/2022 05:28:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=28
06/15/2022 05:28:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=29
06/15/2022 05:28:53 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=31
06/15/2022 05:28:58 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 05:29:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=32
06/15/2022 05:29:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 05:29:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=34
06/15/2022 05:29:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=36
06/15/2022 05:29:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=37
06/15/2022 05:29:27 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 05:29:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=38
06/15/2022 05:29:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=39
06/15/2022 05:29:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=41
06/15/2022 05:29:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 05:29:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=43
06/15/2022 05:29:55 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 05:29:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=44
06/15/2022 05:30:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=46
06/15/2022 05:30:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=47
06/15/2022 05:30:13 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=48
06/15/2022 05:30:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=49
06/15/2022 05:30:23 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 05:30:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=51
06/15/2022 05:30:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=52
06/15/2022 05:30:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=53
06/15/2022 05:30:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=54
06/15/2022 05:30:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=56
06/15/2022 05:30:51 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 05:30:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=57
06/15/2022 05:31:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=58
06/15/2022 05:31:04 - INFO - __main__ - Step 480 Global step 480 Train loss 0.18 on epoch=59
06/15/2022 05:31:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=61
06/15/2022 05:31:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=62
06/15/2022 05:31:19 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 05:31:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=63
06/15/2022 05:31:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=64
06/15/2022 05:31:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=66
06/15/2022 05:31:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=67
06/15/2022 05:31:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=68
06/15/2022 05:31:47 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 05:31:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=69
06/15/2022 05:31:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.22 on epoch=71
06/15/2022 05:32:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 05:32:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=73
06/15/2022 05:32:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=74
06/15/2022 05:32:14 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 05:32:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 05:32:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 05:32:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=78
06/15/2022 05:32:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=79
06/15/2022 05:32:37 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=81
06/15/2022 05:32:42 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 05:32:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=82
06/15/2022 05:32:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 05:32:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=84
06/15/2022 05:33:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=86
06/15/2022 05:33:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=87
06/15/2022 05:33:09 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.1657754010695187 on epoch=87
06/15/2022 05:33:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=88
06/15/2022 05:33:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=89
06/15/2022 05:33:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=91
06/15/2022 05:33:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=92
06/15/2022 05:33:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=93
06/15/2022 05:33:37 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 05:33:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=94
06/15/2022 05:33:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=96
06/15/2022 05:33:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 05:33:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=98
06/15/2022 05:33:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=99
06/15/2022 05:34:04 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 05:34:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=101
06/15/2022 05:34:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 05:34:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=103
06/15/2022 05:34:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=104
06/15/2022 05:34:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=106
06/15/2022 05:34:32 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 05:34:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 05:34:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=108
06/15/2022 05:34:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=109
06/15/2022 05:34:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=111
06/15/2022 05:34:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=112
06/15/2022 05:35:00 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3591989987484355 on epoch=112
06/15/2022 05:35:00 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3591989987484355 on epoch=112, global_step=900
06/15/2022 05:35:05 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=113
06/15/2022 05:35:09 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=114
06/15/2022 05:35:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 05:35:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 05:35:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 05:35:28 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 05:35:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=119
06/15/2022 05:35:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 05:35:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=122
06/15/2022 05:35:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=123
06/15/2022 05:35:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=124
06/15/2022 05:35:56 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 05:36:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=126
06/15/2022 05:36:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=127
06/15/2022 05:36:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=128
06/15/2022 05:36:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=129
06/15/2022 05:36:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=131
06/15/2022 05:36:24 - INFO - __main__ - Global step 1050 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 05:36:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=132
06/15/2022 05:36:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=133
06/15/2022 05:36:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=134
06/15/2022 05:36:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=136
06/15/2022 05:36:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=137
06/15/2022 05:36:51 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=137
06/15/2022 05:36:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=138
06/15/2022 05:37:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=139
06/15/2022 05:37:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=141
06/15/2022 05:37:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 05:37:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=143
06/15/2022 05:37:19 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=143
06/15/2022 05:37:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=144
06/15/2022 05:37:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=146
06/15/2022 05:37:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=147
06/15/2022 05:37:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=148
06/15/2022 05:37:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=149
06/15/2022 05:37:47 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.4848930054295752 on epoch=149
06/15/2022 05:37:47 - INFO - __main__ - Saving model with best Classification-F1: 0.3591989987484355 -> 0.4848930054295752 on epoch=149, global_step=1200
06/15/2022 05:37:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 05:37:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=152
06/15/2022 05:38:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=153
06/15/2022 05:38:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=154
06/15/2022 05:38:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=156
06/15/2022 05:38:16 - INFO - __main__ - Global step 1250 Train loss 0.22 Classification-F1 0.3860677578987438 on epoch=156
06/15/2022 05:38:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=157
06/15/2022 05:38:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=158
06/15/2022 05:38:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=159
06/15/2022 05:38:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=161
06/15/2022 05:38:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=162
06/15/2022 05:38:44 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.440084835630965 on epoch=162
06/15/2022 05:38:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=163
06/15/2022 05:38:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=164
06/15/2022 05:38:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=166
06/15/2022 05:39:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=167
06/15/2022 05:39:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=168
06/15/2022 05:39:11 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.47714452441159305 on epoch=168
06/15/2022 05:39:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=169
06/15/2022 05:39:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=171
06/15/2022 05:39:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=172
06/15/2022 05:39:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=173
06/15/2022 05:39:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=174
06/15/2022 05:39:39 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.40116959064327484 on epoch=174
06/15/2022 05:39:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=176
06/15/2022 05:39:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 05:39:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=178
06/15/2022 05:39:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=179
06/15/2022 05:40:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=181
06/15/2022 05:40:07 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3871086556169429 on epoch=181
06/15/2022 05:40:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=182
06/15/2022 05:40:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=183
06/15/2022 05:40:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=184
06/15/2022 05:40:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=186
06/15/2022 05:40:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=187
06/15/2022 05:40:36 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.3262608402795319 on epoch=187
06/15/2022 05:40:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=188
06/15/2022 05:40:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=189
06/15/2022 05:40:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.24 on epoch=191
06/15/2022 05:40:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=192
06/15/2022 05:40:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=193
06/15/2022 05:41:04 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=193
06/15/2022 05:41:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=194
06/15/2022 05:41:13 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=196
06/15/2022 05:41:17 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=197
06/15/2022 05:41:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=198
06/15/2022 05:41:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=199
06/15/2022 05:41:32 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.439671682626539 on epoch=199
06/15/2022 05:41:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=201
06/15/2022 05:41:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=202
06/15/2022 05:41:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=203
06/15/2022 05:41:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=204
06/15/2022 05:41:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=206
06/15/2022 05:42:00 - INFO - __main__ - Global step 1650 Train loss 0.19 Classification-F1 0.3948694102146787 on epoch=206
06/15/2022 05:42:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=207
06/15/2022 05:42:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=208
06/15/2022 05:42:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=209
06/15/2022 05:42:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.16 on epoch=211
06/15/2022 05:42:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=212
06/15/2022 05:42:29 - INFO - __main__ - Global step 1700 Train loss 0.18 Classification-F1 0.49215650369285235 on epoch=212
06/15/2022 05:42:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4848930054295752 -> 0.49215650369285235 on epoch=212, global_step=1700
06/15/2022 05:42:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=213
06/15/2022 05:42:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=214
06/15/2022 05:42:42 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=216
06/15/2022 05:42:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=217
06/15/2022 05:42:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.16 on epoch=218
06/15/2022 05:42:57 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.28306533391279154 on epoch=218
06/15/2022 05:43:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=219
06/15/2022 05:43:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=221
06/15/2022 05:43:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=222
06/15/2022 05:43:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=223
06/15/2022 05:43:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=224
06/15/2022 05:43:25 - INFO - __main__ - Global step 1800 Train loss 0.18 Classification-F1 0.4740190880169671 on epoch=224
06/15/2022 05:43:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=226
06/15/2022 05:43:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=227
06/15/2022 05:43:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=228
06/15/2022 05:43:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=229
06/15/2022 05:43:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=231
06/15/2022 05:43:53 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.45705196182396607 on epoch=231
06/15/2022 05:43:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.16 on epoch=232
06/15/2022 05:44:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.18 on epoch=233
06/15/2022 05:44:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.17 on epoch=234
06/15/2022 05:44:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=236
06/15/2022 05:44:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=237
06/15/2022 05:44:21 - INFO - __main__ - Global step 1900 Train loss 0.17 Classification-F1 0.4817813765182186 on epoch=237
06/15/2022 05:44:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=238
06/15/2022 05:44:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=239
06/15/2022 05:44:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=241
06/15/2022 05:44:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=242
06/15/2022 05:44:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=243
06/15/2022 05:44:50 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.42840679919331603 on epoch=243
06/15/2022 05:44:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=244
06/15/2022 05:44:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=246
06/15/2022 05:45:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=247
06/15/2022 05:45:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=248
06/15/2022 05:45:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=249
06/15/2022 05:45:18 - INFO - __main__ - Global step 2000 Train loss 0.17 Classification-F1 0.43840100285535205 on epoch=249
06/15/2022 05:45:22 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.15 on epoch=251
06/15/2022 05:45:27 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=252
06/15/2022 05:45:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.15 on epoch=253
06/15/2022 05:45:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.15 on epoch=254
06/15/2022 05:45:40 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.16 on epoch=256
06/15/2022 05:45:46 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.48747093774218553 on epoch=256
06/15/2022 05:45:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=257
06/15/2022 05:45:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.17 on epoch=258
06/15/2022 05:46:00 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.15 on epoch=259
06/15/2022 05:46:04 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.13 on epoch=261
06/15/2022 05:46:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.19 on epoch=262
06/15/2022 05:46:14 - INFO - __main__ - Global step 2100 Train loss 0.15 Classification-F1 0.5700763358778627 on epoch=262
06/15/2022 05:46:14 - INFO - __main__ - Saving model with best Classification-F1: 0.49215650369285235 -> 0.5700763358778627 on epoch=262, global_step=2100
06/15/2022 05:46:19 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=263
06/15/2022 05:46:23 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.17 on epoch=264
06/15/2022 05:46:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=266
06/15/2022 05:46:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.16 on epoch=267
06/15/2022 05:46:37 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=268
06/15/2022 05:46:42 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.48950637463223273 on epoch=268
06/15/2022 05:46:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.14 on epoch=269
06/15/2022 05:46:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=271
06/15/2022 05:46:56 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.11 on epoch=272
06/15/2022 05:47:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.13 on epoch=273
06/15/2022 05:47:05 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.11 on epoch=274
06/15/2022 05:47:11 - INFO - __main__ - Global step 2200 Train loss 0.12 Classification-F1 0.50778245742538 on epoch=274
06/15/2022 05:47:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=276
06/15/2022 05:47:20 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.16 on epoch=277
06/15/2022 05:47:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.14 on epoch=278
06/15/2022 05:47:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.14 on epoch=279
06/15/2022 05:47:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.12 on epoch=281
06/15/2022 05:47:39 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.5241025641025642 on epoch=281
06/15/2022 05:47:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=282
06/15/2022 05:47:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.13 on epoch=283
06/15/2022 05:47:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.15 on epoch=284
06/15/2022 05:47:57 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.15 on epoch=286
06/15/2022 05:48:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.13 on epoch=287
06/15/2022 05:48:07 - INFO - __main__ - Global step 2300 Train loss 0.14 Classification-F1 0.3517481647380863 on epoch=287
06/15/2022 05:48:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=288
06/15/2022 05:48:16 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=289
06/15/2022 05:48:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.10 on epoch=291
06/15/2022 05:48:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.12 on epoch=292
06/15/2022 05:48:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.14 on epoch=293
06/15/2022 05:48:35 - INFO - __main__ - Global step 2350 Train loss 0.12 Classification-F1 0.46895191457525676 on epoch=293
06/15/2022 05:48:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=294
06/15/2022 05:48:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=296
06/15/2022 05:48:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.09 on epoch=297
06/15/2022 05:48:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.11 on epoch=298
06/15/2022 05:48:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.12 on epoch=299
06/15/2022 05:49:03 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.5184705519580636 on epoch=299
06/15/2022 05:49:08 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=301
06/15/2022 05:49:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.16 on epoch=302
06/15/2022 05:49:17 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.10 on epoch=303
06/15/2022 05:49:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=304
06/15/2022 05:49:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.12 on epoch=306
06/15/2022 05:49:32 - INFO - __main__ - Global step 2450 Train loss 0.12 Classification-F1 0.49692950135101943 on epoch=306
06/15/2022 05:49:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.10 on epoch=307
06/15/2022 05:49:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.08 on epoch=308
06/15/2022 05:49:45 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.10 on epoch=309
06/15/2022 05:49:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=311
06/15/2022 05:49:54 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.12 on epoch=312
06/15/2022 05:50:00 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.554006968641115 on epoch=312
06/15/2022 05:50:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.12 on epoch=313
06/15/2022 05:50:09 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.09 on epoch=314
06/15/2022 05:50:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=316
06/15/2022 05:50:18 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.09 on epoch=317
06/15/2022 05:50:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.10 on epoch=318
06/15/2022 05:50:28 - INFO - __main__ - Global step 2550 Train loss 0.11 Classification-F1 0.48747093774218553 on epoch=318
06/15/2022 05:50:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=319
06/15/2022 05:50:37 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=321
06/15/2022 05:50:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.08 on epoch=322
06/15/2022 05:50:46 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.07 on epoch=323
06/15/2022 05:50:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.09 on epoch=324
06/15/2022 05:50:56 - INFO - __main__ - Global step 2600 Train loss 0.09 Classification-F1 0.504537089916873 on epoch=324
06/15/2022 05:51:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.11 on epoch=326
06/15/2022 05:51:05 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=327
06/15/2022 05:51:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=328
06/15/2022 05:51:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.11 on epoch=329
06/15/2022 05:51:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.07 on epoch=331
06/15/2022 05:51:24 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.5137254901960784 on epoch=331
06/15/2022 05:51:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.09 on epoch=332
06/15/2022 05:51:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.14 on epoch=333
06/15/2022 05:51:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.14 on epoch=334
06/15/2022 05:51:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.10 on epoch=336
06/15/2022 05:51:47 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=337
06/15/2022 05:51:52 - INFO - __main__ - Global step 2700 Train loss 0.11 Classification-F1 0.5730170496664195 on epoch=337
06/15/2022 05:51:53 - INFO - __main__ - Saving model with best Classification-F1: 0.5700763358778627 -> 0.5730170496664195 on epoch=337, global_step=2700
06/15/2022 05:51:57 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.09 on epoch=338
06/15/2022 05:52:02 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=339
06/15/2022 05:52:06 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.07 on epoch=341
06/15/2022 05:52:11 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=342
06/15/2022 05:52:15 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=343
06/15/2022 05:52:21 - INFO - __main__ - Global step 2750 Train loss 0.08 Classification-F1 0.4746922024623803 on epoch=343
06/15/2022 05:52:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=344
06/15/2022 05:52:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.08 on epoch=346
06/15/2022 05:52:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.08 on epoch=347
06/15/2022 05:52:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=348
06/15/2022 05:52:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=349
06/15/2022 05:52:49 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.2739921050928622 on epoch=349
06/15/2022 05:52:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.08 on epoch=351
06/15/2022 05:52:58 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.07 on epoch=352
06/15/2022 05:53:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=353
06/15/2022 05:53:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=354
06/15/2022 05:53:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=356
06/15/2022 05:53:17 - INFO - __main__ - Global step 2850 Train loss 0.05 Classification-F1 0.3105590062111801 on epoch=356
06/15/2022 05:53:21 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.11 on epoch=357
06/15/2022 05:53:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.06 on epoch=358
06/15/2022 05:53:31 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.09 on epoch=359
06/15/2022 05:53:35 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=361
06/15/2022 05:53:40 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=362
06/15/2022 05:53:45 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.28603216987729674 on epoch=362
06/15/2022 05:53:50 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.05 on epoch=363
06/15/2022 05:53:54 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.03 on epoch=364
06/15/2022 05:53:59 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=366
06/15/2022 05:54:03 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=367
06/15/2022 05:54:08 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=368
06/15/2022 05:54:14 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.3105590062111801 on epoch=368
06/15/2022 05:54:18 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.08 on epoch=369
06/15/2022 05:54:23 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=371
06/15/2022 05:54:27 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.07 on epoch=372
06/15/2022 05:54:32 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=373
06/15/2022 05:54:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.08 on epoch=374
06/15/2022 05:54:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:54:37 - INFO - __main__ - Printing 3 examples
06/15/2022 05:54:37 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 05:54:37 - INFO - __main__ - ['entailed']
06/15/2022 05:54:37 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 05:54:37 - INFO - __main__ - ['entailed']
06/15/2022 05:54:37 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 05:54:37 - INFO - __main__ - ['entailed']
06/15/2022 05:54:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:54:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:54:38 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 05:54:38 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 05:54:38 - INFO - __main__ - Printing 3 examples
06/15/2022 05:54:38 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 05:54:38 - INFO - __main__ - ['entailed']
06/15/2022 05:54:38 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 05:54:38 - INFO - __main__ - ['entailed']
06/15/2022 05:54:38 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 05:54:38 - INFO - __main__ - ['entailed']
06/15/2022 05:54:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:54:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:54:38 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 05:54:42 - INFO - __main__ - Global step 3000 Train loss 0.07 Classification-F1 0.36811594202898545 on epoch=374
06/15/2022 05:54:42 - INFO - __main__ - save last model!
06/15/2022 05:54:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 05:54:42 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 05:54:42 - INFO - __main__ - Printing 3 examples
06/15/2022 05:54:42 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:54:42 - INFO - __main__ - ['entailed']
06/15/2022 05:54:42 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:54:42 - INFO - __main__ - ['entailed']
06/15/2022 05:54:42 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 05:54:42 - INFO - __main__ - ['entailed']
06/15/2022 05:54:42 - INFO - __main__ - Tokenizing Input ...
06/15/2022 05:54:56 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 05:54:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 05:54:57 - INFO - __main__ - Starting training!
06/15/2022 05:55:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 05:55:19 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 06:04:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_21_0.5_8_predictions.txt
06/15/2022 06:04:09 - INFO - __main__ - Classification-F1 on test data: 0.0980
06/15/2022 06:04:10 - INFO - __main__ - prefix=tab_fact_64_21, lr=0.5, bsz=8, dev_performance=0.5730170496664195, test_performance=0.09800886086352142
06/15/2022 06:04:10 - INFO - __main__ - Running ... prefix=tab_fact_64_21, lr=0.4, bsz=8 ...
06/15/2022 06:04:11 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:04:11 - INFO - __main__ - Printing 3 examples
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:04:11 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:04:11 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 06:04:11 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:04:11 - INFO - __main__ - Printing 3 examples
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 06:04:11 - INFO - __main__ - ['entailed']
06/15/2022 06:04:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:04:11 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:04:12 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 06:04:27 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 06:04:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 06:04:28 - INFO - __main__ - Starting training!
06/15/2022 06:04:33 - INFO - __main__ - Step 10 Global step 10 Train loss 2.96 on epoch=1
06/15/2022 06:04:37 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=2
06/15/2022 06:04:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=3
06/15/2022 06:04:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=4
06/15/2022 06:04:51 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=6
06/15/2022 06:04:56 - INFO - __main__ - Global step 50 Train loss 0.95 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 06:04:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 06:05:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.26 on epoch=7
06/15/2022 06:05:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=8
06/15/2022 06:05:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=9
06/15/2022 06:05:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=11
06/15/2022 06:05:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=12
06/15/2022 06:05:23 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 06:05:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=13
06/15/2022 06:05:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=14
06/15/2022 06:05:36 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=16
06/15/2022 06:05:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=17
06/15/2022 06:05:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=18
06/15/2022 06:05:50 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 06:05:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=19
06/15/2022 06:05:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 06:06:04 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=22
06/15/2022 06:06:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 06:06:12 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=24
06/15/2022 06:06:19 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 06:06:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=26
06/15/2022 06:06:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=27
06/15/2022 06:06:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=28
06/15/2022 06:06:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=29
06/15/2022 06:06:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=31
06/15/2022 06:06:46 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 06:06:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=32
06/15/2022 06:06:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=33
06/15/2022 06:07:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=34
06/15/2022 06:07:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=36
06/15/2022 06:07:08 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=37
06/15/2022 06:07:14 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.40996830884471336 on epoch=37
06/15/2022 06:07:14 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.40996830884471336 on epoch=37, global_step=300
06/15/2022 06:07:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=38
06/15/2022 06:07:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=39
06/15/2022 06:07:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=41
06/15/2022 06:07:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 06:07:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=43
06/15/2022 06:07:42 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 06:07:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 06:07:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=46
06/15/2022 06:07:55 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=47
06/15/2022 06:07:59 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=48
06/15/2022 06:08:04 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=49
06/15/2022 06:08:10 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 06:08:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=51
06/15/2022 06:08:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=52
06/15/2022 06:08:23 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=53
06/15/2022 06:08:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=54
06/15/2022 06:08:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=56
06/15/2022 06:08:37 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 06:08:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=57
06/15/2022 06:08:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.19 on epoch=58
06/15/2022 06:08:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 06:08:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=61
06/15/2022 06:08:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 06:09:05 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 06:09:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=63
06/15/2022 06:09:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=64
06/15/2022 06:09:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 06:09:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 06:09:27 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 06:09:32 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 06:09:37 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 06:09:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=71
06/15/2022 06:09:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 06:09:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=73
06/15/2022 06:09:54 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=74
06/15/2022 06:10:00 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.3682504604051565 on epoch=74
06/15/2022 06:10:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=76
06/15/2022 06:10:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=77
06/15/2022 06:10:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=78
06/15/2022 06:10:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=79
06/15/2022 06:10:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=81
06/15/2022 06:10:28 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.4740190880169671 on epoch=81
06/15/2022 06:10:28 - INFO - __main__ - Saving model with best Classification-F1: 0.40996830884471336 -> 0.4740190880169671 on epoch=81, global_step=650
06/15/2022 06:10:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=82
06/15/2022 06:10:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 06:10:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 06:10:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=86
06/15/2022 06:10:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=87
06/15/2022 06:10:56 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.5294117647058825 on epoch=87
06/15/2022 06:10:56 - INFO - __main__ - Saving model with best Classification-F1: 0.4740190880169671 -> 0.5294117647058825 on epoch=87, global_step=700
06/15/2022 06:11:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=88
06/15/2022 06:11:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=89
06/15/2022 06:11:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=91
06/15/2022 06:11:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=92
06/15/2022 06:11:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=93
06/15/2022 06:11:24 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.37922403003754696 on epoch=93
06/15/2022 06:11:28 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=94
06/15/2022 06:11:33 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=96
06/15/2022 06:11:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=97
06/15/2022 06:11:42 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=98
06/15/2022 06:11:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=99
06/15/2022 06:11:52 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.46843853820598 on epoch=99
06/15/2022 06:11:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=101
06/15/2022 06:12:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 06:12:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=103
06/15/2022 06:12:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=104
06/15/2022 06:12:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=106
06/15/2022 06:12:20 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.47998667998668 on epoch=106
06/15/2022 06:12:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=107
06/15/2022 06:12:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=108
06/15/2022 06:12:34 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=109
06/15/2022 06:12:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=111
06/15/2022 06:12:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=112
06/15/2022 06:12:48 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.5145583557621727 on epoch=112
06/15/2022 06:12:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 06:12:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=114
06/15/2022 06:13:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 06:13:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 06:13:10 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 06:13:16 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.5370264610698648 on epoch=118
06/15/2022 06:13:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5294117647058825 -> 0.5370264610698648 on epoch=118, global_step=950
06/15/2022 06:13:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.14 on epoch=119
06/15/2022 06:13:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=121
06/15/2022 06:13:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=122
06/15/2022 06:13:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=123
06/15/2022 06:13:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=124
06/15/2022 06:13:44 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5302177636408123 on epoch=124
06/15/2022 06:13:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=126
06/15/2022 06:13:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.16 on epoch=127
06/15/2022 06:13:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=128
06/15/2022 06:14:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 06:14:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=131
06/15/2022 06:14:13 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.32246561385634237 on epoch=131
06/15/2022 06:14:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=132
06/15/2022 06:14:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=133
06/15/2022 06:14:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=134
06/15/2022 06:14:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=136
06/15/2022 06:14:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=137
06/15/2022 06:14:41 - INFO - __main__ - Global step 1100 Train loss 0.17 Classification-F1 0.49141145546793813 on epoch=137
06/15/2022 06:14:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=138
06/15/2022 06:14:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=139
06/15/2022 06:14:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=141
06/15/2022 06:14:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=142
06/15/2022 06:15:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=143
06/15/2022 06:15:09 - INFO - __main__ - Global step 1150 Train loss 0.17 Classification-F1 0.4079058031959629 on epoch=143
06/15/2022 06:15:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.16 on epoch=144
06/15/2022 06:15:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.15 on epoch=146
06/15/2022 06:15:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.15 on epoch=147
06/15/2022 06:15:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=148
06/15/2022 06:15:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=149
06/15/2022 06:15:37 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.5484869113187698 on epoch=149
06/15/2022 06:15:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5370264610698648 -> 0.5484869113187698 on epoch=149, global_step=1200
06/15/2022 06:15:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=151
06/15/2022 06:15:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=152
06/15/2022 06:15:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=153
06/15/2022 06:15:55 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=154
06/15/2022 06:15:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=156
06/15/2022 06:16:05 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.5467643467643467 on epoch=156
06/15/2022 06:16:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=157
06/15/2022 06:16:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=158
06/15/2022 06:16:19 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=159
06/15/2022 06:16:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=161
06/15/2022 06:16:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.13 on epoch=162
06/15/2022 06:16:34 - INFO - __main__ - Global step 1300 Train loss 0.16 Classification-F1 0.4439473513583609 on epoch=162
06/15/2022 06:16:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=163
06/15/2022 06:16:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=164
06/15/2022 06:16:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=166
06/15/2022 06:16:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=167
06/15/2022 06:16:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=168
06/15/2022 06:17:02 - INFO - __main__ - Global step 1350 Train loss 0.14 Classification-F1 0.5500462534690101 on epoch=168
06/15/2022 06:17:02 - INFO - __main__ - Saving model with best Classification-F1: 0.5484869113187698 -> 0.5500462534690101 on epoch=168, global_step=1350
06/15/2022 06:17:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.14 on epoch=169
06/15/2022 06:17:11 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=171
06/15/2022 06:17:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=172
06/15/2022 06:17:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=173
06/15/2022 06:17:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=174
06/15/2022 06:17:31 - INFO - __main__ - Global step 1400 Train loss 0.12 Classification-F1 0.504537089916873 on epoch=174
06/15/2022 06:17:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=176
06/15/2022 06:17:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=177
06/15/2022 06:17:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=178
06/15/2022 06:17:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.11 on epoch=179
06/15/2022 06:17:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=181
06/15/2022 06:17:59 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.531135531135531 on epoch=181
06/15/2022 06:18:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.12 on epoch=182
06/15/2022 06:18:08 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=183
06/15/2022 06:18:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.10 on epoch=184
06/15/2022 06:18:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.12 on epoch=186
06/15/2022 06:18:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=187
06/15/2022 06:18:27 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.5145583557621727 on epoch=187
06/15/2022 06:18:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=188
06/15/2022 06:18:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=189
06/15/2022 06:18:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=191
06/15/2022 06:18:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=192
06/15/2022 06:18:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=193
06/15/2022 06:18:56 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.47396184751272774 on epoch=193
06/15/2022 06:19:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=194
06/15/2022 06:19:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.09 on epoch=196
06/15/2022 06:19:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=197
06/15/2022 06:19:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=198
06/15/2022 06:19:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=199
06/15/2022 06:19:24 - INFO - __main__ - Global step 1600 Train loss 0.09 Classification-F1 0.5053671103477888 on epoch=199
06/15/2022 06:19:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=201
06/15/2022 06:19:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=202
06/15/2022 06:19:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=203
06/15/2022 06:19:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=204
06/15/2022 06:19:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=206
06/15/2022 06:19:53 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.5234084111579076 on epoch=206
06/15/2022 06:19:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=207
06/15/2022 06:20:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=208
06/15/2022 06:20:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=209
06/15/2022 06:20:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=211
06/15/2022 06:20:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=212
06/15/2022 06:20:21 - INFO - __main__ - Global step 1700 Train loss 0.08 Classification-F1 0.5041505257332595 on epoch=212
06/15/2022 06:20:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=213
06/15/2022 06:20:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=214
06/15/2022 06:20:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=216
06/15/2022 06:20:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=217
06/15/2022 06:20:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=218
06/15/2022 06:20:50 - INFO - __main__ - Global step 1750 Train loss 0.09 Classification-F1 0.2557692307692308 on epoch=218
06/15/2022 06:20:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=219
06/15/2022 06:20:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=221
06/15/2022 06:21:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=222
06/15/2022 06:21:08 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.08 on epoch=223
06/15/2022 06:21:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=224
06/15/2022 06:21:18 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.49797570850202433 on epoch=224
06/15/2022 06:21:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=226
06/15/2022 06:21:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=227
06/15/2022 06:21:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=228
06/15/2022 06:21:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=229
06/15/2022 06:21:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.03 on epoch=231
06/15/2022 06:21:46 - INFO - __main__ - Global step 1850 Train loss 0.07 Classification-F1 0.5168017822885079 on epoch=231
06/15/2022 06:21:51 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=232
06/15/2022 06:21:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=233
06/15/2022 06:22:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=234
06/15/2022 06:22:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=236
06/15/2022 06:22:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=237
06/15/2022 06:22:14 - INFO - __main__ - Global step 1900 Train loss 0.07 Classification-F1 0.4980392156862745 on epoch=237
06/15/2022 06:22:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=238
06/15/2022 06:22:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=239
06/15/2022 06:22:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=241
06/15/2022 06:22:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=242
06/15/2022 06:22:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=243
06/15/2022 06:22:43 - INFO - __main__ - Global step 1950 Train loss 0.08 Classification-F1 0.3712270341207349 on epoch=243
06/15/2022 06:22:47 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=244
06/15/2022 06:22:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=246
06/15/2022 06:22:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=247
06/15/2022 06:23:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=248
06/15/2022 06:23:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=249
06/15/2022 06:23:11 - INFO - __main__ - Global step 2000 Train loss 0.06 Classification-F1 0.5110771581359816 on epoch=249
06/15/2022 06:23:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=251
06/15/2022 06:23:20 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.03 on epoch=252
06/15/2022 06:23:24 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.04 on epoch=253
06/15/2022 06:23:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.07 on epoch=254
06/15/2022 06:23:33 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.03 on epoch=256
06/15/2022 06:23:39 - INFO - __main__ - Global step 2050 Train loss 0.05 Classification-F1 0.515625 on epoch=256
06/15/2022 06:23:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.07 on epoch=257
06/15/2022 06:23:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.05 on epoch=258
06/15/2022 06:23:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=259
06/15/2022 06:23:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.04 on epoch=261
06/15/2022 06:24:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.03 on epoch=262
06/15/2022 06:24:07 - INFO - __main__ - Global step 2100 Train loss 0.05 Classification-F1 0.4899128268991283 on epoch=262
06/15/2022 06:24:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.06 on epoch=263
06/15/2022 06:24:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.04 on epoch=264
06/15/2022 06:24:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=266
06/15/2022 06:24:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.04 on epoch=267
06/15/2022 06:24:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.05 on epoch=268
06/15/2022 06:24:36 - INFO - __main__ - Global step 2150 Train loss 0.05 Classification-F1 0.5113300492610837 on epoch=268
06/15/2022 06:24:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.05 on epoch=269
06/15/2022 06:24:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.02 on epoch=271
06/15/2022 06:24:49 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.03 on epoch=272
06/15/2022 06:24:54 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.05 on epoch=273
06/15/2022 06:24:58 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=274
06/15/2022 06:25:04 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.5079365079365079 on epoch=274
06/15/2022 06:25:08 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=276
06/15/2022 06:25:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.02 on epoch=277
06/15/2022 06:25:17 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=278
06/15/2022 06:25:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=279
06/15/2022 06:25:26 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=281
06/15/2022 06:25:31 - INFO - __main__ - Global step 2250 Train loss 0.05 Classification-F1 0.47714452441159305 on epoch=281
06/15/2022 06:25:36 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.03 on epoch=282
06/15/2022 06:25:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.04 on epoch=283
06/15/2022 06:25:45 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.04 on epoch=284
06/15/2022 06:25:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=286
06/15/2022 06:25:54 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=287
06/15/2022 06:25:59 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.49566587864460204 on epoch=287
06/15/2022 06:26:04 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.04 on epoch=288
06/15/2022 06:26:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=289
06/15/2022 06:26:13 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.02 on epoch=291
06/15/2022 06:26:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.03 on epoch=292
06/15/2022 06:26:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.05 on epoch=293
06/15/2022 06:26:27 - INFO - __main__ - Global step 2350 Train loss 0.04 Classification-F1 0.5137254901960784 on epoch=293
06/15/2022 06:26:32 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.02 on epoch=294
06/15/2022 06:26:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.01 on epoch=296
06/15/2022 06:26:41 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.04 on epoch=297
06/15/2022 06:26:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.02 on epoch=298
06/15/2022 06:26:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=299
06/15/2022 06:26:55 - INFO - __main__ - Global step 2400 Train loss 0.03 Classification-F1 0.50633608815427 on epoch=299
06/15/2022 06:26:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=301
06/15/2022 06:27:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=302
06/15/2022 06:27:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.03 on epoch=303
06/15/2022 06:27:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.05 on epoch=304
06/15/2022 06:27:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=306
06/15/2022 06:27:23 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.49394613293797873 on epoch=306
06/15/2022 06:27:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.03 on epoch=307
06/15/2022 06:27:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=308
06/15/2022 06:27:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.03 on epoch=309
06/15/2022 06:27:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.01 on epoch=311
06/15/2022 06:27:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=312
06/15/2022 06:27:51 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.5168017822885079 on epoch=312
06/15/2022 06:27:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=313
06/15/2022 06:28:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.03 on epoch=314
06/15/2022 06:28:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=316
06/15/2022 06:28:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.01 on epoch=317
06/15/2022 06:28:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=318
06/15/2022 06:28:19 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.3346456692913386 on epoch=318
06/15/2022 06:28:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=319
06/15/2022 06:28:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=321
06/15/2022 06:28:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=322
06/15/2022 06:28:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.02 on epoch=323
06/15/2022 06:28:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=324
06/15/2022 06:28:47 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.4709377421854818 on epoch=324
06/15/2022 06:28:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.04 on epoch=326
06/15/2022 06:28:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=327
06/15/2022 06:29:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.03 on epoch=328
06/15/2022 06:29:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=329
06/15/2022 06:29:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=331
06/15/2022 06:29:15 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.48424908424908425 on epoch=331
06/15/2022 06:29:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=332
06/15/2022 06:29:24 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=333
06/15/2022 06:29:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=334
06/15/2022 06:29:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.02 on epoch=336
06/15/2022 06:29:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.02 on epoch=337
06/15/2022 06:29:43 - INFO - __main__ - Global step 2700 Train loss 0.02 Classification-F1 0.5283714075165806 on epoch=337
06/15/2022 06:29:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=338
06/15/2022 06:29:52 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=339
06/15/2022 06:29:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.01 on epoch=341
06/15/2022 06:30:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.01 on epoch=342
06/15/2022 06:30:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=343
06/15/2022 06:30:11 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.4682306940371457 on epoch=343
06/15/2022 06:30:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 06:30:20 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=346
06/15/2022 06:30:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=347
06/15/2022 06:30:29 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.01 on epoch=348
06/15/2022 06:30:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=349
06/15/2022 06:30:39 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.523175572519084 on epoch=349
06/15/2022 06:30:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.01 on epoch=351
06/15/2022 06:30:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=352
06/15/2022 06:30:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.01 on epoch=353
06/15/2022 06:30:57 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=354
06/15/2022 06:31:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.04 on epoch=356
06/15/2022 06:31:07 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.49394613293797873 on epoch=356
06/15/2022 06:31:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=357
06/15/2022 06:31:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.02 on epoch=358
06/15/2022 06:31:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.01 on epoch=359
06/15/2022 06:31:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.02 on epoch=361
06/15/2022 06:31:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=362
06/15/2022 06:31:35 - INFO - __main__ - Global step 2900 Train loss 0.02 Classification-F1 0.4832395400048935 on epoch=362
06/15/2022 06:31:39 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=363
06/15/2022 06:31:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=364
06/15/2022 06:31:48 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=366
06/15/2022 06:31:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.01 on epoch=367
06/15/2022 06:31:57 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=368
06/15/2022 06:32:03 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.38030434244711114 on epoch=368
06/15/2022 06:32:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=369
06/15/2022 06:32:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=371
06/15/2022 06:32:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=372
06/15/2022 06:32:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=373
06/15/2022 06:32:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=374
06/15/2022 06:32:27 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:32:27 - INFO - __main__ - Printing 3 examples
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:32:27 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:32:27 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 06:32:27 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:32:27 - INFO - __main__ - Printing 3 examples
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 06:32:27 - INFO - __main__ - ['entailed']
06/15/2022 06:32:27 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:32:27 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:32:28 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 06:32:31 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.5263124882393527 on epoch=374
06/15/2022 06:32:31 - INFO - __main__ - save last model!
06/15/2022 06:32:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 06:32:31 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 06:32:31 - INFO - __main__ - Printing 3 examples
06/15/2022 06:32:31 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 06:32:31 - INFO - __main__ - ['entailed']
06/15/2022 06:32:31 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 06:32:31 - INFO - __main__ - ['entailed']
06/15/2022 06:32:31 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 06:32:31 - INFO - __main__ - ['entailed']
06/15/2022 06:32:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:32:43 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 06:32:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 06:32:44 - INFO - __main__ - Starting training!
06/15/2022 06:32:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:33:08 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 06:41:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_21_0.4_8_predictions.txt
06/15/2022 06:41:35 - INFO - __main__ - Classification-F1 on test data: 0.1181
06/15/2022 06:41:35 - INFO - __main__ - prefix=tab_fact_64_21, lr=0.4, bsz=8, dev_performance=0.5500462534690101, test_performance=0.11810181145272464
06/15/2022 06:41:35 - INFO - __main__ - Running ... prefix=tab_fact_64_21, lr=0.3, bsz=8 ...
06/15/2022 06:41:36 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:41:36 - INFO - __main__ - Printing 3 examples
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:41:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:41:36 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 06:41:36 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 06:41:36 - INFO - __main__ - Printing 3 examples
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 06:41:36 - INFO - __main__ - ['entailed']
06/15/2022 06:41:36 - INFO - __main__ - Tokenizing Input ...
06/15/2022 06:41:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 06:41:37 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 06:41:52 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 06:41:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 06:41:53 - INFO - __main__ - Starting training!
06/15/2022 06:41:58 - INFO - __main__ - Step 10 Global step 10 Train loss 2.99 on epoch=1
06/15/2022 06:42:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.86 on epoch=2
06/15/2022 06:42:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=3
06/15/2022 06:42:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=4
06/15/2022 06:42:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=6
06/15/2022 06:42:21 - INFO - __main__ - Global step 50 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 06:42:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 06:42:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=7
06/15/2022 06:42:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=8
06/15/2022 06:42:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=9
06/15/2022 06:42:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=11
06/15/2022 06:42:43 - INFO - __main__ - Step 100 Global step 100 Train loss 0.24 on epoch=12
06/15/2022 06:42:48 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 06:42:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 06:42:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 06:43:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=16
06/15/2022 06:43:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=17
06/15/2022 06:43:11 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=18
06/15/2022 06:43:16 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 06:43:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=19
06/15/2022 06:43:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=21
06/15/2022 06:43:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=22
06/15/2022 06:43:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=23
06/15/2022 06:43:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=24
06/15/2022 06:43:44 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 06:43:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=26
06/15/2022 06:43:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=27
06/15/2022 06:43:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=28
06/15/2022 06:44:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=29
06/15/2022 06:44:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=31
06/15/2022 06:44:11 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 06:44:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=32
06/15/2022 06:44:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=33
06/15/2022 06:44:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=34
06/15/2022 06:44:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 06:44:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=37
06/15/2022 06:44:39 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.22105263157894736 on epoch=37
06/15/2022 06:44:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=38
06/15/2022 06:44:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=39
06/15/2022 06:44:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=41
06/15/2022 06:44:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=42
06/15/2022 06:45:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 06:45:07 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 06:45:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 06:45:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=46
06/15/2022 06:45:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=47
06/15/2022 06:45:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=48
06/15/2022 06:45:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=49
06/15/2022 06:45:34 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 06:45:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=51
06/15/2022 06:45:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=52
06/15/2022 06:45:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=53
06/15/2022 06:45:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=54
06/15/2022 06:45:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=56
06/15/2022 06:46:01 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 06:46:06 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=57
06/15/2022 06:46:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=58
06/15/2022 06:46:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=59
06/15/2022 06:46:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.19 on epoch=61
06/15/2022 06:46:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=62
06/15/2022 06:46:29 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 06:46:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=63
06/15/2022 06:46:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=64
06/15/2022 06:46:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 06:46:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=67
06/15/2022 06:46:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=68
06/15/2022 06:46:56 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 06:47:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 06:47:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 06:47:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=72
06/15/2022 06:47:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=73
06/15/2022 06:47:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=74
06/15/2022 06:47:23 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 06:47:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=76
06/15/2022 06:47:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=77
06/15/2022 06:47:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=78
06/15/2022 06:47:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=79
06/15/2022 06:47:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=81
06/15/2022 06:47:51 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 06:47:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=82
06/15/2022 06:48:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 06:48:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 06:48:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 06:48:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=87
06/15/2022 06:48:19 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.43636363636363634 on epoch=87
06/15/2022 06:48:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43636363636363634 on epoch=87, global_step=700
06/15/2022 06:48:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=88
06/15/2022 06:48:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=89
06/15/2022 06:48:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=91
06/15/2022 06:48:37 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=92
06/15/2022 06:48:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=93
06/15/2022 06:48:47 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 06:48:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=94
06/15/2022 06:48:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=96
06/15/2022 06:49:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=97
06/15/2022 06:49:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=98
06/15/2022 06:49:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 06:49:15 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.45744466123931915 on epoch=99
06/15/2022 06:49:15 - INFO - __main__ - Saving model with best Classification-F1: 0.43636363636363634 -> 0.45744466123931915 on epoch=99, global_step=800
06/15/2022 06:49:19 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 06:49:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 06:49:28 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=103
06/15/2022 06:49:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=104
06/15/2022 06:49:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=106
06/15/2022 06:49:43 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.42840679919331603 on epoch=106
06/15/2022 06:49:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=107
06/15/2022 06:49:52 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=108
06/15/2022 06:49:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 06:50:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=111
06/15/2022 06:50:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 06:50:11 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.46204360388757554 on epoch=112
06/15/2022 06:50:11 - INFO - __main__ - Saving model with best Classification-F1: 0.45744466123931915 -> 0.46204360388757554 on epoch=112, global_step=900
06/15/2022 06:50:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=113
06/15/2022 06:50:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=114
06/15/2022 06:50:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=116
06/15/2022 06:50:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 06:50:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=118
06/15/2022 06:50:38 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.40996830884471336 on epoch=118
06/15/2022 06:50:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=119
06/15/2022 06:50:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=121
06/15/2022 06:50:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=122
06/15/2022 06:50:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=123
06/15/2022 06:51:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 06:51:06 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.5062438705459301 on epoch=124
06/15/2022 06:51:06 - INFO - __main__ - Saving model with best Classification-F1: 0.46204360388757554 -> 0.5062438705459301 on epoch=124, global_step=1000
06/15/2022 06:51:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=126
06/15/2022 06:51:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 06:51:20 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=128
06/15/2022 06:51:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=129
06/15/2022 06:51:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=131
06/15/2022 06:51:35 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.40096618357487923 on epoch=131
06/15/2022 06:51:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=132
06/15/2022 06:51:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=133
06/15/2022 06:51:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=134
06/15/2022 06:51:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 06:51:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=137
06/15/2022 06:52:02 - INFO - __main__ - Global step 1100 Train loss 0.19 Classification-F1 0.40274392367156486 on epoch=137
06/15/2022 06:52:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.17 on epoch=138
06/15/2022 06:52:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=139
06/15/2022 06:52:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=141
06/15/2022 06:52:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 06:52:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=143
06/15/2022 06:52:30 - INFO - __main__ - Global step 1150 Train loss 0.18 Classification-F1 0.4458874458874459 on epoch=143
06/15/2022 06:52:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=144
06/15/2022 06:52:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=146
06/15/2022 06:52:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=147
06/15/2022 06:52:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.18 on epoch=148
06/15/2022 06:52:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=149
06/15/2022 06:52:58 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.5263124882393527 on epoch=149
06/15/2022 06:52:58 - INFO - __main__ - Saving model with best Classification-F1: 0.5062438705459301 -> 0.5263124882393527 on epoch=149, global_step=1200
06/15/2022 06:53:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=151
06/15/2022 06:53:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=152
06/15/2022 06:53:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=153
06/15/2022 06:53:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=154
06/15/2022 06:53:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.16 on epoch=156
06/15/2022 06:53:26 - INFO - __main__ - Global step 1250 Train loss 0.16 Classification-F1 0.5075370545569221 on epoch=156
06/15/2022 06:53:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=157
06/15/2022 06:53:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=158
06/15/2022 06:53:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=159
06/15/2022 06:53:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=161
06/15/2022 06:53:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=162
06/15/2022 06:53:54 - INFO - __main__ - Global step 1300 Train loss 0.17 Classification-F1 0.5307859583721652 on epoch=162
06/15/2022 06:53:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5263124882393527 -> 0.5307859583721652 on epoch=162, global_step=1300
06/15/2022 06:53:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=163
06/15/2022 06:54:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=164
06/15/2022 06:54:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=166
06/15/2022 06:54:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=167
06/15/2022 06:54:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=168
06/15/2022 06:54:22 - INFO - __main__ - Global step 1350 Train loss 0.17 Classification-F1 0.554442748091603 on epoch=168
06/15/2022 06:54:22 - INFO - __main__ - Saving model with best Classification-F1: 0.5307859583721652 -> 0.554442748091603 on epoch=168, global_step=1350
06/15/2022 06:54:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=169
06/15/2022 06:54:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.14 on epoch=171
06/15/2022 06:54:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.16 on epoch=172
06/15/2022 06:54:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.14 on epoch=173
06/15/2022 06:54:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.13 on epoch=174
06/15/2022 06:54:50 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.5058530510585305 on epoch=174
06/15/2022 06:54:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=176
06/15/2022 06:54:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=177
06/15/2022 06:55:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=178
06/15/2022 06:55:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=179
06/15/2022 06:55:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=181
06/15/2022 06:55:18 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.5376798285889195 on epoch=181
06/15/2022 06:55:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 06:55:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.16 on epoch=183
06/15/2022 06:55:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=184
06/15/2022 06:55:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=186
06/15/2022 06:55:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.15 on epoch=187
06/15/2022 06:55:46 - INFO - __main__ - Global step 1500 Train loss 0.15 Classification-F1 0.5356330320359097 on epoch=187
06/15/2022 06:55:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=188
06/15/2022 06:55:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=189
06/15/2022 06:55:59 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.11 on epoch=191
06/15/2022 06:56:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=192
06/15/2022 06:56:08 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=193
06/15/2022 06:56:14 - INFO - __main__ - Global step 1550 Train loss 0.11 Classification-F1 0.5533516988062442 on epoch=193
06/15/2022 06:56:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=194
06/15/2022 06:56:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=196
06/15/2022 06:56:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=197
06/15/2022 06:56:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=198
06/15/2022 06:56:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=199
06/15/2022 06:56:42 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.4584615384615385 on epoch=199
06/15/2022 06:56:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=201
06/15/2022 06:56:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=202
06/15/2022 06:56:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=203
06/15/2022 06:57:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=204
06/15/2022 06:57:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.11 on epoch=206
06/15/2022 06:57:10 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.5376798285889195 on epoch=206
06/15/2022 06:57:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=207
06/15/2022 06:57:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=208
06/15/2022 06:57:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=209
06/15/2022 06:57:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=211
06/15/2022 06:57:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=212
06/15/2022 06:57:38 - INFO - __main__ - Global step 1700 Train loss 0.10 Classification-F1 0.5390343648904352 on epoch=212
06/15/2022 06:57:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=213
06/15/2022 06:57:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=214
06/15/2022 06:57:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=216
06/15/2022 06:57:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=217
06/15/2022 06:58:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=218
06/15/2022 06:58:06 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.37919506236337924 on epoch=218
06/15/2022 06:58:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=219
06/15/2022 06:58:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=221
06/15/2022 06:58:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=222
06/15/2022 06:58:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=223
06/15/2022 06:58:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=224
06/15/2022 06:58:34 - INFO - __main__ - Global step 1800 Train loss 0.08 Classification-F1 0.3633540372670807 on epoch=224
06/15/2022 06:58:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=226
06/15/2022 06:58:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=227
06/15/2022 06:58:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=228
06/15/2022 06:58:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=229
06/15/2022 06:58:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=231
06/15/2022 06:59:02 - INFO - __main__ - Global step 1850 Train loss 0.09 Classification-F1 0.3623242503421675 on epoch=231
06/15/2022 06:59:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=232
06/15/2022 06:59:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=233
06/15/2022 06:59:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=234
06/15/2022 06:59:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=236
06/15/2022 06:59:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=237
06/15/2022 06:59:30 - INFO - __main__ - Global step 1900 Train loss 0.08 Classification-F1 0.2453862996559274 on epoch=237
06/15/2022 06:59:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=238
06/15/2022 06:59:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=239
06/15/2022 06:59:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=241
06/15/2022 06:59:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=242
06/15/2022 06:59:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=243
06/15/2022 06:59:58 - INFO - __main__ - Global step 1950 Train loss 0.09 Classification-F1 0.2605305209332055 on epoch=243
06/15/2022 07:00:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=244
06/15/2022 07:00:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=246
06/15/2022 07:00:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=247
06/15/2022 07:00:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=248
06/15/2022 07:00:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.08 on epoch=249
06/15/2022 07:00:26 - INFO - __main__ - Global step 2000 Train loss 0.08 Classification-F1 0.33431831240252813 on epoch=249
06/15/2022 07:00:30 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.05 on epoch=251
06/15/2022 07:00:35 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.06 on epoch=252
06/15/2022 07:00:39 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.05 on epoch=253
06/15/2022 07:00:43 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.09 on epoch=254
06/15/2022 07:00:48 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.04 on epoch=256
06/15/2022 07:00:54 - INFO - __main__ - Global step 2050 Train loss 0.06 Classification-F1 0.2111875637104995 on epoch=256
06/15/2022 07:00:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.05 on epoch=257
06/15/2022 07:01:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.09 on epoch=258
06/15/2022 07:01:07 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.06 on epoch=259
06/15/2022 07:01:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.07 on epoch=261
06/15/2022 07:01:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.09 on epoch=262
06/15/2022 07:01:22 - INFO - __main__ - Global step 2100 Train loss 0.07 Classification-F1 0.5484869113187698 on epoch=262
06/15/2022 07:01:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.07 on epoch=263
06/15/2022 07:01:30 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=264
06/15/2022 07:01:35 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=266
06/15/2022 07:01:39 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=267
06/15/2022 07:01:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.06 on epoch=268
06/15/2022 07:01:50 - INFO - __main__ - Global step 2150 Train loss 0.07 Classification-F1 0.2440423930766336 on epoch=268
06/15/2022 07:01:54 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.04 on epoch=269
06/15/2022 07:01:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.04 on epoch=271
06/15/2022 07:02:03 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.06 on epoch=272
06/15/2022 07:02:07 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.04 on epoch=273
06/15/2022 07:02:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.04 on epoch=274
06/15/2022 07:02:18 - INFO - __main__ - Global step 2200 Train loss 0.04 Classification-F1 0.36076935695538054 on epoch=274
06/15/2022 07:02:22 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=276
06/15/2022 07:02:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.04 on epoch=277
06/15/2022 07:02:31 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.04 on epoch=278
06/15/2022 07:02:35 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.04 on epoch=279
06/15/2022 07:02:40 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.05 on epoch=281
06/15/2022 07:02:45 - INFO - __main__ - Global step 2250 Train loss 0.04 Classification-F1 0.25960404642214363 on epoch=281
06/15/2022 07:02:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=282
06/15/2022 07:02:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.03 on epoch=283
06/15/2022 07:02:59 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=284
06/15/2022 07:03:03 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.05 on epoch=286
06/15/2022 07:03:08 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.04 on epoch=287
06/15/2022 07:03:13 - INFO - __main__ - Global step 2300 Train loss 0.05 Classification-F1 0.5141221128482274 on epoch=287
06/15/2022 07:03:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.02 on epoch=288
06/15/2022 07:03:22 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.08 on epoch=289
06/15/2022 07:03:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.03 on epoch=291
06/15/2022 07:03:31 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=292
06/15/2022 07:03:36 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=293
06/15/2022 07:03:41 - INFO - __main__ - Global step 2350 Train loss 0.06 Classification-F1 0.5524750046003803 on epoch=293
06/15/2022 07:03:46 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.05 on epoch=294
06/15/2022 07:03:50 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.04 on epoch=296
06/15/2022 07:03:55 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.07 on epoch=297
06/15/2022 07:03:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.04 on epoch=298
06/15/2022 07:04:04 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.02 on epoch=299
06/15/2022 07:04:09 - INFO - __main__ - Global step 2400 Train loss 0.04 Classification-F1 0.515625 on epoch=299
06/15/2022 07:04:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.03 on epoch=301
06/15/2022 07:04:18 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.03 on epoch=302
06/15/2022 07:04:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=303
06/15/2022 07:04:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.04 on epoch=304
06/15/2022 07:04:31 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.01 on epoch=306
06/15/2022 07:04:37 - INFO - __main__ - Global step 2450 Train loss 0.03 Classification-F1 0.5464320625610948 on epoch=306
06/15/2022 07:04:41 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.04 on epoch=307
06/15/2022 07:04:46 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.02 on epoch=308
06/15/2022 07:04:50 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.01 on epoch=309
06/15/2022 07:04:55 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.04 on epoch=311
06/15/2022 07:04:59 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.02 on epoch=312
06/15/2022 07:05:05 - INFO - __main__ - Global step 2500 Train loss 0.03 Classification-F1 0.5137254901960784 on epoch=312
06/15/2022 07:05:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.04 on epoch=313
06/15/2022 07:05:14 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.04 on epoch=314
06/15/2022 07:05:18 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.04 on epoch=316
06/15/2022 07:05:23 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.02 on epoch=317
06/15/2022 07:05:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.02 on epoch=318
06/15/2022 07:05:33 - INFO - __main__ - Global step 2550 Train loss 0.03 Classification-F1 0.5113300492610837 on epoch=318
06/15/2022 07:05:37 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.05 on epoch=319
06/15/2022 07:05:42 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.02 on epoch=321
06/15/2022 07:05:46 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.02 on epoch=322
06/15/2022 07:05:51 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=323
06/15/2022 07:05:55 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.02 on epoch=324
06/15/2022 07:06:01 - INFO - __main__ - Global step 2600 Train loss 0.03 Classification-F1 0.5607843137254902 on epoch=324
06/15/2022 07:06:01 - INFO - __main__ - Saving model with best Classification-F1: 0.554442748091603 -> 0.5607843137254902 on epoch=324, global_step=2600
06/15/2022 07:06:05 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.01 on epoch=326
06/15/2022 07:06:10 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=327
06/15/2022 07:06:14 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=328
06/15/2022 07:06:19 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.04 on epoch=329
06/15/2022 07:06:23 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=331
06/15/2022 07:06:29 - INFO - __main__ - Global step 2650 Train loss 0.04 Classification-F1 0.3598536244397845 on epoch=331
06/15/2022 07:06:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.05 on epoch=332
06/15/2022 07:06:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=333
06/15/2022 07:06:42 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.05 on epoch=334
06/15/2022 07:06:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=336
06/15/2022 07:06:51 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=337
06/15/2022 07:06:57 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.3607727328657561 on epoch=337
06/15/2022 07:07:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=338
06/15/2022 07:07:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=339
06/15/2022 07:07:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.03 on epoch=341
06/15/2022 07:07:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.04 on epoch=342
06/15/2022 07:07:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=343
06/15/2022 07:07:25 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.3493827160493827 on epoch=343
06/15/2022 07:07:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 07:07:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=346
06/15/2022 07:07:38 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=347
06/15/2022 07:07:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=348
06/15/2022 07:07:47 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.02 on epoch=349
06/15/2022 07:07:53 - INFO - __main__ - Global step 2800 Train loss 0.02 Classification-F1 0.5388091603053435 on epoch=349
06/15/2022 07:07:57 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.04 on epoch=351
06/15/2022 07:08:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.03 on epoch=352
06/15/2022 07:08:06 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=353
06/15/2022 07:08:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.04 on epoch=354
06/15/2022 07:08:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=356
06/15/2022 07:08:21 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.5356330320359097 on epoch=356
06/15/2022 07:08:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=357
06/15/2022 07:08:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.01 on epoch=358
06/15/2022 07:08:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.05 on epoch=359
06/15/2022 07:08:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=361
06/15/2022 07:08:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.01 on epoch=362
06/15/2022 07:08:49 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.35432098765432096 on epoch=362
06/15/2022 07:08:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=363
06/15/2022 07:08:58 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.01 on epoch=364
06/15/2022 07:09:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=366
06/15/2022 07:09:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=367
06/15/2022 07:09:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=368
06/15/2022 07:09:17 - INFO - __main__ - Global step 2950 Train loss 0.02 Classification-F1 0.22095238095238096 on epoch=368
06/15/2022 07:09:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.00 on epoch=369
06/15/2022 07:09:26 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.01 on epoch=371
06/15/2022 07:09:30 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=372
06/15/2022 07:09:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.04 on epoch=373
06/15/2022 07:09:39 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.02 on epoch=374
06/15/2022 07:09:40 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:09:40 - INFO - __main__ - Printing 3 examples
06/15/2022 07:09:40 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 07:09:40 - INFO - __main__ - ['entailed']
06/15/2022 07:09:40 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 07:09:40 - INFO - __main__ - ['entailed']
06/15/2022 07:09:40 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 07:09:40 - INFO - __main__ - ['entailed']
06/15/2022 07:09:40 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:09:41 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:09:41 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 07:09:41 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:09:41 - INFO - __main__ - Printing 3 examples
06/15/2022 07:09:41 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 07:09:41 - INFO - __main__ - ['entailed']
06/15/2022 07:09:41 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 07:09:41 - INFO - __main__ - ['entailed']
06/15/2022 07:09:41 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 07:09:41 - INFO - __main__ - ['entailed']
06/15/2022 07:09:41 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:09:41 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:09:41 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 07:09:44 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.35487850006167515 on epoch=374
06/15/2022 07:09:44 - INFO - __main__ - save last model!
06/15/2022 07:09:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 07:09:44 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 07:09:45 - INFO - __main__ - Printing 3 examples
06/15/2022 07:09:45 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:09:45 - INFO - __main__ - ['entailed']
06/15/2022 07:09:45 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:09:45 - INFO - __main__ - ['entailed']
06/15/2022 07:09:45 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:09:45 - INFO - __main__ - ['entailed']
06/15/2022 07:09:45 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:09:57 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 07:09:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 07:09:58 - INFO - __main__ - Starting training!
06/15/2022 07:10:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:10:22 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 07:19:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_21_0.3_8_predictions.txt
06/15/2022 07:19:10 - INFO - __main__ - Classification-F1 on test data: 0.0431
06/15/2022 07:19:10 - INFO - __main__ - prefix=tab_fact_64_21, lr=0.3, bsz=8, dev_performance=0.5607843137254902, test_performance=0.043139216595000425
06/15/2022 07:19:10 - INFO - __main__ - Running ... prefix=tab_fact_64_21, lr=0.2, bsz=8 ...
06/15/2022 07:19:11 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:19:11 - INFO - __main__ - Printing 3 examples
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gené#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi räikkönen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner björn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner björn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antônio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:19:11 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:19:11 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 07:19:11 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:19:11 - INFO - __main__ - Printing 3 examples
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: the original air date of this season span from 1998 to 1999 [SEP] table_caption: list of jag episodes [SEP] table_text: no in series#no in season#title#directed by#written by#original air date [n] 62#1#gypsy eyes (part 2)#tony wharmby#donald p bellisario#september 22 , 1998 [n] 63#2#embassy#alan j levi#r scott gemmill#september 29 , 1998 [n] 64#3#innocence#tony wharmby#dana coen#october 6 , 1998 [n] 65#4#going after francesca#alan j levi#stephen zito#october 13 , 1998 [n] 66#5#the martin baker fan club#tony wharmby#dana coen#october 20 , 1998 [n] 67#6#act of terror#alan j levi#larry moskowitz#october 27 , 1998 [n] 68#7#angels 30#tony wharmby#r scott gemmill#november 3 , 1998 [n] 69#8#mr rabb goes to washington#jeannot szwarc#stephen zito#november 10 , 1998 [n] 70#9#people v mac#tony wharmby#larry moskowitz#november 17 , 1998 [n] 71#10#the black jet#jeannot szwarc#david zabel#november 24 , 1998 [n] 72#11#jaggle bells#greg beeman#r scott gemmill#december 15 , 1998 [n] 73#12#dungaree justice#hugo cortina#david zabel#january 12 , 1999 [n] 74#13#war stories#greg beeman#dana coen#january 13 , 1999 [n] 75#14#webb of lies#mark horowitz#r scott gemmill#february 9 , 1999 [n] 76#15#rivers' run#greg beeman#larry moskowitz#february 16 , 1999 [n] 77#16#silent service#alan j levi#dana coen & julie b watson#february 23 , 1999 [n] 78#17#nobody 's child#tony wharmby#stephen zito#march 2 , 1999 [n] 79#18#shakedown#tony wharmby#r scott gemmill#march 30 , 1999 [n] 80#19#the adversaries#tony wharmby#larry moskowitz#april 13 , 1999 [n] 81#20#second sight#terrence o'hara#dana coen#april 27 , 1999 [n] 82#21#wilderness of mirrors#alan j levi#paul levine#may 4 , 1999 [n] 83#22#soul searching#jeannot szwarc#donald p bellisario#may 11 , 1999 [n] 84#23#yeah , baby#alan j levi#r scott gemmill#may 18 , 1999 [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: train number 99822 arrives at lonavala at 17:45 [SEP] table_caption: pune suburban railway [SEP] table_text: train number#train name#departure pune#arrival lonavla#frequency#origin [n] 99804#lonavala local#04:45#06:05#daily#pune railway station [n] 99806#lonavala local#05:45#07:05#daily#pune railway station [n] 99808#lonavala local#06:30#07:50#daily#pune railway station [n] 99810#lonavala local#08:05#10:25#daily#pune railway station [n] 99812#lonavala local#09:55#11:15#daily#pune railway station [n] 99814#lonavala local#10:50#12:25#daily#shivajinagar station [n] 99816#lonavala local#12:05#13:17#daily#pune railway station [n] 99820#lonavala local#13:00#14:20#daily#pune railway station [n] 99822#lonavala local#16:25#17:45#daily#pune railway station [n] 99824#lonavala local#16:25#17:37#daily#pune railway station [n] 99826#lonavala local#19:05#20:25#daily#pune railway station [n] 99828#lonavala local#19:35#21:05#daily#shivajinagar station [n] 99830#lonavala local#20:00#21:20#daily#pune railway station [n] 99832#lonavala local#20:00#21:12#daily#pune railway station [n] 99834#lonavala local#20:45#21:57#daily#pune railway station [n] 99836#lonavala local#21:10#22:22#daily#pune railway station [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ -  [tab_fact] statement: 2 of the venue have a crowd larger than 25000 [SEP] table_caption: 1965 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] richmond#12.13 (85)#st kilda#14.12 (96)#mcg#43412#1 may 1965 [n] fitzroy#9.4 (58)#melbourne#14.25 (109)#brunswick street oval#15890#1 may 1965 [n] collingwood#19.16 (130)#hawthorn#10.9 (69)#victoria park#25733#1 may 1965 [n] north melbourne#7.13 (55)#geelong#14.14 (98)#city of coburg oval#17408#1 may 1965 [n] footscray#6.12 (48)#essendon#18.18 (126)#western oval#24365#1 may 1965 [n] south melbourne#12.14 (86)#carlton#10.11 (71)#lake oval#23100#1 may 1965 [n] 
06/15/2022 07:19:11 - INFO - __main__ - ['entailed']
06/15/2022 07:19:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:19:11 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:19:12 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 07:19:30 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 07:19:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 07:19:31 - INFO - __main__ - Starting training!
06/15/2022 07:19:36 - INFO - __main__ - Step 10 Global step 10 Train loss 3.47 on epoch=1
06/15/2022 07:19:40 - INFO - __main__ - Step 20 Global step 20 Train loss 1.13 on epoch=2
06/15/2022 07:19:45 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=3
06/15/2022 07:19:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=4
06/15/2022 07:19:54 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=6
06/15/2022 07:19:59 - INFO - __main__ - Global step 50 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 07:19:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 07:20:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=7
06/15/2022 07:20:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.35 on epoch=8
06/15/2022 07:20:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=9
06/15/2022 07:20:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=11
06/15/2022 07:20:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=12
06/15/2022 07:20:26 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.4331983805668016 on epoch=12
06/15/2022 07:20:26 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4331983805668016 on epoch=12, global_step=100
06/15/2022 07:20:31 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=13
06/15/2022 07:20:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 07:20:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=16
06/15/2022 07:20:44 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=17
06/15/2022 07:20:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=18
06/15/2022 07:20:54 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 07:20:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=19
06/15/2022 07:21:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 07:21:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=22
06/15/2022 07:21:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 07:21:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=24
06/15/2022 07:21:22 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 07:21:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=26
06/15/2022 07:21:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=27
06/15/2022 07:21:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=28
06/15/2022 07:21:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=29
06/15/2022 07:21:44 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=31
06/15/2022 07:21:50 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 07:21:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=32
06/15/2022 07:21:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 07:22:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=34
06/15/2022 07:22:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=36
06/15/2022 07:22:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=37
06/15/2022 07:22:18 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 07:22:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=38
06/15/2022 07:22:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=39
06/15/2022 07:22:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=41
06/15/2022 07:22:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 07:22:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=43
06/15/2022 07:22:46 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 07:22:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=44
06/15/2022 07:22:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=46
06/15/2022 07:23:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=47
06/15/2022 07:23:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=48
06/15/2022 07:23:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=49
06/15/2022 07:23:14 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 07:23:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=51
06/15/2022 07:23:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=52
06/15/2022 07:23:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=53
06/15/2022 07:23:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=54
06/15/2022 07:23:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=56
06/15/2022 07:23:42 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 07:23:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=57
06/15/2022 07:23:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=58
06/15/2022 07:23:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 07:24:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=61
06/15/2022 07:24:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=62
06/15/2022 07:24:10 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 07:24:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=63
06/15/2022 07:24:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=64
06/15/2022 07:24:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 07:24:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 07:24:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=68
06/15/2022 07:24:38 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 07:24:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=69
06/15/2022 07:24:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=71
06/15/2022 07:24:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=72
06/15/2022 07:24:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=73
06/15/2022 07:25:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=74
06/15/2022 07:25:06 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 07:25:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=76
06/15/2022 07:25:15 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=77
06/15/2022 07:25:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=78
06/15/2022 07:25:24 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=79
06/15/2022 07:25:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=81
06/15/2022 07:25:35 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 07:25:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=82
06/15/2022 07:25:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 07:25:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=84
06/15/2022 07:25:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=86
06/15/2022 07:25:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=87
06/15/2022 07:26:03 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.39636200314394787 on epoch=87
06/15/2022 07:26:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.27 on epoch=88
06/15/2022 07:26:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=89
06/15/2022 07:26:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=91
06/15/2022 07:26:20 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=92
06/15/2022 07:26:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=93
06/15/2022 07:26:30 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 07:26:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=94
06/15/2022 07:26:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.17 on epoch=96
06/15/2022 07:26:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 07:26:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=98
06/15/2022 07:26:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=99
06/15/2022 07:26:58 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.46803878883831385 on epoch=99
06/15/2022 07:26:58 - INFO - __main__ - Saving model with best Classification-F1: 0.4331983805668016 -> 0.46803878883831385 on epoch=99, global_step=800
06/15/2022 07:27:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=101
06/15/2022 07:27:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=102
06/15/2022 07:27:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=103
06/15/2022 07:27:16 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=104
06/15/2022 07:27:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=106
06/15/2022 07:27:26 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3298429319371728 on epoch=106
06/15/2022 07:27:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 07:27:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 07:27:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 07:27:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=111
06/15/2022 07:27:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=112
06/15/2022 07:27:54 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3511520737327189 on epoch=112
06/15/2022 07:27:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=113
06/15/2022 07:28:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=114
06/15/2022 07:28:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 07:28:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.15 on epoch=117
06/15/2022 07:28:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 07:28:22 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.2175438596491228 on epoch=118
06/15/2022 07:28:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 07:28:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 07:28:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=122
06/15/2022 07:28:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=123
06/15/2022 07:28:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=124
06/15/2022 07:28:50 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.47276887871853546 on epoch=124
06/15/2022 07:28:50 - INFO - __main__ - Saving model with best Classification-F1: 0.46803878883831385 -> 0.47276887871853546 on epoch=124, global_step=1000
06/15/2022 07:28:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=126
06/15/2022 07:28:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=127
06/15/2022 07:29:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=128
06/15/2022 07:29:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 07:29:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=131
06/15/2022 07:29:18 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.21845574387947272 on epoch=131
06/15/2022 07:29:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=132
06/15/2022 07:29:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=133
06/15/2022 07:29:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=134
06/15/2022 07:29:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=136
06/15/2022 07:29:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=137
06/15/2022 07:29:46 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.15680473372781065 on epoch=137
06/15/2022 07:29:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=138
06/15/2022 07:29:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=139
06/15/2022 07:29:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.17 on epoch=141
06/15/2022 07:30:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 07:30:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=143
06/15/2022 07:30:13 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.22490922294843865 on epoch=143
06/15/2022 07:30:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=144
06/15/2022 07:30:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=146
06/15/2022 07:30:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=147
06/15/2022 07:30:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=148
06/15/2022 07:30:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=149
06/15/2022 07:30:41 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.213903743315508 on epoch=149
06/15/2022 07:30:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 07:30:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=152
06/15/2022 07:30:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=153
06/15/2022 07:30:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=154
06/15/2022 07:31:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=156
06/15/2022 07:31:08 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.36231884057971014 on epoch=156
06/15/2022 07:31:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=157
06/15/2022 07:31:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=158
06/15/2022 07:31:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=159
06/15/2022 07:31:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=161
06/15/2022 07:31:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=162
06/15/2022 07:31:36 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.3598536244397845 on epoch=162
06/15/2022 07:31:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=163
06/15/2022 07:31:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=164
06/15/2022 07:31:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=166
06/15/2022 07:31:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=167
06/15/2022 07:31:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=168
06/15/2022 07:32:04 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.3298429319371728 on epoch=168
06/15/2022 07:32:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=169
06/15/2022 07:32:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=171
06/15/2022 07:32:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=172
06/15/2022 07:32:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=173
06/15/2022 07:32:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=174
06/15/2022 07:32:32 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.34699792960662523 on epoch=174
06/15/2022 07:32:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=176
06/15/2022 07:32:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=177
06/15/2022 07:32:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=178
06/15/2022 07:32:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=179
06/15/2022 07:32:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=181
06/15/2022 07:33:00 - INFO - __main__ - Global step 1450 Train loss 0.16 Classification-F1 0.4989748369058714 on epoch=181
06/15/2022 07:33:00 - INFO - __main__ - Saving model with best Classification-F1: 0.47276887871853546 -> 0.4989748369058714 on epoch=181, global_step=1450
06/15/2022 07:33:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 07:33:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=183
06/15/2022 07:33:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=184
06/15/2022 07:33:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=186
06/15/2022 07:33:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=187
06/15/2022 07:33:28 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.45705196182396607 on epoch=187
06/15/2022 07:33:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=188
06/15/2022 07:33:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=189
06/15/2022 07:33:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=191
06/15/2022 07:33:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=192
06/15/2022 07:33:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=193
06/15/2022 07:33:56 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.4912820512820513 on epoch=193
06/15/2022 07:34:00 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=194
06/15/2022 07:34:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=196
06/15/2022 07:34:09 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=197
06/15/2022 07:34:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=198
06/15/2022 07:34:18 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=199
06/15/2022 07:34:23 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.5075370545569221 on epoch=199
06/15/2022 07:34:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4989748369058714 -> 0.5075370545569221 on epoch=199, global_step=1600
06/15/2022 07:34:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=201
06/15/2022 07:34:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=202
06/15/2022 07:34:37 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=203
06/15/2022 07:34:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.15 on epoch=204
06/15/2022 07:34:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=206
06/15/2022 07:34:51 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.35434173669467794 on epoch=206
06/15/2022 07:34:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=207
06/15/2022 07:35:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=208
06/15/2022 07:35:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=209
06/15/2022 07:35:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.14 on epoch=211
06/15/2022 07:35:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=212
06/15/2022 07:35:19 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.5376798285889195 on epoch=212
06/15/2022 07:35:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5075370545569221 -> 0.5376798285889195 on epoch=212, global_step=1700
06/15/2022 07:35:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=213
06/15/2022 07:35:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=214
06/15/2022 07:35:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=216
06/15/2022 07:35:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=217
06/15/2022 07:35:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=218
06/15/2022 07:35:47 - INFO - __main__ - Global step 1750 Train loss 0.17 Classification-F1 0.501245608431811 on epoch=218
06/15/2022 07:35:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=219
06/15/2022 07:35:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=221
06/15/2022 07:36:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=222
06/15/2022 07:36:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.16 on epoch=223
06/15/2022 07:36:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=224
06/15/2022 07:36:14 - INFO - __main__ - Global step 1800 Train loss 0.16 Classification-F1 0.5302177636408123 on epoch=224
06/15/2022 07:36:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=226
06/15/2022 07:36:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=227
06/15/2022 07:36:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=228
06/15/2022 07:36:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=229
06/15/2022 07:36:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=231
06/15/2022 07:36:42 - INFO - __main__ - Global step 1850 Train loss 0.13 Classification-F1 0.5255744996293551 on epoch=231
06/15/2022 07:36:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=232
06/15/2022 07:36:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=233
06/15/2022 07:36:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=234
06/15/2022 07:36:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=236
06/15/2022 07:37:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=237
06/15/2022 07:37:09 - INFO - __main__ - Global step 1900 Train loss 0.14 Classification-F1 0.5294117647058825 on epoch=237
06/15/2022 07:37:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=238
06/15/2022 07:37:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=239
06/15/2022 07:37:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=241
06/15/2022 07:37:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=242
06/15/2022 07:37:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=243
06/15/2022 07:37:37 - INFO - __main__ - Global step 1950 Train loss 0.14 Classification-F1 0.5307859583721652 on epoch=243
06/15/2022 07:37:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=244
06/15/2022 07:37:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=246
06/15/2022 07:37:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=247
06/15/2022 07:37:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=248
06/15/2022 07:37:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=249
06/15/2022 07:38:05 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.5148803976390183 on epoch=249
06/15/2022 07:38:09 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.11 on epoch=251
06/15/2022 07:38:14 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.10 on epoch=252
06/15/2022 07:38:18 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.10 on epoch=253
06/15/2022 07:38:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=254
06/15/2022 07:38:27 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.11 on epoch=256
06/15/2022 07:38:33 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.5151515151515151 on epoch=256
06/15/2022 07:38:37 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.13 on epoch=257
06/15/2022 07:38:41 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=258
06/15/2022 07:38:46 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=259
06/15/2022 07:38:50 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=261
06/15/2022 07:38:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.14 on epoch=262
06/15/2022 07:39:00 - INFO - __main__ - Global step 2100 Train loss 0.11 Classification-F1 0.5227092120545265 on epoch=262
06/15/2022 07:39:05 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.09 on epoch=263
06/15/2022 07:39:09 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=264
06/15/2022 07:39:14 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.09 on epoch=266
06/15/2022 07:39:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.11 on epoch=267
06/15/2022 07:39:23 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.13 on epoch=268
06/15/2022 07:39:28 - INFO - __main__ - Global step 2150 Train loss 0.10 Classification-F1 0.4740190880169671 on epoch=268
06/15/2022 07:39:32 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=269
06/15/2022 07:39:37 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=271
06/15/2022 07:39:41 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=272
06/15/2022 07:39:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=273
06/15/2022 07:39:50 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.09 on epoch=274
06/15/2022 07:39:56 - INFO - __main__ - Global step 2200 Train loss 0.10 Classification-F1 0.53125 on epoch=274
06/15/2022 07:40:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.10 on epoch=276
06/15/2022 07:40:05 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=277
06/15/2022 07:40:09 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=278
06/15/2022 07:40:14 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.08 on epoch=279
06/15/2022 07:40:18 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.10 on epoch=281
06/15/2022 07:40:24 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.34873949579831937 on epoch=281
06/15/2022 07:40:28 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.14 on epoch=282
06/15/2022 07:40:33 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=283
06/15/2022 07:40:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.07 on epoch=284
06/15/2022 07:40:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.07 on epoch=286
06/15/2022 07:40:46 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=287
06/15/2022 07:40:51 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.497651675995625 on epoch=287
06/15/2022 07:40:56 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.10 on epoch=288
06/15/2022 07:41:00 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=289
06/15/2022 07:41:05 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=291
06/15/2022 07:41:09 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.06 on epoch=292
06/15/2022 07:41:14 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.11 on epoch=293
06/15/2022 07:41:19 - INFO - __main__ - Global step 2350 Train loss 0.07 Classification-F1 0.5126504544338 on epoch=293
06/15/2022 07:41:24 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.06 on epoch=294
06/15/2022 07:41:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.08 on epoch=296
06/15/2022 07:41:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.06 on epoch=297
06/15/2022 07:41:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=298
06/15/2022 07:41:42 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.05 on epoch=299
06/15/2022 07:41:47 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.5205373288555928 on epoch=299
06/15/2022 07:41:51 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.04 on epoch=301
06/15/2022 07:41:56 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=302
06/15/2022 07:42:00 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.09 on epoch=303
06/15/2022 07:42:05 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.11 on epoch=304
06/15/2022 07:42:09 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=306
06/15/2022 07:42:15 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.5464320625610948 on epoch=306
06/15/2022 07:42:15 - INFO - __main__ - Saving model with best Classification-F1: 0.5376798285889195 -> 0.5464320625610948 on epoch=306, global_step=2450
06/15/2022 07:42:19 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=307
06/15/2022 07:42:24 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=308
06/15/2022 07:42:28 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.09 on epoch=309
06/15/2022 07:42:33 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.10 on epoch=311
06/15/2022 07:42:37 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=312
06/15/2022 07:42:43 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.5155067155067155 on epoch=312
06/15/2022 07:42:47 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.06 on epoch=313
06/15/2022 07:42:52 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=314
06/15/2022 07:42:56 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=316
06/15/2022 07:43:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.04 on epoch=317
06/15/2022 07:43:05 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=318
06/15/2022 07:43:11 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.5330817610062892 on epoch=318
06/15/2022 07:43:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.08 on epoch=319
06/15/2022 07:43:20 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.04 on epoch=321
06/15/2022 07:43:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.03 on epoch=322
06/15/2022 07:43:29 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.05 on epoch=323
06/15/2022 07:43:33 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.03 on epoch=324
06/15/2022 07:43:39 - INFO - __main__ - Global step 2600 Train loss 0.05 Classification-F1 0.5220079583715946 on epoch=324
06/15/2022 07:43:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.06 on epoch=326
06/15/2022 07:43:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.01 on epoch=327
06/15/2022 07:43:52 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.04 on epoch=328
06/15/2022 07:43:57 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=329
06/15/2022 07:44:01 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.04 on epoch=331
06/15/2022 07:44:07 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.5166666666666666 on epoch=331
06/15/2022 07:44:11 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=332
06/15/2022 07:44:16 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=333
06/15/2022 07:44:20 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.02 on epoch=334
06/15/2022 07:44:24 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.01 on epoch=336
06/15/2022 07:44:29 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.04 on epoch=337
06/15/2022 07:44:35 - INFO - __main__ - Global step 2700 Train loss 0.03 Classification-F1 0.4998778998778999 on epoch=337
06/15/2022 07:44:39 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.03 on epoch=338
06/15/2022 07:44:44 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=339
06/15/2022 07:44:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.02 on epoch=341
06/15/2022 07:44:53 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.03 on epoch=342
06/15/2022 07:44:57 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.04 on epoch=343
06/15/2022 07:45:03 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.4998778998778999 on epoch=343
06/15/2022 07:45:07 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.05 on epoch=344
06/15/2022 07:45:12 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.02 on epoch=346
06/15/2022 07:45:16 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=347
06/15/2022 07:45:21 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.03 on epoch=348
06/15/2022 07:45:25 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.04 on epoch=349
06/15/2022 07:45:31 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.5198917788845847 on epoch=349
06/15/2022 07:45:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=351
06/15/2022 07:45:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.04 on epoch=352
06/15/2022 07:45:44 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.05 on epoch=353
06/15/2022 07:45:49 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.01 on epoch=354
06/15/2022 07:45:53 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.01 on epoch=356
06/15/2022 07:45:59 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.47653054996032473 on epoch=356
06/15/2022 07:46:03 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=357
06/15/2022 07:46:08 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=358
06/15/2022 07:46:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.02 on epoch=359
06/15/2022 07:46:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=361
06/15/2022 07:46:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=362
06/15/2022 07:46:27 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.4980392156862745 on epoch=362
06/15/2022 07:46:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=363
06/15/2022 07:46:36 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.09 on epoch=364
06/15/2022 07:46:41 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.02 on epoch=366
06/15/2022 07:46:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=367
06/15/2022 07:46:50 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=368
06/15/2022 07:46:55 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.4913907284768212 on epoch=368
06/15/2022 07:47:00 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=369
06/15/2022 07:47:04 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.05 on epoch=371
06/15/2022 07:47:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=372
06/15/2022 07:47:13 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.01 on epoch=373
06/15/2022 07:47:17 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.03 on epoch=374
06/15/2022 07:47:19 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:47:19 - INFO - __main__ - Printing 3 examples
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:47:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:47:19 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 07:47:19 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:47:19 - INFO - __main__ - Printing 3 examples
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 07:47:19 - INFO - __main__ - ['refuted']
06/15/2022 07:47:19 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:47:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:47:20 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 07:47:23 - INFO - __main__ - Global step 3000 Train loss 0.03 Classification-F1 0.5053671103477888 on epoch=374
06/15/2022 07:47:23 - INFO - __main__ - save last model!
06/15/2022 07:47:23 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 07:47:23 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 07:47:23 - INFO - __main__ - Printing 3 examples
06/15/2022 07:47:23 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:47:23 - INFO - __main__ - ['entailed']
06/15/2022 07:47:23 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:47:23 - INFO - __main__ - ['entailed']
06/15/2022 07:47:23 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 07:47:23 - INFO - __main__ - ['entailed']
06/15/2022 07:47:23 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:47:35 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 07:47:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 07:47:36 - INFO - __main__ - Starting training!
06/15/2022 07:47:48 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:48:01 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 07:56:39 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_21_0.2_8_predictions.txt
06/15/2022 07:56:39 - INFO - __main__ - Classification-F1 on test data: 0.1629
06/15/2022 07:56:40 - INFO - __main__ - prefix=tab_fact_64_21, lr=0.2, bsz=8, dev_performance=0.5464320625610948, test_performance=0.1629202895021664
06/15/2022 07:56:40 - INFO - __main__ - Running ... prefix=tab_fact_64_42, lr=0.5, bsz=8 ...
06/15/2022 07:56:41 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:56:41 - INFO - __main__ - Printing 3 examples
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:56:41 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:56:41 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 07:56:41 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 07:56:41 - INFO - __main__ - Printing 3 examples
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 07:56:41 - INFO - __main__ - ['refuted']
06/15/2022 07:56:41 - INFO - __main__ - Tokenizing Input ...
06/15/2022 07:56:41 - INFO - __main__ - Tokenizing Output ...
06/15/2022 07:56:41 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 07:56:57 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 07:56:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 07:56:58 - INFO - __main__ - Starting training!
06/15/2022 07:57:03 - INFO - __main__ - Step 10 Global step 10 Train loss 2.53 on epoch=1
06/15/2022 07:57:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=2
06/15/2022 07:57:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.35 on epoch=3
06/15/2022 07:57:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=4
06/15/2022 07:57:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=6
06/15/2022 07:57:27 - INFO - __main__ - Global step 50 Train loss 0.82 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 07:57:27 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 07:57:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=7
06/15/2022 07:57:36 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=8
06/15/2022 07:57:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=9
06/15/2022 07:57:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=11
06/15/2022 07:57:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=12
06/15/2022 07:57:54 - INFO - __main__ - Global step 100 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 07:57:59 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 07:58:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=14
06/15/2022 07:58:07 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=16
06/15/2022 07:58:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=17
06/15/2022 07:58:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=18
06/15/2022 07:58:21 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 07:58:26 - INFO - __main__ - Step 160 Global step 160 Train loss 0.21 on epoch=19
06/15/2022 07:58:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=21
06/15/2022 07:58:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=22
06/15/2022 07:58:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=23
06/15/2022 07:58:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=24
06/15/2022 07:58:49 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 07:58:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=26
06/15/2022 07:58:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=27
06/15/2022 07:59:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=28
06/15/2022 07:59:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=29
06/15/2022 07:59:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=31
06/15/2022 07:59:17 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 07:59:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=32
06/15/2022 07:59:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=33
06/15/2022 07:59:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=34
06/15/2022 07:59:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=36
06/15/2022 07:59:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=37
06/15/2022 07:59:45 - INFO - __main__ - Global step 300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 07:59:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=38
06/15/2022 07:59:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=39
06/15/2022 07:59:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=41
06/15/2022 08:00:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=42
06/15/2022 08:00:07 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=43
06/15/2022 08:00:12 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 08:00:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=44
06/15/2022 08:00:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.19 on epoch=46
06/15/2022 08:00:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=47
06/15/2022 08:00:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=48
06/15/2022 08:00:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=49
06/15/2022 08:00:40 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 08:00:45 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=51
06/15/2022 08:00:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=52
06/15/2022 08:00:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=53
06/15/2022 08:00:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=54
06/15/2022 08:01:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 08:01:08 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 08:01:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=57
06/15/2022 08:01:17 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=58
06/15/2022 08:01:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 08:01:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=61
06/15/2022 08:01:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=62
06/15/2022 08:01:35 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 08:01:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=63
06/15/2022 08:01:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=64
06/15/2022 08:01:49 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=66
06/15/2022 08:01:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 08:01:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 08:02:03 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.350463149416029 on epoch=68
06/15/2022 08:02:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.350463149416029 on epoch=68, global_step=550
06/15/2022 08:02:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=69
06/15/2022 08:02:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=71
06/15/2022 08:02:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=72
06/15/2022 08:02:21 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=73
06/15/2022 08:02:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 08:02:31 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 08:02:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=76
06/15/2022 08:02:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=77
06/15/2022 08:02:45 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=78
06/15/2022 08:02:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=79
06/15/2022 08:02:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=81
06/15/2022 08:02:59 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.27899846704138986 on epoch=81
06/15/2022 08:03:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=82
06/15/2022 08:03:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=83
06/15/2022 08:03:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 08:03:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 08:03:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=87
06/15/2022 08:03:27 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 08:03:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=88
06/15/2022 08:03:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 08:03:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=91
06/15/2022 08:03:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=92
06/15/2022 08:03:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=93
06/15/2022 08:03:55 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.2250823590207351 on epoch=93
06/15/2022 08:04:00 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=94
06/15/2022 08:04:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=96
06/15/2022 08:04:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=97
06/15/2022 08:04:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=98
06/15/2022 08:04:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=99
06/15/2022 08:04:23 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.34296770117665637 on epoch=99
06/15/2022 08:04:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=101
06/15/2022 08:04:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=102
06/15/2022 08:04:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=103
06/15/2022 08:04:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=104
06/15/2022 08:04:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=106
06/15/2022 08:04:51 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 08:04:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=107
06/15/2022 08:05:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=108
06/15/2022 08:05:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=109
06/15/2022 08:05:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=111
06/15/2022 08:05:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=112
06/15/2022 08:05:20 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.33917396745932415 on epoch=112
06/15/2022 08:05:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 08:05:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.22 on epoch=114
06/15/2022 08:05:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 08:05:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=117
06/15/2022 08:05:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=118
06/15/2022 08:05:48 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.4714714714714715 on epoch=118
06/15/2022 08:05:48 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.4714714714714715 on epoch=118, global_step=950
06/15/2022 08:05:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 08:05:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=121
06/15/2022 08:06:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=122
06/15/2022 08:06:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 08:06:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 08:06:16 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.35518871580252653 on epoch=124
06/15/2022 08:06:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=126
06/15/2022 08:06:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 08:06:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=128
06/15/2022 08:06:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=129
06/15/2022 08:06:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=131
06/15/2022 08:06:44 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3860677578987438 on epoch=131
06/15/2022 08:06:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=132
06/15/2022 08:06:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=133
06/15/2022 08:06:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=134
06/15/2022 08:07:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.18 on epoch=136
06/15/2022 08:07:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.21 on epoch=137
06/15/2022 08:07:13 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.4460264692968701 on epoch=137
06/15/2022 08:07:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=138
06/15/2022 08:07:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=139
06/15/2022 08:07:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=141
06/15/2022 08:07:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=142
06/15/2022 08:07:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=143
06/15/2022 08:07:41 - INFO - __main__ - Global step 1150 Train loss 0.19 Classification-F1 0.47813194959229055 on epoch=143
06/15/2022 08:07:41 - INFO - __main__ - Saving model with best Classification-F1: 0.4714714714714715 -> 0.47813194959229055 on epoch=143, global_step=1150
06/15/2022 08:07:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=144
06/15/2022 08:07:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=146
06/15/2022 08:07:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=147
06/15/2022 08:07:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=148
06/15/2022 08:08:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=149
06/15/2022 08:08:09 - INFO - __main__ - Global step 1200 Train loss 0.18 Classification-F1 0.48074642701117143 on epoch=149
06/15/2022 08:08:09 - INFO - __main__ - Saving model with best Classification-F1: 0.47813194959229055 -> 0.48074642701117143 on epoch=149, global_step=1200
06/15/2022 08:08:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=151
06/15/2022 08:08:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=152
06/15/2022 08:08:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=153
06/15/2022 08:08:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.17 on epoch=154
06/15/2022 08:08:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.19 on epoch=156
06/15/2022 08:08:37 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.4167176766646263 on epoch=156
06/15/2022 08:08:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=157
06/15/2022 08:08:46 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=158
06/15/2022 08:08:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=159
06/15/2022 08:08:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=161
06/15/2022 08:08:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=162
06/15/2022 08:09:05 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.5141221128482274 on epoch=162
06/15/2022 08:09:05 - INFO - __main__ - Saving model with best Classification-F1: 0.48074642701117143 -> 0.5141221128482274 on epoch=162, global_step=1300
06/15/2022 08:09:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=163
06/15/2022 08:09:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.17 on epoch=164
06/15/2022 08:09:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=166
06/15/2022 08:09:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=167
06/15/2022 08:09:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=168
06/15/2022 08:09:34 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.467580132126254 on epoch=168
06/15/2022 08:09:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=169
06/15/2022 08:09:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=171
06/15/2022 08:09:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=172
06/15/2022 08:09:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=173
06/15/2022 08:09:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=174
06/15/2022 08:10:02 - INFO - __main__ - Global step 1400 Train loss 0.17 Classification-F1 0.32418799212598426 on epoch=174
06/15/2022 08:10:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=176
06/15/2022 08:10:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 08:10:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=178
06/15/2022 08:10:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=179
06/15/2022 08:10:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=181
06/15/2022 08:10:30 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.4746686402475947 on epoch=181
06/15/2022 08:10:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 08:10:39 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=183
06/15/2022 08:10:44 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=184
06/15/2022 08:10:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=186
06/15/2022 08:10:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=187
06/15/2022 08:10:59 - INFO - __main__ - Global step 1500 Train loss 0.17 Classification-F1 0.2894409937888199 on epoch=187
06/15/2022 08:11:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=188
06/15/2022 08:11:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=189
06/15/2022 08:11:12 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=191
06/15/2022 08:11:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=192
06/15/2022 08:11:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=193
06/15/2022 08:11:27 - INFO - __main__ - Global step 1550 Train loss 0.16 Classification-F1 0.5184705519580636 on epoch=193
06/15/2022 08:11:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5141221128482274 -> 0.5184705519580636 on epoch=193, global_step=1550
06/15/2022 08:11:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=194
06/15/2022 08:11:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=196
06/15/2022 08:11:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=197
06/15/2022 08:11:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=198
06/15/2022 08:11:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=199
06/15/2022 08:11:55 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.50633608815427 on epoch=199
06/15/2022 08:12:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.16 on epoch=201
06/15/2022 08:12:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.15 on epoch=202
06/15/2022 08:12:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=203
06/15/2022 08:12:13 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=204
06/15/2022 08:12:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.14 on epoch=206
06/15/2022 08:12:24 - INFO - __main__ - Global step 1650 Train loss 0.15 Classification-F1 0.4812085482682388 on epoch=206
06/15/2022 08:12:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=207
06/15/2022 08:12:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=208
06/15/2022 08:12:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.16 on epoch=209
06/15/2022 08:12:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=211
06/15/2022 08:12:46 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=212
06/15/2022 08:12:52 - INFO - __main__ - Global step 1700 Train loss 0.13 Classification-F1 0.5127004930412533 on epoch=212
06/15/2022 08:12:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=213
06/15/2022 08:13:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=214
06/15/2022 08:13:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=216
06/15/2022 08:13:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=217
06/15/2022 08:13:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=218
06/15/2022 08:13:20 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.48424908424908425 on epoch=218
06/15/2022 08:13:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=219
06/15/2022 08:13:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=221
06/15/2022 08:13:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=222
06/15/2022 08:13:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=223
06/15/2022 08:13:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=224
06/15/2022 08:13:49 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.4606412213740458 on epoch=224
06/15/2022 08:13:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=226
06/15/2022 08:13:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=227
06/15/2022 08:14:02 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=228
06/15/2022 08:14:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=229
06/15/2022 08:14:11 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.11 on epoch=231
06/15/2022 08:14:17 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.4749923477196204 on epoch=231
06/15/2022 08:14:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=232
06/15/2022 08:14:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.13 on epoch=233
06/15/2022 08:14:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=234
06/15/2022 08:14:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=236
06/15/2022 08:14:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=237
06/15/2022 08:14:46 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.5220079583715946 on epoch=237
06/15/2022 08:14:46 - INFO - __main__ - Saving model with best Classification-F1: 0.5184705519580636 -> 0.5220079583715946 on epoch=237, global_step=1900
06/15/2022 08:14:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=238
06/15/2022 08:14:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=239
06/15/2022 08:14:59 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=241
06/15/2022 08:15:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=242
06/15/2022 08:15:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=243
06/15/2022 08:15:14 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.5450980392156863 on epoch=243
06/15/2022 08:15:14 - INFO - __main__ - Saving model with best Classification-F1: 0.5220079583715946 -> 0.5450980392156863 on epoch=243, global_step=1950
06/15/2022 08:15:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=244
06/15/2022 08:15:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=246
06/15/2022 08:15:27 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=247
06/15/2022 08:15:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=248
06/15/2022 08:15:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=249
06/15/2022 08:15:42 - INFO - __main__ - Global step 2000 Train loss 0.10 Classification-F1 0.5184705519580636 on epoch=249
06/15/2022 08:15:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=251
06/15/2022 08:15:51 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.12 on epoch=252
06/15/2022 08:15:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.11 on epoch=253
06/15/2022 08:16:00 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.11 on epoch=254
06/15/2022 08:16:05 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=256
06/15/2022 08:16:11 - INFO - __main__ - Global step 2050 Train loss 0.11 Classification-F1 0.5207817754933689 on epoch=256
06/15/2022 08:16:15 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=257
06/15/2022 08:16:20 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.08 on epoch=258
06/15/2022 08:16:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.10 on epoch=259
06/15/2022 08:16:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.08 on epoch=261
06/15/2022 08:16:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.11 on epoch=262
06/15/2022 08:16:40 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.32001990792584295 on epoch=262
06/15/2022 08:16:44 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.10 on epoch=263
06/15/2022 08:16:49 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.10 on epoch=264
06/15/2022 08:16:53 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.06 on epoch=266
06/15/2022 08:16:57 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.05 on epoch=267
06/15/2022 08:17:02 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.07 on epoch=268
06/15/2022 08:17:08 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.24893108651911472 on epoch=268
06/15/2022 08:17:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.09 on epoch=269
06/15/2022 08:17:17 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.07 on epoch=271
06/15/2022 08:17:22 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=272
06/15/2022 08:17:26 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.09 on epoch=273
06/15/2022 08:17:31 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=274
06/15/2022 08:17:37 - INFO - __main__ - Global step 2200 Train loss 0.09 Classification-F1 0.4832395400048935 on epoch=274
06/15/2022 08:17:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.07 on epoch=276
06/15/2022 08:17:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.06 on epoch=277
06/15/2022 08:17:50 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.08 on epoch=278
06/15/2022 08:17:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=279
06/15/2022 08:17:59 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.07 on epoch=281
06/15/2022 08:18:05 - INFO - __main__ - Global step 2250 Train loss 0.07 Classification-F1 0.5035035035035035 on epoch=281
06/15/2022 08:18:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.08 on epoch=282
06/15/2022 08:18:14 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.07 on epoch=283
06/15/2022 08:18:18 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.09 on epoch=284
06/15/2022 08:18:23 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=286
06/15/2022 08:18:27 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.07 on epoch=287
06/15/2022 08:18:33 - INFO - __main__ - Global step 2300 Train loss 0.07 Classification-F1 0.3493827160493827 on epoch=287
06/15/2022 08:18:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.09 on epoch=288
06/15/2022 08:18:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.05 on epoch=289
06/15/2022 08:18:47 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.07 on epoch=291
06/15/2022 08:18:51 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.07 on epoch=292
06/15/2022 08:18:55 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.13 on epoch=293
06/15/2022 08:19:02 - INFO - __main__ - Global step 2350 Train loss 0.08 Classification-F1 0.5079365079365079 on epoch=293
06/15/2022 08:19:06 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.10 on epoch=294
06/15/2022 08:19:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=296
06/15/2022 08:19:15 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=297
06/15/2022 08:19:19 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=298
06/15/2022 08:19:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.04 on epoch=299
06/15/2022 08:19:30 - INFO - __main__ - Global step 2400 Train loss 0.07 Classification-F1 0.25590697674418605 on epoch=299
06/15/2022 08:19:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.10 on epoch=301
06/15/2022 08:19:39 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.06 on epoch=302
06/15/2022 08:19:43 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=303
06/15/2022 08:19:48 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.07 on epoch=304
06/15/2022 08:19:52 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.07 on epoch=306
06/15/2022 08:19:58 - INFO - __main__ - Global step 2450 Train loss 0.07 Classification-F1 0.48424908424908425 on epoch=306
06/15/2022 08:20:03 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.13 on epoch=307
06/15/2022 08:20:07 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.07 on epoch=308
06/15/2022 08:20:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=309
06/15/2022 08:20:16 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=311
06/15/2022 08:20:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.03 on epoch=312
06/15/2022 08:20:27 - INFO - __main__ - Global step 2500 Train loss 0.07 Classification-F1 0.46875 on epoch=312
06/15/2022 08:20:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=313
06/15/2022 08:20:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.06 on epoch=314
06/15/2022 08:20:40 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.05 on epoch=316
06/15/2022 08:20:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.05 on epoch=317
06/15/2022 08:20:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.07 on epoch=318
06/15/2022 08:20:55 - INFO - __main__ - Global step 2550 Train loss 0.05 Classification-F1 0.49556650246305417 on epoch=318
06/15/2022 08:20:59 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=319
06/15/2022 08:21:04 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.06 on epoch=321
06/15/2022 08:21:08 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=322
06/15/2022 08:21:13 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=323
06/15/2022 08:21:17 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=324
06/15/2022 08:21:23 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.1923593185338152 on epoch=324
06/15/2022 08:21:28 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=326
06/15/2022 08:21:32 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.04 on epoch=327
06/15/2022 08:21:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.06 on epoch=328
06/15/2022 08:21:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.07 on epoch=329
06/15/2022 08:21:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.06 on epoch=331
06/15/2022 08:21:52 - INFO - __main__ - Global step 2650 Train loss 0.06 Classification-F1 0.3398540026246719 on epoch=331
06/15/2022 08:21:57 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.02 on epoch=332
06/15/2022 08:22:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.04 on epoch=333
06/15/2022 08:22:05 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.03 on epoch=334
06/15/2022 08:22:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.08 on epoch=336
06/15/2022 08:22:14 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=337
06/15/2022 08:22:20 - INFO - __main__ - Global step 2700 Train loss 0.04 Classification-F1 0.5227092120545265 on epoch=337
06/15/2022 08:22:25 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.04 on epoch=338
06/15/2022 08:22:29 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.04 on epoch=339
06/15/2022 08:22:34 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.05 on epoch=341
06/15/2022 08:22:38 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.02 on epoch=342
06/15/2022 08:22:43 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=343
06/15/2022 08:22:49 - INFO - __main__ - Global step 2750 Train loss 0.04 Classification-F1 0.523175572519084 on epoch=343
06/15/2022 08:22:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.02 on epoch=344
06/15/2022 08:22:58 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.03 on epoch=346
06/15/2022 08:23:02 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.02 on epoch=347
06/15/2022 08:23:07 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=348
06/15/2022 08:23:11 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.06 on epoch=349
06/15/2022 08:23:18 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.5220079583715946 on epoch=349
06/15/2022 08:23:22 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.03 on epoch=351
06/15/2022 08:23:26 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=352
06/15/2022 08:23:31 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.02 on epoch=353
06/15/2022 08:23:35 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.02 on epoch=354
06/15/2022 08:23:40 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.02 on epoch=356
06/15/2022 08:23:46 - INFO - __main__ - Global step 2850 Train loss 0.02 Classification-F1 0.32839932568562147 on epoch=356
06/15/2022 08:23:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=357
06/15/2022 08:23:55 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.05 on epoch=358
06/15/2022 08:23:59 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=359
06/15/2022 08:24:04 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.03 on epoch=361
06/15/2022 08:24:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.02 on epoch=362
06/15/2022 08:24:14 - INFO - __main__ - Global step 2900 Train loss 0.03 Classification-F1 0.3382407521959669 on epoch=362
06/15/2022 08:24:19 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.03 on epoch=363
06/15/2022 08:24:23 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=364
06/15/2022 08:24:28 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.03 on epoch=366
06/15/2022 08:24:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.02 on epoch=367
06/15/2022 08:24:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.02 on epoch=368
06/15/2022 08:24:43 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.4941980806623596 on epoch=368
06/15/2022 08:24:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.01 on epoch=369
06/15/2022 08:24:52 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.03 on epoch=371
06/15/2022 08:24:56 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.02 on epoch=372
06/15/2022 08:25:01 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.02 on epoch=373
06/15/2022 08:25:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.01 on epoch=374
06/15/2022 08:25:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 08:25:06 - INFO - __main__ - Printing 3 examples
06/15/2022 08:25:06 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 08:25:06 - INFO - __main__ - ['refuted']
06/15/2022 08:25:06 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 08:25:06 - INFO - __main__ - ['refuted']
06/15/2022 08:25:06 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 08:25:06 - INFO - __main__ - ['refuted']
06/15/2022 08:25:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 08:25:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 08:25:07 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 08:25:07 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 08:25:07 - INFO - __main__ - Printing 3 examples
06/15/2022 08:25:07 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 08:25:07 - INFO - __main__ - ['refuted']
06/15/2022 08:25:07 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 08:25:07 - INFO - __main__ - ['refuted']
06/15/2022 08:25:07 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 08:25:07 - INFO - __main__ - ['refuted']
06/15/2022 08:25:07 - INFO - __main__ - Tokenizing Input ...
06/15/2022 08:25:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 08:25:07 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 08:25:11 - INFO - __main__ - Global step 3000 Train loss 0.02 Classification-F1 0.3485838779956427 on epoch=374
06/15/2022 08:25:11 - INFO - __main__ - save last model!
06/15/2022 08:25:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 08:25:11 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 08:25:11 - INFO - __main__ - Printing 3 examples
06/15/2022 08:25:11 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 08:25:11 - INFO - __main__ - ['entailed']
06/15/2022 08:25:11 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 08:25:11 - INFO - __main__ - ['entailed']
06/15/2022 08:25:11 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 08:25:11 - INFO - __main__ - ['entailed']
06/15/2022 08:25:11 - INFO - __main__ - Tokenizing Input ...
06/15/2022 08:25:23 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 08:25:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 08:25:23 - INFO - __main__ - Starting training!
06/15/2022 08:25:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 08:25:48 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 08:34:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_42_0.5_8_predictions.txt
06/15/2022 08:34:29 - INFO - __main__ - Classification-F1 on test data: 0.0611
06/15/2022 08:34:29 - INFO - __main__ - prefix=tab_fact_64_42, lr=0.5, bsz=8, dev_performance=0.5450980392156863, test_performance=0.061115500486054145
06/15/2022 08:34:29 - INFO - __main__ - Running ... prefix=tab_fact_64_42, lr=0.4, bsz=8 ...
06/15/2022 08:34:30 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 08:34:30 - INFO - __main__ - Printing 3 examples
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ - Tokenizing Input ...
06/15/2022 08:34:30 - INFO - __main__ - Tokenizing Output ...
06/15/2022 08:34:30 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 08:34:30 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 08:34:30 - INFO - __main__ - Printing 3 examples
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 08:34:30 - INFO - __main__ - ['refuted']
06/15/2022 08:34:30 - INFO - __main__ - Tokenizing Input ...
06/15/2022 08:34:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 08:34:31 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 08:34:49 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 08:34:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 08:34:50 - INFO - __main__ - Starting training!
06/15/2022 08:34:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.86 on epoch=1
06/15/2022 08:35:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=2
06/15/2022 08:35:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=3
06/15/2022 08:35:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=4
06/15/2022 08:35:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=6
06/15/2022 08:35:18 - INFO - __main__ - Global step 50 Train loss 0.91 Classification-F1 0.3727353727353727 on epoch=6
06/15/2022 08:35:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3727353727353727 on epoch=6, global_step=50
06/15/2022 08:35:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=7
06/15/2022 08:35:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=8
06/15/2022 08:35:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=9
06/15/2022 08:35:36 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=11
06/15/2022 08:35:41 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=12
06/15/2022 08:35:46 - INFO - __main__ - Global step 100 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 08:35:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=13
06/15/2022 08:35:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=14
06/15/2022 08:35:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=16
06/15/2022 08:36:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=17
06/15/2022 08:36:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=18
06/15/2022 08:36:13 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.2648955306489553 on epoch=18
06/15/2022 08:36:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=19
06/15/2022 08:36:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 08:36:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=22
06/15/2022 08:36:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=23
06/15/2022 08:36:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=24
06/15/2022 08:36:41 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 08:36:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=26
06/15/2022 08:36:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=27
06/15/2022 08:36:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=28
06/15/2022 08:36:58 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=29
06/15/2022 08:37:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=31
06/15/2022 08:37:09 - INFO - __main__ - Global step 250 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 08:37:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=32
06/15/2022 08:37:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=33
06/15/2022 08:37:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=34
06/15/2022 08:37:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 08:37:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=37
06/15/2022 08:37:37 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 08:37:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=38
06/15/2022 08:37:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=39
06/15/2022 08:37:50 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=41
06/15/2022 08:37:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=42
06/15/2022 08:37:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=43
06/15/2022 08:38:05 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 08:38:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=44
06/15/2022 08:38:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.20 on epoch=46
06/15/2022 08:38:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=47
06/15/2022 08:38:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=48
06/15/2022 08:38:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=49
06/15/2022 08:38:32 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 08:38:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=51
06/15/2022 08:38:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.21 on epoch=52
06/15/2022 08:38:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=53
06/15/2022 08:38:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=54
06/15/2022 08:38:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=56
06/15/2022 08:39:00 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 08:39:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=57
06/15/2022 08:39:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=58
06/15/2022 08:39:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 08:39:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=61
06/15/2022 08:39:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=62
06/15/2022 08:39:28 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.473972602739726 on epoch=62
06/15/2022 08:39:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3727353727353727 -> 0.473972602739726 on epoch=62, global_step=500
06/15/2022 08:39:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=63
06/15/2022 08:39:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=64
06/15/2022 08:39:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 08:39:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=67
06/15/2022 08:39:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 08:39:56 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.22456140350877193 on epoch=68
06/15/2022 08:40:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=69
06/15/2022 08:40:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 08:40:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=72
06/15/2022 08:40:14 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=73
06/15/2022 08:40:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=74
06/15/2022 08:40:24 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 08:40:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 08:40:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=77
06/15/2022 08:40:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=78
06/15/2022 08:40:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=79
06/15/2022 08:40:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=81
06/15/2022 08:40:52 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 08:40:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=82
06/15/2022 08:41:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=83
06/15/2022 08:41:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=84
06/15/2022 08:41:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=86
06/15/2022 08:41:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=87
06/15/2022 08:41:20 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 08:41:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=88
06/15/2022 08:41:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=89
06/15/2022 08:41:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=91
06/15/2022 08:41:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=92
06/15/2022 08:41:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=93
06/15/2022 08:41:48 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.34673046251993617 on epoch=93
06/15/2022 08:41:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=94
06/15/2022 08:41:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=96
06/15/2022 08:42:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=97
06/15/2022 08:42:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=98
06/15/2022 08:42:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 08:42:16 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 08:42:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 08:42:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 08:42:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=103
06/15/2022 08:42:34 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=104
06/15/2022 08:42:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=106
06/15/2022 08:42:44 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 08:42:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=107
06/15/2022 08:42:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=108
06/15/2022 08:42:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.20 on epoch=109
06/15/2022 08:43:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=111
06/15/2022 08:43:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 08:43:12 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 08:43:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=113
06/15/2022 08:43:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=114
06/15/2022 08:43:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=116
06/15/2022 08:43:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=117
06/15/2022 08:43:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 08:43:40 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 08:43:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=119
06/15/2022 08:43:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 08:43:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=122
06/15/2022 08:43:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 08:44:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=124
06/15/2022 08:44:08 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 08:44:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=126
06/15/2022 08:44:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=127
06/15/2022 08:44:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=128
06/15/2022 08:44:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 08:44:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=131
06/15/2022 08:44:36 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 08:44:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=132
06/15/2022 08:44:45 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=133
06/15/2022 08:44:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=134
06/15/2022 08:44:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 08:44:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=137
06/15/2022 08:45:04 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=137
06/15/2022 08:45:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=138
06/15/2022 08:45:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=139
06/15/2022 08:45:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=141
06/15/2022 08:45:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=142
06/15/2022 08:45:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=143
06/15/2022 08:45:32 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.39636200314394787 on epoch=143
06/15/2022 08:45:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=144
06/15/2022 08:45:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=146
06/15/2022 08:45:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=147
06/15/2022 08:45:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=148
06/15/2022 08:45:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=149
06/15/2022 08:46:00 - INFO - __main__ - Global step 1200 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=149
06/15/2022 08:46:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.19 on epoch=151
06/15/2022 08:46:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=152
06/15/2022 08:46:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=153
06/15/2022 08:46:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=154
06/15/2022 08:46:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=156
06/15/2022 08:46:28 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=156
06/15/2022 08:46:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=157
06/15/2022 08:46:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=158
06/15/2022 08:46:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=159
06/15/2022 08:46:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=161
06/15/2022 08:46:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=162
06/15/2022 08:46:56 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.350463149416029 on epoch=162
06/15/2022 08:47:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=163
06/15/2022 08:47:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=164
06/15/2022 08:47:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=166
06/15/2022 08:47:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=167
06/15/2022 08:47:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=168
06/15/2022 08:47:24 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.4167176766646263 on epoch=168
06/15/2022 08:47:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=169
06/15/2022 08:47:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=171
06/15/2022 08:47:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=172
06/15/2022 08:47:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=173
06/15/2022 08:47:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=174
06/15/2022 08:47:52 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.44824036543781764 on epoch=174
06/15/2022 08:47:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=176
06/15/2022 08:48:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=177
06/15/2022 08:48:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=178
06/15/2022 08:48:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=179
06/15/2022 08:48:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=181
06/15/2022 08:48:20 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.46843853820598 on epoch=181
06/15/2022 08:48:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=182
06/15/2022 08:48:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=183
06/15/2022 08:48:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=184
06/15/2022 08:48:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=186
06/15/2022 08:48:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=187
06/15/2022 08:48:48 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.4436486072849709 on epoch=187
06/15/2022 08:48:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.21 on epoch=188
06/15/2022 08:48:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=189
06/15/2022 08:49:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=191
06/15/2022 08:49:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=192
06/15/2022 08:49:10 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=193
06/15/2022 08:49:16 - INFO - __main__ - Global step 1550 Train loss 0.20 Classification-F1 0.4046511627906977 on epoch=193
06/15/2022 08:49:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.20 on epoch=194
06/15/2022 08:49:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=196
06/15/2022 08:49:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=197
06/15/2022 08:49:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=198
06/15/2022 08:49:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=199
06/15/2022 08:49:44 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.41329258976317806 on epoch=199
06/15/2022 08:49:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=201
06/15/2022 08:49:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=202
06/15/2022 08:49:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=203
06/15/2022 08:50:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=204
06/15/2022 08:50:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=206
06/15/2022 08:50:12 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.41666666666666663 on epoch=206
06/15/2022 08:50:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=207
06/15/2022 08:50:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=208
06/15/2022 08:50:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.18 on epoch=209
06/15/2022 08:50:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=211
06/15/2022 08:50:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=212
06/15/2022 08:50:40 - INFO - __main__ - Global step 1700 Train loss 0.19 Classification-F1 0.3727353727353727 on epoch=212
06/15/2022 08:50:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=213
06/15/2022 08:50:49 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=214
06/15/2022 08:50:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=216
06/15/2022 08:50:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=217
06/15/2022 08:51:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=218
06/15/2022 08:51:08 - INFO - __main__ - Global step 1750 Train loss 0.18 Classification-F1 0.42680542415641753 on epoch=218
06/15/2022 08:51:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=219
06/15/2022 08:51:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=221
06/15/2022 08:51:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=222
06/15/2022 08:51:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=223
06/15/2022 08:51:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=224
06/15/2022 08:51:37 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.41832473593711617 on epoch=224
06/15/2022 08:51:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=226
06/15/2022 08:51:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=227
06/15/2022 08:51:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.17 on epoch=228
06/15/2022 08:51:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=229
06/15/2022 08:51:59 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.17 on epoch=231
06/15/2022 08:52:05 - INFO - __main__ - Global step 1850 Train loss 0.17 Classification-F1 0.4196078431372549 on epoch=231
06/15/2022 08:52:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=232
06/15/2022 08:52:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=233
06/15/2022 08:52:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=234
06/15/2022 08:52:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=236
06/15/2022 08:52:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=237
06/15/2022 08:52:34 - INFO - __main__ - Global step 1900 Train loss 0.16 Classification-F1 0.4288159422947613 on epoch=237
06/15/2022 08:52:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=238
06/15/2022 08:52:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=239
06/15/2022 08:52:47 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=241
06/15/2022 08:52:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=242
06/15/2022 08:52:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=243
06/15/2022 08:53:02 - INFO - __main__ - Global step 1950 Train loss 0.16 Classification-F1 0.4340456890198968 on epoch=243
06/15/2022 08:53:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=244
06/15/2022 08:53:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=246
06/15/2022 08:53:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=247
06/15/2022 08:53:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=248
06/15/2022 08:53:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=249
06/15/2022 08:53:30 - INFO - __main__ - Global step 2000 Train loss 0.16 Classification-F1 0.44976664210267747 on epoch=249
06/15/2022 08:53:34 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.16 on epoch=251
06/15/2022 08:53:39 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=252
06/15/2022 08:53:43 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.14 on epoch=253
06/15/2022 08:53:48 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.16 on epoch=254
06/15/2022 08:53:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=256
06/15/2022 08:53:58 - INFO - __main__ - Global step 2050 Train loss 0.15 Classification-F1 0.30327263779527563 on epoch=256
06/15/2022 08:54:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.14 on epoch=257
06/15/2022 08:54:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.15 on epoch=258
06/15/2022 08:54:12 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.14 on epoch=259
06/15/2022 08:54:16 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.12 on epoch=261
06/15/2022 08:54:21 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.15 on epoch=262
06/15/2022 08:54:26 - INFO - __main__ - Global step 2100 Train loss 0.14 Classification-F1 0.40274392367156486 on epoch=262
06/15/2022 08:54:31 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.15 on epoch=263
06/15/2022 08:54:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.16 on epoch=264
06/15/2022 08:54:40 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.14 on epoch=266
06/15/2022 08:54:44 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.18 on epoch=267
06/15/2022 08:54:49 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.15 on epoch=268
06/15/2022 08:54:55 - INFO - __main__ - Global step 2150 Train loss 0.15 Classification-F1 0.4362613163689748 on epoch=268
06/15/2022 08:54:59 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.16 on epoch=269
06/15/2022 08:55:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.13 on epoch=271
06/15/2022 08:55:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.12 on epoch=272
06/15/2022 08:55:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.12 on epoch=273
06/15/2022 08:55:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.12 on epoch=274
06/15/2022 08:55:24 - INFO - __main__ - Global step 2200 Train loss 0.13 Classification-F1 0.4460264692968701 on epoch=274
06/15/2022 08:55:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.15 on epoch=276
06/15/2022 08:55:33 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.13 on epoch=277
06/15/2022 08:55:37 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.15 on epoch=278
06/15/2022 08:55:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.11 on epoch=279
06/15/2022 08:55:46 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.16 on epoch=281
06/15/2022 08:55:53 - INFO - __main__ - Global step 2250 Train loss 0.14 Classification-F1 0.45825921609519715 on epoch=281
06/15/2022 08:55:57 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.13 on epoch=282
06/15/2022 08:56:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.12 on epoch=283
06/15/2022 08:56:06 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.12 on epoch=284
06/15/2022 08:56:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.13 on epoch=286
06/15/2022 08:56:15 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.14 on epoch=287
06/15/2022 08:56:21 - INFO - __main__ - Global step 2300 Train loss 0.13 Classification-F1 0.4601136988813497 on epoch=287
06/15/2022 08:56:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.14 on epoch=288
06/15/2022 08:56:30 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.12 on epoch=289
06/15/2022 08:56:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.15 on epoch=291
06/15/2022 08:56:39 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.13 on epoch=292
06/15/2022 08:56:43 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.12 on epoch=293
06/15/2022 08:56:49 - INFO - __main__ - Global step 2350 Train loss 0.13 Classification-F1 0.4714714714714715 on epoch=293
06/15/2022 08:56:54 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.13 on epoch=294
06/15/2022 08:56:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.15 on epoch=296
06/15/2022 08:57:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.11 on epoch=297
06/15/2022 08:57:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.09 on epoch=298
06/15/2022 08:57:12 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.11 on epoch=299
06/15/2022 08:57:18 - INFO - __main__ - Global step 2400 Train loss 0.12 Classification-F1 0.4436486072849709 on epoch=299
06/15/2022 08:57:22 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.12 on epoch=301
06/15/2022 08:57:27 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.08 on epoch=302
06/15/2022 08:57:31 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.14 on epoch=303
06/15/2022 08:57:36 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.14 on epoch=304
06/15/2022 08:57:40 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.09 on epoch=306
06/15/2022 08:57:46 - INFO - __main__ - Global step 2450 Train loss 0.11 Classification-F1 0.45825921609519715 on epoch=306
06/15/2022 08:57:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.08 on epoch=307
06/15/2022 08:57:55 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.11 on epoch=308
06/15/2022 08:57:59 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.12 on epoch=309
06/15/2022 08:58:04 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=311
06/15/2022 08:58:08 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.11 on epoch=312
06/15/2022 08:58:14 - INFO - __main__ - Global step 2500 Train loss 0.10 Classification-F1 0.46548759518545807 on epoch=312
06/15/2022 08:58:19 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.10 on epoch=313
06/15/2022 08:58:23 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.11 on epoch=314
06/15/2022 08:58:28 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.12 on epoch=316
06/15/2022 08:58:32 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.10 on epoch=317
06/15/2022 08:58:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.08 on epoch=318
06/15/2022 08:58:43 - INFO - __main__ - Global step 2550 Train loss 0.10 Classification-F1 0.4682306940371457 on epoch=318
06/15/2022 08:58:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.12 on epoch=319
06/15/2022 08:58:52 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.09 on epoch=321
06/15/2022 08:58:56 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.06 on epoch=322
06/15/2022 08:59:01 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.10 on epoch=323
06/15/2022 08:59:05 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=324
06/15/2022 08:59:11 - INFO - __main__ - Global step 2600 Train loss 0.10 Classification-F1 0.48235294117647054 on epoch=324
06/15/2022 08:59:11 - INFO - __main__ - Saving model with best Classification-F1: 0.473972602739726 -> 0.48235294117647054 on epoch=324, global_step=2600
06/15/2022 08:59:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.12 on epoch=326
06/15/2022 08:59:20 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.09 on epoch=327
06/15/2022 08:59:25 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.11 on epoch=328
06/15/2022 08:59:29 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.10 on epoch=329
06/15/2022 08:59:34 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=331
06/15/2022 08:59:40 - INFO - __main__ - Global step 2650 Train loss 0.10 Classification-F1 0.3133776792313378 on epoch=331
06/15/2022 08:59:44 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.08 on epoch=332
06/15/2022 08:59:49 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.08 on epoch=333
06/15/2022 08:59:53 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.08 on epoch=334
06/15/2022 08:59:58 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.07 on epoch=336
06/15/2022 09:00:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.07 on epoch=337
06/15/2022 09:00:09 - INFO - __main__ - Global step 2700 Train loss 0.08 Classification-F1 0.27233115468409586 on epoch=337
06/15/2022 09:00:13 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=338
06/15/2022 09:00:18 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.10 on epoch=339
06/15/2022 09:00:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=341
06/15/2022 09:00:27 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.07 on epoch=342
06/15/2022 09:00:31 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.10 on epoch=343
06/15/2022 09:00:37 - INFO - __main__ - Global step 2750 Train loss 0.10 Classification-F1 0.436950146627566 on epoch=343
06/15/2022 09:00:42 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.09 on epoch=344
06/15/2022 09:00:46 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=346
06/15/2022 09:00:51 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.07 on epoch=347
06/15/2022 09:00:55 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.07 on epoch=348
06/15/2022 09:01:00 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.10 on epoch=349
06/15/2022 09:01:06 - INFO - __main__ - Global step 2800 Train loss 0.08 Classification-F1 0.4296526887627419 on epoch=349
06/15/2022 09:01:10 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.07 on epoch=351
06/15/2022 09:01:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.06 on epoch=352
06/15/2022 09:01:19 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.10 on epoch=353
06/15/2022 09:01:24 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=354
06/15/2022 09:01:28 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.07 on epoch=356
06/15/2022 09:01:34 - INFO - __main__ - Global step 2850 Train loss 0.08 Classification-F1 0.3123249299719888 on epoch=356
06/15/2022 09:01:39 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.05 on epoch=357
06/15/2022 09:01:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.09 on epoch=358
06/15/2022 09:01:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.08 on epoch=359
06/15/2022 09:01:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.08 on epoch=361
06/15/2022 09:01:56 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.05 on epoch=362
06/15/2022 09:02:02 - INFO - __main__ - Global step 2900 Train loss 0.07 Classification-F1 0.2856612643820364 on epoch=362
06/15/2022 09:02:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.08 on epoch=363
06/15/2022 09:02:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.08 on epoch=364
06/15/2022 09:02:16 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.08 on epoch=366
06/15/2022 09:02:20 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.06 on epoch=367
06/15/2022 09:02:25 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.06 on epoch=368
06/15/2022 09:02:31 - INFO - __main__ - Global step 2950 Train loss 0.07 Classification-F1 0.3137303149606299 on epoch=368
06/15/2022 09:02:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.07 on epoch=369
06/15/2022 09:02:40 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.07 on epoch=371
06/15/2022 09:02:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.04 on epoch=372
06/15/2022 09:02:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.06 on epoch=373
06/15/2022 09:02:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.06 on epoch=374
06/15/2022 09:02:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:02:55 - INFO - __main__ - Printing 3 examples
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:02:55 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:02:55 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 09:02:55 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:02:55 - INFO - __main__ - Printing 3 examples
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 09:02:55 - INFO - __main__ - ['refuted']
06/15/2022 09:02:55 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:02:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:02:56 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 09:03:00 - INFO - __main__ - Global step 3000 Train loss 0.06 Classification-F1 0.2376331811263318 on epoch=374
06/15/2022 09:03:00 - INFO - __main__ - save last model!
06/15/2022 09:03:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 09:03:00 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 09:03:00 - INFO - __main__ - Printing 3 examples
06/15/2022 09:03:00 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:03:00 - INFO - __main__ - ['entailed']
06/15/2022 09:03:00 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:03:01 - INFO - __main__ - ['entailed']
06/15/2022 09:03:01 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:03:01 - INFO - __main__ - ['entailed']
06/15/2022 09:03:01 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:03:14 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 09:03:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 09:03:15 - INFO - __main__ - Starting training!
06/15/2022 09:03:25 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:03:38 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 09:13:04 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_42_0.4_8_predictions.txt
06/15/2022 09:13:04 - INFO - __main__ - Classification-F1 on test data: 0.0736
06/15/2022 09:13:04 - INFO - __main__ - prefix=tab_fact_64_42, lr=0.4, bsz=8, dev_performance=0.48235294117647054, test_performance=0.07359553496158312
06/15/2022 09:13:04 - INFO - __main__ - Running ... prefix=tab_fact_64_42, lr=0.3, bsz=8 ...
06/15/2022 09:13:05 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:13:05 - INFO - __main__ - Printing 3 examples
06/15/2022 09:13:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 09:13:05 - INFO - __main__ - ['refuted']
06/15/2022 09:13:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 09:13:05 - INFO - __main__ - ['refuted']
06/15/2022 09:13:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 09:13:05 - INFO - __main__ - ['refuted']
06/15/2022 09:13:05 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:13:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:13:06 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 09:13:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:13:06 - INFO - __main__ - Printing 3 examples
06/15/2022 09:13:06 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 09:13:06 - INFO - __main__ - ['refuted']
06/15/2022 09:13:06 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 09:13:06 - INFO - __main__ - ['refuted']
06/15/2022 09:13:06 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 09:13:06 - INFO - __main__ - ['refuted']
06/15/2022 09:13:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:13:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:13:06 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 09:13:21 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 09:13:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 09:13:22 - INFO - __main__ - Starting training!
06/15/2022 09:13:27 - INFO - __main__ - Step 10 Global step 10 Train loss 3.39 on epoch=1
06/15/2022 09:13:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.81 on epoch=2
06/15/2022 09:13:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=3
06/15/2022 09:13:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=4
06/15/2022 09:13:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=6
06/15/2022 09:13:50 - INFO - __main__ - Global step 50 Train loss 1.09 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 09:13:50 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 09:13:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.35 on epoch=7
06/15/2022 09:13:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=8
06/15/2022 09:14:04 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=9
06/15/2022 09:14:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=11
06/15/2022 09:14:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=12
06/15/2022 09:14:18 - INFO - __main__ - Global step 100 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 09:14:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=13
06/15/2022 09:14:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 09:14:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=16
06/15/2022 09:14:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=17
06/15/2022 09:14:40 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=18
06/15/2022 09:14:45 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.47276887871853546 on epoch=18
06/15/2022 09:14:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.47276887871853546 on epoch=18, global_step=150
06/15/2022 09:14:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=19
06/15/2022 09:14:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 09:14:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=22
06/15/2022 09:15:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=23
06/15/2022 09:15:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=24
06/15/2022 09:15:13 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 09:15:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=26
06/15/2022 09:15:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=27
06/15/2022 09:15:26 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=28
06/15/2022 09:15:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=29
06/15/2022 09:15:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=31
06/15/2022 09:15:41 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 09:15:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=32
06/15/2022 09:15:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 09:15:54 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=34
06/15/2022 09:15:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=36
06/15/2022 09:16:03 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=37
06/15/2022 09:16:09 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 09:16:13 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=38
06/15/2022 09:16:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=39
06/15/2022 09:16:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=41
06/15/2022 09:16:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=42
06/15/2022 09:16:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=43
06/15/2022 09:16:38 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 09:16:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=44
06/15/2022 09:16:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=46
06/15/2022 09:16:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=47
06/15/2022 09:16:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=48
06/15/2022 09:17:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=49
06/15/2022 09:17:06 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 09:17:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=51
06/15/2022 09:17:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=52
06/15/2022 09:17:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=53
06/15/2022 09:17:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=54
06/15/2022 09:17:29 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=56
06/15/2022 09:17:34 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 09:17:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.20 on epoch=57
06/15/2022 09:17:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=58
06/15/2022 09:17:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=59
06/15/2022 09:17:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=61
06/15/2022 09:17:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 09:18:02 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 09:18:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=63
06/15/2022 09:18:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=64
06/15/2022 09:18:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 09:18:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 09:18:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=68
06/15/2022 09:18:30 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.4919083969465649 on epoch=68
06/15/2022 09:18:30 - INFO - __main__ - Saving model with best Classification-F1: 0.47276887871853546 -> 0.4919083969465649 on epoch=68, global_step=550
06/15/2022 09:18:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=69
06/15/2022 09:18:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=71
06/15/2022 09:18:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=72
06/15/2022 09:18:48 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=73
06/15/2022 09:18:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 09:18:58 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 09:19:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=76
06/15/2022 09:19:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=77
06/15/2022 09:19:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=78
06/15/2022 09:19:16 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=79
06/15/2022 09:19:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=81
06/15/2022 09:19:26 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.39047619047619053 on epoch=81
06/15/2022 09:19:31 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=82
06/15/2022 09:19:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=83
06/15/2022 09:19:40 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=84
06/15/2022 09:19:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 09:19:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=87
06/15/2022 09:19:54 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 09:19:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=88
06/15/2022 09:20:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=89
06/15/2022 09:20:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=91
06/15/2022 09:20:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=92
06/15/2022 09:20:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=93
06/15/2022 09:20:22 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 09:20:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=94
06/15/2022 09:20:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 09:20:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=97
06/15/2022 09:20:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=98
06/15/2022 09:20:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 09:20:50 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.50633608815427 on epoch=99
06/15/2022 09:20:50 - INFO - __main__ - Saving model with best Classification-F1: 0.4919083969465649 -> 0.50633608815427 on epoch=99, global_step=800
06/15/2022 09:20:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 09:20:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=102
06/15/2022 09:21:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=103
06/15/2022 09:21:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=104
06/15/2022 09:21:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=106
06/15/2022 09:21:18 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.17691622103386806 on epoch=106
06/15/2022 09:21:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=107
06/15/2022 09:21:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=108
06/15/2022 09:21:32 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 09:21:36 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=111
06/15/2022 09:21:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=112
06/15/2022 09:21:46 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 09:21:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=113
06/15/2022 09:21:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=114
06/15/2022 09:22:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=116
06/15/2022 09:22:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=117
06/15/2022 09:22:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 09:22:14 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.4554554554554554 on epoch=118
06/15/2022 09:22:19 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=119
06/15/2022 09:22:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 09:22:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=122
06/15/2022 09:22:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 09:22:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=124
06/15/2022 09:22:43 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.46880856760374834 on epoch=124
06/15/2022 09:22:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=126
06/15/2022 09:22:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=127
06/15/2022 09:22:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.21 on epoch=128
06/15/2022 09:23:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 09:23:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=131
06/15/2022 09:23:11 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.43111111111111106 on epoch=131
06/15/2022 09:23:15 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=132
06/15/2022 09:23:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=133
06/15/2022 09:23:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=134
06/15/2022 09:23:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=136
06/15/2022 09:23:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=137
06/15/2022 09:23:39 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.42857142857142855 on epoch=137
06/15/2022 09:23:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=138
06/15/2022 09:23:48 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=139
06/15/2022 09:23:52 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=141
06/15/2022 09:23:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 09:24:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=143
06/15/2022 09:24:07 - INFO - __main__ - Global step 1150 Train loss 0.22 Classification-F1 0.42216142270861834 on epoch=143
06/15/2022 09:24:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=144
06/15/2022 09:24:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=146
06/15/2022 09:24:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=147
06/15/2022 09:24:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=148
06/15/2022 09:24:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=149
06/15/2022 09:24:35 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.45 on epoch=149
06/15/2022 09:24:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 09:24:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=152
06/15/2022 09:24:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=153
06/15/2022 09:24:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=154
06/15/2022 09:24:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=156
06/15/2022 09:25:03 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.45299145299145294 on epoch=156
06/15/2022 09:25:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=157
06/15/2022 09:25:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=158
06/15/2022 09:25:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=159
06/15/2022 09:25:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=161
06/15/2022 09:25:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=162
06/15/2022 09:25:31 - INFO - __main__ - Global step 1300 Train loss 0.20 Classification-F1 0.37984496124031003 on epoch=162
06/15/2022 09:25:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=163
06/15/2022 09:25:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=164
06/15/2022 09:25:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=166
06/15/2022 09:25:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=167
06/15/2022 09:25:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=168
06/15/2022 09:25:59 - INFO - __main__ - Global step 1350 Train loss 0.18 Classification-F1 0.4341290893015031 on epoch=168
06/15/2022 09:26:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=169
06/15/2022 09:26:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=171
06/15/2022 09:26:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=172
06/15/2022 09:26:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=173
06/15/2022 09:26:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=174
06/15/2022 09:26:27 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.39499118165784836 on epoch=174
06/15/2022 09:26:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=176
06/15/2022 09:26:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 09:26:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=178
06/15/2022 09:26:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=179
06/15/2022 09:26:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.18 on epoch=181
06/15/2022 09:26:55 - INFO - __main__ - Global step 1450 Train loss 0.17 Classification-F1 0.4225563909774436 on epoch=181
06/15/2022 09:27:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=182
06/15/2022 09:27:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=183
06/15/2022 09:27:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=184
06/15/2022 09:27:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=186
06/15/2022 09:27:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.14 on epoch=187
06/15/2022 09:27:24 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.4762748091603053 on epoch=187
06/15/2022 09:27:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=188
06/15/2022 09:27:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=189
06/15/2022 09:27:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=191
06/15/2022 09:27:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=192
06/15/2022 09:27:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=193
06/15/2022 09:27:52 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.5210697417653193 on epoch=193
06/15/2022 09:27:52 - INFO - __main__ - Saving model with best Classification-F1: 0.50633608815427 -> 0.5210697417653193 on epoch=193, global_step=1550
06/15/2022 09:27:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=194
06/15/2022 09:28:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=196
06/15/2022 09:28:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=197
06/15/2022 09:28:10 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=198
06/15/2022 09:28:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=199
06/15/2022 09:28:20 - INFO - __main__ - Global step 1600 Train loss 0.14 Classification-F1 0.41700404858299595 on epoch=199
06/15/2022 09:28:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=201
06/15/2022 09:28:29 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=202
06/15/2022 09:28:33 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=203
06/15/2022 09:28:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.14 on epoch=204
06/15/2022 09:28:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.13 on epoch=206
06/15/2022 09:28:48 - INFO - __main__ - Global step 1650 Train loss 0.14 Classification-F1 0.3969951617010441 on epoch=206
06/15/2022 09:28:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.11 on epoch=207
06/15/2022 09:28:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=208
06/15/2022 09:29:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=209
06/15/2022 09:29:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.10 on epoch=211
06/15/2022 09:29:11 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=212
06/15/2022 09:29:16 - INFO - __main__ - Global step 1700 Train loss 0.12 Classification-F1 0.4362613163689748 on epoch=212
06/15/2022 09:29:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=213
06/15/2022 09:29:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=214
06/15/2022 09:29:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=216
06/15/2022 09:29:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=217
06/15/2022 09:29:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=218
06/15/2022 09:29:45 - INFO - __main__ - Global step 1750 Train loss 0.12 Classification-F1 0.4623177662466024 on epoch=218
06/15/2022 09:29:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=219
06/15/2022 09:29:54 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=221
06/15/2022 09:29:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=222
06/15/2022 09:30:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=223
06/15/2022 09:30:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=224
06/15/2022 09:30:13 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.4306893995552261 on epoch=224
06/15/2022 09:30:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=226
06/15/2022 09:30:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.13 on epoch=227
06/15/2022 09:30:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=228
06/15/2022 09:30:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=229
06/15/2022 09:30:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=231
06/15/2022 09:30:42 - INFO - __main__ - Global step 1850 Train loss 0.12 Classification-F1 0.44446482058805553 on epoch=231
06/15/2022 09:30:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=232
06/15/2022 09:30:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=233
06/15/2022 09:30:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=234
06/15/2022 09:31:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=236
06/15/2022 09:31:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=237
06/15/2022 09:31:10 - INFO - __main__ - Global step 1900 Train loss 0.10 Classification-F1 0.453125 on epoch=237
06/15/2022 09:31:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.11 on epoch=238
06/15/2022 09:31:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=239
06/15/2022 09:31:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=241
06/15/2022 09:31:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=242
06/15/2022 09:31:32 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=243
06/15/2022 09:31:38 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.4196078431372549 on epoch=243
06/15/2022 09:31:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=244
06/15/2022 09:31:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=246
06/15/2022 09:31:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=247
06/15/2022 09:31:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.14 on epoch=248
06/15/2022 09:32:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.13 on epoch=249
06/15/2022 09:32:07 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.4411855131279592 on epoch=249
06/15/2022 09:32:11 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.09 on epoch=251
06/15/2022 09:32:16 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.11 on epoch=252
06/15/2022 09:32:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=253
06/15/2022 09:32:25 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.10 on epoch=254
06/15/2022 09:32:29 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.09 on epoch=256
06/15/2022 09:32:35 - INFO - __main__ - Global step 2050 Train loss 0.10 Classification-F1 0.43529411764705883 on epoch=256
06/15/2022 09:32:39 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.08 on epoch=257
06/15/2022 09:32:44 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.10 on epoch=258
06/15/2022 09:32:48 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.07 on epoch=259
06/15/2022 09:32:53 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.10 on epoch=261
06/15/2022 09:32:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.08 on epoch=262
06/15/2022 09:33:03 - INFO - __main__ - Global step 2100 Train loss 0.09 Classification-F1 0.4375 on epoch=262
06/15/2022 09:33:08 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.11 on epoch=263
06/15/2022 09:33:12 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.07 on epoch=264
06/15/2022 09:33:17 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.07 on epoch=266
06/15/2022 09:33:21 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.06 on epoch=267
06/15/2022 09:33:26 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.09 on epoch=268
06/15/2022 09:33:31 - INFO - __main__ - Global step 2150 Train loss 0.08 Classification-F1 0.4429517502365184 on epoch=268
06/15/2022 09:33:36 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.07 on epoch=269
06/15/2022 09:33:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.10 on epoch=271
06/15/2022 09:33:45 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.09 on epoch=272
06/15/2022 09:33:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.07 on epoch=273
06/15/2022 09:33:54 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.06 on epoch=274
06/15/2022 09:34:00 - INFO - __main__ - Global step 2200 Train loss 0.08 Classification-F1 0.43535259397328363 on epoch=274
06/15/2022 09:34:04 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.06 on epoch=276
06/15/2022 09:34:09 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.10 on epoch=277
06/15/2022 09:34:13 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.05 on epoch=278
06/15/2022 09:34:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.09 on epoch=279
06/15/2022 09:34:22 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=281
06/15/2022 09:34:28 - INFO - __main__ - Global step 2250 Train loss 0.08 Classification-F1 0.46476939399613054 on epoch=281
06/15/2022 09:34:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.06 on epoch=282
06/15/2022 09:34:37 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.05 on epoch=283
06/15/2022 09:34:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.06 on epoch=284
06/15/2022 09:34:46 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.06 on epoch=286
06/15/2022 09:34:51 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.05 on epoch=287
06/15/2022 09:34:56 - INFO - __main__ - Global step 2300 Train loss 0.06 Classification-F1 0.4606412213740458 on epoch=287
06/15/2022 09:35:01 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.05 on epoch=288
06/15/2022 09:35:05 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.04 on epoch=289
06/15/2022 09:35:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.05 on epoch=291
06/15/2022 09:35:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.04 on epoch=292
06/15/2022 09:35:19 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.06 on epoch=293
06/15/2022 09:35:25 - INFO - __main__ - Global step 2350 Train loss 0.05 Classification-F1 0.4919083969465649 on epoch=293
06/15/2022 09:35:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.07 on epoch=294
06/15/2022 09:35:34 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.07 on epoch=296
06/15/2022 09:35:38 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.05 on epoch=297
06/15/2022 09:35:43 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.06 on epoch=298
06/15/2022 09:35:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.07 on epoch=299
06/15/2022 09:35:53 - INFO - __main__ - Global step 2400 Train loss 0.06 Classification-F1 0.4569267662792842 on epoch=299
06/15/2022 09:35:57 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.08 on epoch=301
06/15/2022 09:36:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.04 on epoch=302
06/15/2022 09:36:06 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.05 on epoch=303
06/15/2022 09:36:11 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.06 on epoch=304
06/15/2022 09:36:15 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.04 on epoch=306
06/15/2022 09:36:21 - INFO - __main__ - Global step 2450 Train loss 0.05 Classification-F1 0.4411855131279592 on epoch=306
06/15/2022 09:36:25 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=307
06/15/2022 09:36:30 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.03 on epoch=308
06/15/2022 09:36:34 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.06 on epoch=309
06/15/2022 09:36:39 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.05 on epoch=311
06/15/2022 09:36:43 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.05 on epoch=312
06/15/2022 09:36:49 - INFO - __main__ - Global step 2500 Train loss 0.05 Classification-F1 0.45299145299145294 on epoch=312
06/15/2022 09:36:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.03 on epoch=313
06/15/2022 09:36:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=314
06/15/2022 09:37:03 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.03 on epoch=316
06/15/2022 09:37:07 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.03 on epoch=317
06/15/2022 09:37:12 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.05 on epoch=318
06/15/2022 09:37:17 - INFO - __main__ - Global step 2550 Train loss 0.04 Classification-F1 0.45259042033235586 on epoch=318
06/15/2022 09:37:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.03 on epoch=319
06/15/2022 09:37:26 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.03 on epoch=321
06/15/2022 09:37:31 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.07 on epoch=322
06/15/2022 09:37:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.03 on epoch=323
06/15/2022 09:37:40 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.04 on epoch=324
06/15/2022 09:37:45 - INFO - __main__ - Global step 2600 Train loss 0.04 Classification-F1 0.4580323785803237 on epoch=324
06/15/2022 09:37:50 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.03 on epoch=326
06/15/2022 09:37:54 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=327
06/15/2022 09:37:59 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.07 on epoch=328
06/15/2022 09:38:03 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.06 on epoch=329
06/15/2022 09:38:08 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.02 on epoch=331
06/15/2022 09:38:14 - INFO - __main__ - Global step 2650 Train loss 0.05 Classification-F1 0.43736263736263736 on epoch=331
06/15/2022 09:38:18 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.04 on epoch=332
06/15/2022 09:38:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.06 on epoch=333
06/15/2022 09:38:27 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.04 on epoch=334
06/15/2022 09:38:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=336
06/15/2022 09:38:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.06 on epoch=337
06/15/2022 09:38:42 - INFO - __main__ - Global step 2700 Train loss 0.05 Classification-F1 0.4362613163689748 on epoch=337
06/15/2022 09:38:46 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.02 on epoch=338
06/15/2022 09:38:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.03 on epoch=339
06/15/2022 09:38:55 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.04 on epoch=341
06/15/2022 09:39:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.05 on epoch=342
06/15/2022 09:39:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.03 on epoch=343
06/15/2022 09:39:10 - INFO - __main__ - Global step 2750 Train loss 0.03 Classification-F1 0.4606412213740458 on epoch=343
06/15/2022 09:39:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.04 on epoch=344
06/15/2022 09:39:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.01 on epoch=346
06/15/2022 09:39:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.04 on epoch=347
06/15/2022 09:39:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.02 on epoch=348
06/15/2022 09:39:33 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.03 on epoch=349
06/15/2022 09:39:38 - INFO - __main__ - Global step 2800 Train loss 0.03 Classification-F1 0.467580132126254 on epoch=349
06/15/2022 09:39:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.02 on epoch=351
06/15/2022 09:39:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.02 on epoch=352
06/15/2022 09:39:52 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.04 on epoch=353
06/15/2022 09:39:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.05 on epoch=354
06/15/2022 09:40:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.03 on epoch=356
06/15/2022 09:40:07 - INFO - __main__ - Global step 2850 Train loss 0.03 Classification-F1 0.4623177662466024 on epoch=356
06/15/2022 09:40:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.02 on epoch=357
06/15/2022 09:40:16 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=358
06/15/2022 09:40:20 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.04 on epoch=359
06/15/2022 09:40:25 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.04 on epoch=361
06/15/2022 09:40:30 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.03 on epoch=362
06/15/2022 09:40:35 - INFO - __main__ - Global step 2900 Train loss 0.04 Classification-F1 0.4686202686202686 on epoch=362
06/15/2022 09:40:40 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.02 on epoch=363
06/15/2022 09:40:44 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.04 on epoch=364
06/15/2022 09:40:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.01 on epoch=366
06/15/2022 09:40:53 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.04 on epoch=367
06/15/2022 09:40:58 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.03 on epoch=368
06/15/2022 09:41:03 - INFO - __main__ - Global step 2950 Train loss 0.03 Classification-F1 0.4606412213740458 on epoch=368
06/15/2022 09:41:08 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.02 on epoch=369
06/15/2022 09:41:12 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.04 on epoch=371
06/15/2022 09:41:17 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.05 on epoch=372
06/15/2022 09:41:21 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.03 on epoch=373
06/15/2022 09:41:26 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=374
06/15/2022 09:41:27 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:41:27 - INFO - __main__ - Printing 3 examples
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:41:27 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:41:27 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 09:41:27 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:41:27 - INFO - __main__ - Printing 3 examples
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 09:41:27 - INFO - __main__ - ['refuted']
06/15/2022 09:41:27 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:41:28 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:41:28 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 09:41:32 - INFO - __main__ - Global step 3000 Train loss 0.04 Classification-F1 0.4293740458015267 on epoch=374
06/15/2022 09:41:32 - INFO - __main__ - save last model!
06/15/2022 09:41:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 09:41:32 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 09:41:32 - INFO - __main__ - Printing 3 examples
06/15/2022 09:41:32 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:41:32 - INFO - __main__ - ['entailed']
06/15/2022 09:41:32 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:41:32 - INFO - __main__ - ['entailed']
06/15/2022 09:41:32 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 09:41:32 - INFO - __main__ - ['entailed']
06/15/2022 09:41:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:41:46 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 09:41:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 09:41:47 - INFO - __main__ - Starting training!
06/15/2022 09:41:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:42:09 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 09:51:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_42_0.3_8_predictions.txt
06/15/2022 09:51:08 - INFO - __main__ - Classification-F1 on test data: 0.3328
06/15/2022 09:51:08 - INFO - __main__ - prefix=tab_fact_64_42, lr=0.3, bsz=8, dev_performance=0.5210697417653193, test_performance=0.3327599094539754
06/15/2022 09:51:08 - INFO - __main__ - Running ... prefix=tab_fact_64_42, lr=0.2, bsz=8 ...
06/15/2022 09:51:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:51:09 - INFO - __main__ - Printing 3 examples
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andré#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mário jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:51:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:51:09 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 09:51:09 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 09:51:09 - INFO - __main__ - Printing 3 examples
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: during the may 1952 vfl season , mcg record the highest crowd participation [SEP] table_caption: 1952 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] fitzroy#5.15 (45)#richmond#7.12 (54)#brunswick street oval#20500#19 april 1952 [n] south melbourne#11.16 (82)#st kilda#8.26 (74)#lake oval#18000#19 april 1952 [n] melbourne#14.13 (97)#geelong#15.13 (103)#mcg#25000#19 april 1952 [n] footscray#6.10 (46)#essendon#6.5 (41)#western oval#24000#19 april 1952 [n] hawthorn#2.8 (20)#collingwood#15.16 (106)#glenferrie oval#14000#19 april 1952 [n] north melbourne#8.19 (67)#carlton#13.12 (90)#arden street oval#22000#19 april 1952 [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: jerry barber be the only amateur player to make the top 10 [SEP] table_caption: 1956 u.s. open (golf) [SEP] table_text: place#player#country#score#to par#money [n] 1#cary middlecoff#united states#71 + 70 + 70 + 70 = 281#+ 1#6000 [n] t2#julius boros#united states#71 + 71 + 71 + 69 = 282#+ 2#2650 [n] t2#ben hogan#united states#72 + 68 + 72 + 70 = 282#+ 2#2650 [n] t4#ed furgol#united states#71 + 70 + 73 + 71 = 285#+ 5#1033 [n] t4#ted kroll#united states#72 + 70 + 70 + 73 = 285#+ 5#1033 [n] t4#peter thomson#australia#70 + 69 + 75 + 71 = 285#+ 5#1033 [n] 7#arnold palmer#united states#72 + 70 + 72 + 73 = 287#+ 7#600 [n] 8#ken venturi (a)#united states#77 + 71 + 68 + 73 = 289#+ 9#0 [n] t9#jerry barber#united states#72 + 69 + 74 + 75 = 290#+ 10#416 [n] t9#wes ellis#united states#71 + 70 + 71 + 78 = 290#+ 10#416 [n] t9#doug ford#united states#71 + 75 + 70 + 74 = 290#+ 10#416 [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ -  [tab_fact] statement: 1 be the rank of the swimmer in lane 6 [SEP] table_caption: swimming at the 2008 summer olympics - men 's 100 metre freestyle [SEP] table_text: rank#lane#name#nationality#time [n] 1#5#alain bernard#france#47.20 [n] 2#4#stefan nystrand#sweden#47.91 [n] 3#2#jason lezak#united states#47.98 [n] 4#6#lyndon ferns#south africa#48.00 [n] 5#3#cãsar cielo filho#brazil#48.07 [n] 6#8#christian galenda#italy#48.47 [n] 7#1#jonas persson#sweden#48.59 [n] 8#7#fabien gilot#france#49.00 [n] 
06/15/2022 09:51:09 - INFO - __main__ - ['refuted']
06/15/2022 09:51:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 09:51:10 - INFO - __main__ - Tokenizing Output ...
06/15/2022 09:51:10 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 09:51:29 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 09:51:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 09:51:29 - INFO - __main__ - Starting training!
06/15/2022 09:51:34 - INFO - __main__ - Step 10 Global step 10 Train loss 3.91 on epoch=1
06/15/2022 09:51:39 - INFO - __main__ - Step 20 Global step 20 Train loss 1.27 on epoch=2
06/15/2022 09:51:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=3
06/15/2022 09:51:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=4
06/15/2022 09:51:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.37 on epoch=6
06/15/2022 09:51:58 - INFO - __main__ - Global step 50 Train loss 1.33 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 09:51:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 09:52:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=7
06/15/2022 09:52:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=8
06/15/2022 09:52:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=9
06/15/2022 09:52:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=11
06/15/2022 09:52:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=12
06/15/2022 09:52:25 - INFO - __main__ - Global step 100 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 09:52:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=13
06/15/2022 09:52:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=14
06/15/2022 09:52:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=16
06/15/2022 09:52:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=17
06/15/2022 09:52:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=18
06/15/2022 09:52:54 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.34673046251993617 on epoch=18
06/15/2022 09:52:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34673046251993617 on epoch=18, global_step=150
06/15/2022 09:52:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=19
06/15/2022 09:53:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 09:53:08 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=22
06/15/2022 09:53:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=23
06/15/2022 09:53:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=24
06/15/2022 09:53:23 - INFO - __main__ - Global step 200 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 09:53:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=26
06/15/2022 09:53:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=27
06/15/2022 09:53:36 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=28
06/15/2022 09:53:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.22 on epoch=29
06/15/2022 09:53:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=31
06/15/2022 09:53:50 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 09:53:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=32
06/15/2022 09:53:59 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=33
06/15/2022 09:54:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=34
06/15/2022 09:54:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=36
06/15/2022 09:54:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=37
06/15/2022 09:54:18 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 09:54:23 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=38
06/15/2022 09:54:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=39
06/15/2022 09:54:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=41
06/15/2022 09:54:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 09:54:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=43
06/15/2022 09:54:46 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.4811812391430226 on epoch=43
06/15/2022 09:54:46 - INFO - __main__ - Saving model with best Classification-F1: 0.34673046251993617 -> 0.4811812391430226 on epoch=43, global_step=350
06/15/2022 09:54:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=44
06/15/2022 09:54:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=46
06/15/2022 09:55:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=47
06/15/2022 09:55:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=48
06/15/2022 09:55:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=49
06/15/2022 09:55:14 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.21739130434782608 on epoch=49
06/15/2022 09:55:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=51
06/15/2022 09:55:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=52
06/15/2022 09:55:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=53
06/15/2022 09:55:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=54
06/15/2022 09:55:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=56
06/15/2022 09:55:42 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 09:55:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=57
06/15/2022 09:55:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=58
06/15/2022 09:55:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=59
06/15/2022 09:56:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=61
06/15/2022 09:56:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=62
06/15/2022 09:56:11 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 09:56:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=63
06/15/2022 09:56:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=64
06/15/2022 09:56:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 09:56:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 09:56:34 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 09:56:39 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 09:56:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=69
06/15/2022 09:56:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 09:56:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=72
06/15/2022 09:56:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=73
06/15/2022 09:57:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=74
06/15/2022 09:57:07 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 09:57:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=76
06/15/2022 09:57:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=77
06/15/2022 09:57:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=78
06/15/2022 09:57:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=79
06/15/2022 09:57:29 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=81
06/15/2022 09:57:35 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 09:57:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=82
06/15/2022 09:57:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=83
06/15/2022 09:57:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=84
06/15/2022 09:57:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=86
06/15/2022 09:57:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=87
06/15/2022 09:58:03 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 09:58:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=88
06/15/2022 09:58:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 09:58:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=91
06/15/2022 09:58:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=92
06/15/2022 09:58:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=93
06/15/2022 09:58:31 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 09:58:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=94
06/15/2022 09:58:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=96
06/15/2022 09:58:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=97
06/15/2022 09:58:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=98
06/15/2022 09:58:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=99
06/15/2022 09:58:59 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 09:59:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=101
06/15/2022 09:59:08 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 09:59:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=103
06/15/2022 09:59:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=104
06/15/2022 09:59:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=106
06/15/2022 09:59:27 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 09:59:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=107
06/15/2022 09:59:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=108
06/15/2022 09:59:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 09:59:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=111
06/15/2022 09:59:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 09:59:56 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 10:00:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=113
06/15/2022 10:00:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=114
06/15/2022 10:00:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.27 on epoch=116
06/15/2022 10:00:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=117
06/15/2022 10:00:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 10:00:24 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.21245421245421245 on epoch=118
06/15/2022 10:00:28 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=119
06/15/2022 10:00:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 10:00:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=122
06/15/2022 10:00:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 10:00:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=124
06/15/2022 10:00:52 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 10:00:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=126
06/15/2022 10:01:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=127
06/15/2022 10:01:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 10:01:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 10:01:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.20 on epoch=131
06/15/2022 10:01:20 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 10:01:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=132
06/15/2022 10:01:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=133
06/15/2022 10:01:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=134
06/15/2022 10:01:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=136
06/15/2022 10:01:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=137
06/15/2022 10:01:48 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=137
06/15/2022 10:01:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=138
06/15/2022 10:01:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=139
06/15/2022 10:02:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=141
06/15/2022 10:02:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=142
06/15/2022 10:02:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.18 on epoch=143
06/15/2022 10:02:16 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.3708141321044547 on epoch=143
06/15/2022 10:02:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=144
06/15/2022 10:02:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=146
06/15/2022 10:02:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.19 on epoch=147
06/15/2022 10:02:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=148
06/15/2022 10:02:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=149
06/15/2022 10:02:44 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.34673046251993617 on epoch=149
06/15/2022 10:02:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=151
06/15/2022 10:02:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=152
06/15/2022 10:02:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=153
06/15/2022 10:03:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=154
06/15/2022 10:03:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=156
06/15/2022 10:03:13 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3298429319371728 on epoch=156
06/15/2022 10:03:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=157
06/15/2022 10:03:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=158
06/15/2022 10:03:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=159
06/15/2022 10:03:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=161
06/15/2022 10:03:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=162
06/15/2022 10:03:41 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
06/15/2022 10:03:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=163
06/15/2022 10:03:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=164
06/15/2022 10:03:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=166
06/15/2022 10:03:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=167
06/15/2022 10:04:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=168
06/15/2022 10:04:09 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.34673046251993617 on epoch=168
06/15/2022 10:04:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=169
06/15/2022 10:04:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=171
06/15/2022 10:04:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=172
06/15/2022 10:04:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=173
06/15/2022 10:04:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=174
06/15/2022 10:04:37 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=174
06/15/2022 10:04:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=176
06/15/2022 10:04:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 10:04:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=178
06/15/2022 10:04:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=179
06/15/2022 10:05:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=181
06/15/2022 10:05:06 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=181
06/15/2022 10:05:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=182
06/15/2022 10:05:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=183
06/15/2022 10:05:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=184
06/15/2022 10:05:24 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=186
06/15/2022 10:05:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=187
06/15/2022 10:05:34 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=187
06/15/2022 10:05:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=188
06/15/2022 10:05:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=189
06/15/2022 10:05:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=191
06/15/2022 10:05:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=192
06/15/2022 10:05:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=193
06/15/2022 10:06:02 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.3298429319371728 on epoch=193
06/15/2022 10:06:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=194
06/15/2022 10:06:11 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=196
06/15/2022 10:06:16 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=197
06/15/2022 10:06:20 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=198
06/15/2022 10:06:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=199
06/15/2022 10:06:31 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.3298429319371728 on epoch=199
06/15/2022 10:06:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=201
06/15/2022 10:06:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=202
06/15/2022 10:06:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=203
06/15/2022 10:06:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=204
06/15/2022 10:06:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=206
06/15/2022 10:06:59 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.34673046251993617 on epoch=206
06/15/2022 10:07:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=207
06/15/2022 10:07:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=208
06/15/2022 10:07:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=209
06/15/2022 10:07:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=211
06/15/2022 10:07:22 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=212
06/15/2022 10:07:27 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.34296770117665637 on epoch=212
06/15/2022 10:07:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=213
06/15/2022 10:07:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=214
06/15/2022 10:07:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.18 on epoch=216
06/15/2022 10:07:45 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=217
06/15/2022 10:07:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=218
06/15/2022 10:07:56 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.46471080229042006 on epoch=218
06/15/2022 10:08:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=219
06/15/2022 10:08:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=221
06/15/2022 10:08:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=222
06/15/2022 10:08:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=223
06/15/2022 10:08:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.20 on epoch=224
06/15/2022 10:08:24 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.5388091603053435 on epoch=224
06/15/2022 10:08:24 - INFO - __main__ - Saving model with best Classification-F1: 0.4811812391430226 -> 0.5388091603053435 on epoch=224, global_step=1800
06/15/2022 10:08:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=226
06/15/2022 10:08:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=227
06/15/2022 10:08:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=228
06/15/2022 10:08:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=229
06/15/2022 10:08:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=231
06/15/2022 10:08:52 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.460545937884977 on epoch=231
06/15/2022 10:08:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=232
06/15/2022 10:09:01 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.17 on epoch=233
06/15/2022 10:09:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=234
06/15/2022 10:09:10 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=236
06/15/2022 10:09:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=237
06/15/2022 10:09:20 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.3591989987484355 on epoch=237
06/15/2022 10:09:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=238
06/15/2022 10:09:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.18 on epoch=239
06/15/2022 10:09:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=241
06/15/2022 10:09:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=242
06/15/2022 10:09:43 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=243
06/15/2022 10:09:49 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.37984496124031003 on epoch=243
06/15/2022 10:09:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=244
06/15/2022 10:09:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=246
06/15/2022 10:10:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=247
06/15/2022 10:10:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=248
06/15/2022 10:10:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=249
06/15/2022 10:10:17 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.5113300492610837 on epoch=249
06/15/2022 10:10:21 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.18 on epoch=251
06/15/2022 10:10:26 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=252
06/15/2022 10:10:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=253
06/15/2022 10:10:35 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=254
06/15/2022 10:10:39 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.18 on epoch=256
06/15/2022 10:10:45 - INFO - __main__ - Global step 2050 Train loss 0.19 Classification-F1 0.4709377421854818 on epoch=256
06/15/2022 10:10:50 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=257
06/15/2022 10:10:54 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.21 on epoch=258
06/15/2022 10:10:59 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.20 on epoch=259
06/15/2022 10:11:03 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=261
06/15/2022 10:11:08 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.17 on epoch=262
06/15/2022 10:11:13 - INFO - __main__ - Global step 2100 Train loss 0.19 Classification-F1 0.46880856760374834 on epoch=262
06/15/2022 10:11:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.16 on epoch=263
06/15/2022 10:11:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=264
06/15/2022 10:11:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.18 on epoch=266
06/15/2022 10:11:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.20 on epoch=267
06/15/2022 10:11:36 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=268
06/15/2022 10:11:42 - INFO - __main__ - Global step 2150 Train loss 0.18 Classification-F1 0.3588712522045855 on epoch=268
06/15/2022 10:11:46 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.19 on epoch=269
06/15/2022 10:11:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=271
06/15/2022 10:11:55 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.17 on epoch=272
06/15/2022 10:12:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.16 on epoch=273
06/15/2022 10:12:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=274
06/15/2022 10:12:10 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.5097603162836669 on epoch=274
06/15/2022 10:12:15 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=276
06/15/2022 10:12:19 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=277
06/15/2022 10:12:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.16 on epoch=278
06/15/2022 10:12:28 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.16 on epoch=279
06/15/2022 10:12:33 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=281
06/15/2022 10:12:38 - INFO - __main__ - Global step 2250 Train loss 0.18 Classification-F1 0.5126504544338 on epoch=281
06/15/2022 10:12:43 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.17 on epoch=282
06/15/2022 10:12:47 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=283
06/15/2022 10:12:52 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.18 on epoch=284
06/15/2022 10:12:56 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.17 on epoch=286
06/15/2022 10:13:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.18 on epoch=287
06/15/2022 10:13:06 - INFO - __main__ - Global step 2300 Train loss 0.18 Classification-F1 0.4781408768738631 on epoch=287
06/15/2022 10:13:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.16 on epoch=288
06/15/2022 10:13:15 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.13 on epoch=289
06/15/2022 10:13:20 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.16 on epoch=291
06/15/2022 10:13:25 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.18 on epoch=292
06/15/2022 10:13:29 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=293
06/15/2022 10:13:35 - INFO - __main__ - Global step 2350 Train loss 0.16 Classification-F1 0.4079058031959629 on epoch=293
06/15/2022 10:13:39 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=294
06/15/2022 10:13:44 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.16 on epoch=296
06/15/2022 10:13:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.17 on epoch=297
06/15/2022 10:13:53 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.13 on epoch=298
06/15/2022 10:13:57 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.15 on epoch=299
06/15/2022 10:14:03 - INFO - __main__ - Global step 2400 Train loss 0.16 Classification-F1 0.4560313828048382 on epoch=299
06/15/2022 10:14:07 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.16 on epoch=301
06/15/2022 10:14:12 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=302
06/15/2022 10:14:16 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.16 on epoch=303
06/15/2022 10:14:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.13 on epoch=304
06/15/2022 10:14:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.17 on epoch=306
06/15/2022 10:14:31 - INFO - __main__ - Global step 2450 Train loss 0.16 Classification-F1 0.5110771581359816 on epoch=306
06/15/2022 10:14:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.17 on epoch=307
06/15/2022 10:14:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.15 on epoch=308
06/15/2022 10:14:44 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.15 on epoch=309
06/15/2022 10:14:49 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.15 on epoch=311
06/15/2022 10:14:53 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.13 on epoch=312
06/15/2022 10:14:59 - INFO - __main__ - Global step 2500 Train loss 0.15 Classification-F1 0.5148803976390183 on epoch=312
06/15/2022 10:15:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.14 on epoch=313
06/15/2022 10:15:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.16 on epoch=314
06/15/2022 10:15:13 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.14 on epoch=316
06/15/2022 10:15:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.14 on epoch=317
06/15/2022 10:15:22 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.12 on epoch=318
06/15/2022 10:15:27 - INFO - __main__ - Global step 2550 Train loss 0.14 Classification-F1 0.4685404024273395 on epoch=318
06/15/2022 10:15:32 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.11 on epoch=319
06/15/2022 10:15:36 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.14 on epoch=321
06/15/2022 10:15:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.10 on epoch=322
06/15/2022 10:15:45 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.14 on epoch=323
06/15/2022 10:15:50 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.13 on epoch=324
06/15/2022 10:15:55 - INFO - __main__ - Global step 2600 Train loss 0.12 Classification-F1 0.46558704453441296 on epoch=324
06/15/2022 10:16:00 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.13 on epoch=326
06/15/2022 10:16:04 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.15 on epoch=327
06/15/2022 10:16:09 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.13 on epoch=328
06/15/2022 10:16:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.12 on epoch=329
06/15/2022 10:16:18 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.13 on epoch=331
06/15/2022 10:16:24 - INFO - __main__ - Global step 2650 Train loss 0.13 Classification-F1 0.4848930054295752 on epoch=331
06/15/2022 10:16:28 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.14 on epoch=332
06/15/2022 10:16:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.12 on epoch=333
06/15/2022 10:16:37 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.12 on epoch=334
06/15/2022 10:16:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.14 on epoch=336
06/15/2022 10:16:46 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.11 on epoch=337
06/15/2022 10:16:52 - INFO - __main__ - Global step 2700 Train loss 0.13 Classification-F1 0.5126504544338 on epoch=337
06/15/2022 10:16:56 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.12 on epoch=338
06/15/2022 10:17:01 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.13 on epoch=339
06/15/2022 10:17:05 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.09 on epoch=341
06/15/2022 10:17:10 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.14 on epoch=342
06/15/2022 10:17:14 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.13 on epoch=343
06/15/2022 10:17:20 - INFO - __main__ - Global step 2750 Train loss 0.12 Classification-F1 0.4995112414467253 on epoch=343
06/15/2022 10:17:25 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.12 on epoch=344
06/15/2022 10:17:29 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.11 on epoch=346
06/15/2022 10:17:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.10 on epoch=347
06/15/2022 10:17:38 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.14 on epoch=348
06/15/2022 10:17:43 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.09 on epoch=349
06/15/2022 10:17:48 - INFO - __main__ - Global step 2800 Train loss 0.11 Classification-F1 0.49797570850202433 on epoch=349
06/15/2022 10:17:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.12 on epoch=351
06/15/2022 10:17:57 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.12 on epoch=352
06/15/2022 10:18:02 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.11 on epoch=353
06/15/2022 10:18:06 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.12 on epoch=354
06/15/2022 10:18:11 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.08 on epoch=356
06/15/2022 10:18:16 - INFO - __main__ - Global step 2850 Train loss 0.11 Classification-F1 0.4749923477196204 on epoch=356
06/15/2022 10:18:20 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.10 on epoch=357
06/15/2022 10:18:25 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.13 on epoch=358
06/15/2022 10:18:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.13 on epoch=359
06/15/2022 10:18:34 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.11 on epoch=361
06/15/2022 10:18:38 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.09 on epoch=362
06/15/2022 10:18:44 - INFO - __main__ - Global step 2900 Train loss 0.11 Classification-F1 0.4709377421854818 on epoch=362
06/15/2022 10:18:48 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.06 on epoch=363
06/15/2022 10:18:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.11 on epoch=364
06/15/2022 10:18:57 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.09 on epoch=366
06/15/2022 10:19:02 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.09 on epoch=367
06/15/2022 10:19:06 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.07 on epoch=368
06/15/2022 10:19:12 - INFO - __main__ - Global step 2950 Train loss 0.08 Classification-F1 0.47848230201171377 on epoch=368
06/15/2022 10:19:16 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.11 on epoch=369
06/15/2022 10:19:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.10 on epoch=371
06/15/2022 10:19:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.08 on epoch=372
06/15/2022 10:19:30 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.07 on epoch=373
06/15/2022 10:19:34 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.09 on epoch=374
06/15/2022 10:19:35 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:19:35 - INFO - __main__ - Printing 3 examples
06/15/2022 10:19:35 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 10:19:35 - INFO - __main__ - ['entailed']
06/15/2022 10:19:35 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 10:19:35 - INFO - __main__ - ['entailed']
06/15/2022 10:19:35 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 10:19:35 - INFO - __main__ - ['entailed']
06/15/2022 10:19:35 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:19:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:19:36 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 10:19:36 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:19:36 - INFO - __main__ - Printing 3 examples
06/15/2022 10:19:36 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 10:19:36 - INFO - __main__ - ['entailed']
06/15/2022 10:19:36 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 10:19:36 - INFO - __main__ - ['entailed']
06/15/2022 10:19:36 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 10:19:36 - INFO - __main__ - ['entailed']
06/15/2022 10:19:36 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:19:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:19:36 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 10:19:39 - INFO - __main__ - Global step 3000 Train loss 0.09 Classification-F1 0.504537089916873 on epoch=374
06/15/2022 10:19:39 - INFO - __main__ - save last model!
06/15/2022 10:19:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 10:19:39 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 10:19:39 - INFO - __main__ - Printing 3 examples
06/15/2022 10:19:39 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:19:39 - INFO - __main__ - ['entailed']
06/15/2022 10:19:39 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:19:39 - INFO - __main__ - ['entailed']
06/15/2022 10:19:39 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:19:39 - INFO - __main__ - ['entailed']
06/15/2022 10:19:39 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:19:54 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 10:19:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 10:19:55 - INFO - __main__ - Starting training!
06/15/2022 10:20:04 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:20:17 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 10:28:35 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_42_0.2_8_predictions.txt
06/15/2022 10:28:35 - INFO - __main__ - Classification-F1 on test data: 0.4743
06/15/2022 10:28:36 - INFO - __main__ - prefix=tab_fact_64_42, lr=0.2, bsz=8, dev_performance=0.5388091603053435, test_performance=0.4742916485930634
06/15/2022 10:28:36 - INFO - __main__ - Running ... prefix=tab_fact_64_87, lr=0.5, bsz=8 ...
06/15/2022 10:28:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:28:37 - INFO - __main__ - Printing 3 examples
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:28:37 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:28:37 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 10:28:37 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:28:37 - INFO - __main__ - Printing 3 examples
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 10:28:37 - INFO - __main__ - ['entailed']
06/15/2022 10:28:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:28:37 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:28:38 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 10:28:53 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 10:28:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 10:28:54 - INFO - __main__ - Starting training!
06/15/2022 10:28:59 - INFO - __main__ - Step 10 Global step 10 Train loss 2.93 on epoch=1
06/15/2022 10:29:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.56 on epoch=2
06/15/2022 10:29:08 - INFO - __main__ - Step 30 Global step 30 Train loss 0.38 on epoch=3
06/15/2022 10:29:12 - INFO - __main__ - Step 40 Global step 40 Train loss 0.28 on epoch=4
06/15/2022 10:29:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=6
06/15/2022 10:29:22 - INFO - __main__ - Global step 50 Train loss 0.89 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 10:29:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 10:29:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=7
06/15/2022 10:29:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=8
06/15/2022 10:29:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=9
06/15/2022 10:29:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=11
06/15/2022 10:29:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=12
06/15/2022 10:29:50 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 10:29:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 10:29:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.23 on epoch=14
06/15/2022 10:30:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=16
06/15/2022 10:30:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=17
06/15/2022 10:30:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=18
06/15/2022 10:30:17 - INFO - __main__ - Global step 150 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 10:30:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=19
06/15/2022 10:30:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=21
06/15/2022 10:30:30 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=22
06/15/2022 10:30:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=23
06/15/2022 10:30:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=24
06/15/2022 10:30:45 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 10:30:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=26
06/15/2022 10:30:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=27
06/15/2022 10:30:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=28
06/15/2022 10:31:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=29
06/15/2022 10:31:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=31
06/15/2022 10:31:13 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 10:31:18 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=32
06/15/2022 10:31:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=33
06/15/2022 10:31:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=34
06/15/2022 10:31:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=36
06/15/2022 10:31:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=37
06/15/2022 10:31:41 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 10:31:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=38
06/15/2022 10:31:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=39
06/15/2022 10:31:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=41
06/15/2022 10:31:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=42
06/15/2022 10:32:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 10:32:09 - INFO - __main__ - Global step 350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 10:32:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=44
06/15/2022 10:32:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=46
06/15/2022 10:32:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=47
06/15/2022 10:32:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=48
06/15/2022 10:32:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=49
06/15/2022 10:32:37 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 10:32:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=51
06/15/2022 10:32:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=52
06/15/2022 10:32:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=53
06/15/2022 10:32:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=54
06/15/2022 10:32:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=56
06/15/2022 10:33:05 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 10:33:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=57
06/15/2022 10:33:14 - INFO - __main__ - Step 470 Global step 470 Train loss 2.49 on epoch=58
06/15/2022 10:33:18 - INFO - __main__ - Step 480 Global step 480 Train loss 2.79 on epoch=59
06/15/2022 10:33:23 - INFO - __main__ - Step 490 Global step 490 Train loss 1.47 on epoch=61
06/15/2022 10:33:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=62
06/15/2022 10:33:33 - INFO - __main__ - Global step 500 Train loss 1.51 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 10:33:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=63
06/15/2022 10:33:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=64
06/15/2022 10:33:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.76 on epoch=66
06/15/2022 10:33:51 - INFO - __main__ - Step 540 Global step 540 Train loss 1.07 on epoch=67
06/15/2022 10:33:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=68
06/15/2022 10:34:01 - INFO - __main__ - Global step 550 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 10:34:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=69
06/15/2022 10:34:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.68 on epoch=71
06/15/2022 10:34:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=72
06/15/2022 10:34:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=73
06/15/2022 10:34:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.28 on epoch=74
06/15/2022 10:34:29 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 10:34:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=76
06/15/2022 10:34:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=77
06/15/2022 10:34:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=78
06/15/2022 10:34:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=79
06/15/2022 10:34:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.30 on epoch=81
06/15/2022 10:34:57 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 10:35:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=82
06/15/2022 10:35:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=83
06/15/2022 10:35:10 - INFO - __main__ - Step 680 Global step 680 Train loss 1.25 on epoch=84
06/15/2022 10:35:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=86
06/15/2022 10:35:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=87
06/15/2022 10:35:25 - INFO - __main__ - Global step 700 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 10:35:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=88
06/15/2022 10:35:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 10:35:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=91
06/15/2022 10:35:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=92
06/15/2022 10:35:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=93
06/15/2022 10:35:53 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 10:35:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=94
06/15/2022 10:36:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=96
06/15/2022 10:36:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=97
06/15/2022 10:36:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=98
06/15/2022 10:36:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=99
06/15/2022 10:36:21 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 10:36:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 10:36:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 10:36:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=103
06/15/2022 10:36:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=104
06/15/2022 10:36:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=106
06/15/2022 10:36:49 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 10:36:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 10:36:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=108
06/15/2022 10:37:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=109
06/15/2022 10:37:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=111
06/15/2022 10:37:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=112
06/15/2022 10:37:17 - INFO - __main__ - Global step 900 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 10:37:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=113
06/15/2022 10:37:26 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=114
06/15/2022 10:37:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=116
06/15/2022 10:37:35 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=117
06/15/2022 10:37:40 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=118
06/15/2022 10:37:45 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 10:37:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=119
06/15/2022 10:37:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=121
06/15/2022 10:37:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=122
06/15/2022 10:38:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=123
06/15/2022 10:38:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=124
06/15/2022 10:38:14 - INFO - __main__ - Global step 1000 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 10:38:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=126
06/15/2022 10:38:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=127
06/15/2022 10:38:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 10:38:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=129
06/15/2022 10:38:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=131
06/15/2022 10:38:42 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 10:38:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=132
06/15/2022 10:38:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=133
06/15/2022 10:38:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=134
06/15/2022 10:38:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=136
06/15/2022 10:39:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=137
06/15/2022 10:39:10 - INFO - __main__ - Global step 1100 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=137
06/15/2022 10:39:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=138
06/15/2022 10:39:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=139
06/15/2022 10:39:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=141
06/15/2022 10:39:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=142
06/15/2022 10:39:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=143
06/15/2022 10:39:38 - INFO - __main__ - Global step 1150 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=143
06/15/2022 10:39:42 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=144
06/15/2022 10:39:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=146
06/15/2022 10:39:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=147
06/15/2022 10:39:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=148
06/15/2022 10:40:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=149
06/15/2022 10:40:06 - INFO - __main__ - Global step 1200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=149
06/15/2022 10:40:10 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=151
06/15/2022 10:40:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=152
06/15/2022 10:40:19 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=153
06/15/2022 10:40:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=154
06/15/2022 10:40:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=156
06/15/2022 10:40:34 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=156
06/15/2022 10:40:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=157
06/15/2022 10:40:42 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=158
06/15/2022 10:40:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=159
06/15/2022 10:40:51 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=161
06/15/2022 10:40:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=162
06/15/2022 10:41:01 - INFO - __main__ - Global step 1300 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=162
06/15/2022 10:41:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=163
06/15/2022 10:41:10 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=164
06/15/2022 10:41:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=166
06/15/2022 10:41:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=167
06/15/2022 10:41:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=168
06/15/2022 10:41:29 - INFO - __main__ - Global step 1350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=168
06/15/2022 10:41:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=169
06/15/2022 10:41:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=171
06/15/2022 10:41:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.21 on epoch=172
06/15/2022 10:41:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=173
06/15/2022 10:41:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=174
06/15/2022 10:41:57 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=174
06/15/2022 10:42:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=176
06/15/2022 10:42:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.19 on epoch=177
06/15/2022 10:42:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=178
06/15/2022 10:42:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=179
06/15/2022 10:42:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=181
06/15/2022 10:42:25 - INFO - __main__ - Global step 1450 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=181
06/15/2022 10:42:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=182
06/15/2022 10:42:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=183
06/15/2022 10:42:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=184
06/15/2022 10:42:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=186
06/15/2022 10:42:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=187
06/15/2022 10:42:53 - INFO - __main__ - Global step 1500 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=187
06/15/2022 10:42:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=188
06/15/2022 10:43:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=189
06/15/2022 10:43:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=191
06/15/2022 10:43:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=192
06/15/2022 10:43:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=193
06/15/2022 10:43:21 - INFO - __main__ - Global step 1550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=193
06/15/2022 10:43:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.21 on epoch=194
06/15/2022 10:43:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=196
06/15/2022 10:43:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=197
06/15/2022 10:43:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=198
06/15/2022 10:43:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=199
06/15/2022 10:43:49 - INFO - __main__ - Global step 1600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=199
06/15/2022 10:43:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=201
06/15/2022 10:43:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=202
06/15/2022 10:44:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=203
06/15/2022 10:44:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=204
06/15/2022 10:44:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=206
06/15/2022 10:44:16 - INFO - __main__ - Global step 1650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=206
06/15/2022 10:44:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=207
06/15/2022 10:44:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=208
06/15/2022 10:44:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=209
06/15/2022 10:44:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=211
06/15/2022 10:44:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=212
06/15/2022 10:44:44 - INFO - __main__ - Global step 1700 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=212
06/15/2022 10:44:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=213
06/15/2022 10:44:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=214
06/15/2022 10:44:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=216
06/15/2022 10:45:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=217
06/15/2022 10:45:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=218
06/15/2022 10:45:12 - INFO - __main__ - Global step 1750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=218
06/15/2022 10:45:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=219
06/15/2022 10:45:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.18 on epoch=221
06/15/2022 10:45:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=222
06/15/2022 10:45:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=223
06/15/2022 10:45:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=224
06/15/2022 10:45:40 - INFO - __main__ - Global step 1800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=224
06/15/2022 10:45:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=226
06/15/2022 10:45:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=227
06/15/2022 10:45:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=228
06/15/2022 10:45:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=229
06/15/2022 10:46:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=231
06/15/2022 10:46:08 - INFO - __main__ - Global step 1850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=231
06/15/2022 10:46:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=232
06/15/2022 10:46:17 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=233
06/15/2022 10:46:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.21 on epoch=234
06/15/2022 10:46:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=236
06/15/2022 10:46:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=237
06/15/2022 10:46:36 - INFO - __main__ - Global step 1900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=237
06/15/2022 10:46:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=238
06/15/2022 10:46:45 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=239
06/15/2022 10:46:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=241
06/15/2022 10:46:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=242
06/15/2022 10:46:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=243
06/15/2022 10:47:04 - INFO - __main__ - Global step 1950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=243
06/15/2022 10:47:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=244
06/15/2022 10:47:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=246
06/15/2022 10:47:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=247
06/15/2022 10:47:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=248
06/15/2022 10:47:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=249
06/15/2022 10:47:32 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=249
06/15/2022 10:47:36 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=251
06/15/2022 10:47:41 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.22 on epoch=252
06/15/2022 10:47:45 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.21 on epoch=253
06/15/2022 10:47:49 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.23 on epoch=254
06/15/2022 10:47:54 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.19 on epoch=256
06/15/2022 10:47:59 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=256
06/15/2022 10:48:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.18 on epoch=257
06/15/2022 10:48:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.20 on epoch=258
06/15/2022 10:48:13 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=259
06/15/2022 10:48:17 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=261
06/15/2022 10:48:22 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=262
06/15/2022 10:48:27 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=262
06/15/2022 10:48:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.22 on epoch=263
06/15/2022 10:48:36 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=264
06/15/2022 10:48:41 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=266
06/15/2022 10:48:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=267
06/15/2022 10:48:50 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.19 on epoch=268
06/15/2022 10:48:55 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=268
06/15/2022 10:49:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.22 on epoch=269
06/15/2022 10:49:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.19 on epoch=271
06/15/2022 10:49:09 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.18 on epoch=272
06/15/2022 10:49:13 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=273
06/15/2022 10:49:17 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.21 on epoch=274
06/15/2022 10:49:23 - INFO - __main__ - Global step 2200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=274
06/15/2022 10:49:28 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=276
06/15/2022 10:49:32 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.22 on epoch=277
06/15/2022 10:49:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=278
06/15/2022 10:49:41 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=279
06/15/2022 10:49:45 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.21 on epoch=281
06/15/2022 10:49:51 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=281
06/15/2022 10:49:55 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.18 on epoch=282
06/15/2022 10:50:00 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.18 on epoch=283
06/15/2022 10:50:04 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=284
06/15/2022 10:50:09 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.18 on epoch=286
06/15/2022 10:50:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.21 on epoch=287
06/15/2022 10:50:19 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=287
06/15/2022 10:50:23 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.23 on epoch=288
06/15/2022 10:50:28 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.21 on epoch=289
06/15/2022 10:50:32 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=291
06/15/2022 10:50:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.23 on epoch=292
06/15/2022 10:50:41 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.21 on epoch=293
06/15/2022 10:50:47 - INFO - __main__ - Global step 2350 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=293
06/15/2022 10:50:51 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.21 on epoch=294
06/15/2022 10:50:56 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.18 on epoch=296
06/15/2022 10:51:00 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=297
06/15/2022 10:51:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=298
06/15/2022 10:51:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.21 on epoch=299
06/15/2022 10:51:15 - INFO - __main__ - Global step 2400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=299
06/15/2022 10:51:19 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.22 on epoch=301
06/15/2022 10:51:24 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.20 on epoch=302
06/15/2022 10:51:28 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.20 on epoch=303
06/15/2022 10:51:32 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=304
06/15/2022 10:51:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.26 on epoch=306
06/15/2022 10:51:43 - INFO - __main__ - Global step 2450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=306
06/15/2022 10:51:47 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.24 on epoch=307
06/15/2022 10:51:51 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=308
06/15/2022 10:51:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.21 on epoch=309
06/15/2022 10:52:00 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=311
06/15/2022 10:52:05 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.19 on epoch=312
06/15/2022 10:52:10 - INFO - __main__ - Global step 2500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=312
06/15/2022 10:52:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.23 on epoch=313
06/15/2022 10:52:19 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.18 on epoch=314
06/15/2022 10:52:24 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.22 on epoch=316
06/15/2022 10:52:28 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.20 on epoch=317
06/15/2022 10:52:33 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.23 on epoch=318
06/15/2022 10:52:38 - INFO - __main__ - Global step 2550 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=318
06/15/2022 10:52:43 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=319
06/15/2022 10:52:47 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=321
06/15/2022 10:52:52 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.17 on epoch=322
06/15/2022 10:52:56 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.24 on epoch=323
06/15/2022 10:53:01 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.24 on epoch=324
06/15/2022 10:53:06 - INFO - __main__ - Global step 2600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=324
06/15/2022 10:53:11 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=326
06/15/2022 10:53:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.19 on epoch=327
06/15/2022 10:53:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.20 on epoch=328
06/15/2022 10:53:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.21 on epoch=329
06/15/2022 10:53:29 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.17 on epoch=331
06/15/2022 10:53:34 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=331
06/15/2022 10:53:39 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.21 on epoch=332
06/15/2022 10:53:43 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.21 on epoch=333
06/15/2022 10:53:48 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.21 on epoch=334
06/15/2022 10:53:52 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.17 on epoch=336
06/15/2022 10:53:57 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=337
06/15/2022 10:54:03 - INFO - __main__ - Global step 2700 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=337
06/15/2022 10:54:07 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.19 on epoch=338
06/15/2022 10:54:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.22 on epoch=339
06/15/2022 10:54:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.22 on epoch=341
06/15/2022 10:54:21 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.20 on epoch=342
06/15/2022 10:54:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.21 on epoch=343
06/15/2022 10:54:31 - INFO - __main__ - Global step 2750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=343
06/15/2022 10:54:36 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.20 on epoch=344
06/15/2022 10:54:40 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.21 on epoch=346
06/15/2022 10:54:45 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=347
06/15/2022 10:54:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.21 on epoch=348
06/15/2022 10:54:54 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=349
06/15/2022 10:54:59 - INFO - __main__ - Global step 2800 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=349
06/15/2022 10:55:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.22 on epoch=351
06/15/2022 10:55:08 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.22 on epoch=352
06/15/2022 10:55:13 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=353
06/15/2022 10:55:17 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=354
06/15/2022 10:55:22 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.20 on epoch=356
06/15/2022 10:55:27 - INFO - __main__ - Global step 2850 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=356
06/15/2022 10:55:32 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=357
06/15/2022 10:55:36 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=358
06/15/2022 10:55:41 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.20 on epoch=359
06/15/2022 10:55:45 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=361
06/15/2022 10:55:50 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.20 on epoch=362
06/15/2022 10:55:55 - INFO - __main__ - Global step 2900 Train loss 0.20 Classification-F1 0.3591989987484355 on epoch=362
06/15/2022 10:55:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3591989987484355 on epoch=362, global_step=2900
06/15/2022 10:56:00 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.18 on epoch=363
06/15/2022 10:56:04 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.22 on epoch=364
06/15/2022 10:56:09 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.20 on epoch=366
06/15/2022 10:56:13 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.20 on epoch=367
06/15/2022 10:56:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=368
06/15/2022 10:56:24 - INFO - __main__ - Global step 2950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=368
06/15/2022 10:56:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=369
06/15/2022 10:56:33 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.19 on epoch=371
06/15/2022 10:56:37 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.21 on epoch=372
06/15/2022 10:56:41 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.22 on epoch=373
06/15/2022 10:56:46 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.21 on epoch=374
06/15/2022 10:56:47 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:56:47 - INFO - __main__ - Printing 3 examples
06/15/2022 10:56:47 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 10:56:47 - INFO - __main__ - ['entailed']
06/15/2022 10:56:47 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 10:56:47 - INFO - __main__ - ['entailed']
06/15/2022 10:56:47 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 10:56:47 - INFO - __main__ - ['entailed']
06/15/2022 10:56:47 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:56:47 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:56:48 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 10:56:48 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 10:56:48 - INFO - __main__ - Printing 3 examples
06/15/2022 10:56:48 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 10:56:48 - INFO - __main__ - ['entailed']
06/15/2022 10:56:48 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 10:56:48 - INFO - __main__ - ['entailed']
06/15/2022 10:56:48 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 10:56:48 - INFO - __main__ - ['entailed']
06/15/2022 10:56:48 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:56:48 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:56:48 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 10:56:52 - INFO - __main__ - Global step 3000 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=374
06/15/2022 10:56:52 - INFO - __main__ - save last model!
06/15/2022 10:56:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 10:56:52 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 10:56:52 - INFO - __main__ - Printing 3 examples
06/15/2022 10:56:52 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:56:52 - INFO - __main__ - ['entailed']
06/15/2022 10:56:52 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:56:52 - INFO - __main__ - ['entailed']
06/15/2022 10:56:52 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 10:56:52 - INFO - __main__ - ['entailed']
06/15/2022 10:56:52 - INFO - __main__ - Tokenizing Input ...
06/15/2022 10:57:06 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 10:57:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 10:57:07 - INFO - __main__ - Starting training!
06/15/2022 10:57:17 - INFO - __main__ - Tokenizing Output ...
06/15/2022 10:57:29 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 11:07:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_87_0.5_8_predictions.txt
06/15/2022 11:07:05 - INFO - __main__ - Classification-F1 on test data: 0.3305
06/15/2022 11:07:05 - INFO - __main__ - prefix=tab_fact_64_87, lr=0.5, bsz=8, dev_performance=0.3591989987484355, test_performance=0.33047210300429186
06/15/2022 11:07:05 - INFO - __main__ - Running ... prefix=tab_fact_64_87, lr=0.4, bsz=8 ...
06/15/2022 11:07:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:07:06 - INFO - __main__ - Printing 3 examples
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:07:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:07:06 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 11:07:06 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:07:06 - INFO - __main__ - Printing 3 examples
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 11:07:06 - INFO - __main__ - ['entailed']
06/15/2022 11:07:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:07:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:07:07 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 11:07:25 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 11:07:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 11:07:26 - INFO - __main__ - Starting training!
06/15/2022 11:07:31 - INFO - __main__ - Step 10 Global step 10 Train loss 2.62 on epoch=1
06/15/2022 11:07:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=2
06/15/2022 11:07:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.36 on epoch=3
06/15/2022 11:07:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.32 on epoch=4
06/15/2022 11:07:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=6
06/15/2022 11:07:55 - INFO - __main__ - Global step 50 Train loss 0.85 Classification-F1 0.350463149416029 on epoch=6
06/15/2022 11:07:55 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.350463149416029 on epoch=6, global_step=50
06/15/2022 11:07:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=7
06/15/2022 11:08:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=8
06/15/2022 11:08:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=9
06/15/2022 11:08:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=11
06/15/2022 11:08:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=12
06/15/2022 11:08:23 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 11:08:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=13
06/15/2022 11:08:32 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=14
06/15/2022 11:08:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=16
06/15/2022 11:08:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=17
06/15/2022 11:08:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=18
06/15/2022 11:08:51 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 11:08:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=19
06/15/2022 11:09:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=21
06/15/2022 11:09:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=22
06/15/2022 11:09:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.21 on epoch=23
06/15/2022 11:09:14 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=24
06/15/2022 11:09:19 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.350463149416029 on epoch=24
06/15/2022 11:09:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=26
06/15/2022 11:09:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=27
06/15/2022 11:09:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=28
06/15/2022 11:09:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=29
06/15/2022 11:09:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=31
06/15/2022 11:09:48 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 11:09:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=32
06/15/2022 11:09:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=33
06/15/2022 11:10:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=34
06/15/2022 11:10:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=36
06/15/2022 11:10:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=37
06/15/2022 11:10:15 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 11:10:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=38
06/15/2022 11:10:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=39
06/15/2022 11:10:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=41
06/15/2022 11:10:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.19 on epoch=42
06/15/2022 11:10:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=43
06/15/2022 11:10:43 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 11:10:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=44
06/15/2022 11:10:52 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=46
06/15/2022 11:10:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=47
06/15/2022 11:11:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=48
06/15/2022 11:11:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.22 on epoch=49
06/15/2022 11:11:11 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 11:11:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=51
06/15/2022 11:11:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=52
06/15/2022 11:11:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=53
06/15/2022 11:11:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=54
06/15/2022 11:11:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=56
06/15/2022 11:11:39 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 11:11:43 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=57
06/15/2022 11:11:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=58
06/15/2022 11:11:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=59
06/15/2022 11:11:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=61
06/15/2022 11:12:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 11:12:07 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 11:12:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=63
06/15/2022 11:12:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=64
06/15/2022 11:12:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 11:12:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=67
06/15/2022 11:12:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=68
06/15/2022 11:12:34 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 11:12:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=69
06/15/2022 11:12:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 11:12:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=72
06/15/2022 11:12:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=73
06/15/2022 11:12:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=74
06/15/2022 11:13:02 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 11:13:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 11:13:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=77
06/15/2022 11:13:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=78
06/15/2022 11:13:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=79
06/15/2022 11:13:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=81
06/15/2022 11:13:30 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 11:13:34 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=82
06/15/2022 11:13:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 11:13:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=84
06/15/2022 11:13:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=86
06/15/2022 11:13:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=87
06/15/2022 11:13:59 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.46895191457525676 on epoch=87
06/15/2022 11:13:59 - INFO - __main__ - Saving model with best Classification-F1: 0.350463149416029 -> 0.46895191457525676 on epoch=87, global_step=700
06/15/2022 11:14:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=88
06/15/2022 11:14:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 11:14:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=91
06/15/2022 11:14:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=92
06/15/2022 11:14:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=93
06/15/2022 11:14:27 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 11:14:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=94
06/15/2022 11:14:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 11:14:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=97
06/15/2022 11:14:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=98
06/15/2022 11:14:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=99
06/15/2022 11:14:55 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.45718194254445965 on epoch=99
06/15/2022 11:14:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=101
06/15/2022 11:15:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=102
06/15/2022 11:15:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=103
06/15/2022 11:15:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=104
06/15/2022 11:15:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=106
06/15/2022 11:15:23 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4345381526104417 on epoch=106
06/15/2022 11:15:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=107
06/15/2022 11:15:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=108
06/15/2022 11:15:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=109
06/15/2022 11:15:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=111
06/15/2022 11:15:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=112
06/15/2022 11:15:51 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 11:15:55 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=113
06/15/2022 11:16:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=114
06/15/2022 11:16:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=116
06/15/2022 11:16:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=117
06/15/2022 11:16:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=118
06/15/2022 11:16:19 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 11:16:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=119
06/15/2022 11:16:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=121
06/15/2022 11:16:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=122
06/15/2022 11:16:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 11:16:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=124
06/15/2022 11:16:47 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.40116959064327484 on epoch=124
06/15/2022 11:16:52 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=126
06/15/2022 11:16:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=127
06/15/2022 11:17:01 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=128
06/15/2022 11:17:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 11:17:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=131
06/15/2022 11:17:15 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 11:17:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=132
06/15/2022 11:17:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=133
06/15/2022 11:17:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=134
06/15/2022 11:17:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=136
06/15/2022 11:17:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=137
06/15/2022 11:17:43 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.36318407960199 on epoch=137
06/15/2022 11:17:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=138
06/15/2022 11:17:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=139
06/15/2022 11:17:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=141
06/15/2022 11:18:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=142
06/15/2022 11:18:06 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=143
06/15/2022 11:18:12 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.4198830409356725 on epoch=143
06/15/2022 11:18:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=144
06/15/2022 11:18:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=146
06/15/2022 11:18:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=147
06/15/2022 11:18:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=148
06/15/2022 11:18:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=149
06/15/2022 11:18:40 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.49797570850202433 on epoch=149
06/15/2022 11:18:40 - INFO - __main__ - Saving model with best Classification-F1: 0.46895191457525676 -> 0.49797570850202433 on epoch=149, global_step=1200
06/15/2022 11:18:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=151
06/15/2022 11:18:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=152
06/15/2022 11:18:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=153
06/15/2022 11:18:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=154
06/15/2022 11:19:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=156
06/15/2022 11:19:08 - INFO - __main__ - Global step 1250 Train loss 0.18 Classification-F1 0.40996830884471336 on epoch=156
06/15/2022 11:19:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=157
06/15/2022 11:19:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=158
06/15/2022 11:19:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=159
06/15/2022 11:19:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=161
06/15/2022 11:19:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=162
06/15/2022 11:19:37 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.4155251141552511 on epoch=162
06/15/2022 11:19:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=163
06/15/2022 11:19:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=164
06/15/2022 11:19:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=166
06/15/2022 11:19:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=167
06/15/2022 11:19:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=168
06/15/2022 11:20:05 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=168
06/15/2022 11:20:09 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=169
06/15/2022 11:20:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=171
06/15/2022 11:20:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=172
06/15/2022 11:20:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=173
06/15/2022 11:20:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=174
06/15/2022 11:20:33 - INFO - __main__ - Global step 1400 Train loss 0.18 Classification-F1 0.4458874458874459 on epoch=174
06/15/2022 11:20:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=176
06/15/2022 11:20:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=177
06/15/2022 11:20:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=178
06/15/2022 11:20:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=179
06/15/2022 11:20:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=181
06/15/2022 11:21:01 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.33521821081629694 on epoch=181
06/15/2022 11:21:06 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=182
06/15/2022 11:21:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=183
06/15/2022 11:21:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.17 on epoch=184
06/15/2022 11:21:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=186
06/15/2022 11:21:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=187
06/15/2022 11:21:29 - INFO - __main__ - Global step 1500 Train loss 0.18 Classification-F1 0.28505291005291006 on epoch=187
06/15/2022 11:21:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=188
06/15/2022 11:21:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=189
06/15/2022 11:21:42 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=191
06/15/2022 11:21:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=192
06/15/2022 11:21:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=193
06/15/2022 11:21:57 - INFO - __main__ - Global step 1550 Train loss 0.17 Classification-F1 0.422996703222768 on epoch=193
06/15/2022 11:22:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=194
06/15/2022 11:22:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=196
06/15/2022 11:22:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.18 on epoch=197
06/15/2022 11:22:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=198
06/15/2022 11:22:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.15 on epoch=199
06/15/2022 11:22:25 - INFO - __main__ - Global step 1600 Train loss 0.17 Classification-F1 0.48840927258193445 on epoch=199
06/15/2022 11:22:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=201
06/15/2022 11:22:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=202
06/15/2022 11:22:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=203
06/15/2022 11:22:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=204
06/15/2022 11:22:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=206
06/15/2022 11:22:53 - INFO - __main__ - Global step 1650 Train loss 0.16 Classification-F1 0.4832395400048935 on epoch=206
06/15/2022 11:22:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=207
06/15/2022 11:23:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.15 on epoch=208
06/15/2022 11:23:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=209
06/15/2022 11:23:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=211
06/15/2022 11:23:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=212
06/15/2022 11:23:21 - INFO - __main__ - Global step 1700 Train loss 0.15 Classification-F1 0.4394394394394394 on epoch=212
06/15/2022 11:23:26 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=213
06/15/2022 11:23:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=214
06/15/2022 11:23:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=216
06/15/2022 11:23:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=217
06/15/2022 11:23:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=218
06/15/2022 11:23:49 - INFO - __main__ - Global step 1750 Train loss 0.14 Classification-F1 0.4685404024273395 on epoch=218
06/15/2022 11:23:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=219
06/15/2022 11:23:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.15 on epoch=221
06/15/2022 11:24:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=222
06/15/2022 11:24:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=223
06/15/2022 11:24:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=224
06/15/2022 11:24:17 - INFO - __main__ - Global step 1800 Train loss 0.13 Classification-F1 0.46716371543957746 on epoch=224
06/15/2022 11:24:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.13 on epoch=226
06/15/2022 11:24:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=227
06/15/2022 11:24:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.15 on epoch=228
06/15/2022 11:24:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=229
06/15/2022 11:24:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=231
06/15/2022 11:24:46 - INFO - __main__ - Global step 1850 Train loss 0.14 Classification-F1 0.46471080229042006 on epoch=231
06/15/2022 11:24:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=232
06/15/2022 11:24:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=233
06/15/2022 11:24:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=234
06/15/2022 11:25:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=236
06/15/2022 11:25:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=237
06/15/2022 11:25:14 - INFO - __main__ - Global step 1900 Train loss 0.12 Classification-F1 0.4906642179369452 on epoch=237
06/15/2022 11:25:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=238
06/15/2022 11:25:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=239
06/15/2022 11:25:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=241
06/15/2022 11:25:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=242
06/15/2022 11:25:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=243
06/15/2022 11:25:42 - INFO - __main__ - Global step 1950 Train loss 0.13 Classification-F1 0.4102117061021171 on epoch=243
06/15/2022 11:25:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=244
06/15/2022 11:25:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=246
06/15/2022 11:25:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=247
06/15/2022 11:26:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=248
06/15/2022 11:26:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=249
06/15/2022 11:26:10 - INFO - __main__ - Global step 2000 Train loss 0.12 Classification-F1 0.4217463951977226 on epoch=249
06/15/2022 11:26:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.12 on epoch=251
06/15/2022 11:26:19 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.13 on epoch=252
06/15/2022 11:26:23 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.13 on epoch=253
06/15/2022 11:26:28 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.13 on epoch=254
06/15/2022 11:26:32 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.13 on epoch=256
06/15/2022 11:26:38 - INFO - __main__ - Global step 2050 Train loss 0.13 Classification-F1 0.46558704453441296 on epoch=256
06/15/2022 11:26:43 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.09 on epoch=257
06/15/2022 11:26:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.12 on epoch=258
06/15/2022 11:26:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.09 on epoch=259
06/15/2022 11:26:56 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.11 on epoch=261
06/15/2022 11:27:00 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.10 on epoch=262
06/15/2022 11:27:06 - INFO - __main__ - Global step 2100 Train loss 0.10 Classification-F1 0.45343152422798433 on epoch=262
06/15/2022 11:27:11 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.14 on epoch=263
06/15/2022 11:27:15 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.12 on epoch=264
06/15/2022 11:27:20 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.13 on epoch=266
06/15/2022 11:27:24 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.13 on epoch=267
06/15/2022 11:27:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.14 on epoch=268
06/15/2022 11:27:34 - INFO - __main__ - Global step 2150 Train loss 0.13 Classification-F1 0.46208367308536663 on epoch=268
06/15/2022 11:27:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.12 on epoch=269
06/15/2022 11:27:43 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.12 on epoch=271
06/15/2022 11:27:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.10 on epoch=272
06/15/2022 11:27:52 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.10 on epoch=273
06/15/2022 11:27:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.10 on epoch=274
06/15/2022 11:28:03 - INFO - __main__ - Global step 2200 Train loss 0.11 Classification-F1 0.409703006825309 on epoch=274
06/15/2022 11:28:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.12 on epoch=276
06/15/2022 11:28:11 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.09 on epoch=277
06/15/2022 11:28:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.09 on epoch=278
06/15/2022 11:28:20 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.10 on epoch=279
06/15/2022 11:28:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.08 on epoch=281
06/15/2022 11:28:31 - INFO - __main__ - Global step 2250 Train loss 0.10 Classification-F1 0.4217463951977226 on epoch=281
06/15/2022 11:28:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.09 on epoch=282
06/15/2022 11:28:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.08 on epoch=283
06/15/2022 11:28:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.10 on epoch=284
06/15/2022 11:28:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.10 on epoch=286
06/15/2022 11:28:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.09 on epoch=287
06/15/2022 11:28:59 - INFO - __main__ - Global step 2300 Train loss 0.09 Classification-F1 0.4560313828048382 on epoch=287
06/15/2022 11:29:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.12 on epoch=288
06/15/2022 11:29:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.11 on epoch=289
06/15/2022 11:29:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.12 on epoch=291
06/15/2022 11:29:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.10 on epoch=292
06/15/2022 11:29:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.07 on epoch=293
06/15/2022 11:29:27 - INFO - __main__ - Global step 2350 Train loss 0.10 Classification-F1 0.4812085482682388 on epoch=293
06/15/2022 11:29:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.09 on epoch=294
06/15/2022 11:29:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.12 on epoch=296
06/15/2022 11:29:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.08 on epoch=297
06/15/2022 11:29:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.05 on epoch=298
06/15/2022 11:29:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.08 on epoch=299
06/15/2022 11:29:55 - INFO - __main__ - Global step 2400 Train loss 0.08 Classification-F1 0.4812085482682388 on epoch=299
06/15/2022 11:29:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.09 on epoch=301
06/15/2022 11:30:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.10 on epoch=302
06/15/2022 11:30:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.08 on epoch=303
06/15/2022 11:30:13 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.09 on epoch=304
06/15/2022 11:30:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.10 on epoch=306
06/15/2022 11:30:23 - INFO - __main__ - Global step 2450 Train loss 0.09 Classification-F1 0.4606412213740458 on epoch=306
06/15/2022 11:30:28 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.06 on epoch=307
06/15/2022 11:30:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.09 on epoch=308
06/15/2022 11:30:37 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.07 on epoch=309
06/15/2022 11:30:41 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.08 on epoch=311
06/15/2022 11:30:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.09 on epoch=312
06/15/2022 11:30:51 - INFO - __main__ - Global step 2500 Train loss 0.08 Classification-F1 0.46716371543957746 on epoch=312
06/15/2022 11:30:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.07 on epoch=313
06/15/2022 11:31:00 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.05 on epoch=314
06/15/2022 11:31:05 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.09 on epoch=316
06/15/2022 11:31:09 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.08 on epoch=317
06/15/2022 11:31:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.09 on epoch=318
06/15/2022 11:31:19 - INFO - __main__ - Global step 2550 Train loss 0.07 Classification-F1 0.48424908424908425 on epoch=318
06/15/2022 11:31:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.10 on epoch=319
06/15/2022 11:31:28 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.07 on epoch=321
06/15/2022 11:31:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.05 on epoch=322
06/15/2022 11:31:37 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.04 on epoch=323
06/15/2022 11:31:42 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.07 on epoch=324
06/15/2022 11:31:48 - INFO - __main__ - Global step 2600 Train loss 0.07 Classification-F1 0.45259042033235586 on epoch=324
06/15/2022 11:31:52 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.07 on epoch=326
06/15/2022 11:31:56 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.06 on epoch=327
06/15/2022 11:32:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.08 on epoch=328
06/15/2022 11:32:05 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.08 on epoch=329
06/15/2022 11:32:10 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.09 on epoch=331
06/15/2022 11:32:16 - INFO - __main__ - Global step 2650 Train loss 0.07 Classification-F1 0.4458874458874459 on epoch=331
06/15/2022 11:32:20 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.03 on epoch=332
06/15/2022 11:32:25 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.11 on epoch=333
06/15/2022 11:32:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.06 on epoch=334
06/15/2022 11:32:34 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.06 on epoch=336
06/15/2022 11:32:38 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.05 on epoch=337
06/15/2022 11:32:44 - INFO - __main__ - Global step 2700 Train loss 0.06 Classification-F1 0.43647798742138366 on epoch=337
06/15/2022 11:32:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.05 on epoch=338
06/15/2022 11:32:53 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.05 on epoch=339
06/15/2022 11:32:57 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.11 on epoch=341
06/15/2022 11:33:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.06 on epoch=342
06/15/2022 11:33:06 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.05 on epoch=343
06/15/2022 11:33:12 - INFO - __main__ - Global step 2750 Train loss 0.07 Classification-F1 0.48840927258193445 on epoch=343
06/15/2022 11:33:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.07 on epoch=344
06/15/2022 11:33:21 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.10 on epoch=346
06/15/2022 11:33:25 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.09 on epoch=347
06/15/2022 11:33:30 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.04 on epoch=348
06/15/2022 11:33:34 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.05 on epoch=349
06/15/2022 11:33:40 - INFO - __main__ - Global step 2800 Train loss 0.07 Classification-F1 0.4995112414467253 on epoch=349
06/15/2022 11:33:40 - INFO - __main__ - Saving model with best Classification-F1: 0.49797570850202433 -> 0.4995112414467253 on epoch=349, global_step=2800
06/15/2022 11:33:45 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.05 on epoch=351
06/15/2022 11:33:49 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.01 on epoch=352
06/15/2022 11:33:54 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.06 on epoch=353
06/15/2022 11:33:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.03 on epoch=354
06/15/2022 11:34:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.05 on epoch=356
06/15/2022 11:34:08 - INFO - __main__ - Global step 2850 Train loss 0.04 Classification-F1 0.50633608815427 on epoch=356
06/15/2022 11:34:08 - INFO - __main__ - Saving model with best Classification-F1: 0.4995112414467253 -> 0.50633608815427 on epoch=356, global_step=2850
06/15/2022 11:34:13 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.04 on epoch=357
06/15/2022 11:34:17 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.04 on epoch=358
06/15/2022 11:34:22 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.07 on epoch=359
06/15/2022 11:34:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.07 on epoch=361
06/15/2022 11:34:31 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.06 on epoch=362
06/15/2022 11:34:36 - INFO - __main__ - Global step 2900 Train loss 0.05 Classification-F1 0.48424908424908425 on epoch=362
06/15/2022 11:34:41 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.04 on epoch=363
06/15/2022 11:34:45 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.06 on epoch=364
06/15/2022 11:34:50 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.06 on epoch=366
06/15/2022 11:34:54 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.05 on epoch=367
06/15/2022 11:34:59 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.04 on epoch=368
06/15/2022 11:35:04 - INFO - __main__ - Global step 2950 Train loss 0.05 Classification-F1 0.4749923477196204 on epoch=368
06/15/2022 11:35:09 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.03 on epoch=369
06/15/2022 11:35:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.06 on epoch=371
06/15/2022 11:35:18 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.06 on epoch=372
06/15/2022 11:35:22 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.05 on epoch=373
06/15/2022 11:35:27 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.05 on epoch=374
06/15/2022 11:35:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:35:28 - INFO - __main__ - Printing 3 examples
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:35:28 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:35:28 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 11:35:28 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:35:28 - INFO - __main__ - Printing 3 examples
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 11:35:28 - INFO - __main__ - ['entailed']
06/15/2022 11:35:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:35:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:35:29 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 11:35:32 - INFO - __main__ - Global step 3000 Train loss 0.05 Classification-F1 0.464039408866995 on epoch=374
06/15/2022 11:35:32 - INFO - __main__ - save last model!
06/15/2022 11:35:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 11:35:32 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 11:35:33 - INFO - __main__ - Printing 3 examples
06/15/2022 11:35:33 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 11:35:33 - INFO - __main__ - ['entailed']
06/15/2022 11:35:33 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 11:35:33 - INFO - __main__ - ['entailed']
06/15/2022 11:35:33 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 11:35:33 - INFO - __main__ - ['entailed']
06/15/2022 11:35:33 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:35:47 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 11:35:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 11:35:48 - INFO - __main__ - Starting training!
06/15/2022 11:35:57 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:36:10 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 11:45:17 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_87_0.4_8_predictions.txt
06/15/2022 11:45:17 - INFO - __main__ - Classification-F1 on test data: 0.4980
06/15/2022 11:45:18 - INFO - __main__ - prefix=tab_fact_64_87, lr=0.4, bsz=8, dev_performance=0.50633608815427, test_performance=0.4980070758140711
06/15/2022 11:45:18 - INFO - __main__ - Running ... prefix=tab_fact_64_87, lr=0.3, bsz=8 ...
06/15/2022 11:45:19 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:45:19 - INFO - __main__ - Printing 3 examples
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:45:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:45:19 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 11:45:19 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 11:45:19 - INFO - __main__ - Printing 3 examples
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 11:45:19 - INFO - __main__ - ['entailed']
06/15/2022 11:45:19 - INFO - __main__ - Tokenizing Input ...
06/15/2022 11:45:19 - INFO - __main__ - Tokenizing Output ...
06/15/2022 11:45:19 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 11:45:38 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 11:45:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 11:45:39 - INFO - __main__ - Starting training!
06/15/2022 11:45:44 - INFO - __main__ - Step 10 Global step 10 Train loss 3.55 on epoch=1
06/15/2022 11:45:49 - INFO - __main__ - Step 20 Global step 20 Train loss 0.80 on epoch=2
06/15/2022 11:45:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=3
06/15/2022 11:45:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=4
06/15/2022 11:46:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=6
06/15/2022 11:46:07 - INFO - __main__ - Global step 50 Train loss 1.10 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 11:46:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 11:46:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=7
06/15/2022 11:46:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=8
06/15/2022 11:46:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=9
06/15/2022 11:46:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=11
06/15/2022 11:46:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=12
06/15/2022 11:46:34 - INFO - __main__ - Global step 100 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 11:46:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 11:46:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=14
06/15/2022 11:46:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=16
06/15/2022 11:46:51 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=17
06/15/2022 11:46:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.23 on epoch=18
06/15/2022 11:47:01 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 11:47:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=19
06/15/2022 11:47:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=21
06/15/2022 11:47:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=22
06/15/2022 11:47:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=23
06/15/2022 11:47:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=24
06/15/2022 11:47:29 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 11:47:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=26
06/15/2022 11:47:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=27
06/15/2022 11:47:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=28
06/15/2022 11:47:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=29
06/15/2022 11:47:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=31
06/15/2022 11:47:57 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 11:48:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=32
06/15/2022 11:48:06 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=33
06/15/2022 11:48:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.20 on epoch=34
06/15/2022 11:48:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=36
06/15/2022 11:48:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=37
06/15/2022 11:48:25 - INFO - __main__ - Global step 300 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 11:48:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=38
06/15/2022 11:48:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=39
06/15/2022 11:48:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=41
06/15/2022 11:48:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 11:48:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=43
06/15/2022 11:48:52 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 11:48:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=44
06/15/2022 11:49:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=46
06/15/2022 11:49:05 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=47
06/15/2022 11:49:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=48
06/15/2022 11:49:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.18 on epoch=49
06/15/2022 11:49:21 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 11:49:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=51
06/15/2022 11:49:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=52
06/15/2022 11:49:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=53
06/15/2022 11:49:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=54
06/15/2022 11:49:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 11:49:49 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 11:49:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=57
06/15/2022 11:49:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=58
06/15/2022 11:50:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=59
06/15/2022 11:50:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=61
06/15/2022 11:50:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=62
06/15/2022 11:50:17 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 11:50:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=63
06/15/2022 11:50:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=64
06/15/2022 11:50:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=66
06/15/2022 11:50:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=67
06/15/2022 11:50:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=68
06/15/2022 11:50:44 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 11:50:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 11:50:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=71
06/15/2022 11:50:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=72
06/15/2022 11:51:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=73
06/15/2022 11:51:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=74
06/15/2022 11:51:12 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 11:51:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=76
06/15/2022 11:51:21 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=77
06/15/2022 11:51:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=78
06/15/2022 11:51:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=79
06/15/2022 11:51:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=81
06/15/2022 11:51:40 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 11:51:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=82
06/15/2022 11:51:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=83
06/15/2022 11:51:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=84
06/15/2022 11:51:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.17 on epoch=86
06/15/2022 11:52:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=87
06/15/2022 11:52:07 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 11:52:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=88
06/15/2022 11:52:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 11:52:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=91
06/15/2022 11:52:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=92
06/15/2022 11:52:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=93
06/15/2022 11:52:35 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 11:52:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=94
06/15/2022 11:52:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.21 on epoch=96
06/15/2022 11:52:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=97
06/15/2022 11:52:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=98
06/15/2022 11:52:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=99
06/15/2022 11:53:03 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.4460264692968701 on epoch=99
06/15/2022 11:53:03 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4460264692968701 on epoch=99, global_step=800
06/15/2022 11:53:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=101
06/15/2022 11:53:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=102
06/15/2022 11:53:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=103
06/15/2022 11:53:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=104
06/15/2022 11:53:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=106
06/15/2022 11:53:31 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 11:53:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=107
06/15/2022 11:53:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=108
06/15/2022 11:53:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=109
06/15/2022 11:53:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=111
06/15/2022 11:53:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=112
06/15/2022 11:53:59 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.4258480515839641 on epoch=112
06/15/2022 11:54:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=113
06/15/2022 11:54:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=114
06/15/2022 11:54:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=116
06/15/2022 11:54:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=117
06/15/2022 11:54:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=118
06/15/2022 11:54:27 - INFO - __main__ - Global step 950 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 11:54:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=119
06/15/2022 11:54:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=121
06/15/2022 11:54:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=122
06/15/2022 11:54:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=123
06/15/2022 11:54:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=124
06/15/2022 11:54:54 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.38714090287277697 on epoch=124
06/15/2022 11:54:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=126
06/15/2022 11:55:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=127
06/15/2022 11:55:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=128
06/15/2022 11:55:12 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.21 on epoch=129
06/15/2022 11:55:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.19 on epoch=131
06/15/2022 11:55:22 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.38245614035087716 on epoch=131
06/15/2022 11:55:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=132
06/15/2022 11:55:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.20 on epoch=133
06/15/2022 11:55:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=134
06/15/2022 11:55:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=136
06/15/2022 11:55:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=137
06/15/2022 11:55:50 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.38714090287277697 on epoch=137
06/15/2022 11:55:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=138
06/15/2022 11:55:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=139
06/15/2022 11:56:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=141
06/15/2022 11:56:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.19 on epoch=142
06/15/2022 11:56:12 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=143
06/15/2022 11:56:18 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.4920634920634921 on epoch=143
06/15/2022 11:56:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4460264692968701 -> 0.4920634920634921 on epoch=143, global_step=1150
06/15/2022 11:56:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=144
06/15/2022 11:56:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=146
06/15/2022 11:56:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=147
06/15/2022 11:56:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=148
06/15/2022 11:56:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=149
06/15/2022 11:56:46 - INFO - __main__ - Global step 1200 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=149
06/15/2022 11:56:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 11:56:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=152
06/15/2022 11:56:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=153
06/15/2022 11:57:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.75 on epoch=154
06/15/2022 11:57:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.26 on epoch=156
06/15/2022 11:57:14 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.47396184751272774 on epoch=156
06/15/2022 11:57:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=157
06/15/2022 11:57:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=158
06/15/2022 11:57:27 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=159
06/15/2022 11:57:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=161
06/15/2022 11:57:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=162
06/15/2022 11:57:42 - INFO - __main__ - Global step 1300 Train loss 0.22 Classification-F1 0.48424908424908425 on epoch=162
06/15/2022 11:57:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=163
06/15/2022 11:57:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=164
06/15/2022 11:57:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=166
06/15/2022 11:57:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=167
06/15/2022 11:58:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.23 on epoch=168
06/15/2022 11:58:09 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.4392938868911409 on epoch=168
06/15/2022 11:58:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=169
06/15/2022 11:58:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=171
06/15/2022 11:58:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=172
06/15/2022 11:58:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.80 on epoch=173
06/15/2022 11:58:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.96 on epoch=174
06/15/2022 11:58:37 - INFO - __main__ - Global step 1400 Train loss 0.46 Classification-F1 0.47848230201171377 on epoch=174
06/15/2022 11:58:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.93 on epoch=176
06/15/2022 11:58:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.94 on epoch=177
06/15/2022 11:58:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.49 on epoch=178
06/15/2022 11:58:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=179
06/15/2022 11:58:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.37 on epoch=181
06/15/2022 11:59:05 - INFO - __main__ - Global step 1450 Train loss 0.87 Classification-F1 0.4781408768738631 on epoch=181
06/15/2022 11:59:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=182
06/15/2022 11:59:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.33 on epoch=183
06/15/2022 11:59:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=184
06/15/2022 11:59:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=186
06/15/2022 11:59:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=187
06/15/2022 11:59:33 - INFO - __main__ - Global step 1500 Train loss 0.38 Classification-F1 0.5195195195195195 on epoch=187
06/15/2022 11:59:33 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5195195195195195 on epoch=187, global_step=1500
06/15/2022 11:59:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=188
06/15/2022 11:59:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.17 on epoch=189
06/15/2022 11:59:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=191
06/15/2022 11:59:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=192
06/15/2022 11:59:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=193
06/15/2022 12:00:01 - INFO - __main__ - Global step 1550 Train loss 0.18 Classification-F1 0.5423696920278492 on epoch=193
06/15/2022 12:00:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5423696920278492 on epoch=193, global_step=1550
06/15/2022 12:00:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=194
06/15/2022 12:00:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.17 on epoch=196
06/15/2022 12:00:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=197
06/15/2022 12:00:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=198
06/15/2022 12:00:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=199
06/15/2022 12:00:29 - INFO - __main__ - Global step 1600 Train loss 0.19 Classification-F1 0.5040041332988892 on epoch=199
06/15/2022 12:00:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=201
06/15/2022 12:00:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.20 on epoch=202
06/15/2022 12:00:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=203
06/15/2022 12:00:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=204
06/15/2022 12:00:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=206
06/15/2022 12:00:56 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.5355355355355356 on epoch=206
06/15/2022 12:01:01 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=207
06/15/2022 12:01:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=208
06/15/2022 12:01:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=209
06/15/2022 12:01:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=211
06/15/2022 12:01:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=212
06/15/2022 12:01:24 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.47885474126608885 on epoch=212
06/15/2022 12:01:29 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=213
06/15/2022 12:01:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=214
06/15/2022 12:01:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.19 on epoch=216
06/15/2022 12:01:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=217
06/15/2022 12:01:47 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=218
06/15/2022 12:01:52 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.45071982281284606 on epoch=218
06/15/2022 12:01:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=219
06/15/2022 12:02:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=221
06/15/2022 12:02:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=222
06/15/2022 12:02:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=223
06/15/2022 12:02:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=224
06/15/2022 12:02:20 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.45744466123931915 on epoch=224
06/15/2022 12:02:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=226
06/15/2022 12:02:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=227
06/15/2022 12:02:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=228
06/15/2022 12:02:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=229
06/15/2022 12:02:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=231
06/15/2022 12:02:48 - INFO - __main__ - Global step 1850 Train loss 0.19 Classification-F1 0.5303643724696356 on epoch=231
06/15/2022 12:02:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=232
06/15/2022 12:02:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=233
06/15/2022 12:03:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=234
06/15/2022 12:03:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=236
06/15/2022 12:03:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=237
06/15/2022 12:03:16 - INFO - __main__ - Global step 1900 Train loss 0.19 Classification-F1 0.5141221128482274 on epoch=237
06/15/2022 12:03:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=238
06/15/2022 12:03:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=239
06/15/2022 12:03:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=241
06/15/2022 12:03:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=242
06/15/2022 12:03:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=243
06/15/2022 12:03:44 - INFO - __main__ - Global step 1950 Train loss 0.18 Classification-F1 0.4452012383900929 on epoch=243
06/15/2022 12:03:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=244
06/15/2022 12:03:53 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=246
06/15/2022 12:03:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=247
06/15/2022 12:04:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=248
06/15/2022 12:04:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=249
06/15/2022 12:04:12 - INFO - __main__ - Global step 2000 Train loss 0.19 Classification-F1 0.5273745861981156 on epoch=249
06/15/2022 12:04:16 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.19 on epoch=251
06/15/2022 12:04:21 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=252
06/15/2022 12:04:25 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.20 on epoch=253
06/15/2022 12:04:29 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.20 on epoch=254
06/15/2022 12:04:34 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.24 on epoch=256
06/15/2022 12:04:39 - INFO - __main__ - Global step 2050 Train loss 0.21 Classification-F1 0.4937431109382092 on epoch=256
06/15/2022 12:04:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.19 on epoch=257
06/15/2022 12:04:48 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.18 on epoch=258
06/15/2022 12:04:53 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.19 on epoch=259
06/15/2022 12:04:57 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.23 on epoch=261
06/15/2022 12:05:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.20 on epoch=262
06/15/2022 12:05:07 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.46204360388757554 on epoch=262
06/15/2022 12:05:12 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.21 on epoch=263
06/15/2022 12:05:16 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.20 on epoch=264
06/15/2022 12:05:21 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.21 on epoch=266
06/15/2022 12:05:25 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.19 on epoch=267
06/15/2022 12:05:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.17 on epoch=268
06/15/2022 12:05:35 - INFO - __main__ - Global step 2150 Train loss 0.20 Classification-F1 0.5040041332988892 on epoch=268
06/15/2022 12:05:40 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=269
06/15/2022 12:05:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.17 on epoch=271
06/15/2022 12:05:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.19 on epoch=272
06/15/2022 12:05:53 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.17 on epoch=273
06/15/2022 12:05:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.17 on epoch=274
06/15/2022 12:06:03 - INFO - __main__ - Global step 2200 Train loss 0.18 Classification-F1 0.48689485044711683 on epoch=274
06/15/2022 12:06:07 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.20 on epoch=276
06/15/2022 12:06:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.21 on epoch=277
06/15/2022 12:06:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=278
06/15/2022 12:06:21 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.20 on epoch=279
06/15/2022 12:06:25 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.18 on epoch=281
06/15/2022 12:06:31 - INFO - __main__ - Global step 2250 Train loss 0.20 Classification-F1 0.5102552844508562 on epoch=281
06/15/2022 12:06:35 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.20 on epoch=282
06/15/2022 12:06:40 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.19 on epoch=283
06/15/2022 12:06:44 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.20 on epoch=284
06/15/2022 12:06:49 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.22 on epoch=286
06/15/2022 12:06:53 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=287
06/15/2022 12:06:59 - INFO - __main__ - Global step 2300 Train loss 0.20 Classification-F1 0.49556650246305417 on epoch=287
06/15/2022 12:07:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.17 on epoch=288
06/15/2022 12:07:08 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.19 on epoch=289
06/15/2022 12:07:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.19 on epoch=291
06/15/2022 12:07:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.16 on epoch=292
06/15/2022 12:07:21 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=293
06/15/2022 12:07:27 - INFO - __main__ - Global step 2350 Train loss 0.18 Classification-F1 0.49692950135101943 on epoch=293
06/15/2022 12:07:31 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.16 on epoch=294
06/15/2022 12:07:36 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=296
06/15/2022 12:07:40 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.21 on epoch=297
06/15/2022 12:07:44 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.22 on epoch=298
06/15/2022 12:07:49 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.19 on epoch=299
06/15/2022 12:07:55 - INFO - __main__ - Global step 2400 Train loss 0.19 Classification-F1 0.49672346002621226 on epoch=299
06/15/2022 12:07:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.18 on epoch=301
06/15/2022 12:08:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.18 on epoch=302
06/15/2022 12:08:08 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.19 on epoch=303
06/15/2022 12:08:12 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.17 on epoch=304
06/15/2022 12:08:17 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.19 on epoch=306
06/15/2022 12:08:23 - INFO - __main__ - Global step 2450 Train loss 0.18 Classification-F1 0.4899128268991283 on epoch=306
06/15/2022 12:08:27 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.18 on epoch=307
06/15/2022 12:08:32 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.21 on epoch=308
06/15/2022 12:08:36 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.20 on epoch=309
06/15/2022 12:08:40 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.20 on epoch=311
06/15/2022 12:08:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.16 on epoch=312
06/15/2022 12:08:51 - INFO - __main__ - Global step 2500 Train loss 0.19 Classification-F1 0.5102552844508562 on epoch=312
06/15/2022 12:08:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.18 on epoch=313
06/15/2022 12:08:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=314
06/15/2022 12:09:04 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.19 on epoch=316
06/15/2022 12:09:08 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.19 on epoch=317
06/15/2022 12:09:13 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=318
06/15/2022 12:09:18 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.5075370545569221 on epoch=318
06/15/2022 12:09:23 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.19 on epoch=319
06/15/2022 12:09:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.18 on epoch=321
06/15/2022 12:09:32 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=322
06/15/2022 12:09:36 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.16 on epoch=323
06/15/2022 12:09:41 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.18 on epoch=324
06/15/2022 12:09:46 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.47619047619047616 on epoch=324
06/15/2022 12:09:51 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.19 on epoch=326
06/15/2022 12:09:55 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.17 on epoch=327
06/15/2022 12:10:00 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.18 on epoch=328
06/15/2022 12:10:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.19 on epoch=329
06/15/2022 12:10:09 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.20 on epoch=331
06/15/2022 12:10:14 - INFO - __main__ - Global step 2650 Train loss 0.19 Classification-F1 0.46204360388757554 on epoch=331
06/15/2022 12:10:19 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.17 on epoch=332
06/15/2022 12:10:23 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.22 on epoch=333
06/15/2022 12:10:28 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=334
06/15/2022 12:10:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.19 on epoch=336
06/15/2022 12:10:37 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.15 on epoch=337
06/15/2022 12:10:42 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.49215650369285235 on epoch=337
06/15/2022 12:10:47 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=338
06/15/2022 12:10:51 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.18 on epoch=339
06/15/2022 12:10:56 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.18 on epoch=341
06/15/2022 12:11:00 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.19 on epoch=342
06/15/2022 12:11:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.18 on epoch=343
06/15/2022 12:11:10 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.4899128268991283 on epoch=343
06/15/2022 12:11:15 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.17 on epoch=344
06/15/2022 12:11:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=346
06/15/2022 12:11:24 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.19 on epoch=347
06/15/2022 12:11:28 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.18 on epoch=348
06/15/2022 12:11:32 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=349
06/15/2022 12:11:38 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.49141145546793813 on epoch=349
06/15/2022 12:11:43 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=351
06/15/2022 12:11:47 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=352
06/15/2022 12:11:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.18 on epoch=353
06/15/2022 12:11:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.18 on epoch=354
06/15/2022 12:12:00 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.21 on epoch=356
06/15/2022 12:12:06 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.49692950135101943 on epoch=356
06/15/2022 12:12:11 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.17 on epoch=357
06/15/2022 12:12:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.21 on epoch=358
06/15/2022 12:12:19 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=359
06/15/2022 12:12:24 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.20 on epoch=361
06/15/2022 12:12:28 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.18 on epoch=362
06/15/2022 12:12:34 - INFO - __main__ - Global step 2900 Train loss 0.19 Classification-F1 0.4988989478835331 on epoch=362
06/15/2022 12:12:38 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.16 on epoch=363
06/15/2022 12:12:43 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.18 on epoch=364
06/15/2022 12:12:47 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=366
06/15/2022 12:12:52 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.17 on epoch=367
06/15/2022 12:12:56 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.19 on epoch=368
06/15/2022 12:13:02 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.49692950135101943 on epoch=368
06/15/2022 12:13:06 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.20 on epoch=369
06/15/2022 12:13:11 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.18 on epoch=371
06/15/2022 12:13:15 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.20 on epoch=372
06/15/2022 12:13:20 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=373
06/15/2022 12:13:24 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.19 on epoch=374
06/15/2022 12:13:26 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 12:13:26 - INFO - __main__ - Printing 3 examples
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:13:26 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:13:26 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 12:13:26 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 12:13:26 - INFO - __main__ - Printing 3 examples
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 12:13:26 - INFO - __main__ - ['entailed']
06/15/2022 12:13:26 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:13:26 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:13:26 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 12:13:30 - INFO - __main__ - Global step 3000 Train loss 0.20 Classification-F1 0.4920634920634921 on epoch=374
06/15/2022 12:13:30 - INFO - __main__ - save last model!
06/15/2022 12:13:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 12:13:30 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 12:13:30 - INFO - __main__ - Printing 3 examples
06/15/2022 12:13:30 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:13:30 - INFO - __main__ - ['entailed']
06/15/2022 12:13:30 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:13:30 - INFO - __main__ - ['entailed']
06/15/2022 12:13:30 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:13:30 - INFO - __main__ - ['entailed']
06/15/2022 12:13:30 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:13:45 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 12:13:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 12:13:46 - INFO - __main__ - Starting training!
06/15/2022 12:13:55 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:14:07 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 12:23:20 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_87_0.3_8_predictions.txt
06/15/2022 12:23:20 - INFO - __main__ - Classification-F1 on test data: 0.4895
06/15/2022 12:23:20 - INFO - __main__ - prefix=tab_fact_64_87, lr=0.3, bsz=8, dev_performance=0.5423696920278492, test_performance=0.48954375336724043
06/15/2022 12:23:20 - INFO - __main__ - Running ... prefix=tab_fact_64_87, lr=0.2, bsz=8 ...
06/15/2022 12:23:21 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 12:23:21 - INFO - __main__ - Printing 3 examples
06/15/2022 12:23:21 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin™ away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
06/15/2022 12:23:21 - INFO - __main__ - ['entailed']
06/15/2022 12:23:21 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaraní#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueño#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
06/15/2022 12:23:21 - INFO - __main__ - ['entailed']
06/15/2022 12:23:21 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#josé calderón (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#josé calderón (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#josé calderón (27)#chris bosh , carlos delfino , jamario moon (8)#josé calderón (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#josé calderón (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#josé calderón (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#josé calderón (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterović (8)#josé calderón (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#josé calderón (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#josé calderón (11)#air canada centre 19800#32 - 25 [n] 
06/15/2022 12:23:21 - INFO - __main__ - ['entailed']
06/15/2022 12:23:21 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:23:21 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:23:22 - INFO - __main__ - Loaded 128 examples from train data
06/15/2022 12:23:22 - INFO - __main__ - Start tokenizing ... 128 instances
06/15/2022 12:23:22 - INFO - __main__ - Printing 3 examples
06/15/2022 12:23:22 - INFO - __main__ -  [tab_fact] statement: the lowest attendance figure for a game be 11292 [SEP] table_caption: 2007 - 08 portland trail blazers season [SEP] table_text: #date#visitor#score#home#leading scorer#attendance#record#streak [n] 60#march 2#portland trail blazers#l 104 - 110#golden state warriors#jackson : 29#oracle arena 19596#31 - 29#l1 [n] 61#march 4#phoenix suns#l 97 - 92#portland trail blazers#roy : 25#rose garden 20595#31 - 30#l2 [n] 62#march 7#portland trail blazers#w 103 - 101#milwaukee bucks#aldridge : 29#bradley center 15537#32 - 30#w1 [n] 63#march 8#portland trail blazers#w 120 - 114 ot#new york knicks#robinson : 45#madison square garden 19763#33 - 30#w2 [n] 64#march 10#portland trail blazers#l 80 - 88#cleveland cavaliers#aldridge : 25#quicken loans arena 20213#33 - 31#l1 [n] 65#march 11#portland trail blazers#w 103 - 96#minnesota timberwolves#roy : 27#target center 13433#34 - 31#w1 [n] 66#march 13#portland trail blazers#l 85 - 96#sacramento kings#artest : 22#arco arena 13333#34 - 32#l1 [n] 67#march 15#minnesota timberwolves#w 96 - 107#portland trail blazers#aldridge : 26#rose garden 20079#35 - 32#w1 [n] 68#march 18#phoenix suns#l 111 - 98#portland trail blazers#aldridge : 31#rose garden 20580#35 - 33#l1 [n] 69#march 21#los angeles clippers#w 102 - 107#portland trail blazers#mobley : 24#rose garden 19980#36 - 33#w1 [n] 70#march 22#portland trail blazers#w 83 - 72#los angeles clippers#roy : 23#staples center 18248#37 - 33#w2 [n] 71#march 24#portland trail blazers#l 84 - 97#seattle supersonics#durant : 23#keyarena 11292#37 - 34#l1 [n] 72#march 25#washington wizards#w 82 - 102#portland trail blazers#webster : 23#rose garden 19980#38 - 34#w1 [n] 73#march 27#portland trail blazers#l 95 - 111#golden state warriors#jackson : 24#oracle arena 19732#38 - 35#l1 [n] 
06/15/2022 12:23:22 - INFO - __main__ - ['entailed']
06/15/2022 12:23:22 - INFO - __main__ -  [tab_fact] statement: game be play in april , may and [SEP] table_caption: 2007 cologne centurions season [SEP] table_text: week#date#kickoff#opponent#final score#team record#game site#attendance [n] 1#saturday , april 14#6:00 pm#hamburg sea devils#w 24 - 18#1 - 0#aol arena#20887 [n] 2#saturday , april 21#6:00 pm#frankfurt galaxy#l 13 - 18#1 - 1#rheinenergiestadion#16422 [n] 3#saturday , april 28#7:00 pm#rhein fire#w 14 - 6#2 - 1#ltu arena#21347 [n] 4#saturday , may 5#6:00 pm#berlin thunder#l 28 - 31#2 - 2#rheinenergiestadion#10084 [n] 5#sunday , may 13#4:00 pm#berlin thunder#w 24 - 10#3 - 2#olympic stadium#11995 [n] 6#saturday , may 19#6:00 pm#rhein fire#w 20 - 17#4 - 2#rheinenergiestadion#22154 [n] 7#friday , may 25#8:00 pm#amsterdam admirals#w 30 - 7#5 - 2#amsterdam arena#11714 [n] 8#saturday , june 2#6:00 pm#hamburg sea devils#l 7 - 21#5 - 3#rheinenergiestadion#10221 [n] 9#saturday , june 9#6:00 pm#amsterdam admirals#w 31 - 13#6 - 3#rheinenergiestadion#12878 [n] 
06/15/2022 12:23:22 - INFO - __main__ - ['entailed']
06/15/2022 12:23:22 - INFO - __main__ -  [tab_fact] statement: 5 of the 6 network have broadcasting hour that be 24 hour per day [SEP] table_caption: television in thailand [SEP] table_text: name#network#owner#launch date#channel ( bkk )#broadcasting area#transmitted area#broadcasting hours [n] channel 3#mcot and bangkok entertainment co , ltd#bec - tero#26 march 1970#3 / 32 (vhf / uhf)#rama iv road#bangkok#24 - hours [n] rta tv - 5#royal thai army radio and television#royal thai army#25 january 1958#5 (vhf)#sanam pao#bangkok#24 - hours [n] bbtv channel 7#bangkok broadcasting and tv co , ltd#royal thai army#1 december 1967#7 (vhf)#mo chit#bangkok#24 - hours [n] modernine tv#mcot#mcot#24 june 1955#9 (vhf)#mcot#bangkok#24 - hours [n] nbt#nbt#government#11 july 1988#11 (vhf)#vibhavadi rangsit road din daeng#bangkok#24 - hours [n] thai pbs#thai public broadcasting service#government and public#15 january 2008#29 (uhf)#vibhavadi rangsit road lak si#bangkok#21 - hours (5:00 am - 2:00 am) [n] 
06/15/2022 12:23:22 - INFO - __main__ - ['entailed']
06/15/2022 12:23:22 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:23:22 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:23:22 - INFO - __main__ - Loaded 128 examples from dev data
06/15/2022 12:23:41 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 12:23:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 12:23:41 - INFO - __main__ - Starting training!
06/15/2022 12:23:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.77 on epoch=1
06/15/2022 12:23:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.47 on epoch=2
06/15/2022 12:23:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.68 on epoch=3
06/15/2022 12:24:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=4
06/15/2022 12:24:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=6
06/15/2022 12:24:10 - INFO - __main__ - Global step 50 Train loss 1.38 Classification-F1 0.3333333333333333 on epoch=6
06/15/2022 12:24:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=6, global_step=50
06/15/2022 12:24:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=7
06/15/2022 12:24:19 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=8
06/15/2022 12:24:23 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=9
06/15/2022 12:24:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=11
06/15/2022 12:24:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=12
06/15/2022 12:24:38 - INFO - __main__ - Global step 100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=12
06/15/2022 12:24:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=13
06/15/2022 12:24:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=14
06/15/2022 12:24:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=16
06/15/2022 12:24:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=17
06/15/2022 12:25:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=18
06/15/2022 12:25:06 - INFO - __main__ - Global step 150 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=18
06/15/2022 12:25:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=19
06/15/2022 12:25:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=21
06/15/2022 12:25:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=22
06/15/2022 12:25:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=23
06/15/2022 12:25:28 - INFO - __main__ - Step 200 Global step 200 Train loss 0.28 on epoch=24
06/15/2022 12:25:34 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=24
06/15/2022 12:25:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=26
06/15/2022 12:25:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=27
06/15/2022 12:25:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=28
06/15/2022 12:25:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=29
06/15/2022 12:25:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=31
06/15/2022 12:26:02 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=31
06/15/2022 12:26:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=32
06/15/2022 12:26:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=33
06/15/2022 12:26:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=34
06/15/2022 12:26:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.21 on epoch=36
06/15/2022 12:26:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=37
06/15/2022 12:26:30 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=37
06/15/2022 12:26:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=38
06/15/2022 12:26:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=39
06/15/2022 12:26:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=41
06/15/2022 12:26:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=42
06/15/2022 12:26:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=43
06/15/2022 12:26:58 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=43
06/15/2022 12:27:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=44
06/15/2022 12:27:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=46
06/15/2022 12:27:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=47
06/15/2022 12:27:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=48
06/15/2022 12:27:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=49
06/15/2022 12:27:26 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
06/15/2022 12:27:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=51
06/15/2022 12:27:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=52
06/15/2022 12:27:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=53
06/15/2022 12:27:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=54
06/15/2022 12:27:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=56
06/15/2022 12:27:54 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=56
06/15/2022 12:27:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=57
06/15/2022 12:28:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=58
06/15/2022 12:28:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=59
06/15/2022 12:28:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=61
06/15/2022 12:28:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=62
06/15/2022 12:28:22 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=62
06/15/2022 12:28:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=63
06/15/2022 12:28:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=64
06/15/2022 12:28:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=66
06/15/2022 12:28:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=67
06/15/2022 12:28:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=68
06/15/2022 12:28:50 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=68
06/15/2022 12:28:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=69
06/15/2022 12:28:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=71
06/15/2022 12:29:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=72
06/15/2022 12:29:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=73
06/15/2022 12:29:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=74
06/15/2022 12:29:19 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=74
06/15/2022 12:29:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=76
06/15/2022 12:29:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=77
06/15/2022 12:29:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=78
06/15/2022 12:29:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=79
06/15/2022 12:29:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=81
06/15/2022 12:29:48 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=81
06/15/2022 12:29:52 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=82
06/15/2022 12:29:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=83
06/15/2022 12:30:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=84
06/15/2022 12:30:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=86
06/15/2022 12:30:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=87
06/15/2022 12:30:16 - INFO - __main__ - Global step 700 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=87
06/15/2022 12:30:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=88
06/15/2022 12:30:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=89
06/15/2022 12:30:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=91
06/15/2022 12:30:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=92
06/15/2022 12:30:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=93
06/15/2022 12:30:44 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=93
06/15/2022 12:30:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=94
06/15/2022 12:30:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=96
06/15/2022 12:30:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=97
06/15/2022 12:31:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=98
06/15/2022 12:31:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=99
06/15/2022 12:31:12 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=99
06/15/2022 12:31:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=101
06/15/2022 12:31:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=102
06/15/2022 12:31:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=103
06/15/2022 12:31:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=104
06/15/2022 12:31:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=106
06/15/2022 12:31:40 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=106
06/15/2022 12:31:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=107
06/15/2022 12:31:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=108
06/15/2022 12:31:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=109
06/15/2022 12:31:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=111
06/15/2022 12:32:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=112
06/15/2022 12:32:08 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=112
06/15/2022 12:32:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=113
06/15/2022 12:32:17 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=114
06/15/2022 12:32:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=116
06/15/2022 12:32:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=117
06/15/2022 12:32:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.20 on epoch=118
06/15/2022 12:32:36 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=118
06/15/2022 12:32:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=119
06/15/2022 12:32:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=121
06/15/2022 12:32:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=122
06/15/2022 12:32:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=123
06/15/2022 12:32:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=124
06/15/2022 12:33:04 - INFO - __main__ - Global step 1000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=124
06/15/2022 12:33:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=126
06/15/2022 12:33:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=127
06/15/2022 12:33:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=128
06/15/2022 12:33:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=129
06/15/2022 12:33:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=131
06/15/2022 12:33:33 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=131
06/15/2022 12:33:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=132
06/15/2022 12:33:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=133
06/15/2022 12:33:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=134
06/15/2022 12:33:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=136
06/15/2022 12:33:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=137
06/15/2022 12:34:01 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=137
06/15/2022 12:34:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=138
06/15/2022 12:34:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=139
06/15/2022 12:34:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=141
06/15/2022 12:34:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=142
06/15/2022 12:34:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=143
06/15/2022 12:34:29 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=143
06/15/2022 12:34:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=144
06/15/2022 12:34:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=146
06/15/2022 12:34:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=147
06/15/2022 12:34:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.21 on epoch=148
06/15/2022 12:34:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=149
06/15/2022 12:34:57 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=149
06/15/2022 12:35:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=151
06/15/2022 12:35:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=152
06/15/2022 12:35:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=153
06/15/2022 12:35:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=154
06/15/2022 12:35:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=156
06/15/2022 12:35:25 - INFO - __main__ - Global step 1250 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=156
06/15/2022 12:35:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=157
06/15/2022 12:35:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=158
06/15/2022 12:35:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=159
06/15/2022 12:35:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=161
06/15/2022 12:35:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=162
06/15/2022 12:35:54 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=162
06/15/2022 12:35:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=163
06/15/2022 12:36:03 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=164
06/15/2022 12:36:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=166
06/15/2022 12:36:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=167
06/15/2022 12:36:16 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.20 on epoch=168
06/15/2022 12:36:22 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=168
06/15/2022 12:36:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=169
06/15/2022 12:36:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=171
06/15/2022 12:36:35 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=172
06/15/2022 12:36:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=173
06/15/2022 12:36:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=174
06/15/2022 12:36:50 - INFO - __main__ - Global step 1400 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=174
06/15/2022 12:36:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=176
06/15/2022 12:36:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=177
06/15/2022 12:37:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=178
06/15/2022 12:37:08 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=179
06/15/2022 12:37:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=181
06/15/2022 12:37:18 - INFO - __main__ - Global step 1450 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=181
06/15/2022 12:37:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.20 on epoch=182
06/15/2022 12:37:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.19 on epoch=183
06/15/2022 12:37:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=184
06/15/2022 12:37:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=186
06/15/2022 12:37:41 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=187
06/15/2022 12:37:46 - INFO - __main__ - Global step 1500 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=187
06/15/2022 12:37:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=188
06/15/2022 12:37:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=189
06/15/2022 12:38:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=191
06/15/2022 12:38:04 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=192
06/15/2022 12:38:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=193
06/15/2022 12:38:14 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=193
06/15/2022 12:38:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=194
06/15/2022 12:38:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=196
06/15/2022 12:38:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=197
06/15/2022 12:38:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=198
06/15/2022 12:38:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=199
06/15/2022 12:38:43 - INFO - __main__ - Global step 1600 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=199
06/15/2022 12:38:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=201
06/15/2022 12:38:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=202
06/15/2022 12:38:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=203
06/15/2022 12:39:01 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.19 on epoch=204
06/15/2022 12:39:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=206
06/15/2022 12:39:11 - INFO - __main__ - Global step 1650 Train loss 0.20 Classification-F1 0.34296770117665637 on epoch=206
06/15/2022 12:39:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.34296770117665637 on epoch=206, global_step=1650
06/15/2022 12:39:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.20 on epoch=207
06/15/2022 12:39:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=208
06/15/2022 12:39:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=209
06/15/2022 12:39:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=211
06/15/2022 12:39:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=212
06/15/2022 12:39:39 - INFO - __main__ - Global step 1700 Train loss 0.20 Classification-F1 0.3871086556169429 on epoch=212
06/15/2022 12:39:39 - INFO - __main__ - Saving model with best Classification-F1: 0.34296770117665637 -> 0.3871086556169429 on epoch=212, global_step=1700
06/15/2022 12:39:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=213
06/15/2022 12:39:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=214
06/15/2022 12:39:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=216
06/15/2022 12:39:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=217
06/15/2022 12:40:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=218
06/15/2022 12:40:07 - INFO - __main__ - Global step 1750 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=218
06/15/2022 12:40:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=219
06/15/2022 12:40:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=221
06/15/2022 12:40:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=222
06/15/2022 12:40:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=223
06/15/2022 12:40:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.19 on epoch=224
06/15/2022 12:40:35 - INFO - __main__ - Global step 1800 Train loss 0.20 Classification-F1 0.3999999999999999 on epoch=224
06/15/2022 12:40:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3871086556169429 -> 0.3999999999999999 on epoch=224, global_step=1800
06/15/2022 12:40:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=226
06/15/2022 12:40:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.19 on epoch=227
06/15/2022 12:40:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=228
06/15/2022 12:40:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=229
06/15/2022 12:40:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=231
06/15/2022 12:41:04 - INFO - __main__ - Global step 1850 Train loss 0.20 Classification-F1 0.3298429319371728 on epoch=231
06/15/2022 12:41:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=232
06/15/2022 12:41:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=233
06/15/2022 12:41:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=234
06/15/2022 12:41:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=236
06/15/2022 12:41:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=237
06/15/2022 12:41:32 - INFO - __main__ - Global step 1900 Train loss 0.20 Classification-F1 0.32631578947368417 on epoch=237
06/15/2022 12:41:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=238
06/15/2022 12:41:41 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=239
06/15/2022 12:41:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=241
06/15/2022 12:41:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=242
06/15/2022 12:41:54 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=243
06/15/2022 12:42:00 - INFO - __main__ - Global step 1950 Train loss 0.19 Classification-F1 0.3298429319371728 on epoch=243
06/15/2022 12:42:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=244
06/15/2022 12:42:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=246
06/15/2022 12:42:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=247
06/15/2022 12:42:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=248
06/15/2022 12:42:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=249
06/15/2022 12:42:28 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=249
06/15/2022 12:42:32 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.20 on epoch=251
06/15/2022 12:42:37 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.19 on epoch=252
06/15/2022 12:42:41 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.18 on epoch=253
06/15/2022 12:42:46 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.18 on epoch=254
06/15/2022 12:42:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.17 on epoch=256
06/15/2022 12:42:56 - INFO - __main__ - Global step 2050 Train loss 0.18 Classification-F1 0.3333333333333333 on epoch=256
06/15/2022 12:43:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.20 on epoch=257
06/15/2022 12:43:05 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.22 on epoch=258
06/15/2022 12:43:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.17 on epoch=259
06/15/2022 12:43:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.19 on epoch=261
06/15/2022 12:43:19 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.21 on epoch=262
06/15/2022 12:43:24 - INFO - __main__ - Global step 2100 Train loss 0.20 Classification-F1 0.45257861635220126 on epoch=262
06/15/2022 12:43:24 - INFO - __main__ - Saving model with best Classification-F1: 0.3999999999999999 -> 0.45257861635220126 on epoch=262, global_step=2100
06/15/2022 12:43:29 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.19 on epoch=263
06/15/2022 12:43:33 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.19 on epoch=264
06/15/2022 12:43:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.20 on epoch=266
06/15/2022 12:43:42 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.21 on epoch=267
06/15/2022 12:43:47 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.18 on epoch=268
06/15/2022 12:43:52 - INFO - __main__ - Global step 2150 Train loss 0.19 Classification-F1 0.3727353727353727 on epoch=268
06/15/2022 12:43:57 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.20 on epoch=269
06/15/2022 12:44:01 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.18 on epoch=271
06/15/2022 12:44:06 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.20 on epoch=272
06/15/2022 12:44:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.19 on epoch=273
06/15/2022 12:44:15 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.19 on epoch=274
06/15/2022 12:44:20 - INFO - __main__ - Global step 2200 Train loss 0.19 Classification-F1 0.25963337074448184 on epoch=274
06/15/2022 12:44:24 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.19 on epoch=276
06/15/2022 12:44:29 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.20 on epoch=277
06/15/2022 12:44:33 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.19 on epoch=278
06/15/2022 12:44:38 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.18 on epoch=279
06/15/2022 12:44:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.20 on epoch=281
06/15/2022 12:44:48 - INFO - __main__ - Global step 2250 Train loss 0.19 Classification-F1 0.3860677578987438 on epoch=281
06/15/2022 12:44:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.19 on epoch=282
06/15/2022 12:44:57 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.20 on epoch=283
06/15/2022 12:45:01 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.19 on epoch=284
06/15/2022 12:45:06 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.19 on epoch=286
06/15/2022 12:45:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.20 on epoch=287
06/15/2022 12:45:16 - INFO - __main__ - Global step 2300 Train loss 0.19 Classification-F1 0.46476939399613054 on epoch=287
06/15/2022 12:45:16 - INFO - __main__ - Saving model with best Classification-F1: 0.45257861635220126 -> 0.46476939399613054 on epoch=287, global_step=2300
06/15/2022 12:45:20 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.21 on epoch=288
06/15/2022 12:45:25 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.17 on epoch=289
06/15/2022 12:45:29 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.18 on epoch=291
06/15/2022 12:45:34 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.19 on epoch=292
06/15/2022 12:45:38 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.18 on epoch=293
06/15/2022 12:45:44 - INFO - __main__ - Global step 2350 Train loss 0.19 Classification-F1 0.3771988921326446 on epoch=293
06/15/2022 12:45:48 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.18 on epoch=294
06/15/2022 12:45:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.17 on epoch=296
06/15/2022 12:45:57 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.18 on epoch=297
06/15/2022 12:46:02 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.17 on epoch=298
06/15/2022 12:46:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.20 on epoch=299
06/15/2022 12:46:11 - INFO - __main__ - Global step 2400 Train loss 0.18 Classification-F1 0.43529411764705883 on epoch=299
06/15/2022 12:46:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.19 on epoch=301
06/15/2022 12:46:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.17 on epoch=302
06/15/2022 12:46:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.21 on epoch=303
06/15/2022 12:46:29 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.20 on epoch=304
06/15/2022 12:46:34 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.20 on epoch=306
06/15/2022 12:46:39 - INFO - __main__ - Global step 2450 Train loss 0.19 Classification-F1 0.3708141321044547 on epoch=306
06/15/2022 12:46:43 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.21 on epoch=307
06/15/2022 12:46:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.18 on epoch=308
06/15/2022 12:46:52 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.18 on epoch=309
06/15/2022 12:46:57 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.17 on epoch=311
06/15/2022 12:47:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.17 on epoch=312
06/15/2022 12:47:06 - INFO - __main__ - Global step 2500 Train loss 0.18 Classification-F1 0.28794527620030974 on epoch=312
06/15/2022 12:47:11 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.17 on epoch=313
06/15/2022 12:47:15 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.19 on epoch=314
06/15/2022 12:47:20 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.20 on epoch=316
06/15/2022 12:47:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.17 on epoch=317
06/15/2022 12:47:29 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.18 on epoch=318
06/15/2022 12:47:34 - INFO - __main__ - Global step 2550 Train loss 0.18 Classification-F1 0.3333333333333333 on epoch=318
06/15/2022 12:47:38 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.18 on epoch=319
06/15/2022 12:47:43 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.17 on epoch=321
06/15/2022 12:47:47 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.19 on epoch=322
06/15/2022 12:47:52 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.20 on epoch=323
06/15/2022 12:47:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.16 on epoch=324
06/15/2022 12:48:02 - INFO - __main__ - Global step 2600 Train loss 0.18 Classification-F1 0.4205724632634585 on epoch=324
06/15/2022 12:48:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.17 on epoch=326
06/15/2022 12:48:11 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.18 on epoch=327
06/15/2022 12:48:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.19 on epoch=328
06/15/2022 12:48:20 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.18 on epoch=329
06/15/2022 12:48:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.19 on epoch=331
06/15/2022 12:48:29 - INFO - __main__ - Global step 2650 Train loss 0.18 Classification-F1 0.4181818181818182 on epoch=331
06/15/2022 12:48:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.16 on epoch=332
06/15/2022 12:48:38 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=333
06/15/2022 12:48:43 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.19 on epoch=334
06/15/2022 12:48:47 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.20 on epoch=336
06/15/2022 12:48:52 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.19 on epoch=337
06/15/2022 12:48:57 - INFO - __main__ - Global step 2700 Train loss 0.19 Classification-F1 0.4452324665090622 on epoch=337
06/15/2022 12:49:01 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.18 on epoch=338
06/15/2022 12:49:06 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.19 on epoch=339
06/15/2022 12:49:10 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.19 on epoch=341
06/15/2022 12:49:15 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.17 on epoch=342
06/15/2022 12:49:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.19 on epoch=343
06/15/2022 12:49:25 - INFO - __main__ - Global step 2750 Train loss 0.18 Classification-F1 0.35518871580252653 on epoch=343
06/15/2022 12:49:30 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.18 on epoch=344
06/15/2022 12:49:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.18 on epoch=346
06/15/2022 12:49:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.16 on epoch=347
06/15/2022 12:49:43 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.19 on epoch=348
06/15/2022 12:49:48 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.19 on epoch=349
06/15/2022 12:49:53 - INFO - __main__ - Global step 2800 Train loss 0.18 Classification-F1 0.44209215442092153 on epoch=349
06/15/2022 12:49:58 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.20 on epoch=351
06/15/2022 12:50:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.16 on epoch=352
06/15/2022 12:50:07 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.19 on epoch=353
06/15/2022 12:50:11 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.21 on epoch=354
06/15/2022 12:50:16 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.18 on epoch=356
06/15/2022 12:50:21 - INFO - __main__ - Global step 2850 Train loss 0.19 Classification-F1 0.4340456890198968 on epoch=356
06/15/2022 12:50:25 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.19 on epoch=357
06/15/2022 12:50:30 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.16 on epoch=358
06/15/2022 12:50:34 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.18 on epoch=359
06/15/2022 12:50:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.17 on epoch=361
06/15/2022 12:50:43 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.16 on epoch=362
06/15/2022 12:50:48 - INFO - __main__ - Global step 2900 Train loss 0.17 Classification-F1 0.308058058058058 on epoch=362
06/15/2022 12:50:53 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.19 on epoch=363
06/15/2022 12:50:57 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.17 on epoch=364
06/15/2022 12:51:02 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.19 on epoch=366
06/15/2022 12:51:06 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.16 on epoch=367
06/15/2022 12:51:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.18 on epoch=368
06/15/2022 12:51:16 - INFO - __main__ - Global step 2950 Train loss 0.18 Classification-F1 0.21184894953725597 on epoch=368
06/15/2022 12:51:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.15 on epoch=369
06/15/2022 12:51:25 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.17 on epoch=371
06/15/2022 12:51:29 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.18 on epoch=372
06/15/2022 12:51:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.21 on epoch=373
06/15/2022 12:51:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.15 on epoch=374
06/15/2022 12:51:44 - INFO - __main__ - Global step 3000 Train loss 0.17 Classification-F1 0.4941980806623596 on epoch=374
06/15/2022 12:51:44 - INFO - __main__ - Saving model with best Classification-F1: 0.46476939399613054 -> 0.4941980806623596 on epoch=374, global_step=3000
06/15/2022 12:51:44 - INFO - __main__ - save last model!
06/15/2022 12:51:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 12:51:44 - INFO - __main__ - Start tokenizing ... 12792 instances
06/15/2022 12:51:44 - INFO - __main__ - Printing 3 examples
06/15/2022 12:51:44 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:51:44 - INFO - __main__ - ['entailed']
06/15/2022 12:51:44 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:51:44 - INFO - __main__ - ['entailed']
06/15/2022 12:51:44 - INFO - __main__ -  [tab_fact] statement: süper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#süper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#süper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
06/15/2022 12:51:44 - INFO - __main__ - ['entailed']
06/15/2022 12:51:44 - INFO - __main__ - Tokenizing Input ...
06/15/2022 12:52:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 12:52:22 - INFO - __main__ - Loaded 12792 examples from test data
06/15/2022 13:01:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-tab_fact/tab_fact_64_87_0.2_8_predictions.txt
06/15/2022 13:01:09 - INFO - __main__ - Classification-F1 on test data: 0.2472
06/15/2022 13:01:10 - INFO - __main__ - prefix=tab_fact_64_87, lr=0.2, bsz=8, dev_performance=0.4941980806623596, test_performance=0.247201824320428
