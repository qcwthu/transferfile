05/21/2022 21:25:51 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:25:51 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli
05/21/2022 21:25:51 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
05/21/2022 21:25:51 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli
05/21/2022 21:25:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:25:53 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:25:53 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:25:53 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:53 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/21/2022 21:25:53 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:25:53 - INFO - __main__ - Using 2 gpus
05/21/2022 21:25:53 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
05/21/2022 21:25:59 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.5, bsz=8 ...
06/15/2022 13:01:15 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/15/2022 13:01:15 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli
06/15/2022 13:01:15 - INFO - __main__ - Namespace(task_dir='data_64/anli/', task_name='anli', identifier='T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
06/15/2022 13:01:15 - INFO - __main__ - models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli
06/15/2022 13:01:17 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/15/2022 13:01:17 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/15/2022 13:01:17 - INFO - __main__ - args.device: cuda:1
06/15/2022 13:01:17 - INFO - __main__ - args.device: cuda:0
06/15/2022 13:01:17 - INFO - __main__ - Using 2 gpus
06/15/2022 13:01:17 - INFO - __main__ - Using 2 gpus
06/15/2022 13:01:17 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
06/15/2022 13:01:17 - INFO - __main__ - Fine-tuning the following samples: ['anli_64_100', 'anli_64_13', 'anli_64_21', 'anli_64_42', 'anli_64_87']
06/15/2022 13:01:21 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.5, bsz=8 ...
06/15/2022 13:01:22 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:01:22 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:01:22 - INFO - __main__ - Printing 3 examples
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:01:22 - INFO - __main__ - Printing 3 examples
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:01:22 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:01:22 - INFO - __main__ - ['neutral']
06/15/2022 13:01:22 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:01:22 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:01:22 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:01:23 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:01:23 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:01:23 - INFO - __main__ - Printing 3 examples
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:01:23 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ - Printing 3 examples
06/15/2022 13:01:23 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:01:23 - INFO - __main__ - ['neutral']
06/15/2022 13:01:23 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:01:23 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:01:23 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:01:23 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:01:23 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:01:41 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:01:41 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:01:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:01:42 - INFO - __main__ - Starting training!
06/15/2022 13:01:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:01:46 - INFO - __main__ - Starting training!
06/15/2022 13:01:50 - INFO - __main__ - Step 10 Global step 10 Train loss 0.90 on epoch=0
06/15/2022 13:01:53 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=1
06/15/2022 13:01:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=2
06/15/2022 13:01:58 - INFO - __main__ - Step 40 Global step 40 Train loss 0.66 on epoch=3
06/15/2022 13:02:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=4
06/15/2022 13:02:06 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.23639291465378423 on epoch=4
06/15/2022 13:02:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.23639291465378423 on epoch=4, global_step=50
06/15/2022 13:02:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=4
06/15/2022 13:02:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=5
06/15/2022 13:02:14 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=6
06/15/2022 13:02:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
06/15/2022 13:02:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=8
06/15/2022 13:02:25 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 13:02:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=9
06/15/2022 13:02:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=9
06/15/2022 13:02:32 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=10
06/15/2022 13:02:35 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
06/15/2022 13:02:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.44 on epoch=12
06/15/2022 13:02:43 - INFO - __main__ - Global step 150 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 13:02:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=13
06/15/2022 13:02:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
06/15/2022 13:02:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.44 on epoch=14
06/15/2022 13:02:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=15
06/15/2022 13:02:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=16
06/15/2022 13:03:01 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 13:03:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 13:03:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
06/15/2022 13:03:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
06/15/2022 13:03:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=19
06/15/2022 13:03:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=20
06/15/2022 13:03:17 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.17332310778125185 on epoch=20
06/15/2022 13:03:20 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=21
06/15/2022 13:03:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
06/15/2022 13:03:25 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
06/15/2022 13:03:27 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=24
06/15/2022 13:03:30 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
06/15/2022 13:03:35 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 13:03:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
06/15/2022 13:03:41 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
06/15/2022 13:03:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=27
06/15/2022 13:03:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=28
06/15/2022 13:03:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=29
06/15/2022 13:03:54 - INFO - __main__ - Global step 350 Train loss 0.46 Classification-F1 0.21873496873496875 on epoch=29
06/15/2022 13:03:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=29
06/15/2022 13:03:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
06/15/2022 13:04:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
06/15/2022 13:04:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=32
06/15/2022 13:04:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=33
06/15/2022 13:04:12 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.1843809523809524 on epoch=33
06/15/2022 13:04:14 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=34
06/15/2022 13:04:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
06/15/2022 13:04:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
06/15/2022 13:04:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
06/15/2022 13:04:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
06/15/2022 13:04:30 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 13:04:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
06/15/2022 13:04:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=39
06/15/2022 13:04:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
06/15/2022 13:04:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
06/15/2022 13:04:43 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
06/15/2022 13:04:48 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 13:04:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.48 on epoch=42
06/15/2022 13:04:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=43
06/15/2022 13:04:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=44
06/15/2022 13:04:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
06/15/2022 13:05:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.44 on epoch=45
06/15/2022 13:05:07 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.2733227733227733 on epoch=45
06/15/2022 13:05:07 - INFO - __main__ - Saving model with best Classification-F1: 0.23639291465378423 -> 0.2733227733227733 on epoch=45, global_step=550
06/15/2022 13:05:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
06/15/2022 13:05:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=47
06/15/2022 13:05:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=48
06/15/2022 13:05:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=49
06/15/2022 13:05:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
06/15/2022 13:05:25 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 13:05:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
06/15/2022 13:05:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
06/15/2022 13:05:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=52
06/15/2022 13:05:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=53
06/15/2022 13:05:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=54
06/15/2022 13:05:43 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.23154401455371318 on epoch=54
06/15/2022 13:05:46 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=54
06/15/2022 13:05:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=55
06/15/2022 13:05:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=56
06/15/2022 13:05:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=57
06/15/2022 13:05:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
06/15/2022 13:06:02 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.18422318422318418 on epoch=58
06/15/2022 13:06:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
06/15/2022 13:06:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 13:06:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=60
06/15/2022 13:06:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=61
06/15/2022 13:06:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=62
06/15/2022 13:06:20 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.2779704350427699 on epoch=62
06/15/2022 13:06:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2733227733227733 -> 0.2779704350427699 on epoch=62, global_step=750
06/15/2022 13:06:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
06/15/2022 13:06:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=64
06/15/2022 13:06:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
06/15/2022 13:06:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
06/15/2022 13:06:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=66
06/15/2022 13:06:38 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 13:06:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
06/15/2022 13:06:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=68
06/15/2022 13:06:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=69
06/15/2022 13:06:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
06/15/2022 13:06:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
06/15/2022 13:06:57 - INFO - __main__ - Global step 850 Train loss 0.41 Classification-F1 0.30680402242197685 on epoch=70
06/15/2022 13:06:57 - INFO - __main__ - Saving model with best Classification-F1: 0.2779704350427699 -> 0.30680402242197685 on epoch=70, global_step=850
06/15/2022 13:06:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
06/15/2022 13:07:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=72
06/15/2022 13:07:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=73
06/15/2022 13:07:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=74
06/15/2022 13:07:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
06/15/2022 13:07:15 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.1785932000078658 on epoch=74
06/15/2022 13:07:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
06/15/2022 13:07:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=76
06/15/2022 13:07:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
06/15/2022 13:07:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 13:07:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
06/15/2022 13:07:34 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.19456927871618712 on epoch=79
06/15/2022 13:07:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=79
06/15/2022 13:07:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=80
06/15/2022 13:07:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
06/15/2022 13:07:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.43 on epoch=82
06/15/2022 13:07:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
06/15/2022 13:07:52 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.25004788354721313 on epoch=83
06/15/2022 13:07:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
06/15/2022 13:07:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.42 on epoch=84
06/15/2022 13:08:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
06/15/2022 13:08:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
06/15/2022 13:08:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
06/15/2022 13:08:10 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.2791515016339372 on epoch=87
06/15/2022 13:08:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
06/15/2022 13:08:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=89
06/15/2022 13:08:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=89
06/15/2022 13:08:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
06/15/2022 13:08:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
06/15/2022 13:08:29 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.1800877298885267 on epoch=91
06/15/2022 13:08:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.39 on epoch=92
06/15/2022 13:08:34 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
06/15/2022 13:08:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
06/15/2022 13:08:39 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=94
06/15/2022 13:08:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=95
06/15/2022 13:08:47 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.260639694474281 on epoch=95
06/15/2022 13:08:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
06/15/2022 13:08:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
06/15/2022 13:08:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=98
06/15/2022 13:08:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
06/15/2022 13:09:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
06/15/2022 13:09:05 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.28788350922577927 on epoch=99
06/15/2022 13:09:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
06/15/2022 13:09:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
06/15/2022 13:09:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=102
06/15/2022 13:09:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=103
06/15/2022 13:09:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=104
06/15/2022 13:09:24 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.2963361157975601 on epoch=104
06/15/2022 13:09:27 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=104
06/15/2022 13:09:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=105
06/15/2022 13:09:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=106
06/15/2022 13:09:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=107
06/15/2022 13:09:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=108
06/15/2022 13:09:42 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.24228784951487226 on epoch=108
06/15/2022 13:09:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=109
06/15/2022 13:09:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=109
06/15/2022 13:09:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=110
06/15/2022 13:09:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=111
06/15/2022 13:09:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=112
06/15/2022 13:10:01 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.30676448233700143 on epoch=112
06/15/2022 13:10:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
06/15/2022 13:10:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.39 on epoch=114
06/15/2022 13:10:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=114
06/15/2022 13:10:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
06/15/2022 13:10:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
06/15/2022 13:10:19 - INFO - __main__ - Global step 1400 Train loss 0.40 Classification-F1 0.260559869431894 on epoch=116
06/15/2022 13:10:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=117
06/15/2022 13:10:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=118
06/15/2022 13:10:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
06/15/2022 13:10:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.39 on epoch=119
06/15/2022 13:10:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=120
06/15/2022 13:10:37 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.2385083713850837 on epoch=120
06/15/2022 13:10:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=121
06/15/2022 13:10:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=122
06/15/2022 13:10:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=123
06/15/2022 13:10:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=124
06/15/2022 13:10:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=124
06/15/2022 13:10:56 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.24678453010545343 on epoch=124
06/15/2022 13:10:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=125
06/15/2022 13:11:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=126
06/15/2022 13:11:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=127
06/15/2022 13:11:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=128
06/15/2022 13:11:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=129
06/15/2022 13:11:14 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.3564768564768565 on epoch=129
06/15/2022 13:11:14 - INFO - __main__ - Saving model with best Classification-F1: 0.30680402242197685 -> 0.3564768564768565 on epoch=129, global_step=1550
06/15/2022 13:11:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
06/15/2022 13:11:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=130
06/15/2022 13:11:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=131
06/15/2022 13:11:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=132
06/15/2022 13:11:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
06/15/2022 13:11:33 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.268095378440206 on epoch=133
06/15/2022 13:11:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=134
06/15/2022 13:11:38 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.34 on epoch=134
06/15/2022 13:11:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=135
06/15/2022 13:11:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=136
06/15/2022 13:11:45 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=137
06/15/2022 13:11:51 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.24361038853792474 on epoch=137
06/15/2022 13:11:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=138
06/15/2022 13:11:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
06/15/2022 13:11:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
06/15/2022 13:12:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=140
06/15/2022 13:12:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.41 on epoch=141
06/15/2022 13:12:09 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.21457461645746165 on epoch=141
06/15/2022 13:12:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=142
06/15/2022 13:12:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
06/15/2022 13:12:17 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=144
06/15/2022 13:12:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=144
06/15/2022 13:12:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=145
06/15/2022 13:12:27 - INFO - __main__ - Global step 1750 Train loss 0.40 Classification-F1 0.327274139831702 on epoch=145
06/15/2022 13:12:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
06/15/2022 13:12:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=147
06/15/2022 13:12:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 13:12:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
06/15/2022 13:12:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=149
06/15/2022 13:12:45 - INFO - __main__ - Global step 1800 Train loss 0.40 Classification-F1 0.22006106137010736 on epoch=149
06/15/2022 13:12:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=150
06/15/2022 13:12:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.43 on epoch=151
06/15/2022 13:12:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=152
06/15/2022 13:12:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
06/15/2022 13:12:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.38 on epoch=154
06/15/2022 13:13:03 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.27825350059425036 on epoch=154
06/15/2022 13:13:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.38 on epoch=154
06/15/2022 13:13:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=155
06/15/2022 13:13:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
06/15/2022 13:13:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=157
06/15/2022 13:13:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=158
06/15/2022 13:13:22 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.3400930032027593 on epoch=158
06/15/2022 13:13:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=159
06/15/2022 13:13:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=159
06/15/2022 13:13:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
06/15/2022 13:13:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=161
06/15/2022 13:13:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=162
06/15/2022 13:13:40 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.23344965104685944 on epoch=162
06/15/2022 13:13:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=163
06/15/2022 13:13:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=164
06/15/2022 13:13:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
06/15/2022 13:13:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=165
06/15/2022 13:13:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=166
06/15/2022 13:13:58 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.19579945799457996 on epoch=166
06/15/2022 13:14:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.36 on epoch=167
06/15/2022 13:14:03 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=168
06/15/2022 13:14:06 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
06/15/2022 13:14:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=169
06/15/2022 13:14:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.39 on epoch=170
06/15/2022 13:14:16 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.33235361130097973 on epoch=170
06/15/2022 13:14:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.37 on epoch=171
06/15/2022 13:14:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=172
06/15/2022 13:14:24 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=173
06/15/2022 13:14:27 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=174
06/15/2022 13:14:29 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
06/15/2022 13:14:35 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.2413332301421459 on epoch=174
06/15/2022 13:14:37 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=175
06/15/2022 13:14:40 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=176
06/15/2022 13:14:43 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=177
06/15/2022 13:14:45 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=178
06/15/2022 13:14:48 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=179
06/15/2022 13:14:53 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.32379554903399366 on epoch=179
06/15/2022 13:14:56 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.39 on epoch=179
06/15/2022 13:14:58 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.34 on epoch=180
06/15/2022 13:15:01 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=181
06/15/2022 13:15:03 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=182
06/15/2022 13:15:06 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
06/15/2022 13:15:11 - INFO - __main__ - Global step 2200 Train loss 0.39 Classification-F1 0.2141464667835338 on epoch=183
06/15/2022 13:15:14 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.38 on epoch=184
06/15/2022 13:15:17 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
06/15/2022 13:15:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=185
06/15/2022 13:15:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
06/15/2022 13:15:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.34 on epoch=187
06/15/2022 13:15:29 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.22882449022933046 on epoch=187
06/15/2022 13:15:32 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
06/15/2022 13:15:35 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.39 on epoch=189
06/15/2022 13:15:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=189
06/15/2022 13:15:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.39 on epoch=190
06/15/2022 13:15:42 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=191
06/15/2022 13:15:48 - INFO - __main__ - Global step 2300 Train loss 0.39 Classification-F1 0.18746353404412744 on epoch=191
06/15/2022 13:15:50 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=192
06/15/2022 13:15:53 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
06/15/2022 13:15:55 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=194
06/15/2022 13:15:58 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=194
06/15/2022 13:16:01 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.36 on epoch=195
06/15/2022 13:16:06 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.255026455026455 on epoch=195
06/15/2022 13:16:09 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=196
06/15/2022 13:16:11 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.35 on epoch=197
06/15/2022 13:16:14 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.39 on epoch=198
06/15/2022 13:16:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=199
06/15/2022 13:16:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=199
06/15/2022 13:16:24 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.1984834146905148 on epoch=199
06/15/2022 13:16:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=200
06/15/2022 13:16:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.37 on epoch=201
06/15/2022 13:16:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=202
06/15/2022 13:16:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.36 on epoch=203
06/15/2022 13:16:37 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=204
06/15/2022 13:16:43 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.34678394722374933 on epoch=204
06/15/2022 13:16:45 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=204
06/15/2022 13:16:48 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=205
06/15/2022 13:16:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.37 on epoch=206
06/15/2022 13:16:53 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
06/15/2022 13:16:56 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.36 on epoch=208
06/15/2022 13:17:01 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.2479382406211675 on epoch=208
06/15/2022 13:17:04 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=209
06/15/2022 13:17:06 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.41 on epoch=209
06/15/2022 13:17:09 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.34 on epoch=210
06/15/2022 13:17:11 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=211
06/15/2022 13:17:14 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=212
06/15/2022 13:17:19 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.2777196659448791 on epoch=212
06/15/2022 13:17:22 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.33 on epoch=213
06/15/2022 13:17:25 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=214
06/15/2022 13:17:27 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=214
06/15/2022 13:17:30 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=215
06/15/2022 13:17:32 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=216
06/15/2022 13:17:38 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.1717340461075844 on epoch=216
06/15/2022 13:17:40 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
06/15/2022 13:17:43 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=218
06/15/2022 13:17:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=219
06/15/2022 13:17:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.33 on epoch=219
06/15/2022 13:17:51 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.32 on epoch=220
06/15/2022 13:17:56 - INFO - __main__ - Global step 2650 Train loss 0.34 Classification-F1 0.23591378209849265 on epoch=220
06/15/2022 13:17:58 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.35 on epoch=221
06/15/2022 13:18:01 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.37 on epoch=222
06/15/2022 13:18:04 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.35 on epoch=223
06/15/2022 13:18:06 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=224
06/15/2022 13:18:09 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.36 on epoch=224
06/15/2022 13:18:14 - INFO - __main__ - Global step 2700 Train loss 0.36 Classification-F1 0.27989027795837745 on epoch=224
06/15/2022 13:18:17 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.36 on epoch=225
06/15/2022 13:18:19 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=226
06/15/2022 13:18:22 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.35 on epoch=227
06/15/2022 13:18:24 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.29 on epoch=228
06/15/2022 13:18:27 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=229
06/15/2022 13:18:32 - INFO - __main__ - Global step 2750 Train loss 0.34 Classification-F1 0.12460639428185068 on epoch=229
06/15/2022 13:18:34 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.35 on epoch=229
06/15/2022 13:18:37 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.34 on epoch=230
06/15/2022 13:18:40 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.32 on epoch=231
06/15/2022 13:18:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.27 on epoch=232
06/15/2022 13:18:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=233
06/15/2022 13:18:50 - INFO - __main__ - Global step 2800 Train loss 0.32 Classification-F1 0.0949527959331881 on epoch=233
06/15/2022 13:18:52 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.29 on epoch=234
06/15/2022 13:18:55 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.34 on epoch=234
06/15/2022 13:18:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.34 on epoch=235
06/15/2022 13:19:00 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.34 on epoch=236
06/15/2022 13:19:03 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.31 on epoch=237
06/15/2022 13:19:08 - INFO - __main__ - Global step 2850 Train loss 0.33 Classification-F1 0.051664008506113766 on epoch=237
06/15/2022 13:19:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.36 on epoch=238
06/15/2022 13:19:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.34 on epoch=239
06/15/2022 13:19:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.34 on epoch=239
06/15/2022 13:19:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.35 on epoch=240
06/15/2022 13:19:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.33 on epoch=241
06/15/2022 13:19:26 - INFO - __main__ - Global step 2900 Train loss 0.34 Classification-F1 0.2360292108881956 on epoch=241
06/15/2022 13:19:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.31 on epoch=242
06/15/2022 13:19:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.30 on epoch=243
06/15/2022 13:19:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.32 on epoch=244
06/15/2022 13:19:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.30 on epoch=244
06/15/2022 13:19:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=245
06/15/2022 13:19:44 - INFO - __main__ - Global step 2950 Train loss 0.31 Classification-F1 0.14690064391887825 on epoch=245
06/15/2022 13:19:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=246
06/15/2022 13:19:49 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=247
06/15/2022 13:19:52 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.33 on epoch=248
06/15/2022 13:19:54 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.27 on epoch=249
06/15/2022 13:19:57 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.30 on epoch=249
06/15/2022 13:19:58 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:19:58 - INFO - __main__ - Printing 3 examples
06/15/2022 13:19:58 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:19:58 - INFO - __main__ - ['neutral']
06/15/2022 13:19:58 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:19:58 - INFO - __main__ - ['neutral']
06/15/2022 13:19:58 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:19:58 - INFO - __main__ - ['neutral']
06/15/2022 13:19:58 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:19:58 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:19:59 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:19:59 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:19:59 - INFO - __main__ - Printing 3 examples
06/15/2022 13:19:59 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:19:59 - INFO - __main__ - ['neutral']
06/15/2022 13:19:59 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:19:59 - INFO - __main__ - ['neutral']
06/15/2022 13:19:59 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:19:59 - INFO - __main__ - ['neutral']
06/15/2022 13:19:59 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:19:59 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:19:59 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:20:02 - INFO - __main__ - Global step 3000 Train loss 0.32 Classification-F1 0.24679224679224676 on epoch=249
06/15/2022 13:20:02 - INFO - __main__ - save last model!
06/15/2022 13:20:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 13:20:02 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 13:20:02 - INFO - __main__ - Printing 3 examples
06/15/2022 13:20:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 13:20:02 - INFO - __main__ - ['contradiction']
06/15/2022 13:20:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 13:20:02 - INFO - __main__ - ['entailment']
06/15/2022 13:20:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 13:20:02 - INFO - __main__ - ['contradiction']
06/15/2022 13:20:02 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:20:03 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:20:04 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 13:20:18 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:20:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:20:19 - INFO - __main__ - Starting training!
06/15/2022 13:20:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_100_0.5_8_predictions.txt
06/15/2022 13:20:33 - INFO - __main__ - Classification-F1 on test data: 0.2430
06/15/2022 13:20:33 - INFO - __main__ - prefix=anli_64_100, lr=0.5, bsz=8, dev_performance=0.3564768564768565, test_performance=0.24297218932721498
06/15/2022 13:20:33 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.4, bsz=8 ...
06/15/2022 13:20:34 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:20:34 - INFO - __main__ - Printing 3 examples
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:20:34 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:20:34 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:20:34 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:20:34 - INFO - __main__ - Printing 3 examples
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:20:34 - INFO - __main__ - ['neutral']
06/15/2022 13:20:34 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:20:34 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:20:35 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:20:53 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:20:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:20:54 - INFO - __main__ - Starting training!
06/15/2022 13:20:58 - INFO - __main__ - Step 10 Global step 10 Train loss 0.85 on epoch=0
06/15/2022 13:21:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.67 on epoch=1
06/15/2022 13:21:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=2
06/15/2022 13:21:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=3
06/15/2022 13:21:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=4
06/15/2022 13:21:12 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 13:21:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 13:21:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.61 on epoch=4
06/15/2022 13:21:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=5
06/15/2022 13:21:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=6
06/15/2022 13:21:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=7
06/15/2022 13:21:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=8
06/15/2022 13:21:31 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 13:21:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.52 on epoch=9
06/15/2022 13:21:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
06/15/2022 13:21:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=10
06/15/2022 13:21:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
06/15/2022 13:21:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=12
06/15/2022 13:21:50 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 13:21:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=13
06/15/2022 13:21:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
06/15/2022 13:21:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
06/15/2022 13:22:01 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
06/15/2022 13:22:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
06/15/2022 13:22:09 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 13:22:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=17
06/15/2022 13:22:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
06/15/2022 13:22:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.52 on epoch=19
06/15/2022 13:22:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
06/15/2022 13:22:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=20
06/15/2022 13:22:28 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 13:22:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
06/15/2022 13:22:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
06/15/2022 13:22:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=23
06/15/2022 13:22:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=24
06/15/2022 13:22:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
06/15/2022 13:22:47 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 13:22:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
06/15/2022 13:22:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
06/15/2022 13:22:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.45 on epoch=27
06/15/2022 13:22:57 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
06/15/2022 13:23:00 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
06/15/2022 13:23:06 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 13:23:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=29
06/15/2022 13:23:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
06/15/2022 13:23:14 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=31
06/15/2022 13:23:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
06/15/2022 13:23:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
06/15/2022 13:23:25 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.26009531774795225 on epoch=33
06/15/2022 13:23:25 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.26009531774795225 on epoch=33, global_step=400
06/15/2022 13:23:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
06/15/2022 13:23:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
06/15/2022 13:23:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
06/15/2022 13:23:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
06/15/2022 13:23:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.41 on epoch=37
06/15/2022 13:23:44 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 13:23:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
06/15/2022 13:23:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.45 on epoch=39
06/15/2022 13:23:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
06/15/2022 13:23:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=40
06/15/2022 13:23:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=41
06/15/2022 13:24:03 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 13:24:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
06/15/2022 13:24:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
06/15/2022 13:24:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=44
06/15/2022 13:24:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
06/15/2022 13:24:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
06/15/2022 13:24:22 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 13:24:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
06/15/2022 13:24:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
06/15/2022 13:24:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
06/15/2022 13:24:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
06/15/2022 13:24:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
06/15/2022 13:24:42 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.18971428571428572 on epoch=49
06/15/2022 13:24:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=50
06/15/2022 13:24:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
06/15/2022 13:24:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.43 on epoch=52
06/15/2022 13:24:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 13:24:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=54
06/15/2022 13:25:01 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 13:25:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=54
06/15/2022 13:25:07 - INFO - __main__ - Step 670 Global step 670 Train loss 0.41 on epoch=55
06/15/2022 13:25:09 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
06/15/2022 13:25:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
06/15/2022 13:25:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=58
06/15/2022 13:25:21 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.2787735228319516 on epoch=58
06/15/2022 13:25:21 - INFO - __main__ - Saving model with best Classification-F1: 0.26009531774795225 -> 0.2787735228319516 on epoch=58, global_step=700
06/15/2022 13:25:23 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=59
06/15/2022 13:25:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=59
06/15/2022 13:25:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=60
06/15/2022 13:25:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
06/15/2022 13:25:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
06/15/2022 13:25:40 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 13:25:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
06/15/2022 13:25:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
06/15/2022 13:25:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
06/15/2022 13:25:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=65
06/15/2022 13:25:53 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
06/15/2022 13:25:59 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 13:26:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
06/15/2022 13:26:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
06/15/2022 13:26:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
06/15/2022 13:26:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=69
06/15/2022 13:26:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=70
06/15/2022 13:26:18 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.30199294742263955 on epoch=70
06/15/2022 13:26:18 - INFO - __main__ - Saving model with best Classification-F1: 0.2787735228319516 -> 0.30199294742263955 on epoch=70, global_step=850
06/15/2022 13:26:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
06/15/2022 13:26:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
06/15/2022 13:26:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=73
06/15/2022 13:26:29 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=74
06/15/2022 13:26:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=74
06/15/2022 13:26:37 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.2229606188466948 on epoch=74
06/15/2022 13:26:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=75
06/15/2022 13:26:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=76
06/15/2022 13:26:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=77
06/15/2022 13:26:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=78
06/15/2022 13:26:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
06/15/2022 13:26:57 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.19722222222222222 on epoch=79
06/15/2022 13:26:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
06/15/2022 13:27:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
06/15/2022 13:27:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
06/15/2022 13:27:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=82
06/15/2022 13:27:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=83
06/15/2022 13:27:16 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.2557622557622558 on epoch=83
06/15/2022 13:27:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.39 on epoch=84
06/15/2022 13:27:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=84
06/15/2022 13:27:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
06/15/2022 13:27:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
06/15/2022 13:27:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=87
06/15/2022 13:27:35 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.28771929824561404 on epoch=87
06/15/2022 13:27:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=88
06/15/2022 13:27:41 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=89
06/15/2022 13:27:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
06/15/2022 13:27:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=90
06/15/2022 13:27:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.45 on epoch=91
06/15/2022 13:27:55 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 13:27:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.43 on epoch=92
06/15/2022 13:28:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
06/15/2022 13:28:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.47 on epoch=94
06/15/2022 13:28:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=94
06/15/2022 13:28:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=95
06/15/2022 13:28:14 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.2240875770865938 on epoch=95
06/15/2022 13:28:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=96
06/15/2022 13:28:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.39 on epoch=97
06/15/2022 13:28:22 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
06/15/2022 13:28:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
06/15/2022 13:28:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=99
06/15/2022 13:28:33 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.16732026143790854 on epoch=99
06/15/2022 13:28:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=100
06/15/2022 13:28:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
06/15/2022 13:28:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=102
06/15/2022 13:28:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=103
06/15/2022 13:28:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=104
06/15/2022 13:28:52 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.20016029641698088 on epoch=104
06/15/2022 13:28:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=104
06/15/2022 13:28:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=105
06/15/2022 13:29:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.39 on epoch=106
06/15/2022 13:29:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
06/15/2022 13:29:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
06/15/2022 13:29:12 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.18147828587863102 on epoch=108
06/15/2022 13:29:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
06/15/2022 13:29:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
06/15/2022 13:29:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=110
06/15/2022 13:29:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=111
06/15/2022 13:29:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=112
06/15/2022 13:29:31 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.20600275197798415 on epoch=112
06/15/2022 13:29:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
06/15/2022 13:29:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.45 on epoch=114
06/15/2022 13:29:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=114
06/15/2022 13:29:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=115
06/15/2022 13:29:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=116
06/15/2022 13:29:50 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 13:29:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=117
06/15/2022 13:29:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=118
06/15/2022 13:29:58 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
06/15/2022 13:30:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=119
06/15/2022 13:30:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
06/15/2022 13:30:09 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.2391989378888942 on epoch=120
06/15/2022 13:30:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
06/15/2022 13:30:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
06/15/2022 13:30:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.36 on epoch=123
06/15/2022 13:30:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=124
06/15/2022 13:30:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=124
06/15/2022 13:30:28 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.24716682494460274 on epoch=124
06/15/2022 13:30:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=125
06/15/2022 13:30:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=126
06/15/2022 13:30:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=127
06/15/2022 13:30:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.38 on epoch=128
06/15/2022 13:30:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.46 on epoch=129
06/15/2022 13:30:48 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3176084913518084 on epoch=129
06/15/2022 13:30:48 - INFO - __main__ - Saving model with best Classification-F1: 0.30199294742263955 -> 0.3176084913518084 on epoch=129, global_step=1550
06/15/2022 13:30:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=129
06/15/2022 13:30:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=130
06/15/2022 13:30:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
06/15/2022 13:30:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=132
06/15/2022 13:31:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=133
06/15/2022 13:31:07 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.27666870166870167 on epoch=133
06/15/2022 13:31:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
06/15/2022 13:31:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 13:31:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=135
06/15/2022 13:31:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.41 on epoch=136
06/15/2022 13:31:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=137
06/15/2022 13:31:27 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.24649225781206321 on epoch=137
06/15/2022 13:31:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=138
06/15/2022 13:31:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=139
06/15/2022 13:31:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=139
06/15/2022 13:31:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=140
06/15/2022 13:31:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=141
06/15/2022 13:31:46 - INFO - __main__ - Global step 1700 Train loss 0.40 Classification-F1 0.23188405797101444 on epoch=141
06/15/2022 13:31:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
06/15/2022 13:31:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
06/15/2022 13:31:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=144
06/15/2022 13:31:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.41 on epoch=144
06/15/2022 13:31:59 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=145
06/15/2022 13:32:05 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.2807422680412371 on epoch=145
06/15/2022 13:32:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
06/15/2022 13:32:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=147
06/15/2022 13:32:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 13:32:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
06/15/2022 13:32:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=149
06/15/2022 13:32:24 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.2535369671933549 on epoch=149
06/15/2022 13:32:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=150
06/15/2022 13:32:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
06/15/2022 13:32:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.39 on epoch=152
06/15/2022 13:32:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=153
06/15/2022 13:32:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.42 on epoch=154
06/15/2022 13:32:43 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.3271347817612043 on epoch=154
06/15/2022 13:32:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3176084913518084 -> 0.3271347817612043 on epoch=154, global_step=1850
06/15/2022 13:32:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
06/15/2022 13:32:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.38 on epoch=155
06/15/2022 13:32:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
06/15/2022 13:32:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=157
06/15/2022 13:32:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=158
06/15/2022 13:33:03 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.273605401208963 on epoch=158
06/15/2022 13:33:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=159
06/15/2022 13:33:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=159
06/15/2022 13:33:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=160
06/15/2022 13:33:13 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
06/15/2022 13:33:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=162
06/15/2022 13:33:22 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.2013329146166998 on epoch=162
06/15/2022 13:33:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=163
06/15/2022 13:33:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=164
06/15/2022 13:33:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=164
06/15/2022 13:33:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=165
06/15/2022 13:33:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
06/15/2022 13:33:41 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.20058755209400833 on epoch=166
06/15/2022 13:33:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
06/15/2022 13:33:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=168
06/15/2022 13:33:49 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.41 on epoch=169
06/15/2022 13:33:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=169
06/15/2022 13:33:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=170
06/15/2022 13:34:00 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.2641509433962264 on epoch=170
06/15/2022 13:34:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=171
06/15/2022 13:34:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.37 on epoch=172
06/15/2022 13:34:08 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.37 on epoch=173
06/15/2022 13:34:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=174
06/15/2022 13:34:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.35 on epoch=174
06/15/2022 13:34:19 - INFO - __main__ - Global step 2100 Train loss 0.37 Classification-F1 0.17823129251700678 on epoch=174
06/15/2022 13:34:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
06/15/2022 13:34:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.37 on epoch=176
06/15/2022 13:34:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.37 on epoch=177
06/15/2022 13:34:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.36 on epoch=178
06/15/2022 13:34:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.38 on epoch=179
06/15/2022 13:34:39 - INFO - __main__ - Global step 2150 Train loss 0.38 Classification-F1 0.3217846967846968 on epoch=179
06/15/2022 13:34:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=179
06/15/2022 13:34:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.37 on epoch=180
06/15/2022 13:34:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=181
06/15/2022 13:34:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=182
06/15/2022 13:34:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
06/15/2022 13:34:58 - INFO - __main__ - Global step 2200 Train loss 0.39 Classification-F1 0.23097624935648597 on epoch=183
06/15/2022 13:35:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.35 on epoch=184
06/15/2022 13:35:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
06/15/2022 13:35:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=185
06/15/2022 13:35:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
06/15/2022 13:35:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=187
06/15/2022 13:35:17 - INFO - __main__ - Global step 2250 Train loss 0.37 Classification-F1 0.2599979928747052 on epoch=187
06/15/2022 13:35:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.39 on epoch=188
06/15/2022 13:35:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=189
06/15/2022 13:35:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
06/15/2022 13:35:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=190
06/15/2022 13:35:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
06/15/2022 13:35:37 - INFO - __main__ - Global step 2300 Train loss 0.38 Classification-F1 0.242147502421475 on epoch=191
06/15/2022 13:35:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.38 on epoch=192
06/15/2022 13:35:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
06/15/2022 13:35:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.37 on epoch=194
06/15/2022 13:35:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=194
06/15/2022 13:35:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.37 on epoch=195
06/15/2022 13:35:56 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.3147847034025895 on epoch=195
06/15/2022 13:35:59 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=196
06/15/2022 13:36:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
06/15/2022 13:36:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=198
06/15/2022 13:36:07 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.37 on epoch=199
06/15/2022 13:36:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.38 on epoch=199
06/15/2022 13:36:15 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.25237964083243986 on epoch=199
06/15/2022 13:36:18 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.35 on epoch=200
06/15/2022 13:36:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=201
06/15/2022 13:36:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=202
06/15/2022 13:36:26 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
06/15/2022 13:36:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.34 on epoch=204
06/15/2022 13:36:34 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.3610652421729545 on epoch=204
06/15/2022 13:36:34 - INFO - __main__ - Saving model with best Classification-F1: 0.3271347817612043 -> 0.3610652421729545 on epoch=204, global_step=2450
06/15/2022 13:36:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.36 on epoch=204
06/15/2022 13:36:40 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.37 on epoch=205
06/15/2022 13:36:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.38 on epoch=206
06/15/2022 13:36:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.37 on epoch=207
06/15/2022 13:36:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.35 on epoch=208
06/15/2022 13:36:53 - INFO - __main__ - Global step 2500 Train loss 0.37 Classification-F1 0.29679713089944076 on epoch=208
06/15/2022 13:36:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=209
06/15/2022 13:36:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
06/15/2022 13:37:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=210
06/15/2022 13:37:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.35 on epoch=211
06/15/2022 13:37:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
06/15/2022 13:37:13 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.2932603310848177 on epoch=212
06/15/2022 13:37:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.38 on epoch=213
06/15/2022 13:37:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=214
06/15/2022 13:37:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.37 on epoch=214
06/15/2022 13:37:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.41 on epoch=215
06/15/2022 13:37:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.37 on epoch=216
06/15/2022 13:37:32 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.1771996469820805 on epoch=216
06/15/2022 13:37:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
06/15/2022 13:37:37 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=218
06/15/2022 13:37:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=219
06/15/2022 13:37:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.45 on epoch=219
06/15/2022 13:37:45 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.41 on epoch=220
06/15/2022 13:37:51 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.25587583148558757 on epoch=220
06/15/2022 13:37:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=221
06/15/2022 13:37:56 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.41 on epoch=222
06/15/2022 13:37:59 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=223
06/15/2022 13:38:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.31 on epoch=224
06/15/2022 13:38:04 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.37 on epoch=224
06/15/2022 13:38:10 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.2832826536534327 on epoch=224
06/15/2022 13:38:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
06/15/2022 13:38:15 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.39 on epoch=226
06/15/2022 13:38:18 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=227
06/15/2022 13:38:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.37 on epoch=228
06/15/2022 13:38:23 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=229
06/15/2022 13:38:29 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.2888712165027955 on epoch=229
06/15/2022 13:38:32 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.34 on epoch=229
06/15/2022 13:38:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.36 on epoch=230
06/15/2022 13:38:37 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.34 on epoch=231
06/15/2022 13:38:40 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.33 on epoch=232
06/15/2022 13:38:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.34 on epoch=233
06/15/2022 13:38:48 - INFO - __main__ - Global step 2800 Train loss 0.34 Classification-F1 0.23434410019775867 on epoch=233
06/15/2022 13:38:51 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.33 on epoch=234
06/15/2022 13:38:54 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=234
06/15/2022 13:38:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=235
06/15/2022 13:38:59 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=236
06/15/2022 13:39:02 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.36 on epoch=237
06/15/2022 13:39:07 - INFO - __main__ - Global step 2850 Train loss 0.37 Classification-F1 0.22879948914431672 on epoch=237
06/15/2022 13:39:10 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=238
06/15/2022 13:39:13 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.32 on epoch=239
06/15/2022 13:39:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.33 on epoch=239
06/15/2022 13:39:18 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=240
06/15/2022 13:39:21 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
06/15/2022 13:39:26 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.1863320670734047 on epoch=241
06/15/2022 13:39:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.36 on epoch=242
06/15/2022 13:39:32 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.36 on epoch=243
06/15/2022 13:39:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=244
06/15/2022 13:39:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.32 on epoch=244
06/15/2022 13:39:40 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.32 on epoch=245
06/15/2022 13:39:46 - INFO - __main__ - Global step 2950 Train loss 0.34 Classification-F1 0.3087650314037121 on epoch=245
06/15/2022 13:39:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.38 on epoch=246
06/15/2022 13:39:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.34 on epoch=247
06/15/2022 13:39:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=248
06/15/2022 13:39:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.32 on epoch=249
06/15/2022 13:39:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=249
06/15/2022 13:40:00 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:40:00 - INFO - __main__ - Printing 3 examples
06/15/2022 13:40:00 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:40:00 - INFO - __main__ - ['neutral']
06/15/2022 13:40:00 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:40:00 - INFO - __main__ - ['neutral']
06/15/2022 13:40:00 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:40:00 - INFO - __main__ - ['neutral']
06/15/2022 13:40:00 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:40:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:40:01 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:40:01 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:40:01 - INFO - __main__ - Printing 3 examples
06/15/2022 13:40:01 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:40:01 - INFO - __main__ - ['neutral']
06/15/2022 13:40:01 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:40:01 - INFO - __main__ - ['neutral']
06/15/2022 13:40:01 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:40:01 - INFO - __main__ - ['neutral']
06/15/2022 13:40:01 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:40:01 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:40:01 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:40:05 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.2389938037729892 on epoch=249
06/15/2022 13:40:05 - INFO - __main__ - save last model!
06/15/2022 13:40:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 13:40:05 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 13:40:05 - INFO - __main__ - Printing 3 examples
06/15/2022 13:40:05 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 13:40:05 - INFO - __main__ - ['contradiction']
06/15/2022 13:40:05 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 13:40:05 - INFO - __main__ - ['entailment']
06/15/2022 13:40:05 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 13:40:05 - INFO - __main__ - ['contradiction']
06/15/2022 13:40:05 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:40:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:40:06 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 13:40:19 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:40:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:40:20 - INFO - __main__ - Starting training!
06/15/2022 13:40:37 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_100_0.4_8_predictions.txt
06/15/2022 13:40:37 - INFO - __main__ - Classification-F1 on test data: 0.2933
06/15/2022 13:40:37 - INFO - __main__ - prefix=anli_64_100, lr=0.4, bsz=8, dev_performance=0.3610652421729545, test_performance=0.29334259256239487
06/15/2022 13:40:37 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.3, bsz=8 ...
06/15/2022 13:40:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:40:38 - INFO - __main__ - Printing 3 examples
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:40:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:40:38 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:40:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:40:38 - INFO - __main__ - Printing 3 examples
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:40:38 - INFO - __main__ - ['neutral']
06/15/2022 13:40:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:40:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:40:38 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 13:40:57 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 13:40:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 13:40:58 - INFO - __main__ - Starting training!
06/15/2022 13:41:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.92 on epoch=0
06/15/2022 13:41:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=1
06/15/2022 13:41:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=2
06/15/2022 13:41:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
06/15/2022 13:41:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=4
06/15/2022 13:41:18 - INFO - __main__ - Global step 50 Train loss 0.67 Classification-F1 0.21306672526184722 on epoch=4
06/15/2022 13:41:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.21306672526184722 on epoch=4, global_step=50
06/15/2022 13:41:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=4
06/15/2022 13:41:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=5
06/15/2022 13:41:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=6
06/15/2022 13:41:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
06/15/2022 13:41:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=8
06/15/2022 13:41:35 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 13:41:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=9
06/15/2022 13:41:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.51 on epoch=9
06/15/2022 13:41:43 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=10
06/15/2022 13:41:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
06/15/2022 13:41:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=12
06/15/2022 13:41:54 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 13:41:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=13
06/15/2022 13:42:00 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=14
06/15/2022 13:42:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=14
06/15/2022 13:42:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
06/15/2022 13:42:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
06/15/2022 13:42:13 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 13:42:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 13:42:19 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=18
06/15/2022 13:42:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=19
06/15/2022 13:42:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
06/15/2022 13:42:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=20
06/15/2022 13:42:32 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 13:42:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=21
06/15/2022 13:42:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
06/15/2022 13:42:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
06/15/2022 13:42:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
06/15/2022 13:42:46 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
06/15/2022 13:42:51 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 13:42:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=25
06/15/2022 13:42:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=26
06/15/2022 13:42:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=27
06/15/2022 13:43:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=28
06/15/2022 13:43:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.44 on epoch=29
06/15/2022 13:43:11 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 13:43:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=29
06/15/2022 13:43:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=30
06/15/2022 13:43:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.44 on epoch=31
06/15/2022 13:43:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.42 on epoch=32
06/15/2022 13:43:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=33
06/15/2022 13:43:30 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.24954212454212454 on epoch=33
06/15/2022 13:43:30 - INFO - __main__ - Saving model with best Classification-F1: 0.21306672526184722 -> 0.24954212454212454 on epoch=33, global_step=400
06/15/2022 13:43:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=34
06/15/2022 13:43:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
06/15/2022 13:43:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
06/15/2022 13:43:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=36
06/15/2022 13:43:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.40 on epoch=37
06/15/2022 13:43:49 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 13:43:51 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=38
06/15/2022 13:43:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
06/15/2022 13:43:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.50 on epoch=39
06/15/2022 13:43:59 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
06/15/2022 13:44:02 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=41
06/15/2022 13:44:08 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 13:44:11 - INFO - __main__ - Step 510 Global step 510 Train loss 0.41 on epoch=42
06/15/2022 13:44:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
06/15/2022 13:44:16 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=44
06/15/2022 13:44:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
06/15/2022 13:44:21 - INFO - __main__ - Step 550 Global step 550 Train loss 0.41 on epoch=45
06/15/2022 13:44:27 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 13:44:30 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
06/15/2022 13:44:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
06/15/2022 13:44:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=48
06/15/2022 13:44:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
06/15/2022 13:44:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=49
06/15/2022 13:44:46 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 13:44:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
06/15/2022 13:44:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
06/15/2022 13:44:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
06/15/2022 13:44:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
06/15/2022 13:44:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
06/15/2022 13:45:05 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 13:45:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=54
06/15/2022 13:45:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
06/15/2022 13:45:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
06/15/2022 13:45:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
06/15/2022 13:45:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
06/15/2022 13:45:24 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 13:45:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.50 on epoch=59
06/15/2022 13:45:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 13:45:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
06/15/2022 13:45:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=61
06/15/2022 13:45:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=62
06/15/2022 13:45:43 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 13:45:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=63
06/15/2022 13:45:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.43 on epoch=64
06/15/2022 13:45:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=64
06/15/2022 13:45:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.61 on epoch=65
06/15/2022 13:45:57 - INFO - __main__ - Step 800 Global step 800 Train loss 3.61 on epoch=66
06/15/2022 13:46:02 - INFO - __main__ - Global step 800 Train loss 1.10 Classification-F1 0.20661383829028165 on epoch=66
06/15/2022 13:46:05 - INFO - __main__ - Step 810 Global step 810 Train loss 5.49 on epoch=67
06/15/2022 13:46:08 - INFO - __main__ - Step 820 Global step 820 Train loss 3.23 on epoch=68
06/15/2022 13:46:11 - INFO - __main__ - Step 830 Global step 830 Train loss 1.39 on epoch=69
06/15/2022 13:46:13 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=69
06/15/2022 13:46:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.47 on epoch=70
06/15/2022 13:46:22 - INFO - __main__ - Global step 850 Train loss 2.22 Classification-F1 0.23010667196713708 on epoch=70
06/15/2022 13:46:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=71
06/15/2022 13:46:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
06/15/2022 13:46:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.57 on epoch=73
06/15/2022 13:46:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.58 on epoch=74
06/15/2022 13:46:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.55 on epoch=74
06/15/2022 13:46:41 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.1957146965775093 on epoch=74
06/15/2022 13:46:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
06/15/2022 13:46:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=76
06/15/2022 13:46:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=77
06/15/2022 13:46:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
06/15/2022 13:46:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.53 on epoch=79
06/15/2022 13:47:00 - INFO - __main__ - Global step 950 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 13:47:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.48 on epoch=79
06/15/2022 13:47:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.46 on epoch=80
06/15/2022 13:47:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=81
06/15/2022 13:47:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=82
06/15/2022 13:47:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
06/15/2022 13:47:19 - INFO - __main__ - Global step 1000 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 13:47:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
06/15/2022 13:47:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.50 on epoch=84
06/15/2022 13:47:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=85
06/15/2022 13:47:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=86
06/15/2022 13:47:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=87
06/15/2022 13:47:38 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 13:47:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.47 on epoch=88
06/15/2022 13:47:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
06/15/2022 13:47:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
06/15/2022 13:47:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=90
06/15/2022 13:47:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=91
06/15/2022 13:47:57 - INFO - __main__ - Global step 1100 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 13:48:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=92
06/15/2022 13:48:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=93
06/15/2022 13:48:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=94
06/15/2022 13:48:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
06/15/2022 13:48:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
06/15/2022 13:48:16 - INFO - __main__ - Global step 1150 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 13:48:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=96
06/15/2022 13:48:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=97
06/15/2022 13:48:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=98
06/15/2022 13:48:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=99
06/15/2022 13:48:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=99
06/15/2022 13:48:35 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 13:48:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=100
06/15/2022 13:48:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=101
06/15/2022 13:48:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.44 on epoch=102
06/15/2022 13:48:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
06/15/2022 13:48:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=104
06/15/2022 13:48:54 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 13:48:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=104
06/15/2022 13:49:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=105
06/15/2022 13:49:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=106
06/15/2022 13:49:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.46 on epoch=107
06/15/2022 13:49:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=108
06/15/2022 13:49:13 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 13:49:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.47 on epoch=109
06/15/2022 13:49:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=109
06/15/2022 13:49:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.43 on epoch=110
06/15/2022 13:49:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=111
06/15/2022 13:49:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
06/15/2022 13:49:32 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 13:49:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
06/15/2022 13:49:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
06/15/2022 13:49:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=114
06/15/2022 13:49:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=115
06/15/2022 13:49:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.47 on epoch=116
06/15/2022 13:49:51 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 13:49:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=117
06/15/2022 13:49:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.43 on epoch=118
06/15/2022 13:49:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=119
06/15/2022 13:50:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=119
06/15/2022 13:50:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=120
06/15/2022 13:50:10 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=120
06/15/2022 13:50:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=121
06/15/2022 13:50:15 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=122
06/15/2022 13:50:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=123
06/15/2022 13:50:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=124
06/15/2022 13:50:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=124
06/15/2022 13:50:29 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.17911609088079675 on epoch=124
06/15/2022 13:50:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=125
06/15/2022 13:50:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.48 on epoch=126
06/15/2022 13:50:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=127
06/15/2022 13:50:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=128
06/15/2022 13:50:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=129
06/15/2022 13:50:48 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 13:50:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
06/15/2022 13:50:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=130
06/15/2022 13:50:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=131
06/15/2022 13:50:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=132
06/15/2022 13:51:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.41 on epoch=133
06/15/2022 13:51:07 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 13:51:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=134
06/15/2022 13:51:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
06/15/2022 13:51:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
06/15/2022 13:51:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=136
06/15/2022 13:51:20 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=137
06/15/2022 13:51:26 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.1775766716943188 on epoch=137
06/15/2022 13:51:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=138
06/15/2022 13:51:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=139
06/15/2022 13:51:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
06/15/2022 13:51:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=140
06/15/2022 13:51:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=141
06/15/2022 13:51:45 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.18892001244942422 on epoch=141
06/15/2022 13:51:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=142
06/15/2022 13:51:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
06/15/2022 13:51:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.44 on epoch=144
06/15/2022 13:51:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=144
06/15/2022 13:51:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=145
06/15/2022 13:52:04 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.2166040884438882 on epoch=145
06/15/2022 13:52:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.42 on epoch=146
06/15/2022 13:52:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
06/15/2022 13:52:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 13:52:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
06/15/2022 13:52:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=149
06/15/2022 13:52:23 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.17964855553630887 on epoch=149
06/15/2022 13:52:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=150
06/15/2022 13:52:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
06/15/2022 13:52:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=152
06/15/2022 13:52:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=153
06/15/2022 13:52:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=154
06/15/2022 13:52:42 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.19610474326716987 on epoch=154
06/15/2022 13:52:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=154
06/15/2022 13:52:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=155
06/15/2022 13:52:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
06/15/2022 13:52:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
06/15/2022 13:52:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=158
06/15/2022 13:53:01 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.18951882832479847 on epoch=158
06/15/2022 13:53:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
06/15/2022 13:53:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=159
06/15/2022 13:53:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.44 on epoch=160
06/15/2022 13:53:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=161
06/15/2022 13:53:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=162
06/15/2022 13:53:20 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.18931039128510116 on epoch=162
06/15/2022 13:53:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
06/15/2022 13:53:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=164
06/15/2022 13:53:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=164
06/15/2022 13:53:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=165
06/15/2022 13:53:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=166
06/15/2022 13:53:39 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.20311539249846278 on epoch=166
06/15/2022 13:53:42 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
06/15/2022 13:53:44 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.46 on epoch=168
06/15/2022 13:53:47 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.43 on epoch=169
06/15/2022 13:53:50 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.44 on epoch=169
06/15/2022 13:53:52 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.44 on epoch=170
06/15/2022 13:53:58 - INFO - __main__ - Global step 2050 Train loss 0.43 Classification-F1 0.18971428571428572 on epoch=170
06/15/2022 13:54:01 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.43 on epoch=171
06/15/2022 13:54:03 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.39 on epoch=172
06/15/2022 13:54:06 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
06/15/2022 13:54:09 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
06/15/2022 13:54:11 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.37 on epoch=174
06/15/2022 13:54:17 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.2138736596252936 on epoch=174
06/15/2022 13:54:20 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.41 on epoch=175
06/15/2022 13:54:22 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=176
06/15/2022 13:54:25 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
06/15/2022 13:54:28 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=178
06/15/2022 13:54:30 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.47 on epoch=179
06/15/2022 13:54:36 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.22311119271482815 on epoch=179
06/15/2022 13:54:39 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=179
06/15/2022 13:54:41 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
06/15/2022 13:54:44 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.46 on epoch=181
06/15/2022 13:54:47 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
06/15/2022 13:54:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
06/15/2022 13:54:55 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.18951882832479847 on epoch=183
06/15/2022 13:54:58 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 13:55:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
06/15/2022 13:55:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=185
06/15/2022 13:55:06 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=186
06/15/2022 13:55:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.39 on epoch=187
06/15/2022 13:55:14 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.21122030109901513 on epoch=187
06/15/2022 13:55:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.41 on epoch=188
06/15/2022 13:55:19 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
06/15/2022 13:55:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
06/15/2022 13:55:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.40 on epoch=190
06/15/2022 13:55:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=191
06/15/2022 13:55:33 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.22713479331126388 on epoch=191
06/15/2022 13:55:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.41 on epoch=192
06/15/2022 13:55:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=193
06/15/2022 13:55:41 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=194
06/15/2022 13:55:44 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=194
06/15/2022 13:55:47 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=195
06/15/2022 13:55:52 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.2328874078366979 on epoch=195
06/15/2022 13:55:55 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=196
06/15/2022 13:55:58 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
06/15/2022 13:56:01 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 13:56:03 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.39 on epoch=199
06/15/2022 13:56:06 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=199
06/15/2022 13:56:12 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.22332569391392923 on epoch=199
06/15/2022 13:56:14 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=200
06/15/2022 13:56:17 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=201
06/15/2022 13:56:20 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=202
06/15/2022 13:56:22 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.39 on epoch=203
06/15/2022 13:56:25 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.43 on epoch=204
06/15/2022 13:56:31 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.22466169566418115 on epoch=204
06/15/2022 13:56:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.46 on epoch=204
06/15/2022 13:56:36 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.42 on epoch=205
06/15/2022 13:56:39 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=206
06/15/2022 13:56:42 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.47 on epoch=207
06/15/2022 13:56:44 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.42 on epoch=208
06/15/2022 13:56:50 - INFO - __main__ - Global step 2500 Train loss 0.43 Classification-F1 0.26572993569897596 on epoch=208
06/15/2022 13:56:50 - INFO - __main__ - Saving model with best Classification-F1: 0.24954212454212454 -> 0.26572993569897596 on epoch=208, global_step=2500
06/15/2022 13:56:53 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
06/15/2022 13:56:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=209
06/15/2022 13:56:58 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=210
06/15/2022 13:57:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.43 on epoch=211
06/15/2022 13:57:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=212
06/15/2022 13:57:09 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.23489132626669126 on epoch=212
06/15/2022 13:57:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=213
06/15/2022 13:57:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.48 on epoch=214
06/15/2022 13:57:18 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.41 on epoch=214
06/15/2022 13:57:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=215
06/15/2022 13:57:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.42 on epoch=216
06/15/2022 13:57:29 - INFO - __main__ - Global step 2600 Train loss 0.43 Classification-F1 0.2434599943257119 on epoch=216
06/15/2022 13:57:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=217
06/15/2022 13:57:34 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.36 on epoch=218
06/15/2022 13:57:37 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=219
06/15/2022 13:57:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=219
06/15/2022 13:57:42 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.36 on epoch=220
06/15/2022 13:57:48 - INFO - __main__ - Global step 2650 Train loss 0.38 Classification-F1 0.24291413220902233 on epoch=220
06/15/2022 13:57:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
06/15/2022 13:57:53 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.38 on epoch=222
06/15/2022 13:57:56 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=223
06/15/2022 13:57:59 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.42 on epoch=224
06/15/2022 13:58:01 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
06/15/2022 13:58:07 - INFO - __main__ - Global step 2700 Train loss 0.40 Classification-F1 0.24299810014095727 on epoch=224
06/15/2022 13:58:10 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.42 on epoch=225
06/15/2022 13:58:12 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
06/15/2022 13:58:15 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.42 on epoch=227
06/15/2022 13:58:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=228
06/15/2022 13:58:20 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=229
06/15/2022 13:58:26 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.23197138015439656 on epoch=229
06/15/2022 13:58:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.44 on epoch=229
06/15/2022 13:58:31 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=230
06/15/2022 13:58:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=231
06/15/2022 13:58:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
06/15/2022 13:58:40 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.40 on epoch=233
06/15/2022 13:58:45 - INFO - __main__ - Global step 2800 Train loss 0.42 Classification-F1 0.24600496201439292 on epoch=233
06/15/2022 13:58:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=234
06/15/2022 13:58:51 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
06/15/2022 13:58:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
06/15/2022 13:58:56 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=236
06/15/2022 13:58:59 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.39 on epoch=237
06/15/2022 13:59:04 - INFO - __main__ - Global step 2850 Train loss 0.40 Classification-F1 0.2487050960735171 on epoch=237
06/15/2022 13:59:07 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
06/15/2022 13:59:10 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=239
06/15/2022 13:59:12 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.38 on epoch=239
06/15/2022 13:59:15 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.41 on epoch=240
06/15/2022 13:59:18 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=241
06/15/2022 13:59:23 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.23204678362573097 on epoch=241
06/15/2022 13:59:26 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=242
06/15/2022 13:59:29 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.41 on epoch=243
06/15/2022 13:59:31 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
06/15/2022 13:59:34 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.40 on epoch=244
06/15/2022 13:59:37 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=245
06/15/2022 13:59:43 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.23908730158730163 on epoch=245
06/15/2022 13:59:45 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.39 on epoch=246
06/15/2022 13:59:48 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.43 on epoch=247
06/15/2022 13:59:51 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=248
06/15/2022 13:59:53 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=249
06/15/2022 13:59:56 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.42 on epoch=249
06/15/2022 13:59:57 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:59:57 - INFO - __main__ - Printing 3 examples
06/15/2022 13:59:57 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 13:59:57 - INFO - __main__ - ['neutral']
06/15/2022 13:59:57 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 13:59:57 - INFO - __main__ - ['neutral']
06/15/2022 13:59:57 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 13:59:57 - INFO - __main__ - ['neutral']
06/15/2022 13:59:57 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:59:58 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:59:58 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 13:59:58 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 13:59:58 - INFO - __main__ - Printing 3 examples
06/15/2022 13:59:58 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 13:59:58 - INFO - __main__ - ['neutral']
06/15/2022 13:59:58 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 13:59:58 - INFO - __main__ - ['neutral']
06/15/2022 13:59:58 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 13:59:58 - INFO - __main__ - ['neutral']
06/15/2022 13:59:58 - INFO - __main__ - Tokenizing Input ...
06/15/2022 13:59:58 - INFO - __main__ - Tokenizing Output ...
06/15/2022 13:59:58 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:00:02 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.23485386765193503 on epoch=249
06/15/2022 14:00:02 - INFO - __main__ - save last model!
06/15/2022 14:00:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 14:00:02 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 14:00:02 - INFO - __main__ - Printing 3 examples
06/15/2022 14:00:02 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 14:00:02 - INFO - __main__ - ['contradiction']
06/15/2022 14:00:02 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 14:00:02 - INFO - __main__ - ['entailment']
06/15/2022 14:00:02 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 14:00:02 - INFO - __main__ - ['contradiction']
06/15/2022 14:00:02 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:00:02 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:00:03 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 14:00:15 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:00:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:00:16 - INFO - __main__ - Starting training!
06/15/2022 14:00:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_100_0.3_8_predictions.txt
06/15/2022 14:00:33 - INFO - __main__ - Classification-F1 on test data: 0.2470
06/15/2022 14:00:34 - INFO - __main__ - prefix=anli_64_100, lr=0.3, bsz=8, dev_performance=0.26572993569897596, test_performance=0.247048520000661
06/15/2022 14:00:34 - INFO - __main__ - Running ... prefix=anli_64_100, lr=0.2, bsz=8 ...
06/15/2022 14:00:34 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:00:34 - INFO - __main__ - Printing 3 examples
06/15/2022 14:00:34 - INFO - __main__ -  [anli] premise: John Zdechlik ("Zuh-DEK-lik") (born 2 May 1937) is an American composer, music teacher, and conductor. Zdechlik has been elected to the American Bandmasters Association and many of his compositions have become standard concert band repertoire, including Chorale and Shaker Dance and Psalm 46. [SEP] hypothesis: John Zdechlik is a savant of music. 
06/15/2022 14:00:34 - INFO - __main__ - ['neutral']
06/15/2022 14:00:34 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run" even though Laine had written a few songs before for other singers
06/15/2022 14:00:34 - INFO - __main__ - ['neutral']
06/15/2022 14:00:34 - INFO - __main__ -  [anli] premise: Askold Anatolievich Makarov (Russian: Аско́льд Анато́льевич Мака́ров ; 3 May 1925 – 25 December 2000) was a Russian ballet dancer and ballet professor, leading soloist at the Kirov Ballet during the 1960s and early 1970s. Director of the Saint-Petesburg State Academic Ballet from 1976 to 2000. Awarded with: State Prize of the USSR (1951) and People's Artist of the USSR (1983). [SEP] hypothesis: The USSR is home to some of the finest ballet dancers in the world.
06/15/2022 14:00:34 - INFO - __main__ - ['neutral']
06/15/2022 14:00:34 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:00:35 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:00:35 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:00:35 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:00:35 - INFO - __main__ - Printing 3 examples
06/15/2022 14:00:35 - INFO - __main__ -  [anli] premise: Rudolph the Red-Nosed Reindeer is a legendary reindeer, created by Robert Lewis May, usually depicted as a young fawn who barely has antlers, with a glowing red nose, popularly known as "Santa's ninth reindeer." When depicted, he is the lead reindeer pulling Santa's sleigh on Christmas Eve. The luminosity of his nose is so great that it illuminates the team's path through inclement winter weather. [SEP] hypothesis: Rudolph the Red-nosed Reindeer can guide the pack through clear weather.
06/15/2022 14:00:35 - INFO - __main__ - ['neutral']
06/15/2022 14:00:35 - INFO - __main__ -  [anli] premise: The Bessbrook bombing took place on the 17 April 1979 when four Royal Ulster Constabulary (RUC) officers were killed when the Provisional IRA exploded an estimated 1,000 pound roadside van bomb at Bessbrook, County Armagh, believed to be the largest bomb used by the IRA up to that point. [SEP] hypothesis: Bessbrook County was averse to the Irish Republic Army's bomb.
06/15/2022 14:00:35 - INFO - __main__ - ['neutral']
06/15/2022 14:00:35 - INFO - __main__ -  [anli] premise: The Bathurst 1000 (currently branded as the Supercheap Auto Bathurst 1000 for sponsorship reasons) is a 1,000 km touring car race held annually on the Mount Panorama Circuit in Bathurst, New South Wales, Australia. It is currently run as a championship event for Supercars. [SEP] hypothesis: The Bathurst 1000 is the most popular race in Australia.
06/15/2022 14:00:35 - INFO - __main__ - ['neutral']
06/15/2022 14:00:35 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:00:35 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:00:35 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:00:50 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:00:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:00:51 - INFO - __main__ - Starting training!
06/15/2022 14:00:55 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=0
06/15/2022 14:00:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=1
06/15/2022 14:01:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=2
06/15/2022 14:01:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
06/15/2022 14:01:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.64 on epoch=4
06/15/2022 14:01:11 - INFO - __main__ - Global step 50 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 14:01:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 14:01:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=4
06/15/2022 14:01:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=5
06/15/2022 14:01:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.49 on epoch=6
06/15/2022 14:01:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=7
06/15/2022 14:01:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=8
06/15/2022 14:01:28 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 14:01:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
06/15/2022 14:01:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=9
06/15/2022 14:01:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=10
06/15/2022 14:01:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=11
06/15/2022 14:01:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=12
06/15/2022 14:01:46 - INFO - __main__ - Global step 150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 14:01:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=13
06/15/2022 14:01:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=14
06/15/2022 14:01:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
06/15/2022 14:01:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=15
06/15/2022 14:01:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.43 on epoch=16
06/15/2022 14:02:04 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 14:02:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 14:02:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.46 on epoch=18
06/15/2022 14:02:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
06/15/2022 14:02:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
06/15/2022 14:02:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.52 on epoch=20
06/15/2022 14:02:22 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 14:02:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=21
06/15/2022 14:02:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
06/15/2022 14:02:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
06/15/2022 14:02:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.43 on epoch=24
06/15/2022 14:02:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
06/15/2022 14:02:41 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 14:02:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=25
06/15/2022 14:02:46 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
06/15/2022 14:02:48 - INFO - __main__ - Step 330 Global step 330 Train loss 0.47 on epoch=27
06/15/2022 14:02:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
06/15/2022 14:02:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=29
06/15/2022 14:02:59 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.27208605184270845 on epoch=29
06/15/2022 14:02:59 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27208605184270845 on epoch=29, global_step=350
06/15/2022 14:03:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=29
06/15/2022 14:03:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=30
06/15/2022 14:03:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
06/15/2022 14:03:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=32
06/15/2022 14:03:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.46 on epoch=33
06/15/2022 14:03:17 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 14:03:19 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=34
06/15/2022 14:03:22 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=34
06/15/2022 14:03:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.46 on epoch=35
06/15/2022 14:03:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=36
06/15/2022 14:03:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=37
06/15/2022 14:03:35 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 14:03:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.42 on epoch=38
06/15/2022 14:03:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
06/15/2022 14:03:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=39
06/15/2022 14:03:45 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=40
06/15/2022 14:03:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.49 on epoch=41
06/15/2022 14:03:53 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 14:03:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
06/15/2022 14:03:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
06/15/2022 14:04:01 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=44
06/15/2022 14:04:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.51 on epoch=44
06/15/2022 14:04:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=45
06/15/2022 14:04:11 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.22214051858465955 on epoch=45
06/15/2022 14:04:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
06/15/2022 14:04:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.47 on epoch=47
06/15/2022 14:04:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.39 on epoch=48
06/15/2022 14:04:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=49
06/15/2022 14:04:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
06/15/2022 14:04:30 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 14:04:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
06/15/2022 14:04:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=51
06/15/2022 14:04:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=52
06/15/2022 14:04:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=53
06/15/2022 14:04:43 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
06/15/2022 14:04:47 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.27439153439153435 on epoch=54
06/15/2022 14:04:48 - INFO - __main__ - Saving model with best Classification-F1: 0.27208605184270845 -> 0.27439153439153435 on epoch=54, global_step=650
06/15/2022 14:04:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=54
06/15/2022 14:04:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=55
06/15/2022 14:04:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.45 on epoch=56
06/15/2022 14:04:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.45 on epoch=57
06/15/2022 14:05:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
06/15/2022 14:05:06 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.19301342158485016 on epoch=58
06/15/2022 14:05:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=59
06/15/2022 14:05:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 14:05:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=60
06/15/2022 14:05:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.40 on epoch=61
06/15/2022 14:05:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
06/15/2022 14:05:24 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 14:05:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
06/15/2022 14:05:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.41 on epoch=64
06/15/2022 14:05:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
06/15/2022 14:05:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
06/15/2022 14:05:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=66
06/15/2022 14:05:42 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 14:05:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
06/15/2022 14:05:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.43 on epoch=68
06/15/2022 14:05:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=69
06/15/2022 14:05:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
06/15/2022 14:05:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 14:06:01 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.29476740971516074 on epoch=70
06/15/2022 14:06:01 - INFO - __main__ - Saving model with best Classification-F1: 0.27439153439153435 -> 0.29476740971516074 on epoch=70, global_step=850
06/15/2022 14:06:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.45 on epoch=71
06/15/2022 14:06:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=72
06/15/2022 14:06:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
06/15/2022 14:06:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.50 on epoch=74
06/15/2022 14:06:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
06/15/2022 14:06:19 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 14:06:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
06/15/2022 14:06:24 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=76
06/15/2022 14:06:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=77
06/15/2022 14:06:30 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=78
06/15/2022 14:06:32 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
06/15/2022 14:06:38 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.2194161522434532 on epoch=79
06/15/2022 14:06:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
06/15/2022 14:06:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=80
06/15/2022 14:06:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.47 on epoch=81
06/15/2022 14:06:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
06/15/2022 14:06:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=83
06/15/2022 14:06:56 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.1819250708139597 on epoch=83
06/15/2022 14:06:59 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=84
06/15/2022 14:07:01 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=84
06/15/2022 14:07:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=85
06/15/2022 14:07:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=86
06/15/2022 14:07:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=87
06/15/2022 14:07:14 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.202169212527028 on epoch=87
06/15/2022 14:07:17 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
06/15/2022 14:07:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
06/15/2022 14:07:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=89
06/15/2022 14:07:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
06/15/2022 14:07:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=91
06/15/2022 14:07:33 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.20331113483469743 on epoch=91
06/15/2022 14:07:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=92
06/15/2022 14:07:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
06/15/2022 14:07:40 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=94
06/15/2022 14:07:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=94
06/15/2022 14:07:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=95
06/15/2022 14:07:51 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.24269251366951186 on epoch=95
06/15/2022 14:07:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=96
06/15/2022 14:07:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
06/15/2022 14:07:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
06/15/2022 14:08:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=99
06/15/2022 14:08:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=99
06/15/2022 14:08:09 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.20999848761685203 on epoch=99
06/15/2022 14:08:12 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=100
06/15/2022 14:08:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=101
06/15/2022 14:08:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=102
06/15/2022 14:08:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=103
06/15/2022 14:08:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=104
06/15/2022 14:08:28 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.34030402234622165 on epoch=104
06/15/2022 14:08:28 - INFO - __main__ - Saving model with best Classification-F1: 0.29476740971516074 -> 0.34030402234622165 on epoch=104, global_step=1250
06/15/2022 14:08:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=104
06/15/2022 14:08:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
06/15/2022 14:08:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=106
06/15/2022 14:08:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.38 on epoch=107
06/15/2022 14:08:41 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
06/15/2022 14:08:46 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.26604081823415565 on epoch=108
06/15/2022 14:08:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=109
06/15/2022 14:08:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
06/15/2022 14:08:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=110
06/15/2022 14:08:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=111
06/15/2022 14:08:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.38 on epoch=112
06/15/2022 14:09:05 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.23128051552709086 on epoch=112
06/15/2022 14:09:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
06/15/2022 14:09:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=114
06/15/2022 14:09:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=114
06/15/2022 14:09:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
06/15/2022 14:09:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
06/15/2022 14:09:23 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.19624991718014972 on epoch=116
06/15/2022 14:09:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.41 on epoch=117
06/15/2022 14:09:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=118
06/15/2022 14:09:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.36 on epoch=119
06/15/2022 14:09:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
06/15/2022 14:09:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=120
06/15/2022 14:09:41 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.2474747474747475 on epoch=120
06/15/2022 14:09:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.43 on epoch=121
06/15/2022 14:09:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=122
06/15/2022 14:09:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.42 on epoch=123
06/15/2022 14:09:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=124
06/15/2022 14:09:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=124
06/15/2022 14:09:59 - INFO - __main__ - Global step 1500 Train loss 0.41 Classification-F1 0.19874843554443053 on epoch=124
06/15/2022 14:10:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=125
06/15/2022 14:10:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=126
06/15/2022 14:10:07 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=127
06/15/2022 14:10:09 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=128
06/15/2022 14:10:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=129
06/15/2022 14:10:17 - INFO - __main__ - Global step 1550 Train loss 0.39 Classification-F1 0.22243503798761413 on epoch=129
06/15/2022 14:10:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=129
06/15/2022 14:10:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=130
06/15/2022 14:10:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=131
06/15/2022 14:10:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=132
06/15/2022 14:10:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=133
06/15/2022 14:10:35 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.23420546948040968 on epoch=133
06/15/2022 14:10:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.42 on epoch=134
06/15/2022 14:10:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=134
06/15/2022 14:10:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=135
06/15/2022 14:10:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=136
06/15/2022 14:10:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=137
06/15/2022 14:10:53 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.1959115561118064 on epoch=137
06/15/2022 14:10:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=138
06/15/2022 14:10:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
06/15/2022 14:11:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
06/15/2022 14:11:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=140
06/15/2022 14:11:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=141
06/15/2022 14:11:12 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.28596296882418104 on epoch=141
06/15/2022 14:11:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
06/15/2022 14:11:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
06/15/2022 14:11:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=144
06/15/2022 14:11:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=144
06/15/2022 14:11:25 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
06/15/2022 14:11:30 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.27471052189964756 on epoch=145
06/15/2022 14:11:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.37 on epoch=146
06/15/2022 14:11:35 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.36 on epoch=147
06/15/2022 14:11:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=148
06/15/2022 14:11:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=149
06/15/2022 14:11:43 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
06/15/2022 14:11:48 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.21295903847900458 on epoch=149
06/15/2022 14:11:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=150
06/15/2022 14:11:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=151
06/15/2022 14:11:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=152
06/15/2022 14:11:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=153
06/15/2022 14:12:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=154
06/15/2022 14:12:07 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.2268791229388892 on epoch=154
06/15/2022 14:12:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
06/15/2022 14:12:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=155
06/15/2022 14:12:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=156
06/15/2022 14:12:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
06/15/2022 14:12:20 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.39 on epoch=158
06/15/2022 14:12:25 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.2679906471308095 on epoch=158
06/15/2022 14:12:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
06/15/2022 14:12:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=159
06/15/2022 14:12:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
06/15/2022 14:12:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=161
06/15/2022 14:12:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=162
06/15/2022 14:12:44 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.20445785151667503 on epoch=162
06/15/2022 14:12:46 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=163
06/15/2022 14:12:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=164
06/15/2022 14:12:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=164
06/15/2022 14:12:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=165
06/15/2022 14:12:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=166
06/15/2022 14:13:02 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.25521176212420454 on epoch=166
06/15/2022 14:13:05 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.35 on epoch=167
06/15/2022 14:13:07 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.37 on epoch=168
06/15/2022 14:13:10 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.32 on epoch=169
06/15/2022 14:13:12 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=169
06/15/2022 14:13:15 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.37 on epoch=170
06/15/2022 14:13:20 - INFO - __main__ - Global step 2050 Train loss 0.35 Classification-F1 0.19879832810867293 on epoch=170
06/15/2022 14:13:23 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.40 on epoch=171
06/15/2022 14:13:26 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=172
06/15/2022 14:13:28 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=173
06/15/2022 14:13:31 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.36 on epoch=174
06/15/2022 14:13:33 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.39 on epoch=174
06/15/2022 14:13:39 - INFO - __main__ - Global step 2100 Train loss 0.39 Classification-F1 0.30907461713913326 on epoch=174
06/15/2022 14:13:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=175
06/15/2022 14:13:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=176
06/15/2022 14:13:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.39 on epoch=177
06/15/2022 14:13:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.39 on epoch=178
06/15/2022 14:13:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.37 on epoch=179
06/15/2022 14:13:57 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.3334591796097172 on epoch=179
06/15/2022 14:14:00 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.37 on epoch=179
06/15/2022 14:14:03 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.38 on epoch=180
06/15/2022 14:14:05 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=181
06/15/2022 14:14:08 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.36 on epoch=182
06/15/2022 14:14:10 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=183
06/15/2022 14:14:16 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.25938659518138724 on epoch=183
06/15/2022 14:14:18 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.36 on epoch=184
06/15/2022 14:14:21 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=184
06/15/2022 14:14:24 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=185
06/15/2022 14:14:26 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.42 on epoch=186
06/15/2022 14:14:29 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=187
06/15/2022 14:14:34 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.22509315252580783 on epoch=187
06/15/2022 14:14:37 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.34 on epoch=188
06/15/2022 14:14:39 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.36 on epoch=189
06/15/2022 14:14:42 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.37 on epoch=189
06/15/2022 14:14:45 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.36 on epoch=190
06/15/2022 14:14:47 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
06/15/2022 14:14:53 - INFO - __main__ - Global step 2300 Train loss 0.36 Classification-F1 0.24992390973520587 on epoch=191
06/15/2022 14:14:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=192
06/15/2022 14:14:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.38 on epoch=193
06/15/2022 14:15:00 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.36 on epoch=194
06/15/2022 14:15:03 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.37 on epoch=194
06/15/2022 14:15:06 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
06/15/2022 14:15:11 - INFO - __main__ - Global step 2350 Train loss 0.37 Classification-F1 0.26827401454267125 on epoch=195
06/15/2022 14:15:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.33 on epoch=196
06/15/2022 14:15:16 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=197
06/15/2022 14:15:19 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.36 on epoch=198
06/15/2022 14:15:21 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=199
06/15/2022 14:15:24 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.33 on epoch=199
06/15/2022 14:15:29 - INFO - __main__ - Global step 2400 Train loss 0.34 Classification-F1 0.27755221187272777 on epoch=199
06/15/2022 14:15:32 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.37 on epoch=200
06/15/2022 14:15:35 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=201
06/15/2022 14:15:37 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=202
06/15/2022 14:15:40 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.37 on epoch=203
06/15/2022 14:15:42 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.33 on epoch=204
06/15/2022 14:15:48 - INFO - __main__ - Global step 2450 Train loss 0.37 Classification-F1 0.29630024806318517 on epoch=204
06/15/2022 14:15:51 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.39 on epoch=204
06/15/2022 14:15:53 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.36 on epoch=205
06/15/2022 14:15:56 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=206
06/15/2022 14:15:58 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
06/15/2022 14:16:01 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
06/15/2022 14:16:06 - INFO - __main__ - Global step 2500 Train loss 0.38 Classification-F1 0.30884331329105236 on epoch=208
06/15/2022 14:16:09 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.32 on epoch=209
06/15/2022 14:16:12 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
06/15/2022 14:16:14 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=210
06/15/2022 14:16:17 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=211
06/15/2022 14:16:19 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
06/15/2022 14:16:25 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.2867171717171717 on epoch=212
06/15/2022 14:16:28 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=213
06/15/2022 14:16:30 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=214
06/15/2022 14:16:33 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.33 on epoch=214
06/15/2022 14:16:35 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=215
06/15/2022 14:16:38 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.33 on epoch=216
06/15/2022 14:16:43 - INFO - __main__ - Global step 2600 Train loss 0.34 Classification-F1 0.2898329369757375 on epoch=216
06/15/2022 14:16:46 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.34 on epoch=217
06/15/2022 14:16:48 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.31 on epoch=218
06/15/2022 14:16:51 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.28 on epoch=219
06/15/2022 14:16:54 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.27 on epoch=219
06/15/2022 14:16:56 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.40 on epoch=220
06/15/2022 14:17:02 - INFO - __main__ - Global step 2650 Train loss 0.32 Classification-F1 0.26966242488511843 on epoch=220
06/15/2022 14:17:04 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.32 on epoch=221
06/15/2022 14:17:07 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.31 on epoch=222
06/15/2022 14:17:09 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=223
06/15/2022 14:17:12 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=224
06/15/2022 14:17:15 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.30 on epoch=224
06/15/2022 14:17:20 - INFO - __main__ - Global step 2700 Train loss 0.30 Classification-F1 0.28358165858165857 on epoch=224
06/15/2022 14:17:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.32 on epoch=225
06/15/2022 14:17:25 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.31 on epoch=226
06/15/2022 14:17:28 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.26 on epoch=227
06/15/2022 14:17:30 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=228
06/15/2022 14:17:33 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.29 on epoch=229
06/15/2022 14:17:39 - INFO - __main__ - Global step 2750 Train loss 0.30 Classification-F1 0.3371203799600373 on epoch=229
06/15/2022 14:17:41 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.31 on epoch=229
06/15/2022 14:17:44 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.29 on epoch=230
06/15/2022 14:17:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.32 on epoch=231
06/15/2022 14:17:49 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.31 on epoch=232
06/15/2022 14:17:52 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.30 on epoch=233
06/15/2022 14:17:57 - INFO - __main__ - Global step 2800 Train loss 0.31 Classification-F1 0.3166679446872684 on epoch=233
06/15/2022 14:18:00 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.30 on epoch=234
06/15/2022 14:18:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.27 on epoch=234
06/15/2022 14:18:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=235
06/15/2022 14:18:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.33 on epoch=236
06/15/2022 14:18:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.29 on epoch=237
06/15/2022 14:18:16 - INFO - __main__ - Global step 2850 Train loss 0.29 Classification-F1 0.2925425846330359 on epoch=237
06/15/2022 14:18:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.34 on epoch=238
06/15/2022 14:18:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.31 on epoch=239
06/15/2022 14:18:23 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.26 on epoch=239
06/15/2022 14:18:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.32 on epoch=240
06/15/2022 14:18:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=241
06/15/2022 14:18:34 - INFO - __main__ - Global step 2900 Train loss 0.31 Classification-F1 0.28960385406118627 on epoch=241
06/15/2022 14:18:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.31 on epoch=242
06/15/2022 14:18:39 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.28 on epoch=243
06/15/2022 14:18:42 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.29 on epoch=244
06/15/2022 14:18:44 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.27 on epoch=244
06/15/2022 14:18:47 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=245
06/15/2022 14:18:52 - INFO - __main__ - Global step 2950 Train loss 0.30 Classification-F1 0.3033233062766693 on epoch=245
06/15/2022 14:18:55 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=246
06/15/2022 14:18:57 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.29 on epoch=247
06/15/2022 14:19:00 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.27 on epoch=248
06/15/2022 14:19:03 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.28 on epoch=249
06/15/2022 14:19:05 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.27 on epoch=249
06/15/2022 14:19:07 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:19:07 - INFO - __main__ - Printing 3 examples
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:19:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:19:07 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:19:07 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:19:07 - INFO - __main__ - Printing 3 examples
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:19:07 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:07 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:19:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:19:07 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:19:10 - INFO - __main__ - Global step 3000 Train loss 0.29 Classification-F1 0.26279809220985695 on epoch=249
06/15/2022 14:19:10 - INFO - __main__ - save last model!
06/15/2022 14:19:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 14:19:10 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 14:19:10 - INFO - __main__ - Printing 3 examples
06/15/2022 14:19:10 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 14:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:10 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 14:19:10 - INFO - __main__ - ['entailment']
06/15/2022 14:19:10 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 14:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:10 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:19:11 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:19:12 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 14:19:25 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:19:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:19:26 - INFO - __main__ - Starting training!
06/15/2022 14:19:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_100_0.2_8_predictions.txt
06/15/2022 14:19:36 - INFO - __main__ - Classification-F1 on test data: 0.2607
06/15/2022 14:19:36 - INFO - __main__ - prefix=anli_64_100, lr=0.2, bsz=8, dev_performance=0.34030402234622165, test_performance=0.26065238606875235
06/15/2022 14:19:36 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.5, bsz=8 ...
06/15/2022 14:19:37 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:19:37 - INFO - __main__ - Printing 3 examples
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:19:37 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:19:37 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:19:37 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:19:37 - INFO - __main__ - Printing 3 examples
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:19:37 - INFO - __main__ - ['contradiction']
06/15/2022 14:19:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:19:37 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:19:38 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:19:56 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:19:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:19:57 - INFO - __main__ - Starting training!
06/15/2022 14:20:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.92 on epoch=0
06/15/2022 14:20:03 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=1
06/15/2022 14:20:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=2
06/15/2022 14:20:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
06/15/2022 14:20:11 - INFO - __main__ - Step 50 Global step 50 Train loss 0.60 on epoch=4
06/15/2022 14:20:16 - INFO - __main__ - Global step 50 Train loss 0.65 Classification-F1 0.17595815389455882 on epoch=4
06/15/2022 14:20:16 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.17595815389455882 on epoch=4, global_step=50
06/15/2022 14:20:18 - INFO - __main__ - Step 60 Global step 60 Train loss 0.55 on epoch=4
06/15/2022 14:20:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=5
06/15/2022 14:20:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=6
06/15/2022 14:20:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=7
06/15/2022 14:20:29 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=8
06/15/2022 14:20:34 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 14:20:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=9
06/15/2022 14:20:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
06/15/2022 14:20:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.49 on epoch=10
06/15/2022 14:20:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=11
06/15/2022 14:20:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=12
06/15/2022 14:20:53 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 14:20:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=13
06/15/2022 14:20:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
06/15/2022 14:21:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
06/15/2022 14:21:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=15
06/15/2022 14:21:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=16
06/15/2022 14:21:12 - INFO - __main__ - Global step 200 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 14:21:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=17
06/15/2022 14:21:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=18
06/15/2022 14:21:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=19
06/15/2022 14:21:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=19
06/15/2022 14:21:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=20
06/15/2022 14:21:32 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.2511262994562979 on epoch=20
06/15/2022 14:21:32 - INFO - __main__ - Saving model with best Classification-F1: 0.17595815389455882 -> 0.2511262994562979 on epoch=20, global_step=250
06/15/2022 14:21:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=21
06/15/2022 14:21:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=22
06/15/2022 14:21:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=23
06/15/2022 14:21:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=24
06/15/2022 14:21:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.45 on epoch=24
06/15/2022 14:21:50 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 14:21:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.43 on epoch=25
06/15/2022 14:21:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=26
06/15/2022 14:21:58 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=27
06/15/2022 14:22:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
06/15/2022 14:22:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
06/15/2022 14:22:10 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.18849182313749244 on epoch=29
06/15/2022 14:22:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=29
06/15/2022 14:22:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=30
06/15/2022 14:22:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
06/15/2022 14:22:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.46 on epoch=32
06/15/2022 14:22:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
06/15/2022 14:22:29 - INFO - __main__ - Global step 400 Train loss 0.45 Classification-F1 0.16533333333333333 on epoch=33
06/15/2022 14:22:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.43 on epoch=34
06/15/2022 14:22:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
06/15/2022 14:22:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=35
06/15/2022 14:22:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.49 on epoch=36
06/15/2022 14:22:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
06/15/2022 14:22:48 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.22497779599644738 on epoch=37
06/15/2022 14:22:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
06/15/2022 14:22:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=39
06/15/2022 14:22:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
06/15/2022 14:22:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=40
06/15/2022 14:23:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=41
06/15/2022 14:23:07 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 14:23:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=42
06/15/2022 14:23:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=43
06/15/2022 14:23:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
06/15/2022 14:23:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.41 on epoch=44
06/15/2022 14:23:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
06/15/2022 14:23:26 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 14:23:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
06/15/2022 14:23:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=47
06/15/2022 14:23:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 14:23:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=49
06/15/2022 14:23:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
06/15/2022 14:23:45 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.178080012725682 on epoch=49
06/15/2022 14:23:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.43 on epoch=50
06/15/2022 14:23:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=51
06/15/2022 14:23:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=52
06/15/2022 14:23:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 14:23:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=54
06/15/2022 14:24:04 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 14:24:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=54
06/15/2022 14:24:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.93 on epoch=55
06/15/2022 14:24:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.74 on epoch=56
06/15/2022 14:24:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
06/15/2022 14:24:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.43 on epoch=58
06/15/2022 14:24:23 - INFO - __main__ - Global step 700 Train loss 0.60 Classification-F1 0.29156223893065997 on epoch=58
06/15/2022 14:24:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2511262994562979 -> 0.29156223893065997 on epoch=58, global_step=700
06/15/2022 14:24:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
06/15/2022 14:24:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.41 on epoch=59
06/15/2022 14:24:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=60
06/15/2022 14:24:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=61
06/15/2022 14:24:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
06/15/2022 14:24:42 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.28347509763012896 on epoch=62
06/15/2022 14:24:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
06/15/2022 14:24:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
06/15/2022 14:24:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=64
06/15/2022 14:24:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.40 on epoch=65
06/15/2022 14:24:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=66
06/15/2022 14:25:01 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16732026143790854 on epoch=66
06/15/2022 14:25:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.42 on epoch=67
06/15/2022 14:25:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=68
06/15/2022 14:25:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=69
06/15/2022 14:25:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=69
06/15/2022 14:25:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 14:25:20 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 14:25:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
06/15/2022 14:25:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
06/15/2022 14:25:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
06/15/2022 14:25:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
06/15/2022 14:25:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=74
06/15/2022 14:25:40 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.2279808047249908 on epoch=74
06/15/2022 14:25:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.69 on epoch=75
06/15/2022 14:25:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.59 on epoch=76
06/15/2022 14:25:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=77
06/15/2022 14:25:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
06/15/2022 14:25:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=79
06/15/2022 14:25:59 - INFO - __main__ - Global step 950 Train loss 0.54 Classification-F1 0.1647058823529412 on epoch=79
06/15/2022 14:26:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
06/15/2022 14:26:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=80
06/15/2022 14:26:07 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=81
06/15/2022 14:26:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
06/15/2022 14:26:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=83
06/15/2022 14:26:18 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.2449230769230769 on epoch=83
06/15/2022 14:26:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=84
06/15/2022 14:26:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=84
06/15/2022 14:26:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
06/15/2022 14:26:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
06/15/2022 14:26:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=87
06/15/2022 14:26:37 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.21035669816157623 on epoch=87
06/15/2022 14:26:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=88
06/15/2022 14:26:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=89
06/15/2022 14:26:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
06/15/2022 14:26:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=90
06/15/2022 14:26:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
06/15/2022 14:26:56 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.22787313696404607 on epoch=91
06/15/2022 14:26:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=92
06/15/2022 14:27:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
06/15/2022 14:27:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=94
06/15/2022 14:27:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=94
06/15/2022 14:27:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=95
06/15/2022 14:27:16 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.22420634920634921 on epoch=95
06/15/2022 14:27:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=96
06/15/2022 14:27:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
06/15/2022 14:27:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
06/15/2022 14:27:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=99
06/15/2022 14:27:29 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=99
06/15/2022 14:27:35 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.2022321113230204 on epoch=99
06/15/2022 14:27:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=100
06/15/2022 14:27:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
06/15/2022 14:27:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
06/15/2022 14:27:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=103
06/15/2022 14:27:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.40 on epoch=104
06/15/2022 14:27:54 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.2113056803898273 on epoch=104
06/15/2022 14:27:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.36 on epoch=104
06/15/2022 14:27:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=105
06/15/2022 14:28:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
06/15/2022 14:28:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
06/15/2022 14:28:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
06/15/2022 14:28:13 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.21052631578947367 on epoch=108
06/15/2022 14:28:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=109
06/15/2022 14:28:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.44 on epoch=109
06/15/2022 14:28:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=110
06/15/2022 14:28:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=111
06/15/2022 14:28:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.35 on epoch=112
06/15/2022 14:28:33 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 14:28:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=113
06/15/2022 14:28:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.43 on epoch=114
06/15/2022 14:28:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
06/15/2022 14:28:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.42 on epoch=115
06/15/2022 14:28:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.38 on epoch=116
06/15/2022 14:28:52 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.2595074354905072 on epoch=116
06/15/2022 14:28:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=117
06/15/2022 14:28:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
06/15/2022 14:29:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=119
06/15/2022 14:29:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
06/15/2022 14:29:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
06/15/2022 14:29:11 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.1836290071584189 on epoch=120
06/15/2022 14:29:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
06/15/2022 14:29:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
06/15/2022 14:29:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=123
06/15/2022 14:29:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.40 on epoch=124
06/15/2022 14:29:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=124
06/15/2022 14:29:30 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.1721904761904762 on epoch=124
06/15/2022 14:29:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=125
06/15/2022 14:29:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=126
06/15/2022 14:29:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=127
06/15/2022 14:29:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
06/15/2022 14:29:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=129
06/15/2022 14:29:49 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.18399830629174121 on epoch=129
06/15/2022 14:29:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=129
06/15/2022 14:29:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.39 on epoch=130
06/15/2022 14:29:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=131
06/15/2022 14:30:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.41 on epoch=132
06/15/2022 14:30:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
06/15/2022 14:30:09 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 14:30:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=134
06/15/2022 14:30:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.45 on epoch=134
06/15/2022 14:30:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=135
06/15/2022 14:30:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=136
06/15/2022 14:30:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=137
06/15/2022 14:30:28 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 14:30:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
06/15/2022 14:30:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=139
06/15/2022 14:30:36 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.42 on epoch=139
06/15/2022 14:30:39 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
06/15/2022 14:30:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=141
06/15/2022 14:30:47 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.22030111105974448 on epoch=141
06/15/2022 14:30:50 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
06/15/2022 14:30:53 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=143
06/15/2022 14:30:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=144
06/15/2022 14:30:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=144
06/15/2022 14:31:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.40 on epoch=145
06/15/2022 14:31:07 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.2520499453164295 on epoch=145
06/15/2022 14:31:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
06/15/2022 14:31:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=147
06/15/2022 14:31:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 14:31:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=149
06/15/2022 14:31:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.44 on epoch=149
06/15/2022 14:31:26 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=149
06/15/2022 14:31:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.42 on epoch=150
06/15/2022 14:31:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
06/15/2022 14:31:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=152
06/15/2022 14:31:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
06/15/2022 14:31:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=154
06/15/2022 14:31:45 - INFO - __main__ - Global step 1850 Train loss 0.43 Classification-F1 0.28329218752040963 on epoch=154
06/15/2022 14:31:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.39 on epoch=154
06/15/2022 14:31:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=155
06/15/2022 14:31:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.42 on epoch=156
06/15/2022 14:31:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.37 on epoch=157
06/15/2022 14:31:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
06/15/2022 14:32:04 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.2816993464052287 on epoch=158
06/15/2022 14:32:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=159
06/15/2022 14:32:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
06/15/2022 14:32:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
06/15/2022 14:32:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
06/15/2022 14:32:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
06/15/2022 14:32:23 - INFO - __main__ - Global step 1950 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=162
06/15/2022 14:32:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=163
06/15/2022 14:32:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=164
06/15/2022 14:32:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.40 on epoch=164
06/15/2022 14:32:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=165
06/15/2022 14:32:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
06/15/2022 14:32:42 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.16666666666666666 on epoch=166
06/15/2022 14:32:45 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.41 on epoch=167
06/15/2022 14:32:48 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
06/15/2022 14:32:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=169
06/15/2022 14:32:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
06/15/2022 14:32:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=170
06/15/2022 14:33:01 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=170
06/15/2022 14:33:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=171
06/15/2022 14:33:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
06/15/2022 14:33:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.42 on epoch=173
06/15/2022 14:33:12 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.43 on epoch=174
06/15/2022 14:33:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
06/15/2022 14:33:20 - INFO - __main__ - Global step 2100 Train loss 0.43 Classification-F1 0.2922417521922063 on epoch=174
06/15/2022 14:33:20 - INFO - __main__ - Saving model with best Classification-F1: 0.29156223893065997 -> 0.2922417521922063 on epoch=174, global_step=2100
06/15/2022 14:33:23 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=175
06/15/2022 14:33:26 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
06/15/2022 14:33:28 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
06/15/2022 14:33:31 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.41 on epoch=178
06/15/2022 14:33:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=179
06/15/2022 14:33:39 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.16535433070866143 on epoch=179
06/15/2022 14:33:42 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=179
06/15/2022 14:33:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
06/15/2022 14:33:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=181
06/15/2022 14:33:50 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.38 on epoch=182
06/15/2022 14:33:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.40 on epoch=183
06/15/2022 14:33:58 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.18171428571428572 on epoch=183
06/15/2022 14:34:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.44 on epoch=184
06/15/2022 14:34:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.37 on epoch=184
06/15/2022 14:34:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.43 on epoch=185
06/15/2022 14:34:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.46 on epoch=186
06/15/2022 14:34:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=187
06/15/2022 14:34:17 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.1647058823529412 on epoch=187
06/15/2022 14:34:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=188
06/15/2022 14:34:23 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.39 on epoch=189
06/15/2022 14:34:26 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
06/15/2022 14:34:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=190
06/15/2022 14:34:31 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.43 on epoch=191
06/15/2022 14:34:37 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.3439814814814815 on epoch=191
06/15/2022 14:34:37 - INFO - __main__ - Saving model with best Classification-F1: 0.2922417521922063 -> 0.3439814814814815 on epoch=191, global_step=2300
06/15/2022 14:34:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.39 on epoch=192
06/15/2022 14:34:42 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=193
06/15/2022 14:34:45 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=194
06/15/2022 14:34:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=194
06/15/2022 14:34:50 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.44 on epoch=195
06/15/2022 14:34:56 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.21146085552865215 on epoch=195
06/15/2022 14:34:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.41 on epoch=196
06/15/2022 14:35:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=197
06/15/2022 14:35:04 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 14:35:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.41 on epoch=199
06/15/2022 14:35:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.45 on epoch=199
06/15/2022 14:35:15 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.175386993985331 on epoch=199
06/15/2022 14:35:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.46 on epoch=200
06/15/2022 14:35:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.40 on epoch=201
06/15/2022 14:35:23 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=202
06/15/2022 14:35:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
06/15/2022 14:35:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.41 on epoch=204
06/15/2022 14:35:34 - INFO - __main__ - Global step 2450 Train loss 0.41 Classification-F1 0.24679781822638966 on epoch=204
06/15/2022 14:35:37 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.42 on epoch=204
06/15/2022 14:35:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.38 on epoch=205
06/15/2022 14:35:42 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
06/15/2022 14:35:45 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.39 on epoch=207
06/15/2022 14:35:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.40 on epoch=208
06/15/2022 14:35:53 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.25410682791635175 on epoch=208
06/15/2022 14:35:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=209
06/15/2022 14:35:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.39 on epoch=209
06/15/2022 14:36:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.41 on epoch=210
06/15/2022 14:36:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.41 on epoch=211
06/15/2022 14:36:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.45 on epoch=212
06/15/2022 14:36:12 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.34605722886169393 on epoch=212
06/15/2022 14:36:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3439814814814815 -> 0.34605722886169393 on epoch=212, global_step=2550
06/15/2022 14:36:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=213
06/15/2022 14:36:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.41 on epoch=214
06/15/2022 14:36:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.39 on epoch=214
06/15/2022 14:36:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.35 on epoch=215
06/15/2022 14:36:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=216
06/15/2022 14:36:31 - INFO - __main__ - Global step 2600 Train loss 0.40 Classification-F1 0.17823541288108216 on epoch=216
06/15/2022 14:36:34 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.40 on epoch=217
06/15/2022 14:36:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
06/15/2022 14:36:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=219
06/15/2022 14:36:42 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
06/15/2022 14:36:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.41 on epoch=220
06/15/2022 14:36:50 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.19872393401805166 on epoch=220
06/15/2022 14:36:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.44 on epoch=221
06/15/2022 14:36:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.43 on epoch=222
06/15/2022 14:36:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.57 on epoch=223
06/15/2022 14:37:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=224
06/15/2022 14:37:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.38 on epoch=224
06/15/2022 14:37:09 - INFO - __main__ - Global step 2700 Train loss 0.45 Classification-F1 0.2313829787234043 on epoch=224
06/15/2022 14:37:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
06/15/2022 14:37:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
06/15/2022 14:37:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
06/15/2022 14:37:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.41 on epoch=228
06/15/2022 14:37:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=229
06/15/2022 14:37:28 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.24754599423969906 on epoch=229
06/15/2022 14:37:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.40 on epoch=229
06/15/2022 14:37:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=230
06/15/2022 14:37:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=231
06/15/2022 14:37:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.38 on epoch=232
06/15/2022 14:37:41 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=233
06/15/2022 14:37:47 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.29988178692841905 on epoch=233
06/15/2022 14:37:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=234
06/15/2022 14:37:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
06/15/2022 14:37:55 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.38 on epoch=235
06/15/2022 14:37:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
06/15/2022 14:38:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.40 on epoch=237
06/15/2022 14:38:06 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.2645927903871829 on epoch=237
06/15/2022 14:38:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=238
06/15/2022 14:38:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.38 on epoch=239
06/15/2022 14:38:14 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=239
06/15/2022 14:38:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=240
06/15/2022 14:38:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
06/15/2022 14:38:26 - INFO - __main__ - Global step 2900 Train loss 0.41 Classification-F1 0.278838794562065 on epoch=241
06/15/2022 14:38:28 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=242
06/15/2022 14:38:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
06/15/2022 14:38:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=244
06/15/2022 14:38:36 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.38 on epoch=244
06/15/2022 14:38:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.39 on epoch=245
06/15/2022 14:38:45 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.20476190476190478 on epoch=245
06/15/2022 14:38:47 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=246
06/15/2022 14:38:50 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.41 on epoch=247
06/15/2022 14:38:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.40 on epoch=248
06/15/2022 14:38:55 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=249
06/15/2022 14:38:58 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.36 on epoch=249
06/15/2022 14:38:59 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:38:59 - INFO - __main__ - Printing 3 examples
06/15/2022 14:38:59 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:38:59 - INFO - __main__ - ['contradiction']
06/15/2022 14:38:59 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:38:59 - INFO - __main__ - ['contradiction']
06/15/2022 14:38:59 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:38:59 - INFO - __main__ - ['contradiction']
06/15/2022 14:38:59 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:39:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:39:00 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:39:00 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:39:00 - INFO - __main__ - Printing 3 examples
06/15/2022 14:39:00 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:39:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:00 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:39:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:00 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:39:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:00 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:39:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:39:00 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:39:04 - INFO - __main__ - Global step 3000 Train loss 0.40 Classification-F1 0.2803587832252367 on epoch=249
06/15/2022 14:39:04 - INFO - __main__ - save last model!
06/15/2022 14:39:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 14:39:04 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 14:39:04 - INFO - __main__ - Printing 3 examples
06/15/2022 14:39:04 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 14:39:04 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:04 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 14:39:04 - INFO - __main__ - ['entailment']
06/15/2022 14:39:04 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 14:39:04 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:04 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:39:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:39:06 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 14:39:17 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:39:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:39:17 - INFO - __main__ - Starting training!
06/15/2022 14:39:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_13_0.5_8_predictions.txt
06/15/2022 14:39:36 - INFO - __main__ - Classification-F1 on test data: 0.2452
06/15/2022 14:39:37 - INFO - __main__ - prefix=anli_64_13, lr=0.5, bsz=8, dev_performance=0.34605722886169393, test_performance=0.24516348908752428
06/15/2022 14:39:37 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.4, bsz=8 ...
06/15/2022 14:39:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:39:38 - INFO - __main__ - Printing 3 examples
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:39:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:39:38 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:39:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:39:38 - INFO - __main__ - Printing 3 examples
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:39:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:39:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:39:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:39:38 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:39:57 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:39:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:39:58 - INFO - __main__ - Starting training!
06/15/2022 14:40:01 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=0
06/15/2022 14:40:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=1
06/15/2022 14:40:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=2
06/15/2022 14:40:09 - INFO - __main__ - Step 40 Global step 40 Train loss 0.58 on epoch=3
06/15/2022 14:40:12 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=4
06/15/2022 14:40:17 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 14:40:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 14:40:20 - INFO - __main__ - Step 60 Global step 60 Train loss 0.52 on epoch=4
06/15/2022 14:40:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.53 on epoch=5
06/15/2022 14:40:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
06/15/2022 14:40:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=7
06/15/2022 14:40:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=8
06/15/2022 14:40:37 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 14:40:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=9
06/15/2022 14:40:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.53 on epoch=9
06/15/2022 14:40:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.47 on epoch=10
06/15/2022 14:40:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.51 on epoch=11
06/15/2022 14:40:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.46 on epoch=12
06/15/2022 14:40:55 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 14:40:58 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=13
06/15/2022 14:41:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=14
06/15/2022 14:41:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=14
06/15/2022 14:41:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=15
06/15/2022 14:41:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.50 on epoch=16
06/15/2022 14:41:14 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 14:41:17 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=17
06/15/2022 14:41:20 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=18
06/15/2022 14:41:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
06/15/2022 14:41:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=19
06/15/2022 14:41:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
06/15/2022 14:41:33 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 14:41:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
06/15/2022 14:41:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.45 on epoch=22
06/15/2022 14:41:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=23
06/15/2022 14:41:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.49 on epoch=24
06/15/2022 14:41:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
06/15/2022 14:41:52 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 14:41:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=25
06/15/2022 14:41:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=26
06/15/2022 14:42:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.40 on epoch=27
06/15/2022 14:42:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
06/15/2022 14:42:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=29
06/15/2022 14:42:12 - INFO - __main__ - Global step 350 Train loss 0.44 Classification-F1 0.2588262378109944 on epoch=29
06/15/2022 14:42:12 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2588262378109944 on epoch=29, global_step=350
06/15/2022 14:42:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.48 on epoch=29
06/15/2022 14:42:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=30
06/15/2022 14:42:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=31
06/15/2022 14:42:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
06/15/2022 14:42:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
06/15/2022 14:42:29 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 14:42:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=34
06/15/2022 14:42:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
06/15/2022 14:42:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=35
06/15/2022 14:42:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
06/15/2022 14:42:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=37
06/15/2022 14:42:48 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 14:42:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=38
06/15/2022 14:42:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=39
06/15/2022 14:42:56 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
06/15/2022 14:42:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
06/15/2022 14:43:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.46 on epoch=41
06/15/2022 14:43:07 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 14:43:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=42
06/15/2022 14:43:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.44 on epoch=43
06/15/2022 14:43:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=44
06/15/2022 14:43:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
06/15/2022 14:43:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
06/15/2022 14:43:26 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 14:43:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
06/15/2022 14:43:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
06/15/2022 14:43:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=48
06/15/2022 14:43:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
06/15/2022 14:43:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.48 on epoch=49
06/15/2022 14:43:45 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.18627450980392157 on epoch=49
06/15/2022 14:43:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
06/15/2022 14:43:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=51
06/15/2022 14:43:53 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
06/15/2022 14:43:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=53
06/15/2022 14:43:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=54
06/15/2022 14:44:04 - INFO - __main__ - Global step 650 Train loss 0.44 Classification-F1 0.2137017000192587 on epoch=54
06/15/2022 14:44:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=54
06/15/2022 14:44:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=55
06/15/2022 14:44:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
06/15/2022 14:44:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=57
06/15/2022 14:44:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.48 on epoch=58
06/15/2022 14:44:23 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.27612233666168723 on epoch=58
06/15/2022 14:44:23 - INFO - __main__ - Saving model with best Classification-F1: 0.2588262378109944 -> 0.27612233666168723 on epoch=58, global_step=700
06/15/2022 14:44:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=59
06/15/2022 14:44:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
06/15/2022 14:44:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
06/15/2022 14:44:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
06/15/2022 14:44:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
06/15/2022 14:44:42 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 14:44:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.43 on epoch=63
06/15/2022 14:44:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
06/15/2022 14:44:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=64
06/15/2022 14:44:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.41 on epoch=65
06/15/2022 14:44:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=66
06/15/2022 14:45:01 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.19743774610393214 on epoch=66
06/15/2022 14:45:04 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=67
06/15/2022 14:45:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=68
06/15/2022 14:45:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
06/15/2022 14:45:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=69
06/15/2022 14:45:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=70
06/15/2022 14:45:20 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 14:45:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.47 on epoch=71
06/15/2022 14:45:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
06/15/2022 14:45:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=73
06/15/2022 14:45:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=74
06/15/2022 14:45:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.43 on epoch=74
06/15/2022 14:45:39 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.22622779519331246 on epoch=74
06/15/2022 14:45:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=75
06/15/2022 14:45:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
06/15/2022 14:45:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=77
06/15/2022 14:45:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
06/15/2022 14:45:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.50 on epoch=79
06/15/2022 14:45:58 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.30141066039115577 on epoch=79
06/15/2022 14:45:58 - INFO - __main__ - Saving model with best Classification-F1: 0.27612233666168723 -> 0.30141066039115577 on epoch=79, global_step=950
06/15/2022 14:46:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=79
06/15/2022 14:46:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
06/15/2022 14:46:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
06/15/2022 14:46:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=82
06/15/2022 14:46:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=83
06/15/2022 14:46:18 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.26142829630407755 on epoch=83
06/15/2022 14:46:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=84
06/15/2022 14:46:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.46 on epoch=84
06/15/2022 14:46:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=85
06/15/2022 14:46:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.44 on epoch=86
06/15/2022 14:46:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=87
06/15/2022 14:46:37 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.2551095958813299 on epoch=87
06/15/2022 14:46:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=88
06/15/2022 14:46:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
06/15/2022 14:46:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
06/15/2022 14:46:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=90
06/15/2022 14:46:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=91
06/15/2022 14:46:56 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 14:46:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=92
06/15/2022 14:47:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
06/15/2022 14:47:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=94
06/15/2022 14:47:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
06/15/2022 14:47:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=95
06/15/2022 14:47:15 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 14:47:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
06/15/2022 14:47:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
06/15/2022 14:47:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=98
06/15/2022 14:47:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
06/15/2022 14:47:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=99
06/15/2022 14:47:34 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.26234897395489126 on epoch=99
06/15/2022 14:47:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=100
06/15/2022 14:47:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=101
06/15/2022 14:47:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
06/15/2022 14:47:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=103
06/15/2022 14:47:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
06/15/2022 14:47:54 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.23785290808014647 on epoch=104
06/15/2022 14:47:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=104
06/15/2022 14:47:59 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=105
06/15/2022 14:48:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=106
06/15/2022 14:48:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
06/15/2022 14:48:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 14:48:13 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.2404926354331364 on epoch=108
06/15/2022 14:48:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=109
06/15/2022 14:48:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
06/15/2022 14:48:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=110
06/15/2022 14:48:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=111
06/15/2022 14:48:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.41 on epoch=112
06/15/2022 14:48:32 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.26357249531982924 on epoch=112
06/15/2022 14:48:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
06/15/2022 14:48:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
06/15/2022 14:48:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
06/15/2022 14:48:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.40 on epoch=115
06/15/2022 14:48:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
06/15/2022 14:48:51 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.1775766716943188 on epoch=116
06/15/2022 14:48:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.39 on epoch=117
06/15/2022 14:48:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=118
06/15/2022 14:48:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=119
06/15/2022 14:49:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=119
06/15/2022 14:49:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=120
06/15/2022 14:49:10 - INFO - __main__ - Global step 1450 Train loss 0.40 Classification-F1 0.2875372042162038 on epoch=120
06/15/2022 14:49:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=121
06/15/2022 14:49:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
06/15/2022 14:49:18 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=123
06/15/2022 14:49:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=124
06/15/2022 14:49:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
06/15/2022 14:49:29 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.16337285902503296 on epoch=124
06/15/2022 14:49:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=125
06/15/2022 14:49:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
06/15/2022 14:49:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.42 on epoch=127
06/15/2022 14:49:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=128
06/15/2022 14:49:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=129
06/15/2022 14:49:49 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 14:49:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
06/15/2022 14:49:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=130
06/15/2022 14:49:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
06/15/2022 14:49:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=132
06/15/2022 14:50:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
06/15/2022 14:50:08 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.18477666534683335 on epoch=133
06/15/2022 14:50:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=134
06/15/2022 14:50:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=134
06/15/2022 14:50:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.42 on epoch=135
06/15/2022 14:50:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
06/15/2022 14:50:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=137
06/15/2022 14:50:27 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.2613636363636364 on epoch=137
06/15/2022 14:50:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=138
06/15/2022 14:50:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
06/15/2022 14:50:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.45 on epoch=139
06/15/2022 14:50:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=140
06/15/2022 14:50:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=141
06/15/2022 14:50:47 - INFO - __main__ - Global step 1700 Train loss 0.42 Classification-F1 0.21007147236655435 on epoch=141
06/15/2022 14:50:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
06/15/2022 14:50:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=143
06/15/2022 14:50:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=144
06/15/2022 14:50:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
06/15/2022 14:51:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=145
06/15/2022 14:51:06 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.231276150627615 on epoch=145
06/15/2022 14:51:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=146
06/15/2022 14:51:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
06/15/2022 14:51:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.40 on epoch=148
06/15/2022 14:51:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
06/15/2022 14:51:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.42 on epoch=149
06/15/2022 14:51:25 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.2592836353531365 on epoch=149
06/15/2022 14:51:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.34 on epoch=150
06/15/2022 14:51:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
06/15/2022 14:51:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=152
06/15/2022 14:51:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
06/15/2022 14:51:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=154
06/15/2022 14:51:44 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.16233120113717125 on epoch=154
06/15/2022 14:51:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.37 on epoch=154
06/15/2022 14:51:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=155
06/15/2022 14:51:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
06/15/2022 14:51:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.42 on epoch=157
06/15/2022 14:51:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=158
06/15/2022 14:52:04 - INFO - __main__ - Global step 1900 Train loss 0.39 Classification-F1 0.375854097593228 on epoch=158
06/15/2022 14:52:04 - INFO - __main__ - Saving model with best Classification-F1: 0.30141066039115577 -> 0.375854097593228 on epoch=158, global_step=1900
06/15/2022 14:52:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=159
06/15/2022 14:52:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=159
06/15/2022 14:52:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=160
06/15/2022 14:52:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
06/15/2022 14:52:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.44 on epoch=162
06/15/2022 14:52:23 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.21185642037745156 on epoch=162
06/15/2022 14:52:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
06/15/2022 14:52:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=164
06/15/2022 14:52:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=164
06/15/2022 14:52:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 14:52:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=166
06/15/2022 14:52:42 - INFO - __main__ - Global step 2000 Train loss 0.42 Classification-F1 0.16732026143790854 on epoch=166
06/15/2022 14:52:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
06/15/2022 14:52:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=168
06/15/2022 14:52:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.39 on epoch=169
06/15/2022 14:52:52 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.38 on epoch=169
06/15/2022 14:52:55 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
06/15/2022 14:53:01 - INFO - __main__ - Global step 2050 Train loss 0.40 Classification-F1 0.3046494880595741 on epoch=170
06/15/2022 14:53:03 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.39 on epoch=171
06/15/2022 14:53:06 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.40 on epoch=172
06/15/2022 14:53:09 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
06/15/2022 14:53:11 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.40 on epoch=174
06/15/2022 14:53:14 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
06/15/2022 14:53:19 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.2864510323105412 on epoch=174
06/15/2022 14:53:22 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.43 on epoch=175
06/15/2022 14:53:25 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=176
06/15/2022 14:53:27 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.43 on epoch=177
06/15/2022 14:53:30 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
06/15/2022 14:53:33 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=179
06/15/2022 14:53:39 - INFO - __main__ - Global step 2150 Train loss 0.40 Classification-F1 0.3123309178743961 on epoch=179
06/15/2022 14:53:41 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.40 on epoch=179
06/15/2022 14:53:44 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
06/15/2022 14:53:47 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
06/15/2022 14:53:49 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=182
06/15/2022 14:53:52 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.45 on epoch=183
06/15/2022 14:53:58 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.29044357546312755 on epoch=183
06/15/2022 14:54:00 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.44 on epoch=184
06/15/2022 14:54:03 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
06/15/2022 14:54:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.40 on epoch=185
06/15/2022 14:54:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.39 on epoch=186
06/15/2022 14:54:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=187
06/15/2022 14:54:17 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.30227272727272725 on epoch=187
06/15/2022 14:54:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
06/15/2022 14:54:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=189
06/15/2022 14:54:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.39 on epoch=189
06/15/2022 14:54:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.37 on epoch=190
06/15/2022 14:54:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=191
06/15/2022 14:54:36 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.23067322060362852 on epoch=191
06/15/2022 14:54:39 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=192
06/15/2022 14:54:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=193
06/15/2022 14:54:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=194
06/15/2022 14:54:47 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.39 on epoch=194
06/15/2022 14:54:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=195
06/15/2022 14:54:55 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.3594442837863891 on epoch=195
06/15/2022 14:54:58 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=196
06/15/2022 14:55:01 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
06/15/2022 14:55:03 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.40 on epoch=198
06/15/2022 14:55:06 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.43 on epoch=199
06/15/2022 14:55:09 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=199
06/15/2022 14:55:14 - INFO - __main__ - Global step 2400 Train loss 0.39 Classification-F1 0.24237610311216096 on epoch=199
06/15/2022 14:55:17 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=200
06/15/2022 14:55:20 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.39 on epoch=201
06/15/2022 14:55:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=202
06/15/2022 14:55:25 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
06/15/2022 14:55:28 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=204
06/15/2022 14:55:33 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.26362586362586365 on epoch=204
06/15/2022 14:55:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.35 on epoch=204
06/15/2022 14:55:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.39 on epoch=205
06/15/2022 14:55:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.36 on epoch=206
06/15/2022 14:55:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.35 on epoch=207
06/15/2022 14:55:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.36 on epoch=208
06/15/2022 14:55:52 - INFO - __main__ - Global step 2500 Train loss 0.36 Classification-F1 0.3256075782281233 on epoch=208
06/15/2022 14:55:55 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.37 on epoch=209
06/15/2022 14:55:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.40 on epoch=209
06/15/2022 14:56:00 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.36 on epoch=210
06/15/2022 14:56:03 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.39 on epoch=211
06/15/2022 14:56:06 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.36 on epoch=212
06/15/2022 14:56:11 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.3268028188286809 on epoch=212
06/15/2022 14:56:14 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.37 on epoch=213
06/15/2022 14:56:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.34 on epoch=214
06/15/2022 14:56:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.34 on epoch=214
06/15/2022 14:56:22 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.37 on epoch=215
06/15/2022 14:56:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.38 on epoch=216
06/15/2022 14:56:31 - INFO - __main__ - Global step 2600 Train loss 0.36 Classification-F1 0.3098464317976513 on epoch=216
06/15/2022 14:56:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=217
06/15/2022 14:56:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.34 on epoch=218
06/15/2022 14:56:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=219
06/15/2022 14:56:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.35 on epoch=219
06/15/2022 14:56:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.37 on epoch=220
06/15/2022 14:56:50 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.3257541743235664 on epoch=220
06/15/2022 14:56:53 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.36 on epoch=221
06/15/2022 14:56:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=222
06/15/2022 14:56:58 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.31 on epoch=223
06/15/2022 14:57:01 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.30 on epoch=224
06/15/2022 14:57:03 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.35 on epoch=224
06/15/2022 14:57:09 - INFO - __main__ - Global step 2700 Train loss 0.33 Classification-F1 0.3685984219155247 on epoch=224
06/15/2022 14:57:12 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
06/15/2022 14:57:14 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=226
06/15/2022 14:57:17 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.33 on epoch=227
06/15/2022 14:57:20 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.33 on epoch=228
06/15/2022 14:57:22 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.31 on epoch=229
06/15/2022 14:57:28 - INFO - __main__ - Global step 2750 Train loss 0.35 Classification-F1 0.3103050157806795 on epoch=229
06/15/2022 14:57:31 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.34 on epoch=229
06/15/2022 14:57:34 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=230
06/15/2022 14:57:36 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=231
06/15/2022 14:57:39 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.35 on epoch=232
06/15/2022 14:57:42 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.37 on epoch=233
06/15/2022 14:57:47 - INFO - __main__ - Global step 2800 Train loss 0.36 Classification-F1 0.3941941310046731 on epoch=233
06/15/2022 14:57:48 - INFO - __main__ - Saving model with best Classification-F1: 0.375854097593228 -> 0.3941941310046731 on epoch=233, global_step=2800
06/15/2022 14:57:50 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.37 on epoch=234
06/15/2022 14:57:53 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.33 on epoch=234
06/15/2022 14:57:56 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.36 on epoch=235
06/15/2022 14:57:58 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.36 on epoch=236
06/15/2022 14:58:01 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.41 on epoch=237
06/15/2022 14:58:07 - INFO - __main__ - Global step 2850 Train loss 0.36 Classification-F1 0.3859108591031857 on epoch=237
06/15/2022 14:58:09 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.35 on epoch=238
06/15/2022 14:58:12 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.32 on epoch=239
06/15/2022 14:58:15 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.31 on epoch=239
06/15/2022 14:58:17 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.33 on epoch=240
06/15/2022 14:58:20 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.32 on epoch=241
06/15/2022 14:58:26 - INFO - __main__ - Global step 2900 Train loss 0.32 Classification-F1 0.3209132870787006 on epoch=241
06/15/2022 14:58:29 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=242
06/15/2022 14:58:31 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.37 on epoch=243
06/15/2022 14:58:34 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.30 on epoch=244
06/15/2022 14:58:37 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.29 on epoch=244
06/15/2022 14:58:39 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.35 on epoch=245
06/15/2022 14:58:45 - INFO - __main__ - Global step 2950 Train loss 0.34 Classification-F1 0.3846716998398536 on epoch=245
06/15/2022 14:58:48 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.34 on epoch=246
06/15/2022 14:58:51 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.32 on epoch=247
06/15/2022 14:58:53 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.38 on epoch=248
06/15/2022 14:58:56 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.32 on epoch=249
06/15/2022 14:58:59 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.31 on epoch=249
06/15/2022 14:59:00 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:59:00 - INFO - __main__ - Printing 3 examples
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:59:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:59:00 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:59:00 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:59:00 - INFO - __main__ - Printing 3 examples
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:59:00 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:00 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:59:01 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:59:01 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:59:05 - INFO - __main__ - Global step 3000 Train loss 0.33 Classification-F1 0.34078399883203153 on epoch=249
06/15/2022 14:59:05 - INFO - __main__ - save last model!
06/15/2022 14:59:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 14:59:05 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 14:59:05 - INFO - __main__ - Printing 3 examples
06/15/2022 14:59:05 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 14:59:05 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:05 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 14:59:05 - INFO - __main__ - ['entailment']
06/15/2022 14:59:05 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 14:59:05 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:05 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:59:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:59:06 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 14:59:17 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:59:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:59:18 - INFO - __main__ - Starting training!
06/15/2022 14:59:37 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_13_0.4_8_predictions.txt
06/15/2022 14:59:37 - INFO - __main__ - Classification-F1 on test data: 0.2767
06/15/2022 14:59:37 - INFO - __main__ - prefix=anli_64_13, lr=0.4, bsz=8, dev_performance=0.3941941310046731, test_performance=0.276663050180367
06/15/2022 14:59:37 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.3, bsz=8 ...
06/15/2022 14:59:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:59:38 - INFO - __main__ - Printing 3 examples
06/15/2022 14:59:38 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 14:59:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:38 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 14:59:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:38 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 14:59:38 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:59:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:59:39 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 14:59:39 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 14:59:39 - INFO - __main__ - Printing 3 examples
06/15/2022 14:59:39 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 14:59:39 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:39 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 14:59:39 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:39 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 14:59:39 - INFO - __main__ - ['contradiction']
06/15/2022 14:59:39 - INFO - __main__ - Tokenizing Input ...
06/15/2022 14:59:39 - INFO - __main__ - Tokenizing Output ...
06/15/2022 14:59:39 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 14:59:57 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 14:59:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 14:59:58 - INFO - __main__ - Starting training!
06/15/2022 15:00:02 - INFO - __main__ - Step 10 Global step 10 Train loss 1.02 on epoch=0
06/15/2022 15:00:04 - INFO - __main__ - Step 20 Global step 20 Train loss 0.69 on epoch=1
06/15/2022 15:00:07 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=2
06/15/2022 15:00:10 - INFO - __main__ - Step 40 Global step 40 Train loss 0.55 on epoch=3
06/15/2022 15:00:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=4
06/15/2022 15:00:17 - INFO - __main__ - Global step 50 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 15:00:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 15:00:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=4
06/15/2022 15:00:22 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=5
06/15/2022 15:00:25 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=6
06/15/2022 15:00:27 - INFO - __main__ - Step 90 Global step 90 Train loss 0.52 on epoch=7
06/15/2022 15:00:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.58 on epoch=8
06/15/2022 15:00:36 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 15:00:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=9
06/15/2022 15:00:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=9
06/15/2022 15:00:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.46 on epoch=10
06/15/2022 15:00:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=11
06/15/2022 15:00:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.47 on epoch=12
06/15/2022 15:00:54 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 15:00:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=13
06/15/2022 15:00:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.52 on epoch=14
06/15/2022 15:01:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.51 on epoch=14
06/15/2022 15:01:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
06/15/2022 15:01:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
06/15/2022 15:01:13 - INFO - __main__ - Global step 200 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 15:01:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.42 on epoch=17
06/15/2022 15:01:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.45 on epoch=18
06/15/2022 15:01:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
06/15/2022 15:01:23 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
06/15/2022 15:01:26 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
06/15/2022 15:01:31 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 15:01:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
06/15/2022 15:01:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=22
06/15/2022 15:01:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=23
06/15/2022 15:01:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=24
06/15/2022 15:01:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
06/15/2022 15:01:50 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 15:01:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.48 on epoch=25
06/15/2022 15:01:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
06/15/2022 15:01:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=27
06/15/2022 15:02:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
06/15/2022 15:02:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
06/15/2022 15:02:08 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.1647058823529412 on epoch=29
06/15/2022 15:02:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
06/15/2022 15:02:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=30
06/15/2022 15:02:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.48 on epoch=31
06/15/2022 15:02:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
06/15/2022 15:02:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=33
06/15/2022 15:02:27 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.27268376068376066 on epoch=33
06/15/2022 15:02:27 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27268376068376066 on epoch=33, global_step=400
06/15/2022 15:02:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=34
06/15/2022 15:02:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=34
06/15/2022 15:02:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=35
06/15/2022 15:02:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=36
06/15/2022 15:02:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=37
06/15/2022 15:02:45 - INFO - __main__ - Global step 450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 15:02:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=38
06/15/2022 15:02:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.52 on epoch=39
06/15/2022 15:02:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.47 on epoch=39
06/15/2022 15:02:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
06/15/2022 15:02:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=41
06/15/2022 15:03:04 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 15:03:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
06/15/2022 15:03:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
06/15/2022 15:03:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
06/15/2022 15:03:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
06/15/2022 15:03:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
06/15/2022 15:03:22 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 15:03:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
06/15/2022 15:03:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
06/15/2022 15:03:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 15:03:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=49
06/15/2022 15:03:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
06/15/2022 15:03:41 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.21930058967096003 on epoch=49
06/15/2022 15:03:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
06/15/2022 15:03:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
06/15/2022 15:03:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=52
06/15/2022 15:03:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=53
06/15/2022 15:03:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.47 on epoch=54
06/15/2022 15:04:00 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.20069948883508207 on epoch=54
06/15/2022 15:04:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.47 on epoch=54
06/15/2022 15:04:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=55
06/15/2022 15:04:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
06/15/2022 15:04:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.38 on epoch=57
06/15/2022 15:04:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=58
06/15/2022 15:04:18 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16399092970521542 on epoch=58
06/15/2022 15:04:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
06/15/2022 15:04:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 15:04:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=60
06/15/2022 15:04:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
06/15/2022 15:04:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=62
06/15/2022 15:04:37 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 15:04:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
06/15/2022 15:04:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
06/15/2022 15:04:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
06/15/2022 15:04:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
06/15/2022 15:04:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=66
06/15/2022 15:04:56 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.17382508558979146 on epoch=66
06/15/2022 15:04:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.40 on epoch=67
06/15/2022 15:05:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
06/15/2022 15:05:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.45 on epoch=69
06/15/2022 15:05:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.42 on epoch=69
06/15/2022 15:05:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=70
06/15/2022 15:05:14 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 15:05:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=71
06/15/2022 15:05:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=72
06/15/2022 15:05:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
06/15/2022 15:05:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.43 on epoch=74
06/15/2022 15:05:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.38 on epoch=74
06/15/2022 15:05:33 - INFO - __main__ - Global step 900 Train loss 0.44 Classification-F1 0.19872393401805166 on epoch=74
06/15/2022 15:05:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=75
06/15/2022 15:05:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=76
06/15/2022 15:05:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=77
06/15/2022 15:05:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=78
06/15/2022 15:05:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
06/15/2022 15:05:52 - INFO - __main__ - Global step 950 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 15:05:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=79
06/15/2022 15:05:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.41 on epoch=80
06/15/2022 15:05:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
06/15/2022 15:06:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.42 on epoch=82
06/15/2022 15:06:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
06/15/2022 15:06:10 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 15:06:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
06/15/2022 15:06:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=84
06/15/2022 15:06:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=85
06/15/2022 15:06:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
06/15/2022 15:06:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.45 on epoch=87
06/15/2022 15:06:29 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.18627450980392157 on epoch=87
06/15/2022 15:06:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=88
06/15/2022 15:06:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=89
06/15/2022 15:06:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.41 on epoch=89
06/15/2022 15:06:39 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.44 on epoch=90
06/15/2022 15:06:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=91
06/15/2022 15:06:47 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 15:06:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=92
06/15/2022 15:06:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=93
06/15/2022 15:06:55 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
06/15/2022 15:06:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
06/15/2022 15:07:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=95
06/15/2022 15:07:06 - INFO - __main__ - Global step 1150 Train loss 0.41 Classification-F1 0.19607843137254902 on epoch=95
06/15/2022 15:07:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
06/15/2022 15:07:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=97
06/15/2022 15:07:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
06/15/2022 15:07:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.41 on epoch=99
06/15/2022 15:07:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=99
06/15/2022 15:07:24 - INFO - __main__ - Global step 1200 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 15:07:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
06/15/2022 15:07:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
06/15/2022 15:07:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=102
06/15/2022 15:07:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.45 on epoch=103
06/15/2022 15:07:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=104
06/15/2022 15:07:43 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.34521806061080446 on epoch=104
06/15/2022 15:07:43 - INFO - __main__ - Saving model with best Classification-F1: 0.27268376068376066 -> 0.34521806061080446 on epoch=104, global_step=1250
06/15/2022 15:07:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.41 on epoch=104
06/15/2022 15:07:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=105
06/15/2022 15:07:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
06/15/2022 15:07:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=107
06/15/2022 15:07:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
06/15/2022 15:08:02 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.16535433070866143 on epoch=108
06/15/2022 15:08:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=109
06/15/2022 15:08:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=109
06/15/2022 15:08:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=110
06/15/2022 15:08:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.44 on epoch=111
06/15/2022 15:08:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=112
06/15/2022 15:08:20 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.2428037793891452 on epoch=112
06/15/2022 15:08:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.41 on epoch=113
06/15/2022 15:08:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=114
06/15/2022 15:08:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=114
06/15/2022 15:08:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=115
06/15/2022 15:08:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
06/15/2022 15:08:39 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.23491687632578562 on epoch=116
06/15/2022 15:08:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=117
06/15/2022 15:08:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=118
06/15/2022 15:08:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
06/15/2022 15:08:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=119
06/15/2022 15:08:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
06/15/2022 15:08:57 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=120
06/15/2022 15:09:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
06/15/2022 15:09:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
06/15/2022 15:09:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=123
06/15/2022 15:09:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=124
06/15/2022 15:09:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=124
06/15/2022 15:09:16 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.3324979114452799 on epoch=124
06/15/2022 15:09:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=125
06/15/2022 15:09:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=126
06/15/2022 15:09:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=127
06/15/2022 15:09:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=128
06/15/2022 15:09:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
06/15/2022 15:09:35 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.20833333333333334 on epoch=129
06/15/2022 15:09:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.36 on epoch=129
06/15/2022 15:09:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.41 on epoch=130
06/15/2022 15:09:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
06/15/2022 15:09:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=132
06/15/2022 15:09:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
06/15/2022 15:09:53 - INFO - __main__ - Global step 1600 Train loss 0.40 Classification-F1 0.22953761214630777 on epoch=133
06/15/2022 15:09:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=134
06/15/2022 15:09:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 15:10:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.40 on epoch=135
06/15/2022 15:10:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.42 on epoch=136
06/15/2022 15:10:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
06/15/2022 15:10:12 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.3668842905781587 on epoch=137
06/15/2022 15:10:12 - INFO - __main__ - Saving model with best Classification-F1: 0.34521806061080446 -> 0.3668842905781587 on epoch=137, global_step=1650
06/15/2022 15:10:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=138
06/15/2022 15:10:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=139
06/15/2022 15:10:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.39 on epoch=139
06/15/2022 15:10:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=140
06/15/2022 15:10:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=141
06/15/2022 15:10:31 - INFO - __main__ - Global step 1700 Train loss 0.39 Classification-F1 0.3130817469052763 on epoch=141
06/15/2022 15:10:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=142
06/15/2022 15:10:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
06/15/2022 15:10:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=144
06/15/2022 15:10:41 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.33 on epoch=144
06/15/2022 15:10:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.34 on epoch=145
06/15/2022 15:10:50 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.2903288205723398 on epoch=145
06/15/2022 15:10:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
06/15/2022 15:10:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=147
06/15/2022 15:10:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.36 on epoch=148
06/15/2022 15:11:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
06/15/2022 15:11:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.38 on epoch=149
06/15/2022 15:11:08 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3593599383073068 on epoch=149
06/15/2022 15:11:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=150
06/15/2022 15:11:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=151
06/15/2022 15:11:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=152
06/15/2022 15:11:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=153
06/15/2022 15:11:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=154
06/15/2022 15:11:27 - INFO - __main__ - Global step 1850 Train loss 0.37 Classification-F1 0.32288175423186866 on epoch=154
06/15/2022 15:11:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.36 on epoch=154
06/15/2022 15:11:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=155
06/15/2022 15:11:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=156
06/15/2022 15:11:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=157
06/15/2022 15:11:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=158
06/15/2022 15:11:46 - INFO - __main__ - Global step 1900 Train loss 0.36 Classification-F1 0.40439613964722043 on epoch=158
06/15/2022 15:11:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3668842905781587 -> 0.40439613964722043 on epoch=158, global_step=1900
06/15/2022 15:11:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=159
06/15/2022 15:11:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=159
06/15/2022 15:11:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.37 on epoch=160
06/15/2022 15:11:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=161
06/15/2022 15:11:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=162
06/15/2022 15:12:04 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.25558562963881254 on epoch=162
06/15/2022 15:12:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=163
06/15/2022 15:12:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=164
06/15/2022 15:12:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=164
06/15/2022 15:12:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=165
06/15/2022 15:12:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=166
06/15/2022 15:12:23 - INFO - __main__ - Global step 2000 Train loss 0.38 Classification-F1 0.32317016317016317 on epoch=166
06/15/2022 15:12:26 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.34 on epoch=167
06/15/2022 15:12:28 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.35 on epoch=168
06/15/2022 15:12:31 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.36 on epoch=169
06/15/2022 15:12:34 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.36 on epoch=169
06/15/2022 15:12:36 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.31 on epoch=170
06/15/2022 15:12:42 - INFO - __main__ - Global step 2050 Train loss 0.34 Classification-F1 0.359855604853458 on epoch=170
06/15/2022 15:12:44 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.35 on epoch=171
06/15/2022 15:12:47 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.32 on epoch=172
06/15/2022 15:12:50 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.34 on epoch=173
06/15/2022 15:12:52 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.32 on epoch=174
06/15/2022 15:12:55 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.34 on epoch=174
06/15/2022 15:13:01 - INFO - __main__ - Global step 2100 Train loss 0.33 Classification-F1 0.27802822063404803 on epoch=174
06/15/2022 15:13:03 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.35 on epoch=175
06/15/2022 15:13:06 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.30 on epoch=176
06/15/2022 15:13:08 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.29 on epoch=177
06/15/2022 15:13:11 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.32 on epoch=178
06/15/2022 15:13:14 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.35 on epoch=179
06/15/2022 15:13:19 - INFO - __main__ - Global step 2150 Train loss 0.32 Classification-F1 0.3885720555208494 on epoch=179
06/15/2022 15:13:22 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.27 on epoch=179
06/15/2022 15:13:25 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.31 on epoch=180
06/15/2022 15:13:27 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.32 on epoch=181
06/15/2022 15:13:30 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.34 on epoch=182
06/15/2022 15:13:32 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.35 on epoch=183
06/15/2022 15:13:38 - INFO - __main__ - Global step 2200 Train loss 0.32 Classification-F1 0.26489828294139606 on epoch=183
06/15/2022 15:13:41 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.32 on epoch=184
06/15/2022 15:13:43 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.26 on epoch=184
06/15/2022 15:13:46 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.27 on epoch=185
06/15/2022 15:13:49 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.29 on epoch=186
06/15/2022 15:13:51 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.29 on epoch=187
06/15/2022 15:13:57 - INFO - __main__ - Global step 2250 Train loss 0.29 Classification-F1 0.26848431918390614 on epoch=187
06/15/2022 15:13:59 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.36 on epoch=188
06/15/2022 15:14:02 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.29 on epoch=189
06/15/2022 15:14:05 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.36 on epoch=189
06/15/2022 15:14:07 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.30 on epoch=190
06/15/2022 15:14:10 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.31 on epoch=191
06/15/2022 15:14:15 - INFO - __main__ - Global step 2300 Train loss 0.33 Classification-F1 0.3085633839764612 on epoch=191
06/15/2022 15:14:18 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.28 on epoch=192
06/15/2022 15:14:21 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.36 on epoch=193
06/15/2022 15:14:23 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.32 on epoch=194
06/15/2022 15:14:26 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.30 on epoch=194
06/15/2022 15:14:28 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.33 on epoch=195
06/15/2022 15:14:34 - INFO - __main__ - Global step 2350 Train loss 0.32 Classification-F1 0.15945171136752356 on epoch=195
06/15/2022 15:14:37 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.33 on epoch=196
06/15/2022 15:14:39 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.29 on epoch=197
06/15/2022 15:14:42 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.31 on epoch=198
06/15/2022 15:14:45 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=199
06/15/2022 15:14:47 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.29 on epoch=199
06/15/2022 15:14:53 - INFO - __main__ - Global step 2400 Train loss 0.32 Classification-F1 0.34036017648343053 on epoch=199
06/15/2022 15:14:55 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.24 on epoch=200
06/15/2022 15:14:58 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.27 on epoch=201
06/15/2022 15:15:01 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.31 on epoch=202
06/15/2022 15:15:03 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.33 on epoch=203
06/15/2022 15:15:06 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.29 on epoch=204
06/15/2022 15:15:12 - INFO - __main__ - Global step 2450 Train loss 0.29 Classification-F1 0.19157938971797175 on epoch=204
06/15/2022 15:15:14 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.25 on epoch=204
06/15/2022 15:15:17 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.28 on epoch=205
06/15/2022 15:15:19 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.29 on epoch=206
06/15/2022 15:15:22 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.27 on epoch=207
06/15/2022 15:15:25 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.29 on epoch=208
06/15/2022 15:15:30 - INFO - __main__ - Global step 2500 Train loss 0.27 Classification-F1 0.31881238711951015 on epoch=208
06/15/2022 15:15:33 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.31 on epoch=209
06/15/2022 15:15:36 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.29 on epoch=209
06/15/2022 15:15:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.24 on epoch=210
06/15/2022 15:15:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.25 on epoch=211
06/15/2022 15:15:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.27 on epoch=212
06/15/2022 15:15:49 - INFO - __main__ - Global step 2550 Train loss 0.27 Classification-F1 0.08885857987654394 on epoch=212
06/15/2022 15:15:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.33 on epoch=213
06/15/2022 15:15:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.27 on epoch=214
06/15/2022 15:15:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.26 on epoch=214
06/15/2022 15:15:59 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.31 on epoch=215
06/15/2022 15:16:02 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.25 on epoch=216
06/15/2022 15:16:08 - INFO - __main__ - Global step 2600 Train loss 0.28 Classification-F1 0.35717860439325183 on epoch=216
06/15/2022 15:16:10 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=217
06/15/2022 15:16:13 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.26 on epoch=218
06/15/2022 15:16:16 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.26 on epoch=219
06/15/2022 15:16:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.24 on epoch=219
06/15/2022 15:16:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.29 on epoch=220
06/15/2022 15:16:26 - INFO - __main__ - Global step 2650 Train loss 0.27 Classification-F1 0.33007964257964256 on epoch=220
06/15/2022 15:16:29 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.26 on epoch=221
06/15/2022 15:16:32 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.20 on epoch=222
06/15/2022 15:16:34 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.26 on epoch=223
06/15/2022 15:16:37 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.29 on epoch=224
06/15/2022 15:16:39 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.24 on epoch=224
06/15/2022 15:16:45 - INFO - __main__ - Global step 2700 Train loss 0.25 Classification-F1 0.17695883793263187 on epoch=224
06/15/2022 15:16:48 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=225
06/15/2022 15:16:50 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.21 on epoch=226
06/15/2022 15:16:53 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.27 on epoch=227
06/15/2022 15:16:56 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.26 on epoch=228
06/15/2022 15:16:58 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.25 on epoch=229
06/15/2022 15:17:04 - INFO - __main__ - Global step 2750 Train loss 0.25 Classification-F1 0.3127362189288795 on epoch=229
06/15/2022 15:17:06 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.26 on epoch=229
06/15/2022 15:17:09 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.30 on epoch=230
06/15/2022 15:17:12 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.24 on epoch=231
06/15/2022 15:17:14 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.28 on epoch=232
06/15/2022 15:17:17 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.27 on epoch=233
06/15/2022 15:17:22 - INFO - __main__ - Global step 2800 Train loss 0.27 Classification-F1 0.16862609932168915 on epoch=233
06/15/2022 15:17:25 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.24 on epoch=234
06/15/2022 15:17:28 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.30 on epoch=234
06/15/2022 15:17:30 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.21 on epoch=235
06/15/2022 15:17:33 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.25 on epoch=236
06/15/2022 15:17:36 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.23 on epoch=237
06/15/2022 15:17:41 - INFO - __main__ - Global step 2850 Train loss 0.24 Classification-F1 0.1376769817122251 on epoch=237
06/15/2022 15:17:44 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.20 on epoch=238
06/15/2022 15:17:46 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.26 on epoch=239
06/15/2022 15:17:49 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.23 on epoch=239
06/15/2022 15:17:52 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=240
06/15/2022 15:17:54 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.24 on epoch=241
06/15/2022 15:18:00 - INFO - __main__ - Global step 2900 Train loss 0.23 Classification-F1 0.18842203364675275 on epoch=241
06/15/2022 15:18:03 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.26 on epoch=242
06/15/2022 15:18:05 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.19 on epoch=243
06/15/2022 15:18:08 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.26 on epoch=244
06/15/2022 15:18:10 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=244
06/15/2022 15:18:13 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.21 on epoch=245
06/15/2022 15:18:19 - INFO - __main__ - Global step 2950 Train loss 0.23 Classification-F1 0.14627484296158996 on epoch=245
06/15/2022 15:18:21 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.22 on epoch=246
06/15/2022 15:18:24 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.22 on epoch=247
06/15/2022 15:18:26 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.23 on epoch=248
06/15/2022 15:18:29 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.19 on epoch=249
06/15/2022 15:18:32 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.30 on epoch=249
06/15/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:18:33 - INFO - __main__ - Printing 3 examples
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:18:33 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:18:33 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:18:33 - INFO - __main__ - Printing 3 examples
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 15:18:33 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:18:34 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:18:34 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:18:37 - INFO - __main__ - Global step 3000 Train loss 0.23 Classification-F1 0.2220693910175858 on epoch=249
06/15/2022 15:18:37 - INFO - __main__ - save last model!
06/15/2022 15:18:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 15:18:37 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 15:18:37 - INFO - __main__ - Printing 3 examples
06/15/2022 15:18:37 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 15:18:37 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:37 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 15:18:37 - INFO - __main__ - ['entailment']
06/15/2022 15:18:37 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 15:18:37 - INFO - __main__ - ['contradiction']
06/15/2022 15:18:37 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:18:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:18:39 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 15:18:49 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:18:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:18:50 - INFO - __main__ - Starting training!
06/15/2022 15:19:09 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_13_0.3_8_predictions.txt
06/15/2022 15:19:09 - INFO - __main__ - Classification-F1 on test data: 0.0729
06/15/2022 15:19:09 - INFO - __main__ - prefix=anli_64_13, lr=0.3, bsz=8, dev_performance=0.40439613964722043, test_performance=0.07292312347702186
06/15/2022 15:19:09 - INFO - __main__ - Running ... prefix=anli_64_13, lr=0.2, bsz=8 ...
06/15/2022 15:19:10 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:19:10 - INFO - __main__ - Printing 3 examples
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: The South Kalgoorlie Gold Mine is a gold mine located south-west of Kalgoorlie, Western Australia. The mine is sometimes also referred to as "South Kal Mines - New Celebration", being a merger of the former "New Celebration Gold Mine" and the "Jubilee Gold Mine", which were combined in 2002. [SEP] hypothesis: The South Kalgoorlie Gold Mine is located northwest of Perth,Australia. 
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: Julia Gjika (born 1949) is an Albanian-born poet living in the United States. She is one of the few writers publishing in the Albanian language and writes poetry as well working as a journalist. Her poems have been praised by her peers and have been included in several publications of collected works. [SEP] hypothesis: Julia Gjika publishes her work in French.
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: Curtis Lee Hanson (March 24, 1945 – September 20, 2016) was an American film director, producer, and screenwriter. His directing work included the psychological thriller "The Hand That Rocks the Cradle" (1992), the neo-noir crime film "L.A. Confidential" (1997), the comedy "Wonder Boys" (2000), the hip hop drama "8 Mile" (2002), and the romantic comedy-drama "In Her Shoes" (2005). [SEP] hypothesis: Curtis Lee Hanson was born in Italy.
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:19:10 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:19:10 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:19:10 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:19:10 - INFO - __main__ - Printing 3 examples
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: Logo TV (often shorted to Logo) is an American digital cable and satellite television channel that is owned by Viacom Media Networks. Launched in 2005, it was originally aimed primarily at LGBT viewers, but in 2012 it shifted its focus towards general cultural and lifestyle programming. [SEP] hypothesis: Logo TV shifted its focus to LGBT programming in 2012
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: Earl of Burlington is a title that has been created twice, the first time in the Peerage of England in 1664 and the second in the Peerage of the United Kingdom in 1831. Since 1858, Earl of Burlington has been a courtesy title used by the Dukes of Devonshire, traditionally borne by the duke's grandson, who is the eldest son of the duke's eldest son, the Marquess of Hartington. [SEP] hypothesis: Earl of Burlington is a title that has been created no less than 3 times.
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ -  [anli] premise: The Royal Irish Rangers (27th (Inniskilling), 83rd and 87th) was a regular infantry regiment of the British Army with a relatively short existence, formed in 1968 and later merged with the Ulster Defence Regiment in 1992 to form the Royal Irish Regiment. [SEP] hypothesis: The Royal Irish Rangers was formed in 1992.
06/15/2022 15:19:10 - INFO - __main__ - ['contradiction']
06/15/2022 15:19:10 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:19:10 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:19:11 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:19:26 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:19:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:19:27 - INFO - __main__ - Starting training!
06/15/2022 15:19:30 - INFO - __main__ - Step 10 Global step 10 Train loss 1.18 on epoch=0
06/15/2022 15:19:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.72 on epoch=1
06/15/2022 15:19:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.75 on epoch=2
06/15/2022 15:19:38 - INFO - __main__ - Step 40 Global step 40 Train loss 1.18 on epoch=3
06/15/2022 15:19:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=4
06/15/2022 15:19:45 - INFO - __main__ - Global step 50 Train loss 0.88 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 15:19:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 15:19:47 - INFO - __main__ - Step 60 Global step 60 Train loss 0.59 on epoch=4
06/15/2022 15:19:50 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=5
06/15/2022 15:19:52 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
06/15/2022 15:19:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=7
06/15/2022 15:19:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=8
06/15/2022 15:20:01 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 15:20:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.60 on epoch=9
06/15/2022 15:20:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.57 on epoch=9
06/15/2022 15:20:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=10
06/15/2022 15:20:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=11
06/15/2022 15:20:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
06/15/2022 15:20:20 - INFO - __main__ - Global step 150 Train loss 0.55 Classification-F1 0.19743774610393214 on epoch=12
06/15/2022 15:20:20 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.19743774610393214 on epoch=12, global_step=150
06/15/2022 15:20:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=13
06/15/2022 15:20:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=14
06/15/2022 15:20:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=14
06/15/2022 15:20:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
06/15/2022 15:20:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=16
06/15/2022 15:20:38 - INFO - __main__ - Global step 200 Train loss 0.50 Classification-F1 0.291445995827315 on epoch=16
06/15/2022 15:20:38 - INFO - __main__ - Saving model with best Classification-F1: 0.19743774610393214 -> 0.291445995827315 on epoch=16, global_step=200
06/15/2022 15:20:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.51 on epoch=17
06/15/2022 15:20:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=18
06/15/2022 15:20:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=19
06/15/2022 15:20:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
06/15/2022 15:20:51 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
06/15/2022 15:20:56 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 15:20:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
06/15/2022 15:21:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=22
06/15/2022 15:21:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.49 on epoch=23
06/15/2022 15:21:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.51 on epoch=24
06/15/2022 15:21:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.48 on epoch=24
06/15/2022 15:21:15 - INFO - __main__ - Global step 300 Train loss 0.50 Classification-F1 0.16732026143790854 on epoch=24
06/15/2022 15:21:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=25
06/15/2022 15:21:20 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
06/15/2022 15:21:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.42 on epoch=27
06/15/2022 15:21:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.49 on epoch=28
06/15/2022 15:21:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
06/15/2022 15:21:33 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 15:21:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
06/15/2022 15:21:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=30
06/15/2022 15:21:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=31
06/15/2022 15:21:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=32
06/15/2022 15:21:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
06/15/2022 15:21:52 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 15:21:54 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=34
06/15/2022 15:21:57 - INFO - __main__ - Step 420 Global step 420 Train loss 0.50 on epoch=34
06/15/2022 15:22:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.47 on epoch=35
06/15/2022 15:22:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=36
06/15/2022 15:22:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=37
06/15/2022 15:22:10 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.291626221445674 on epoch=37
06/15/2022 15:22:11 - INFO - __main__ - Saving model with best Classification-F1: 0.291445995827315 -> 0.291626221445674 on epoch=37, global_step=450
06/15/2022 15:22:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.46 on epoch=38
06/15/2022 15:22:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.46 on epoch=39
06/15/2022 15:22:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=39
06/15/2022 15:22:21 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
06/15/2022 15:22:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.51 on epoch=41
06/15/2022 15:22:29 - INFO - __main__ - Global step 500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 15:22:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=42
06/15/2022 15:22:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=43
06/15/2022 15:22:37 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=44
06/15/2022 15:22:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=44
06/15/2022 15:22:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=45
06/15/2022 15:22:48 - INFO - __main__ - Global step 550 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=45
06/15/2022 15:22:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.44 on epoch=46
06/15/2022 15:22:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
06/15/2022 15:22:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
06/15/2022 15:22:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=49
06/15/2022 15:23:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=49
06/15/2022 15:23:04 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.2275985663082437 on epoch=49
06/15/2022 15:23:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=50
06/15/2022 15:23:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
06/15/2022 15:23:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=52
06/15/2022 15:23:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
06/15/2022 15:23:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.53 on epoch=54
06/15/2022 15:23:22 - INFO - __main__ - Global step 650 Train loss 0.47 Classification-F1 0.1775766716943188 on epoch=54
06/15/2022 15:23:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=54
06/15/2022 15:23:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
06/15/2022 15:23:30 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
06/15/2022 15:23:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=57
06/15/2022 15:23:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=58
06/15/2022 15:23:41 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.32826805774566975 on epoch=58
06/15/2022 15:23:41 - INFO - __main__ - Saving model with best Classification-F1: 0.291626221445674 -> 0.32826805774566975 on epoch=58, global_step=700
06/15/2022 15:23:44 - INFO - __main__ - Step 710 Global step 710 Train loss 0.44 on epoch=59
06/15/2022 15:23:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.45 on epoch=59
06/15/2022 15:23:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=60
06/15/2022 15:23:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
06/15/2022 15:23:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=62
06/15/2022 15:23:59 - INFO - __main__ - Global step 750 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 15:24:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=63
06/15/2022 15:24:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
06/15/2022 15:24:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
06/15/2022 15:24:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=65
06/15/2022 15:24:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=66
06/15/2022 15:24:18 - INFO - __main__ - Global step 800 Train loss 0.47 Classification-F1 0.17595815389455882 on epoch=66
06/15/2022 15:24:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.45 on epoch=67
06/15/2022 15:24:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=68
06/15/2022 15:24:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.47 on epoch=69
06/15/2022 15:24:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=69
06/15/2022 15:24:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=70
06/15/2022 15:24:36 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 15:24:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
06/15/2022 15:24:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=72
06/15/2022 15:24:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
06/15/2022 15:24:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.44 on epoch=74
06/15/2022 15:24:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
06/15/2022 15:24:55 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3514499420539689 on epoch=74
06/15/2022 15:24:55 - INFO - __main__ - Saving model with best Classification-F1: 0.32826805774566975 -> 0.3514499420539689 on epoch=74, global_step=900
06/15/2022 15:24:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
06/15/2022 15:25:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.50 on epoch=76
06/15/2022 15:25:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=77
06/15/2022 15:25:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 15:25:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.47 on epoch=79
06/15/2022 15:25:14 - INFO - __main__ - Global step 950 Train loss 0.45 Classification-F1 0.18906810035842292 on epoch=79
06/15/2022 15:25:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=79
06/15/2022 15:25:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
06/15/2022 15:25:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=81
06/15/2022 15:25:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
06/15/2022 15:25:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.43 on epoch=83
06/15/2022 15:25:33 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.3054831595829992 on epoch=83
06/15/2022 15:25:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
06/15/2022 15:25:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=84
06/15/2022 15:25:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=85
06/15/2022 15:25:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.47 on epoch=86
06/15/2022 15:25:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=87
06/15/2022 15:25:50 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.2062333842411901 on epoch=87
06/15/2022 15:25:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
06/15/2022 15:25:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=89
06/15/2022 15:25:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
06/15/2022 15:26:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=90
06/15/2022 15:26:03 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=91
06/15/2022 15:26:08 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=91
06/15/2022 15:26:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.41 on epoch=92
06/15/2022 15:26:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
06/15/2022 15:26:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.44 on epoch=94
06/15/2022 15:26:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=94
06/15/2022 15:26:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
06/15/2022 15:26:27 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 15:26:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.43 on epoch=96
06/15/2022 15:26:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=97
06/15/2022 15:26:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
06/15/2022 15:26:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.46 on epoch=99
06/15/2022 15:26:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.45 on epoch=99
06/15/2022 15:26:45 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 15:26:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
06/15/2022 15:26:51 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
06/15/2022 15:26:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=102
06/15/2022 15:26:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.41 on epoch=103
06/15/2022 15:26:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
06/15/2022 15:27:04 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.21325855622663212 on epoch=104
06/15/2022 15:27:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
06/15/2022 15:27:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.43 on epoch=105
06/15/2022 15:27:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.44 on epoch=106
06/15/2022 15:27:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.43 on epoch=107
06/15/2022 15:27:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=108
06/15/2022 15:27:23 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.2694283180132237 on epoch=108
06/15/2022 15:27:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
06/15/2022 15:27:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
06/15/2022 15:27:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=110
06/15/2022 15:27:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=111
06/15/2022 15:27:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=112
06/15/2022 15:27:41 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.24377999261720193 on epoch=112
06/15/2022 15:27:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=113
06/15/2022 15:27:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=114
06/15/2022 15:27:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=114
06/15/2022 15:27:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=115
06/15/2022 15:27:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=116
06/15/2022 15:28:00 - INFO - __main__ - Global step 1400 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 15:28:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
06/15/2022 15:28:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=118
06/15/2022 15:28:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=119
06/15/2022 15:28:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=119
06/15/2022 15:28:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
06/15/2022 15:28:18 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=120
06/15/2022 15:28:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
06/15/2022 15:28:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.44 on epoch=122
06/15/2022 15:28:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.45 on epoch=123
06/15/2022 15:28:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=124
06/15/2022 15:28:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=124
06/15/2022 15:28:37 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.38727947319278594 on epoch=124
06/15/2022 15:28:37 - INFO - __main__ - Saving model with best Classification-F1: 0.3514499420539689 -> 0.38727947319278594 on epoch=124, global_step=1500
06/15/2022 15:28:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=125
06/15/2022 15:28:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.41 on epoch=126
06/15/2022 15:28:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
06/15/2022 15:28:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=128
06/15/2022 15:28:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=129
06/15/2022 15:28:56 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.18892001244942422 on epoch=129
06/15/2022 15:28:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.40 on epoch=129
06/15/2022 15:29:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=130
06/15/2022 15:29:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=131
06/15/2022 15:29:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=132
06/15/2022 15:29:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.44 on epoch=133
06/15/2022 15:29:14 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.26289682539682535 on epoch=133
06/15/2022 15:29:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=134
06/15/2022 15:29:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 15:29:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=135
06/15/2022 15:29:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=136
06/15/2022 15:29:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
06/15/2022 15:29:33 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.25613309874048057 on epoch=137
06/15/2022 15:29:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.41 on epoch=138
06/15/2022 15:29:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
06/15/2022 15:29:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
06/15/2022 15:29:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=140
06/15/2022 15:29:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.40 on epoch=141
06/15/2022 15:29:52 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.1965714285714286 on epoch=141
06/15/2022 15:29:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
06/15/2022 15:29:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.45 on epoch=143
06/15/2022 15:30:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=144
06/15/2022 15:30:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=144
06/15/2022 15:30:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=145
06/15/2022 15:30:11 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.1885434487640847 on epoch=145
06/15/2022 15:30:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=146
06/15/2022 15:30:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
06/15/2022 15:30:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=148
06/15/2022 15:30:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=149
06/15/2022 15:30:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=149
06/15/2022 15:30:29 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.2589510124721392 on epoch=149
06/15/2022 15:30:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.37 on epoch=150
06/15/2022 15:30:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
06/15/2022 15:30:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.41 on epoch=152
06/15/2022 15:30:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=153
06/15/2022 15:30:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=154
06/15/2022 15:30:48 - INFO - __main__ - Global step 1850 Train loss 0.41 Classification-F1 0.20415420405450316 on epoch=154
06/15/2022 15:30:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=154
06/15/2022 15:30:53 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=155
06/15/2022 15:30:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=156
06/15/2022 15:30:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=157
06/15/2022 15:31:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=158
06/15/2022 15:31:06 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.36865598952759404 on epoch=158
06/15/2022 15:31:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.36 on epoch=159
06/15/2022 15:31:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=159
06/15/2022 15:31:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=160
06/15/2022 15:31:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
06/15/2022 15:31:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=162
06/15/2022 15:31:25 - INFO - __main__ - Global step 1950 Train loss 0.39 Classification-F1 0.3291052114060964 on epoch=162
06/15/2022 15:31:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
06/15/2022 15:31:31 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.40 on epoch=164
06/15/2022 15:31:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=164
06/15/2022 15:31:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 15:31:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.41 on epoch=166
06/15/2022 15:31:44 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.2645604986030518 on epoch=166
06/15/2022 15:31:47 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.36 on epoch=167
06/15/2022 15:31:49 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.38 on epoch=168
06/15/2022 15:31:52 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.37 on epoch=169
06/15/2022 15:31:55 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.40 on epoch=169
06/15/2022 15:31:57 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
06/15/2022 15:32:03 - INFO - __main__ - Global step 2050 Train loss 0.38 Classification-F1 0.3548382048467699 on epoch=170
06/15/2022 15:32:05 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.42 on epoch=171
06/15/2022 15:32:08 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
06/15/2022 15:32:11 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.41 on epoch=173
06/15/2022 15:32:13 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=174
06/15/2022 15:32:16 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=174
06/15/2022 15:32:21 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.3612441555782501 on epoch=174
06/15/2022 15:32:24 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.39 on epoch=175
06/15/2022 15:32:27 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.36 on epoch=176
06/15/2022 15:32:29 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.36 on epoch=177
06/15/2022 15:32:32 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.40 on epoch=178
06/15/2022 15:32:34 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.44 on epoch=179
06/15/2022 15:32:40 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.289011127817098 on epoch=179
06/15/2022 15:32:43 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=179
06/15/2022 15:32:45 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.41 on epoch=180
06/15/2022 15:32:48 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.36 on epoch=181
06/15/2022 15:32:51 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.39 on epoch=182
06/15/2022 15:32:53 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=183
06/15/2022 15:32:58 - INFO - __main__ - Global step 2200 Train loss 0.39 Classification-F1 0.2238562091503268 on epoch=183
06/15/2022 15:33:01 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=184
06/15/2022 15:33:04 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.41 on epoch=184
06/15/2022 15:33:06 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=185
06/15/2022 15:33:09 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
06/15/2022 15:33:11 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
06/15/2022 15:33:17 - INFO - __main__ - Global step 2250 Train loss 0.39 Classification-F1 0.33872303224014527 on epoch=187
06/15/2022 15:33:20 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=188
06/15/2022 15:33:22 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
06/15/2022 15:33:25 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=189
06/15/2022 15:33:28 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=190
06/15/2022 15:33:30 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.40 on epoch=191
06/15/2022 15:33:36 - INFO - __main__ - Global step 2300 Train loss 0.41 Classification-F1 0.36739014263986514 on epoch=191
06/15/2022 15:33:38 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.35 on epoch=192
06/15/2022 15:33:41 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=193
06/15/2022 15:33:44 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.35 on epoch=194
06/15/2022 15:33:46 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.36 on epoch=194
06/15/2022 15:33:49 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.40 on epoch=195
06/15/2022 15:33:55 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.3717207128250073 on epoch=195
06/15/2022 15:33:57 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.39 on epoch=196
06/15/2022 15:34:00 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.36 on epoch=197
06/15/2022 15:34:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 15:34:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.34 on epoch=199
06/15/2022 15:34:08 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.36 on epoch=199
06/15/2022 15:34:13 - INFO - __main__ - Global step 2400 Train loss 0.37 Classification-F1 0.4168212468193384 on epoch=199
06/15/2022 15:34:13 - INFO - __main__ - Saving model with best Classification-F1: 0.38727947319278594 -> 0.4168212468193384 on epoch=199, global_step=2400
06/15/2022 15:34:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.36 on epoch=200
06/15/2022 15:34:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=201
06/15/2022 15:34:21 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.38 on epoch=202
06/15/2022 15:34:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.40 on epoch=203
06/15/2022 15:34:26 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.35 on epoch=204
06/15/2022 15:34:32 - INFO - __main__ - Global step 2450 Train loss 0.38 Classification-F1 0.33937078878644905 on epoch=204
06/15/2022 15:34:35 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.38 on epoch=204
06/15/2022 15:34:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.33 on epoch=205
06/15/2022 15:34:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.34 on epoch=206
06/15/2022 15:34:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=207
06/15/2022 15:34:45 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
06/15/2022 15:34:51 - INFO - __main__ - Global step 2500 Train loss 0.36 Classification-F1 0.3747529200359389 on epoch=208
06/15/2022 15:34:54 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.36 on epoch=209
06/15/2022 15:34:56 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
06/15/2022 15:34:59 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.38 on epoch=210
06/15/2022 15:35:01 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.36 on epoch=211
06/15/2022 15:35:04 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
06/15/2022 15:35:10 - INFO - __main__ - Global step 2550 Train loss 0.37 Classification-F1 0.372254444710197 on epoch=212
06/15/2022 15:35:12 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.36 on epoch=213
06/15/2022 15:35:15 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=214
06/15/2022 15:35:17 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=214
06/15/2022 15:35:20 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.38 on epoch=215
06/15/2022 15:35:23 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.36 on epoch=216
06/15/2022 15:35:28 - INFO - __main__ - Global step 2600 Train loss 0.37 Classification-F1 0.31220357112469765 on epoch=216
06/15/2022 15:35:31 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=217
06/15/2022 15:35:33 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=218
06/15/2022 15:35:36 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=219
06/15/2022 15:35:39 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.43 on epoch=219
06/15/2022 15:35:41 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
06/15/2022 15:35:47 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.37052631578947365 on epoch=220
06/15/2022 15:35:50 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.34 on epoch=221
06/15/2022 15:35:52 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.37 on epoch=222
06/15/2022 15:35:55 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.34 on epoch=223
06/15/2022 15:35:57 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=224
06/15/2022 15:36:00 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.41 on epoch=224
06/15/2022 15:36:06 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.4036377763650491 on epoch=224
06/15/2022 15:36:08 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
06/15/2022 15:36:11 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=226
06/15/2022 15:36:14 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=227
06/15/2022 15:36:16 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=228
06/15/2022 15:36:19 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.41 on epoch=229
06/15/2022 15:36:24 - INFO - __main__ - Global step 2750 Train loss 0.38 Classification-F1 0.3785358367216774 on epoch=229
06/15/2022 15:36:27 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.60 on epoch=229
06/15/2022 15:36:30 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.48 on epoch=230
06/15/2022 15:36:32 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.41 on epoch=231
06/15/2022 15:36:35 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.34 on epoch=232
06/15/2022 15:36:38 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.39 on epoch=233
06/15/2022 15:36:43 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.3403547671840355 on epoch=233
06/15/2022 15:36:46 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=234
06/15/2022 15:36:48 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
06/15/2022 15:36:51 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
06/15/2022 15:36:54 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.37 on epoch=236
06/15/2022 15:36:56 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.37 on epoch=237
06/15/2022 15:37:02 - INFO - __main__ - Global step 2850 Train loss 0.39 Classification-F1 0.37851024473053263 on epoch=237
06/15/2022 15:37:05 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.36 on epoch=238
06/15/2022 15:37:07 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.36 on epoch=239
06/15/2022 15:37:10 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.40 on epoch=239
06/15/2022 15:37:12 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.34 on epoch=240
06/15/2022 15:37:15 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.40 on epoch=241
06/15/2022 15:37:21 - INFO - __main__ - Global step 2900 Train loss 0.37 Classification-F1 0.36837583148558756 on epoch=241
06/15/2022 15:37:23 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.35 on epoch=242
06/15/2022 15:37:26 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=243
06/15/2022 15:37:29 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.36 on epoch=244
06/15/2022 15:37:31 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.37 on epoch=244
06/15/2022 15:37:34 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.37 on epoch=245
06/15/2022 15:37:40 - INFO - __main__ - Global step 2950 Train loss 0.37 Classification-F1 0.4111188206169318 on epoch=245
06/15/2022 15:37:42 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.35 on epoch=246
06/15/2022 15:37:45 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.31 on epoch=247
06/15/2022 15:37:47 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.36 on epoch=248
06/15/2022 15:37:50 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.41 on epoch=249
06/15/2022 15:37:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.34 on epoch=249
06/15/2022 15:37:54 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:37:54 - INFO - __main__ - Printing 3 examples
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:37:54 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:37:54 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:37:54 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:37:54 - INFO - __main__ - Printing 3 examples
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 15:37:54 - INFO - __main__ - ['entailment']
06/15/2022 15:37:54 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:37:55 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:37:55 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:37:58 - INFO - __main__ - Global step 3000 Train loss 0.35 Classification-F1 0.39260052401463347 on epoch=249
06/15/2022 15:37:58 - INFO - __main__ - save last model!
06/15/2022 15:37:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 15:37:58 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 15:37:58 - INFO - __main__ - Printing 3 examples
06/15/2022 15:37:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 15:37:58 - INFO - __main__ - ['contradiction']
06/15/2022 15:37:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 15:37:58 - INFO - __main__ - ['entailment']
06/15/2022 15:37:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 15:37:58 - INFO - __main__ - ['contradiction']
06/15/2022 15:37:58 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:37:59 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:38:00 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 15:38:11 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:38:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:38:12 - INFO - __main__ - Starting training!
06/15/2022 15:38:29 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_13_0.2_8_predictions.txt
06/15/2022 15:38:29 - INFO - __main__ - Classification-F1 on test data: 0.3186
06/15/2022 15:38:30 - INFO - __main__ - prefix=anli_64_13, lr=0.2, bsz=8, dev_performance=0.4168212468193384, test_performance=0.3185562270140219
06/15/2022 15:38:30 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.5, bsz=8 ...
06/15/2022 15:38:30 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:38:31 - INFO - __main__ - Printing 3 examples
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:38:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:38:31 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:38:31 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:38:31 - INFO - __main__ - Printing 3 examples
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 15:38:31 - INFO - __main__ - ['entailment']
06/15/2022 15:38:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:38:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:38:31 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:38:50 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:38:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:38:51 - INFO - __main__ - Starting training!
06/15/2022 15:38:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.02 on epoch=0
06/15/2022 15:38:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.67 on epoch=1
06/15/2022 15:38:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=2
06/15/2022 15:39:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=3
06/15/2022 15:39:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=4
06/15/2022 15:39:10 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 15:39:10 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 15:39:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=4
06/15/2022 15:39:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=5
06/15/2022 15:39:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=6
06/15/2022 15:39:21 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
06/15/2022 15:39:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=8
06/15/2022 15:39:28 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 15:39:30 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
06/15/2022 15:39:33 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
06/15/2022 15:39:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=10
06/15/2022 15:39:38 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=11
06/15/2022 15:39:41 - INFO - __main__ - Step 150 Global step 150 Train loss 0.54 on epoch=12
06/15/2022 15:39:46 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 15:39:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.52 on epoch=13
06/15/2022 15:39:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=14
06/15/2022 15:39:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.53 on epoch=14
06/15/2022 15:39:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=15
06/15/2022 15:39:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
06/15/2022 15:40:05 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 15:40:08 - INFO - __main__ - Step 210 Global step 210 Train loss 0.47 on epoch=17
06/15/2022 15:40:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=18
06/15/2022 15:40:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.53 on epoch=19
06/15/2022 15:40:16 - INFO - __main__ - Step 240 Global step 240 Train loss 0.45 on epoch=19
06/15/2022 15:40:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=20
06/15/2022 15:40:24 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.1836290071584189 on epoch=20
06/15/2022 15:40:24 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.1836290071584189 on epoch=20, global_step=250
06/15/2022 15:40:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=21
06/15/2022 15:40:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.50 on epoch=22
06/15/2022 15:40:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=23
06/15/2022 15:40:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=24
06/15/2022 15:40:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.44 on epoch=24
06/15/2022 15:40:43 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 15:40:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
06/15/2022 15:40:48 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=26
06/15/2022 15:40:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=27
06/15/2022 15:40:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
06/15/2022 15:40:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=29
06/15/2022 15:41:02 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 15:41:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=29
06/15/2022 15:41:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
06/15/2022 15:41:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.41 on epoch=31
06/15/2022 15:41:12 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
06/15/2022 15:41:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=33
06/15/2022 15:41:21 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 15:41:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=34
06/15/2022 15:41:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
06/15/2022 15:41:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.48 on epoch=35
06/15/2022 15:41:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.45 on epoch=36
06/15/2022 15:41:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
06/15/2022 15:41:39 - INFO - __main__ - Global step 450 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 15:41:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
06/15/2022 15:41:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
06/15/2022 15:41:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
06/15/2022 15:41:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.50 on epoch=40
06/15/2022 15:41:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=41
06/15/2022 15:41:58 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 15:42:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=42
06/15/2022 15:42:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=43
06/15/2022 15:42:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.44 on epoch=44
06/15/2022 15:42:09 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=44
06/15/2022 15:42:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.48 on epoch=45
06/15/2022 15:42:17 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16732026143790854 on epoch=45
06/15/2022 15:42:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
06/15/2022 15:42:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=47
06/15/2022 15:42:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
06/15/2022 15:42:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.45 on epoch=49
06/15/2022 15:42:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.47 on epoch=49
06/15/2022 15:42:36 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 15:42:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=50
06/15/2022 15:42:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
06/15/2022 15:42:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.48 on epoch=52
06/15/2022 15:42:47 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=53
06/15/2022 15:42:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.42 on epoch=54
06/15/2022 15:42:55 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 15:42:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=54
06/15/2022 15:43:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=55
06/15/2022 15:43:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=56
06/15/2022 15:43:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
06/15/2022 15:43:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
06/15/2022 15:43:14 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 15:43:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=59
06/15/2022 15:43:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=59
06/15/2022 15:43:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.46 on epoch=60
06/15/2022 15:43:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=61
06/15/2022 15:43:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
06/15/2022 15:43:33 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 15:43:35 - INFO - __main__ - Step 760 Global step 760 Train loss 0.44 on epoch=63
06/15/2022 15:43:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.46 on epoch=64
06/15/2022 15:43:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
06/15/2022 15:43:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=65
06/15/2022 15:43:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=66
06/15/2022 15:43:52 - INFO - __main__ - Global step 800 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 15:43:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=67
06/15/2022 15:43:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=68
06/15/2022 15:43:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
06/15/2022 15:44:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
06/15/2022 15:44:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=70
06/15/2022 15:44:11 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.24181344675080516 on epoch=70
06/15/2022 15:44:11 - INFO - __main__ - Saving model with best Classification-F1: 0.1836290071584189 -> 0.24181344675080516 on epoch=70, global_step=850
06/15/2022 15:44:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=71
06/15/2022 15:44:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=72
06/15/2022 15:44:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
06/15/2022 15:44:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=74
06/15/2022 15:44:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=74
06/15/2022 15:44:30 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 15:44:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.40 on epoch=75
06/15/2022 15:44:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=76
06/15/2022 15:44:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=77
06/15/2022 15:44:40 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 15:44:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
06/15/2022 15:44:48 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 15:44:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
06/15/2022 15:44:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
06/15/2022 15:44:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.45 on epoch=81
06/15/2022 15:44:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=82
06/15/2022 15:45:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.42 on epoch=83
06/15/2022 15:45:07 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.18704761904761905 on epoch=83
06/15/2022 15:45:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
06/15/2022 15:45:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=84
06/15/2022 15:45:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=85
06/15/2022 15:45:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
06/15/2022 15:45:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.43 on epoch=87
06/15/2022 15:45:26 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 15:45:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=88
06/15/2022 15:45:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=89
06/15/2022 15:45:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=89
06/15/2022 15:45:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
06/15/2022 15:45:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=91
06/15/2022 15:45:45 - INFO - __main__ - Global step 1100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 15:45:48 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=92
06/15/2022 15:45:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
06/15/2022 15:45:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
06/15/2022 15:45:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.45 on epoch=94
06/15/2022 15:45:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=95
06/15/2022 15:46:04 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.20639187574671447 on epoch=95
06/15/2022 15:46:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=96
06/15/2022 15:46:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=97
06/15/2022 15:46:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
06/15/2022 15:46:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.42 on epoch=99
06/15/2022 15:46:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=99
06/15/2022 15:46:23 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 15:46:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.45 on epoch=100
06/15/2022 15:46:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
06/15/2022 15:46:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
06/15/2022 15:46:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
06/15/2022 15:46:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
06/15/2022 15:46:42 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 15:46:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=104
06/15/2022 15:46:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.42 on epoch=105
06/15/2022 15:46:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=106
06/15/2022 15:46:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
06/15/2022 15:46:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 15:47:01 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.1775766716943188 on epoch=108
06/15/2022 15:47:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=109
06/15/2022 15:47:06 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=109
06/15/2022 15:47:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=110
06/15/2022 15:47:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=111
06/15/2022 15:47:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=112
06/15/2022 15:47:20 - INFO - __main__ - Global step 1350 Train loss 0.41 Classification-F1 0.17433647054294896 on epoch=112
06/15/2022 15:47:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
06/15/2022 15:47:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
06/15/2022 15:47:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.40 on epoch=114
06/15/2022 15:47:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=115
06/15/2022 15:47:33 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
06/15/2022 15:47:39 - INFO - __main__ - Global step 1400 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 15:47:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.43 on epoch=117
06/15/2022 15:47:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
06/15/2022 15:47:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
06/15/2022 15:47:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=119
06/15/2022 15:47:52 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=120
06/15/2022 15:47:58 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.17699251303962893 on epoch=120
06/15/2022 15:48:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.41 on epoch=121
06/15/2022 15:48:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.43 on epoch=122
06/15/2022 15:48:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
06/15/2022 15:48:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=124
06/15/2022 15:48:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=124
06/15/2022 15:48:16 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=124
06/15/2022 15:48:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=125
06/15/2022 15:48:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
06/15/2022 15:48:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.45 on epoch=127
06/15/2022 15:48:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=128
06/15/2022 15:48:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
06/15/2022 15:48:34 - INFO - __main__ - Global step 1550 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 15:48:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=129
06/15/2022 15:48:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=130
06/15/2022 15:48:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.40 on epoch=131
06/15/2022 15:48:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=132
06/15/2022 15:48:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=133
06/15/2022 15:48:53 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.30440264855622207 on epoch=133
06/15/2022 15:48:53 - INFO - __main__ - Saving model with best Classification-F1: 0.24181344675080516 -> 0.30440264855622207 on epoch=133, global_step=1600
06/15/2022 15:48:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.39 on epoch=134
06/15/2022 15:48:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=134
06/15/2022 15:49:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=135
06/15/2022 15:49:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=136
06/15/2022 15:49:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=137
06/15/2022 15:49:12 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 15:49:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
06/15/2022 15:49:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.41 on epoch=139
06/15/2022 15:49:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
06/15/2022 15:49:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.44 on epoch=140
06/15/2022 15:49:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=141
06/15/2022 15:49:31 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=141
06/15/2022 15:49:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=142
06/15/2022 15:49:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
06/15/2022 15:49:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.40 on epoch=144
06/15/2022 15:49:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.43 on epoch=144
06/15/2022 15:49:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
06/15/2022 15:49:50 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=145
06/15/2022 15:49:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=146
06/15/2022 15:49:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
06/15/2022 15:49:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
06/15/2022 15:50:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=149
06/15/2022 15:50:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
06/15/2022 15:50:09 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=149
06/15/2022 15:50:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=150
06/15/2022 15:50:15 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
06/15/2022 15:50:17 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.47 on epoch=152
06/15/2022 15:50:20 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
06/15/2022 15:50:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=154
06/15/2022 15:50:28 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=154
06/15/2022 15:50:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=154
06/15/2022 15:50:33 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=155
06/15/2022 15:50:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=156
06/15/2022 15:50:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=157
06/15/2022 15:50:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
06/15/2022 15:50:47 - INFO - __main__ - Global step 1900 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=158
06/15/2022 15:50:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=159
06/15/2022 15:50:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.41 on epoch=159
06/15/2022 15:50:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=160
06/15/2022 15:50:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.38 on epoch=161
06/15/2022 15:51:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
06/15/2022 15:51:06 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.2712291838775599 on epoch=162
06/15/2022 15:51:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=163
06/15/2022 15:51:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=164
06/15/2022 15:51:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=164
06/15/2022 15:51:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 15:51:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.38 on epoch=166
06/15/2022 15:51:25 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.235846338942267 on epoch=166
06/15/2022 15:51:28 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
06/15/2022 15:51:30 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.43 on epoch=168
06/15/2022 15:51:33 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.38 on epoch=169
06/15/2022 15:51:36 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
06/15/2022 15:51:38 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.43 on epoch=170
06/15/2022 15:51:44 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=170
06/15/2022 15:51:47 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=171
06/15/2022 15:51:50 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=172
06/15/2022 15:51:52 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=173
06/15/2022 15:51:55 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=174
06/15/2022 15:51:57 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=174
06/15/2022 15:52:03 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=174
06/15/2022 15:52:06 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=175
06/15/2022 15:52:08 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.39 on epoch=176
06/15/2022 15:52:11 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
06/15/2022 15:52:14 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=178
06/15/2022 15:52:16 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=179
06/15/2022 15:52:22 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=179
06/15/2022 15:52:25 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=179
06/15/2022 15:52:27 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
06/15/2022 15:52:30 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=181
06/15/2022 15:52:33 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.40 on epoch=182
06/15/2022 15:52:35 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.41 on epoch=183
06/15/2022 15:52:41 - INFO - __main__ - Global step 2200 Train loss 0.41 Classification-F1 0.20816919656144406 on epoch=183
06/15/2022 15:52:44 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 15:52:46 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
06/15/2022 15:52:49 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.38 on epoch=185
06/15/2022 15:52:52 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.41 on epoch=186
06/15/2022 15:52:54 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.43 on epoch=187
06/15/2022 15:53:00 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.20770099497101271 on epoch=187
06/15/2022 15:53:03 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=188
06/15/2022 15:53:05 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.41 on epoch=189
06/15/2022 15:53:08 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.43 on epoch=189
06/15/2022 15:53:11 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.42 on epoch=190
06/15/2022 15:53:13 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=191
06/15/2022 15:53:19 - INFO - __main__ - Global step 2300 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=191
06/15/2022 15:53:21 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=192
06/15/2022 15:53:24 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=193
06/15/2022 15:53:27 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.40 on epoch=194
06/15/2022 15:53:29 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.42 on epoch=194
06/15/2022 15:53:32 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=195
06/15/2022 15:53:38 - INFO - __main__ - Global step 2350 Train loss 0.42 Classification-F1 0.22500127090641045 on epoch=195
06/15/2022 15:53:40 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=196
06/15/2022 15:53:43 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
06/15/2022 15:53:46 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=198
06/15/2022 15:53:48 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.35 on epoch=199
06/15/2022 15:53:51 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.40 on epoch=199
06/15/2022 15:53:57 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.1679790026246719 on epoch=199
06/15/2022 15:53:59 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.44 on epoch=200
06/15/2022 15:54:02 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.40 on epoch=201
06/15/2022 15:54:05 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.44 on epoch=202
06/15/2022 15:54:07 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
06/15/2022 15:54:10 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=204
06/15/2022 15:54:16 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=204
06/15/2022 15:54:18 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
06/15/2022 15:54:21 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.40 on epoch=205
06/15/2022 15:54:23 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.41 on epoch=206
06/15/2022 15:54:26 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=207
06/15/2022 15:54:29 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.40 on epoch=208
06/15/2022 15:54:34 - INFO - __main__ - Global step 2500 Train loss 0.40 Classification-F1 0.1775766716943188 on epoch=208
06/15/2022 15:54:37 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.41 on epoch=209
06/15/2022 15:54:40 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.42 on epoch=209
06/15/2022 15:54:42 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=210
06/15/2022 15:54:45 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.40 on epoch=211
06/15/2022 15:54:48 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.40 on epoch=212
06/15/2022 15:54:53 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.2721381711072433 on epoch=212
06/15/2022 15:54:56 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.43 on epoch=213
06/15/2022 15:54:59 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
06/15/2022 15:55:01 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
06/15/2022 15:55:04 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.40 on epoch=215
06/15/2022 15:55:06 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=216
06/15/2022 15:55:12 - INFO - __main__ - Global step 2600 Train loss 0.41 Classification-F1 0.1794219130041292 on epoch=216
06/15/2022 15:55:15 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.44 on epoch=217
06/15/2022 15:55:17 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
06/15/2022 15:55:20 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=219
06/15/2022 15:55:23 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
06/15/2022 15:55:25 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
06/15/2022 15:55:31 - INFO - __main__ - Global step 2650 Train loss 0.41 Classification-F1 0.2303693395396452 on epoch=220
06/15/2022 15:55:34 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.39 on epoch=221
06/15/2022 15:55:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.42 on epoch=222
06/15/2022 15:55:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.40 on epoch=223
06/15/2022 15:55:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.36 on epoch=224
06/15/2022 15:55:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.39 on epoch=224
06/15/2022 15:55:50 - INFO - __main__ - Global step 2700 Train loss 0.39 Classification-F1 0.18020816002070505 on epoch=224
06/15/2022 15:55:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
06/15/2022 15:55:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
06/15/2022 15:55:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.38 on epoch=227
06/15/2022 15:56:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=228
06/15/2022 15:56:03 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.39 on epoch=229
06/15/2022 15:56:09 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.1775766716943188 on epoch=229
06/15/2022 15:56:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.37 on epoch=229
06/15/2022 15:56:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.37 on epoch=230
06/15/2022 15:56:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=231
06/15/2022 15:56:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.42 on epoch=232
06/15/2022 15:56:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
06/15/2022 15:56:28 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.2906767337807606 on epoch=233
06/15/2022 15:56:31 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
06/15/2022 15:56:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=234
06/15/2022 15:56:36 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.45 on epoch=235
06/15/2022 15:56:39 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.39 on epoch=236
06/15/2022 15:56:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.44 on epoch=237
06/15/2022 15:56:47 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.3243496659565526 on epoch=237
06/15/2022 15:56:47 - INFO - __main__ - Saving model with best Classification-F1: 0.30440264855622207 -> 0.3243496659565526 on epoch=237, global_step=2850
06/15/2022 15:56:50 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
06/15/2022 15:56:52 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.37 on epoch=239
06/15/2022 15:56:55 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=239
06/15/2022 15:56:58 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.42 on epoch=240
06/15/2022 15:57:00 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.34 on epoch=241
06/15/2022 15:57:06 - INFO - __main__ - Global step 2900 Train loss 0.39 Classification-F1 0.18888888888888888 on epoch=241
06/15/2022 15:57:09 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.44 on epoch=242
06/15/2022 15:57:11 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.41 on epoch=243
06/15/2022 15:57:14 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.33 on epoch=244
06/15/2022 15:57:16 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=244
06/15/2022 15:57:19 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.42 on epoch=245
06/15/2022 15:57:25 - INFO - __main__ - Global step 2950 Train loss 0.40 Classification-F1 0.26294355798999763 on epoch=245
06/15/2022 15:57:28 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.39 on epoch=246
06/15/2022 15:57:30 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=247
06/15/2022 15:57:33 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
06/15/2022 15:57:36 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.39 on epoch=249
06/15/2022 15:57:38 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.39 on epoch=249
06/15/2022 15:57:40 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:57:40 - INFO - __main__ - Printing 3 examples
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:57:40 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:57:40 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:57:40 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:57:40 - INFO - __main__ - Printing 3 examples
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 15:57:40 - INFO - __main__ - ['entailment']
06/15/2022 15:57:40 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:57:40 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:57:41 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:57:44 - INFO - __main__ - Global step 3000 Train loss 0.40 Classification-F1 0.20128193525969973 on epoch=249
06/15/2022 15:57:44 - INFO - __main__ - save last model!
06/15/2022 15:57:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 15:57:44 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 15:57:44 - INFO - __main__ - Printing 3 examples
06/15/2022 15:57:44 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 15:57:44 - INFO - __main__ - ['contradiction']
06/15/2022 15:57:44 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 15:57:44 - INFO - __main__ - ['entailment']
06/15/2022 15:57:44 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 15:57:44 - INFO - __main__ - ['contradiction']
06/15/2022 15:57:44 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:57:45 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:57:46 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 15:57:56 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:57:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:57:57 - INFO - __main__ - Starting training!
06/15/2022 15:58:16 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_21_0.5_8_predictions.txt
06/15/2022 15:58:16 - INFO - __main__ - Classification-F1 on test data: 0.1908
06/15/2022 15:58:16 - INFO - __main__ - prefix=anli_64_21, lr=0.5, bsz=8, dev_performance=0.3243496659565526, test_performance=0.19077526049993473
06/15/2022 15:58:16 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.4, bsz=8 ...
06/15/2022 15:58:17 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:58:17 - INFO - __main__ - Printing 3 examples
06/15/2022 15:58:17 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 15:58:17 - INFO - __main__ - ['entailment']
06/15/2022 15:58:17 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 15:58:17 - INFO - __main__ - ['entailment']
06/15/2022 15:58:17 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 15:58:17 - INFO - __main__ - ['entailment']
06/15/2022 15:58:17 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:58:18 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:58:18 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 15:58:18 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 15:58:18 - INFO - __main__ - Printing 3 examples
06/15/2022 15:58:18 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 15:58:18 - INFO - __main__ - ['entailment']
06/15/2022 15:58:18 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 15:58:18 - INFO - __main__ - ['entailment']
06/15/2022 15:58:18 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 15:58:18 - INFO - __main__ - ['entailment']
06/15/2022 15:58:18 - INFO - __main__ - Tokenizing Input ...
06/15/2022 15:58:18 - INFO - __main__ - Tokenizing Output ...
06/15/2022 15:58:18 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 15:58:33 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 15:58:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 15:58:34 - INFO - __main__ - Starting training!
06/15/2022 15:58:38 - INFO - __main__ - Step 10 Global step 10 Train loss 0.95 on epoch=0
06/15/2022 15:58:41 - INFO - __main__ - Step 20 Global step 20 Train loss 0.80 on epoch=1
06/15/2022 15:58:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.70 on epoch=2
06/15/2022 15:58:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.97 on epoch=3
06/15/2022 15:58:48 - INFO - __main__ - Step 50 Global step 50 Train loss 1.29 on epoch=4
06/15/2022 15:58:52 - INFO - __main__ - Global step 50 Train loss 0.94 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 15:58:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 15:58:55 - INFO - __main__ - Step 60 Global step 60 Train loss 0.68 on epoch=4
06/15/2022 15:58:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.73 on epoch=5
06/15/2022 15:59:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.63 on epoch=6
06/15/2022 15:59:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.62 on epoch=7
06/15/2022 15:59:05 - INFO - __main__ - Step 100 Global step 100 Train loss 0.64 on epoch=8
06/15/2022 15:59:09 - INFO - __main__ - Global step 100 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 15:59:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.59 on epoch=9
06/15/2022 15:59:14 - INFO - __main__ - Step 120 Global step 120 Train loss 0.59 on epoch=9
06/15/2022 15:59:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=10
06/15/2022 15:59:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=11
06/15/2022 15:59:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
06/15/2022 15:59:27 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 15:59:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.56 on epoch=13
06/15/2022 15:59:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.51 on epoch=14
06/15/2022 15:59:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.60 on epoch=14
06/15/2022 15:59:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.60 on epoch=15
06/15/2022 15:59:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=16
06/15/2022 15:59:46 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 15:59:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=17
06/15/2022 15:59:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=18
06/15/2022 15:59:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.49 on epoch=19
06/15/2022 15:59:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
06/15/2022 15:59:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.59 on epoch=20
06/15/2022 16:00:04 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 16:00:07 - INFO - __main__ - Step 260 Global step 260 Train loss 0.52 on epoch=21
06/15/2022 16:00:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=22
06/15/2022 16:00:12 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
06/15/2022 16:00:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.54 on epoch=24
06/15/2022 16:00:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.61 on epoch=24
06/15/2022 16:00:22 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 16:00:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.49 on epoch=25
06/15/2022 16:00:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
06/15/2022 16:00:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=27
06/15/2022 16:00:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=28
06/15/2022 16:00:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=29
06/15/2022 16:00:41 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.16732026143790854 on epoch=29
06/15/2022 16:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16732026143790854 on epoch=29, global_step=350
06/15/2022 16:00:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.50 on epoch=29
06/15/2022 16:00:46 - INFO - __main__ - Step 370 Global step 370 Train loss 0.47 on epoch=30
06/15/2022 16:00:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=31
06/15/2022 16:00:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=32
06/15/2022 16:00:53 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=33
06/15/2022 16:00:59 - INFO - __main__ - Global step 400 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 16:01:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
06/15/2022 16:01:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=34
06/15/2022 16:01:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=35
06/15/2022 16:01:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
06/15/2022 16:01:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.46 on epoch=37
06/15/2022 16:01:17 - INFO - __main__ - Global step 450 Train loss 0.48 Classification-F1 0.20387111086462203 on epoch=37
06/15/2022 16:01:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16732026143790854 -> 0.20387111086462203 on epoch=37, global_step=450
06/15/2022 16:01:20 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=38
06/15/2022 16:01:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.50 on epoch=39
06/15/2022 16:01:25 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
06/15/2022 16:01:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.51 on epoch=40
06/15/2022 16:01:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=41
06/15/2022 16:01:35 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 16:01:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
06/15/2022 16:01:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=43
06/15/2022 16:01:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
06/15/2022 16:01:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.46 on epoch=44
06/15/2022 16:01:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.51 on epoch=45
06/15/2022 16:01:54 - INFO - __main__ - Global step 550 Train loss 0.49 Classification-F1 0.21253342651159143 on epoch=45
06/15/2022 16:01:54 - INFO - __main__ - Saving model with best Classification-F1: 0.20387111086462203 -> 0.21253342651159143 on epoch=45, global_step=550
06/15/2022 16:01:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
06/15/2022 16:01:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=47
06/15/2022 16:02:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
06/15/2022 16:02:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=49
06/15/2022 16:02:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
06/15/2022 16:02:13 - INFO - __main__ - Global step 600 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 16:02:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.56 on epoch=50
06/15/2022 16:02:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=51
06/15/2022 16:02:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.49 on epoch=52
06/15/2022 16:02:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
06/15/2022 16:02:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
06/15/2022 16:02:31 - INFO - __main__ - Global step 650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 16:02:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.44 on epoch=54
06/15/2022 16:02:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=55
06/15/2022 16:02:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=56
06/15/2022 16:02:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=57
06/15/2022 16:02:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=58
06/15/2022 16:02:49 - INFO - __main__ - Global step 700 Train loss 0.44 Classification-F1 0.19743774610393214 on epoch=58
06/15/2022 16:02:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=59
06/15/2022 16:02:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
06/15/2022 16:02:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.47 on epoch=60
06/15/2022 16:03:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=61
06/15/2022 16:03:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.46 on epoch=62
06/15/2022 16:03:08 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 16:03:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
06/15/2022 16:03:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=64
06/15/2022 16:03:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=64
06/15/2022 16:03:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
06/15/2022 16:03:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=66
06/15/2022 16:03:26 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 16:03:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=67
06/15/2022 16:03:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=68
06/15/2022 16:03:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
06/15/2022 16:03:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.47 on epoch=69
06/15/2022 16:03:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=70
06/15/2022 16:03:45 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.2998033777361095 on epoch=70
06/15/2022 16:03:45 - INFO - __main__ - Saving model with best Classification-F1: 0.21253342651159143 -> 0.2998033777361095 on epoch=70, global_step=850
06/15/2022 16:03:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.43 on epoch=71
06/15/2022 16:03:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.51 on epoch=72
06/15/2022 16:03:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.47 on epoch=73
06/15/2022 16:03:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=74
06/15/2022 16:03:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
06/15/2022 16:04:03 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 16:04:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=75
06/15/2022 16:04:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=76
06/15/2022 16:04:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=77
06/15/2022 16:04:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=78
06/15/2022 16:04:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
06/15/2022 16:04:22 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.199241088129977 on epoch=79
06/15/2022 16:04:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=79
06/15/2022 16:04:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.45 on epoch=80
06/15/2022 16:04:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.46 on epoch=81
06/15/2022 16:04:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
06/15/2022 16:04:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.46 on epoch=83
06/15/2022 16:04:40 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.1916050456720313 on epoch=83
06/15/2022 16:04:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=84
06/15/2022 16:04:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.43 on epoch=84
06/15/2022 16:04:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=85
06/15/2022 16:04:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=86
06/15/2022 16:04:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=87
06/15/2022 16:04:58 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 16:05:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=88
06/15/2022 16:05:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
06/15/2022 16:05:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.48 on epoch=89
06/15/2022 16:05:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
06/15/2022 16:05:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.42 on epoch=91
06/15/2022 16:05:17 - INFO - __main__ - Global step 1100 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 16:05:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.46 on epoch=92
06/15/2022 16:05:22 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=93
06/15/2022 16:05:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.42 on epoch=94
06/15/2022 16:05:27 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
06/15/2022 16:05:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
06/15/2022 16:05:35 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.19444444444444445 on epoch=95
06/15/2022 16:05:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=96
06/15/2022 16:05:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.45 on epoch=97
06/15/2022 16:05:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=98
06/15/2022 16:05:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
06/15/2022 16:05:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=99
06/15/2022 16:05:54 - INFO - __main__ - Global step 1200 Train loss 0.45 Classification-F1 0.16732026143790854 on epoch=99
06/15/2022 16:05:56 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
06/15/2022 16:05:59 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.43 on epoch=101
06/15/2022 16:06:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.46 on epoch=102
06/15/2022 16:06:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.48 on epoch=103
06/15/2022 16:06:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=104
06/15/2022 16:06:12 - INFO - __main__ - Global step 1250 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 16:06:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=104
06/15/2022 16:06:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=105
06/15/2022 16:06:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=106
06/15/2022 16:06:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
06/15/2022 16:06:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=108
06/15/2022 16:06:31 - INFO - __main__ - Global step 1300 Train loss 0.44 Classification-F1 0.24284511784511786 on epoch=108
06/15/2022 16:06:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=109
06/15/2022 16:06:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
06/15/2022 16:06:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=110
06/15/2022 16:06:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=111
06/15/2022 16:06:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=112
06/15/2022 16:06:49 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 16:06:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.43 on epoch=113
06/15/2022 16:06:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=114
06/15/2022 16:06:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
06/15/2022 16:06:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.43 on epoch=115
06/15/2022 16:07:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.42 on epoch=116
06/15/2022 16:07:07 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 16:07:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
06/15/2022 16:07:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.46 on epoch=118
06/15/2022 16:07:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
06/15/2022 16:07:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=119
06/15/2022 16:07:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=120
06/15/2022 16:07:26 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.2356273799572769 on epoch=120
06/15/2022 16:07:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.39 on epoch=121
06/15/2022 16:07:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=122
06/15/2022 16:07:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=123
06/15/2022 16:07:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.38 on epoch=124
06/15/2022 16:07:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=124
06/15/2022 16:07:44 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=124
06/15/2022 16:07:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=125
06/15/2022 16:07:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
06/15/2022 16:07:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
06/15/2022 16:07:54 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=128
06/15/2022 16:07:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=129
06/15/2022 16:08:02 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.18627450980392157 on epoch=129
06/15/2022 16:08:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
06/15/2022 16:08:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.42 on epoch=130
06/15/2022 16:08:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
06/15/2022 16:08:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=132
06/15/2022 16:08:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=133
06/15/2022 16:08:21 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 16:08:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=134
06/15/2022 16:08:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=134
06/15/2022 16:08:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.46 on epoch=135
06/15/2022 16:08:31 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.38 on epoch=136
06/15/2022 16:08:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=137
06/15/2022 16:08:39 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 16:08:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=138
06/15/2022 16:08:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=139
06/15/2022 16:08:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
06/15/2022 16:08:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=140
06/15/2022 16:08:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=141
06/15/2022 16:08:58 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=141
06/15/2022 16:09:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
06/15/2022 16:09:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.46 on epoch=143
06/15/2022 16:09:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=144
06/15/2022 16:09:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=144
06/15/2022 16:09:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=145
06/15/2022 16:09:16 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.28406846609611586 on epoch=145
06/15/2022 16:09:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=146
06/15/2022 16:09:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=147
06/15/2022 16:09:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
06/15/2022 16:09:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=149
06/15/2022 16:09:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.41 on epoch=149
06/15/2022 16:09:35 - INFO - __main__ - Global step 1800 Train loss 0.42 Classification-F1 0.24699463951533607 on epoch=149
06/15/2022 16:09:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=150
06/15/2022 16:09:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
06/15/2022 16:09:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=152
06/15/2022 16:09:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=153
06/15/2022 16:09:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=154
06/15/2022 16:09:53 - INFO - __main__ - Global step 1850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=154
06/15/2022 16:09:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=154
06/15/2022 16:09:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=155
06/15/2022 16:10:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=156
06/15/2022 16:10:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=157
06/15/2022 16:10:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.48 on epoch=158
06/15/2022 16:10:12 - INFO - __main__ - Global step 1900 Train loss 0.44 Classification-F1 0.189265974482153 on epoch=158
06/15/2022 16:10:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.38 on epoch=159
06/15/2022 16:10:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=159
06/15/2022 16:10:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=160
06/15/2022 16:10:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=161
06/15/2022 16:10:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=162
06/15/2022 16:10:30 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.2519105959703794 on epoch=162
06/15/2022 16:10:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
06/15/2022 16:10:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=164
06/15/2022 16:10:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=164
06/15/2022 16:10:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=165
06/15/2022 16:10:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
06/15/2022 16:10:49 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=166
06/15/2022 16:10:51 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
06/15/2022 16:10:54 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.45 on epoch=168
06/15/2022 16:10:56 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
06/15/2022 16:10:59 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=169
06/15/2022 16:11:02 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=170
06/15/2022 16:11:07 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.20066248256624827 on epoch=170
06/15/2022 16:11:10 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.38 on epoch=171
06/15/2022 16:11:13 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.38 on epoch=172
06/15/2022 16:11:15 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.43 on epoch=173
06/15/2022 16:11:18 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.42 on epoch=174
06/15/2022 16:11:20 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.45 on epoch=174
06/15/2022 16:11:26 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.17433647054294896 on epoch=174
06/15/2022 16:11:28 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
06/15/2022 16:11:31 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.41 on epoch=176
06/15/2022 16:11:34 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
06/15/2022 16:11:36 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.45 on epoch=178
06/15/2022 16:11:39 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=179
06/15/2022 16:11:44 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.18256716417910446 on epoch=179
06/15/2022 16:11:47 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=179
06/15/2022 16:11:50 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
06/15/2022 16:11:52 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
06/15/2022 16:11:55 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=182
06/15/2022 16:11:57 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
06/15/2022 16:12:03 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.29012177596248395 on epoch=183
06/15/2022 16:12:06 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 16:12:08 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.42 on epoch=184
06/15/2022 16:12:11 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.42 on epoch=185
06/15/2022 16:12:13 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
06/15/2022 16:12:16 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.46 on epoch=187
06/15/2022 16:12:21 - INFO - __main__ - Global step 2250 Train loss 0.42 Classification-F1 0.2977349664941467 on epoch=187
06/15/2022 16:12:24 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
06/15/2022 16:12:27 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=189
06/15/2022 16:12:29 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
06/15/2022 16:12:32 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.43 on epoch=190
06/15/2022 16:12:34 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.41 on epoch=191
06/15/2022 16:12:40 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.1647058823529412 on epoch=191
06/15/2022 16:12:42 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.44 on epoch=192
06/15/2022 16:12:45 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
06/15/2022 16:12:48 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=194
06/15/2022 16:12:50 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=194
06/15/2022 16:12:53 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.41 on epoch=195
06/15/2022 16:12:58 - INFO - __main__ - Global step 2350 Train loss 0.43 Classification-F1 0.2796410014170997 on epoch=195
06/15/2022 16:13:01 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=196
06/15/2022 16:13:04 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=197
06/15/2022 16:13:06 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 16:13:09 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.36 on epoch=199
06/15/2022 16:13:11 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=199
06/15/2022 16:13:17 - INFO - __main__ - Global step 2400 Train loss 0.40 Classification-F1 0.1647058823529412 on epoch=199
06/15/2022 16:13:20 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.42 on epoch=200
06/15/2022 16:13:22 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.45 on epoch=201
06/15/2022 16:13:25 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.41 on epoch=202
06/15/2022 16:13:27 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
06/15/2022 16:13:30 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.39 on epoch=204
06/15/2022 16:13:35 - INFO - __main__ - Global step 2450 Train loss 0.42 Classification-F1 0.16402116402116398 on epoch=204
06/15/2022 16:13:38 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
06/15/2022 16:13:41 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=205
06/15/2022 16:13:43 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
06/15/2022 16:13:46 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
06/15/2022 16:13:48 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.41 on epoch=208
06/15/2022 16:13:54 - INFO - __main__ - Global step 2500 Train loss 0.42 Classification-F1 0.2703335116818263 on epoch=208
06/15/2022 16:13:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.42 on epoch=209
06/15/2022 16:13:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.43 on epoch=209
06/15/2022 16:14:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=210
06/15/2022 16:14:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.39 on epoch=211
06/15/2022 16:14:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.42 on epoch=212
06/15/2022 16:14:12 - INFO - __main__ - Global step 2550 Train loss 0.42 Classification-F1 0.18192935710869715 on epoch=212
06/15/2022 16:14:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
06/15/2022 16:14:17 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.40 on epoch=214
06/15/2022 16:14:20 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=214
06/15/2022 16:14:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=215
06/15/2022 16:14:25 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.39 on epoch=216
06/15/2022 16:14:31 - INFO - __main__ - Global step 2600 Train loss 0.41 Classification-F1 0.18993197278911564 on epoch=216
06/15/2022 16:14:33 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.41 on epoch=217
06/15/2022 16:14:36 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.40 on epoch=218
06/15/2022 16:14:39 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.36 on epoch=219
06/15/2022 16:14:41 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.41 on epoch=219
06/15/2022 16:14:44 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
06/15/2022 16:14:49 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.23141837065887702 on epoch=220
06/15/2022 16:14:52 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.38 on epoch=221
06/15/2022 16:14:55 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=222
06/15/2022 16:14:57 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=223
06/15/2022 16:15:00 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=224
06/15/2022 16:15:02 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.40 on epoch=224
06/15/2022 16:15:08 - INFO - __main__ - Global step 2700 Train loss 0.41 Classification-F1 0.28851383134607195 on epoch=224
06/15/2022 16:15:11 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.46 on epoch=225
06/15/2022 16:15:13 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.36 on epoch=226
06/15/2022 16:15:16 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.43 on epoch=227
06/15/2022 16:15:18 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=228
06/15/2022 16:15:21 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=229
06/15/2022 16:15:26 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.16600790513833993 on epoch=229
06/15/2022 16:15:29 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.42 on epoch=229
06/15/2022 16:15:32 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.39 on epoch=230
06/15/2022 16:15:34 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.40 on epoch=231
06/15/2022 16:15:37 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=232
06/15/2022 16:15:39 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.40 on epoch=233
06/15/2022 16:15:45 - INFO - __main__ - Global step 2800 Train loss 0.40 Classification-F1 0.29528432511146324 on epoch=233
06/15/2022 16:15:48 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.36 on epoch=234
06/15/2022 16:15:50 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
06/15/2022 16:15:53 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.37 on epoch=235
06/15/2022 16:15:55 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.43 on epoch=236
06/15/2022 16:15:58 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=237
06/15/2022 16:16:03 - INFO - __main__ - Global step 2850 Train loss 0.40 Classification-F1 0.27433751595191463 on epoch=237
06/15/2022 16:16:06 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.41 on epoch=238
06/15/2022 16:16:09 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.39 on epoch=239
06/15/2022 16:16:11 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.41 on epoch=239
06/15/2022 16:16:14 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
06/15/2022 16:16:16 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.41 on epoch=241
06/15/2022 16:16:22 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.19233868014355818 on epoch=241
06/15/2022 16:16:25 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.38 on epoch=242
06/15/2022 16:16:27 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.39 on epoch=243
06/15/2022 16:16:30 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
06/15/2022 16:16:32 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.36 on epoch=244
06/15/2022 16:16:35 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=245
06/15/2022 16:16:41 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.3277479723539181 on epoch=245
06/15/2022 16:16:41 - INFO - __main__ - Saving model with best Classification-F1: 0.2998033777361095 -> 0.3277479723539181 on epoch=245, global_step=2950
06/15/2022 16:16:43 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.33 on epoch=246
06/15/2022 16:16:46 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.36 on epoch=247
06/15/2022 16:16:49 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.39 on epoch=248
06/15/2022 16:16:51 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.35 on epoch=249
06/15/2022 16:16:54 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.41 on epoch=249
06/15/2022 16:16:55 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:16:55 - INFO - __main__ - Printing 3 examples
06/15/2022 16:16:55 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 16:16:55 - INFO - __main__ - ['entailment']
06/15/2022 16:16:55 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 16:16:55 - INFO - __main__ - ['entailment']
06/15/2022 16:16:55 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 16:16:55 - INFO - __main__ - ['entailment']
06/15/2022 16:16:55 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:16:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:16:56 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:16:56 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:16:56 - INFO - __main__ - Printing 3 examples
06/15/2022 16:16:56 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 16:16:56 - INFO - __main__ - ['entailment']
06/15/2022 16:16:56 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 16:16:56 - INFO - __main__ - ['entailment']
06/15/2022 16:16:56 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 16:16:56 - INFO - __main__ - ['entailment']
06/15/2022 16:16:56 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:16:56 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:16:56 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:16:59 - INFO - __main__ - Global step 3000 Train loss 0.37 Classification-F1 0.22218833138461017 on epoch=249
06/15/2022 16:16:59 - INFO - __main__ - save last model!
06/15/2022 16:16:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 16:16:59 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 16:16:59 - INFO - __main__ - Printing 3 examples
06/15/2022 16:16:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 16:16:59 - INFO - __main__ - ['contradiction']
06/15/2022 16:16:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 16:16:59 - INFO - __main__ - ['entailment']
06/15/2022 16:16:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 16:16:59 - INFO - __main__ - ['contradiction']
06/15/2022 16:16:59 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:17:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:17:01 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 16:17:12 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:17:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:17:13 - INFO - __main__ - Starting training!
06/15/2022 16:17:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_21_0.4_8_predictions.txt
06/15/2022 16:17:30 - INFO - __main__ - Classification-F1 on test data: 0.1934
06/15/2022 16:17:31 - INFO - __main__ - prefix=anli_64_21, lr=0.4, bsz=8, dev_performance=0.3277479723539181, test_performance=0.19342346718480138
06/15/2022 16:17:31 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.3, bsz=8 ...
06/15/2022 16:17:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:17:32 - INFO - __main__ - Printing 3 examples
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:17:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:17:32 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:17:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:17:32 - INFO - __main__ - Printing 3 examples
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 16:17:32 - INFO - __main__ - ['entailment']
06/15/2022 16:17:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:17:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:17:32 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:17:51 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:17:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:17:52 - INFO - __main__ - Starting training!
06/15/2022 16:17:55 - INFO - __main__ - Step 10 Global step 10 Train loss 0.91 on epoch=0
06/15/2022 16:17:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=1
06/15/2022 16:18:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=2
06/15/2022 16:18:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=3
06/15/2022 16:18:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=4
06/15/2022 16:18:11 - INFO - __main__ - Global step 50 Train loss 0.66 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 16:18:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 16:18:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.62 on epoch=4
06/15/2022 16:18:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=5
06/15/2022 16:18:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.52 on epoch=6
06/15/2022 16:18:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.50 on epoch=7
06/15/2022 16:18:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.51 on epoch=8
06/15/2022 16:18:30 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.20655998184088067 on epoch=8
06/15/2022 16:18:30 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20655998184088067 on epoch=8, global_step=100
06/15/2022 16:18:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=9
06/15/2022 16:18:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.52 on epoch=9
06/15/2022 16:18:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=10
06/15/2022 16:18:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=11
06/15/2022 16:18:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=12
06/15/2022 16:18:49 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 16:18:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=13
06/15/2022 16:18:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.45 on epoch=14
06/15/2022 16:18:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=14
06/15/2022 16:19:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=15
06/15/2022 16:19:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.45 on epoch=16
06/15/2022 16:19:08 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 16:19:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 16:19:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=18
06/15/2022 16:19:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=19
06/15/2022 16:19:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=19
06/15/2022 16:19:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.53 on epoch=20
06/15/2022 16:19:27 - INFO - __main__ - Global step 250 Train loss 0.49 Classification-F1 0.2743762385944273 on epoch=20
06/15/2022 16:19:27 - INFO - __main__ - Saving model with best Classification-F1: 0.20655998184088067 -> 0.2743762385944273 on epoch=20, global_step=250
06/15/2022 16:19:29 - INFO - __main__ - Step 260 Global step 260 Train loss 0.47 on epoch=21
06/15/2022 16:19:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
06/15/2022 16:19:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.50 on epoch=23
06/15/2022 16:19:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.45 on epoch=24
06/15/2022 16:19:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=24
06/15/2022 16:19:46 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 16:19:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
06/15/2022 16:19:51 - INFO - __main__ - Step 320 Global step 320 Train loss 0.47 on epoch=26
06/15/2022 16:19:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=27
06/15/2022 16:19:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=28
06/15/2022 16:19:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=29
06/15/2022 16:20:04 - INFO - __main__ - Global step 350 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 16:20:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.45 on epoch=29
06/15/2022 16:20:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.50 on epoch=30
06/15/2022 16:20:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.47 on epoch=31
06/15/2022 16:20:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.50 on epoch=32
06/15/2022 16:20:18 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=33
06/15/2022 16:20:23 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 16:20:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.46 on epoch=34
06/15/2022 16:20:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.49 on epoch=34
06/15/2022 16:20:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=35
06/15/2022 16:20:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=36
06/15/2022 16:20:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.50 on epoch=37
06/15/2022 16:20:42 - INFO - __main__ - Global step 450 Train loss 0.47 Classification-F1 0.24652494521490154 on epoch=37
06/15/2022 16:20:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
06/15/2022 16:20:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.49 on epoch=39
06/15/2022 16:20:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.42 on epoch=39
06/15/2022 16:20:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.47 on epoch=40
06/15/2022 16:20:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.42 on epoch=41
06/15/2022 16:21:01 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 16:21:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.51 on epoch=42
06/15/2022 16:21:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=43
06/15/2022 16:21:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=44
06/15/2022 16:21:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
06/15/2022 16:21:14 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=45
06/15/2022 16:21:20 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.30199294742263955 on epoch=45
06/15/2022 16:21:20 - INFO - __main__ - Saving model with best Classification-F1: 0.2743762385944273 -> 0.30199294742263955 on epoch=45, global_step=550
06/15/2022 16:21:23 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=46
06/15/2022 16:21:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
06/15/2022 16:21:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 16:21:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=49
06/15/2022 16:21:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.43 on epoch=49
06/15/2022 16:21:39 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 16:21:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.48 on epoch=50
06/15/2022 16:21:44 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
06/15/2022 16:21:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=52
06/15/2022 16:21:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.48 on epoch=53
06/15/2022 16:21:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
06/15/2022 16:21:58 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 16:22:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
06/15/2022 16:22:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=55
06/15/2022 16:22:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.41 on epoch=56
06/15/2022 16:22:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
06/15/2022 16:22:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=58
06/15/2022 16:22:17 - INFO - __main__ - Global step 700 Train loss 0.46 Classification-F1 0.27146464646464646 on epoch=58
06/15/2022 16:22:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=59
06/15/2022 16:22:22 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 16:22:25 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=60
06/15/2022 16:22:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
06/15/2022 16:22:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=62
06/15/2022 16:22:35 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 16:22:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
06/15/2022 16:22:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=64
06/15/2022 16:22:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
06/15/2022 16:22:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
06/15/2022 16:22:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=66
06/15/2022 16:22:54 - INFO - __main__ - Global step 800 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 16:22:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
06/15/2022 16:23:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=68
06/15/2022 16:23:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=69
06/15/2022 16:23:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=69
06/15/2022 16:23:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 16:23:13 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 16:23:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=71
06/15/2022 16:23:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
06/15/2022 16:23:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
06/15/2022 16:23:24 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=74
06/15/2022 16:23:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.46 on epoch=74
06/15/2022 16:23:32 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 16:23:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=75
06/15/2022 16:23:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.40 on epoch=76
06/15/2022 16:23:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=77
06/15/2022 16:23:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 16:23:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=79
06/15/2022 16:23:51 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.19631618453721345 on epoch=79
06/15/2022 16:23:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=79
06/15/2022 16:23:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=80
06/15/2022 16:23:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=81
06/15/2022 16:24:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
06/15/2022 16:24:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=83
06/15/2022 16:24:10 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.34684095860566444 on epoch=83
06/15/2022 16:24:10 - INFO - __main__ - Saving model with best Classification-F1: 0.30199294742263955 -> 0.34684095860566444 on epoch=83, global_step=1000
06/15/2022 16:24:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
06/15/2022 16:24:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=84
06/15/2022 16:24:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=85
06/15/2022 16:24:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.40 on epoch=86
06/15/2022 16:24:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=87
06/15/2022 16:24:29 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 16:24:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=88
06/15/2022 16:24:34 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.41 on epoch=89
06/15/2022 16:24:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
06/15/2022 16:24:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=90
06/15/2022 16:24:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=91
06/15/2022 16:24:48 - INFO - __main__ - Global step 1100 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 16:24:50 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.44 on epoch=92
06/15/2022 16:24:53 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=93
06/15/2022 16:24:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=94
06/15/2022 16:24:58 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=94
06/15/2022 16:25:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.40 on epoch=95
06/15/2022 16:25:07 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.22608695652173916 on epoch=95
06/15/2022 16:25:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.42 on epoch=96
06/15/2022 16:25:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
06/15/2022 16:25:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
06/15/2022 16:25:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
06/15/2022 16:25:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=99
06/15/2022 16:25:26 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.16600790513833993 on epoch=99
06/15/2022 16:25:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.43 on epoch=100
06/15/2022 16:25:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.44 on epoch=101
06/15/2022 16:25:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.43 on epoch=102
06/15/2022 16:25:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=103
06/15/2022 16:25:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=104
06/15/2022 16:25:45 - INFO - __main__ - Global step 1250 Train loss 0.43 Classification-F1 0.20508383997121557 on epoch=104
06/15/2022 16:25:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=104
06/15/2022 16:25:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.46 on epoch=105
06/15/2022 16:25:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=106
06/15/2022 16:25:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=107
06/15/2022 16:25:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 16:26:04 - INFO - __main__ - Global step 1300 Train loss 0.43 Classification-F1 0.3029951690821256 on epoch=108
06/15/2022 16:26:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=109
06/15/2022 16:26:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.43 on epoch=109
06/15/2022 16:26:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=110
06/15/2022 16:26:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
06/15/2022 16:26:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.44 on epoch=112
06/15/2022 16:26:23 - INFO - __main__ - Global step 1350 Train loss 0.42 Classification-F1 0.3345099345099345 on epoch=112
06/15/2022 16:26:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
06/15/2022 16:26:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=114
06/15/2022 16:26:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=114
06/15/2022 16:26:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.45 on epoch=115
06/15/2022 16:26:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=116
06/15/2022 16:26:42 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.16535433070866143 on epoch=116
06/15/2022 16:26:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
06/15/2022 16:26:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=118
06/15/2022 16:26:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=119
06/15/2022 16:26:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=119
06/15/2022 16:26:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.39 on epoch=120
06/15/2022 16:27:01 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.33339067910628817 on epoch=120
06/15/2022 16:27:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=121
06/15/2022 16:27:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=122
06/15/2022 16:27:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
06/15/2022 16:27:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=124
06/15/2022 16:27:14 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=124
06/15/2022 16:27:20 - INFO - __main__ - Global step 1500 Train loss 0.40 Classification-F1 0.16666666666666666 on epoch=124
06/15/2022 16:27:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.42 on epoch=125
06/15/2022 16:27:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.39 on epoch=126
06/15/2022 16:27:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=127
06/15/2022 16:27:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
06/15/2022 16:27:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
06/15/2022 16:27:39 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.3276069793482728 on epoch=129
06/15/2022 16:27:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=129
06/15/2022 16:27:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=130
06/15/2022 16:27:47 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=131
06/15/2022 16:27:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=132
06/15/2022 16:27:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
06/15/2022 16:27:58 - INFO - __main__ - Global step 1600 Train loss 0.43 Classification-F1 0.2655442837614217 on epoch=133
06/15/2022 16:28:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
06/15/2022 16:28:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=134
06/15/2022 16:28:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=135
06/15/2022 16:28:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=136
06/15/2022 16:28:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
06/15/2022 16:28:17 - INFO - __main__ - Global step 1650 Train loss 0.42 Classification-F1 0.1990221455277538 on epoch=137
06/15/2022 16:28:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
06/15/2022 16:28:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=139
06/15/2022 16:28:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=139
06/15/2022 16:28:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=140
06/15/2022 16:28:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=141
06/15/2022 16:28:36 - INFO - __main__ - Global step 1700 Train loss 0.41 Classification-F1 0.1775766716943188 on epoch=141
06/15/2022 16:28:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=142
06/15/2022 16:28:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
06/15/2022 16:28:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=144
06/15/2022 16:28:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=144
06/15/2022 16:28:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
06/15/2022 16:28:56 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.1929897743218085 on epoch=145
06/15/2022 16:28:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=146
06/15/2022 16:29:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
06/15/2022 16:29:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 16:29:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.42 on epoch=149
06/15/2022 16:29:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
06/15/2022 16:29:15 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.17632850241545894 on epoch=149
06/15/2022 16:29:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=150
06/15/2022 16:29:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=151
06/15/2022 16:29:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=152
06/15/2022 16:29:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.41 on epoch=153
06/15/2022 16:29:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=154
06/15/2022 16:29:34 - INFO - __main__ - Global step 1850 Train loss 0.40 Classification-F1 0.2517127553562531 on epoch=154
06/15/2022 16:29:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=154
06/15/2022 16:29:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=155
06/15/2022 16:29:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
06/15/2022 16:29:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.41 on epoch=157
06/15/2022 16:29:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
06/15/2022 16:29:53 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.28646470560729614 on epoch=158
06/15/2022 16:29:55 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=159
06/15/2022 16:29:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=159
06/15/2022 16:30:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
06/15/2022 16:30:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.41 on epoch=161
06/15/2022 16:30:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=162
06/15/2022 16:30:11 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.21529847596412965 on epoch=162
06/15/2022 16:30:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=163
06/15/2022 16:30:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=164
06/15/2022 16:30:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=164
06/15/2022 16:30:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.41 on epoch=165
06/15/2022 16:30:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
06/15/2022 16:30:31 - INFO - __main__ - Global step 2000 Train loss 0.40 Classification-F1 0.18693852042809156 on epoch=166
06/15/2022 16:30:33 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
06/15/2022 16:30:36 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.41 on epoch=168
06/15/2022 16:30:38 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
06/15/2022 16:30:41 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=169
06/15/2022 16:30:44 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.41 on epoch=170
06/15/2022 16:30:49 - INFO - __main__ - Global step 2050 Train loss 0.42 Classification-F1 0.3192759232381874 on epoch=170
06/15/2022 16:30:52 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.33 on epoch=171
06/15/2022 16:30:55 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.36 on epoch=172
06/15/2022 16:30:57 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.41 on epoch=173
06/15/2022 16:31:00 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=174
06/15/2022 16:31:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.38 on epoch=174
06/15/2022 16:31:08 - INFO - __main__ - Global step 2100 Train loss 0.38 Classification-F1 0.22218833138461017 on epoch=174
06/15/2022 16:31:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.38 on epoch=175
06/15/2022 16:31:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
06/15/2022 16:31:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
06/15/2022 16:31:18 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.38 on epoch=178
06/15/2022 16:31:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.39 on epoch=179
06/15/2022 16:31:26 - INFO - __main__ - Global step 2150 Train loss 0.39 Classification-F1 0.36886608015640276 on epoch=179
06/15/2022 16:31:26 - INFO - __main__ - Saving model with best Classification-F1: 0.34684095860566444 -> 0.36886608015640276 on epoch=179, global_step=2150
06/15/2022 16:31:29 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.41 on epoch=179
06/15/2022 16:31:31 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
06/15/2022 16:31:34 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.38 on epoch=181
06/15/2022 16:31:37 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.35 on epoch=182
06/15/2022 16:31:39 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.37 on epoch=183
06/15/2022 16:31:45 - INFO - __main__ - Global step 2200 Train loss 0.38 Classification-F1 0.3804179262487956 on epoch=183
06/15/2022 16:31:45 - INFO - __main__ - Saving model with best Classification-F1: 0.36886608015640276 -> 0.3804179262487956 on epoch=183, global_step=2200
06/15/2022 16:31:47 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.39 on epoch=184
06/15/2022 16:31:50 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.36 on epoch=184
06/15/2022 16:31:53 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.35 on epoch=185
06/15/2022 16:31:55 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.40 on epoch=186
06/15/2022 16:31:58 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.40 on epoch=187
06/15/2022 16:32:03 - INFO - __main__ - Global step 2250 Train loss 0.38 Classification-F1 0.30843599033816427 on epoch=187
06/15/2022 16:32:06 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.38 on epoch=188
06/15/2022 16:32:09 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.40 on epoch=189
06/15/2022 16:32:11 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
06/15/2022 16:32:14 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=190
06/15/2022 16:32:16 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.37 on epoch=191
06/15/2022 16:32:22 - INFO - __main__ - Global step 2300 Train loss 0.39 Classification-F1 0.2851004834392105 on epoch=191
06/15/2022 16:32:25 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.37 on epoch=192
06/15/2022 16:32:27 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.40 on epoch=193
06/15/2022 16:32:30 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.38 on epoch=194
06/15/2022 16:32:32 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=194
06/15/2022 16:32:35 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.35 on epoch=195
06/15/2022 16:32:41 - INFO - __main__ - Global step 2350 Train loss 0.38 Classification-F1 0.36038507178083695 on epoch=195
06/15/2022 16:32:43 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=196
06/15/2022 16:32:46 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.34 on epoch=197
06/15/2022 16:32:48 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.37 on epoch=198
06/15/2022 16:32:51 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.33 on epoch=199
06/15/2022 16:32:54 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.39 on epoch=199
06/15/2022 16:32:59 - INFO - __main__ - Global step 2400 Train loss 0.36 Classification-F1 0.2495567375886525 on epoch=199
06/15/2022 16:33:02 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.38 on epoch=200
06/15/2022 16:33:04 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.33 on epoch=201
06/15/2022 16:33:07 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.43 on epoch=202
06/15/2022 16:33:09 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.30 on epoch=203
06/15/2022 16:33:12 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.36 on epoch=204
06/15/2022 16:33:18 - INFO - __main__ - Global step 2450 Train loss 0.36 Classification-F1 0.3117643041986611 on epoch=204
06/15/2022 16:33:20 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.34 on epoch=204
06/15/2022 16:33:23 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.34 on epoch=205
06/15/2022 16:33:25 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.33 on epoch=206
06/15/2022 16:33:28 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.36 on epoch=207
06/15/2022 16:33:31 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.32 on epoch=208
06/15/2022 16:33:36 - INFO - __main__ - Global step 2500 Train loss 0.34 Classification-F1 0.3357538384092626 on epoch=208
06/15/2022 16:33:39 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.33 on epoch=209
06/15/2022 16:33:42 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.41 on epoch=209
06/15/2022 16:33:44 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.37 on epoch=210
06/15/2022 16:33:47 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.34 on epoch=211
06/15/2022 16:33:49 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.33 on epoch=212
06/15/2022 16:33:55 - INFO - __main__ - Global step 2550 Train loss 0.36 Classification-F1 0.3410037435862399 on epoch=212
06/15/2022 16:33:57 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.29 on epoch=213
06/15/2022 16:34:00 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.35 on epoch=214
06/15/2022 16:34:03 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.31 on epoch=214
06/15/2022 16:34:05 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=215
06/15/2022 16:34:08 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.34 on epoch=216
06/15/2022 16:34:13 - INFO - __main__ - Global step 2600 Train loss 0.33 Classification-F1 0.3104086845466156 on epoch=216
06/15/2022 16:34:16 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.30 on epoch=217
06/15/2022 16:34:19 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.28 on epoch=218
06/15/2022 16:34:21 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.30 on epoch=219
06/15/2022 16:34:24 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.29 on epoch=219
06/15/2022 16:34:27 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.38 on epoch=220
06/15/2022 16:34:32 - INFO - __main__ - Global step 2650 Train loss 0.31 Classification-F1 0.34386098126221043 on epoch=220
06/15/2022 16:34:35 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.27 on epoch=221
06/15/2022 16:34:37 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.29 on epoch=222
06/15/2022 16:34:40 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.29 on epoch=223
06/15/2022 16:34:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.28 on epoch=224
06/15/2022 16:34:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.33 on epoch=224
06/15/2022 16:34:51 - INFO - __main__ - Global step 2700 Train loss 0.29 Classification-F1 0.2974065508312084 on epoch=224
06/15/2022 16:34:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.25 on epoch=225
06/15/2022 16:34:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.27 on epoch=226
06/15/2022 16:34:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.34 on epoch=227
06/15/2022 16:35:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.31 on epoch=228
06/15/2022 16:35:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.27 on epoch=229
06/15/2022 16:35:09 - INFO - __main__ - Global step 2750 Train loss 0.29 Classification-F1 0.30040829359825777 on epoch=229
06/15/2022 16:35:12 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.28 on epoch=229
06/15/2022 16:35:14 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.28 on epoch=230
06/15/2022 16:35:17 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.34 on epoch=231
06/15/2022 16:35:19 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.24 on epoch=232
06/15/2022 16:35:22 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.27 on epoch=233
06/15/2022 16:35:28 - INFO - __main__ - Global step 2800 Train loss 0.28 Classification-F1 0.3121353508422638 on epoch=233
06/15/2022 16:35:30 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.22 on epoch=234
06/15/2022 16:35:33 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.27 on epoch=234
06/15/2022 16:35:35 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.27 on epoch=235
06/15/2022 16:35:38 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.32 on epoch=236
06/15/2022 16:35:41 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.25 on epoch=237
06/15/2022 16:35:46 - INFO - __main__ - Global step 2850 Train loss 0.27 Classification-F1 0.33116270574464557 on epoch=237
06/15/2022 16:35:49 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.23 on epoch=238
06/15/2022 16:35:51 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.23 on epoch=239
06/15/2022 16:35:54 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.29 on epoch=239
06/15/2022 16:35:57 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.24 on epoch=240
06/15/2022 16:35:59 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.30 on epoch=241
06/15/2022 16:36:05 - INFO - __main__ - Global step 2900 Train loss 0.26 Classification-F1 0.3712361712361712 on epoch=241
06/15/2022 16:36:07 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.27 on epoch=242
06/15/2022 16:36:10 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.24 on epoch=243
06/15/2022 16:36:12 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.34 on epoch=244
06/15/2022 16:36:15 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.24 on epoch=244
06/15/2022 16:36:18 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.22 on epoch=245
06/15/2022 16:36:23 - INFO - __main__ - Global step 2950 Train loss 0.26 Classification-F1 0.35500803697006234 on epoch=245
06/15/2022 16:36:26 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.23 on epoch=246
06/15/2022 16:36:28 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.25 on epoch=247
06/15/2022 16:36:31 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.26 on epoch=248
06/15/2022 16:36:34 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.31 on epoch=249
06/15/2022 16:36:36 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.25 on epoch=249
06/15/2022 16:36:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:36:38 - INFO - __main__ - Printing 3 examples
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:36:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:36:38 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:36:38 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:36:38 - INFO - __main__ - Printing 3 examples
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 16:36:38 - INFO - __main__ - ['entailment']
06/15/2022 16:36:38 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:36:38 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:36:38 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:36:42 - INFO - __main__ - Global step 3000 Train loss 0.26 Classification-F1 0.2953478786911837 on epoch=249
06/15/2022 16:36:42 - INFO - __main__ - save last model!
06/15/2022 16:36:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 16:36:42 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 16:36:42 - INFO - __main__ - Printing 3 examples
06/15/2022 16:36:42 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 16:36:42 - INFO - __main__ - ['contradiction']
06/15/2022 16:36:42 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 16:36:42 - INFO - __main__ - ['entailment']
06/15/2022 16:36:42 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 16:36:42 - INFO - __main__ - ['contradiction']
06/15/2022 16:36:42 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:36:42 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:36:43 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 16:36:56 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:36:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:36:57 - INFO - __main__ - Starting training!
06/15/2022 16:37:12 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_21_0.3_8_predictions.txt
06/15/2022 16:37:12 - INFO - __main__ - Classification-F1 on test data: 0.2676
06/15/2022 16:37:12 - INFO - __main__ - prefix=anli_64_21, lr=0.3, bsz=8, dev_performance=0.3804179262487956, test_performance=0.2675733108021769
06/15/2022 16:37:12 - INFO - __main__ - Running ... prefix=anli_64_21, lr=0.2, bsz=8 ...
06/15/2022 16:37:13 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:37:13 - INFO - __main__ - Printing 3 examples
06/15/2022 16:37:13 - INFO - __main__ -  [anli] premise: Bill Lowrey (born January 29, 1963) is an American musical entertainer and banjoist from California. He has been a featured performer or headliner at a variety of jazz festivals around the U.S. for over fifteen years. Lowrey has established himself in the four-string banjo community as one of its key figures as compared to the likes of Sean Moyses, Steve Peterson, and Buddy Wachter. [SEP] hypothesis: Bill Lowrey has played festivals for over a decade.
06/15/2022 16:37:13 - INFO - __main__ - ['entailment']
06/15/2022 16:37:13 - INFO - __main__ -  [anli] premise: Glassroth v. Moore, CV-01-T-1268-N, 229 F. Supp. 2d 1290 (M.D. Ala. 2002), and its companion case Maddox and Howard v. Moore, CV-01-T-1269-N, concern then-Alabama Supreme Court Chief Justice Roy S. Moore and a stone monument of the Ten Commandments in the rotunda of the Alabama Judicial Building in Montgomery, Alabama. [SEP] hypothesis: Justice Roy S. Moore was the Supreme Court Chief in Alabama. 
06/15/2022 16:37:13 - INFO - __main__ - ['entailment']
06/15/2022 16:37:13 - INFO - __main__ -  [anli] premise: 5...GO is an album by South Korean rock band F.T. Island. It was released on 13 May 2015. The album was released to celebrate the band's fifth anniversary in Japan. The title track "Primavera" is a collaboration with Japanese rock singer Takahiro Moriuchi from One Ok Rock. [SEP] hypothesis: 5 GO is a rock album.
06/15/2022 16:37:13 - INFO - __main__ - ['entailment']
06/15/2022 16:37:13 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:37:13 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:37:13 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:37:13 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:37:14 - INFO - __main__ - Printing 3 examples
06/15/2022 16:37:14 - INFO - __main__ -  [anli] premise: The city's name derives from the Greek words "άργυρος" ("árgyros" meaning "silver") and "πόλη" ("poli" meaning "city"). The name's older form was "Argyroupolis". The first name of the settlement was "New Argyroupolis", given by the refugees from Gümüşhane. [SEP] hypothesis: The city's name derives from the Greek words.
06/15/2022 16:37:14 - INFO - __main__ - ['entailment']
06/15/2022 16:37:14 - INFO - __main__ -  [anli] premise: Operation Epsilon was the codename of a program in which Allied forces near the end of World War II detained ten German scientists who were thought to have worked on Nazi Germany's nuclear program. The scientists were captured between May 1 and June 30, 1945, as part of the Allied Alsos Mission, mainly as part of its Operation Big sweep through southwestern Germany. [SEP] hypothesis: Ten scientists were captured between May 1 and June 30, 1945.
06/15/2022 16:37:14 - INFO - __main__ - ['entailment']
06/15/2022 16:37:14 - INFO - __main__ -  [anli] premise: "No Words" is a song written by Paul McCartney and Denny Laine, and first released on 7 December 1973 on "Band on the Run" by Paul McCartney and Wings. The song was Laine's first co-writing on a Wings album and his only writing credit on "Band on the Run". [SEP] hypothesis: No Words was released in 1973
06/15/2022 16:37:14 - INFO - __main__ - ['entailment']
06/15/2022 16:37:14 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:37:14 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:37:14 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:37:32 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:37:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:37:33 - INFO - __main__ - Starting training!
06/15/2022 16:37:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.07 on epoch=0
06/15/2022 16:37:39 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=1
06/15/2022 16:37:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.68 on epoch=2
06/15/2022 16:37:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.59 on epoch=3
06/15/2022 16:37:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=4
06/15/2022 16:37:53 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.1975867967269197 on epoch=4
06/15/2022 16:37:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1975867967269197 on epoch=4, global_step=50
06/15/2022 16:37:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.54 on epoch=4
06/15/2022 16:37:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.50 on epoch=5
06/15/2022 16:38:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=6
06/15/2022 16:38:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.64 on epoch=7
06/15/2022 16:38:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=8
06/15/2022 16:38:12 - INFO - __main__ - Global step 100 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 16:38:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.48 on epoch=9
06/15/2022 16:38:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=9
06/15/2022 16:38:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=10
06/15/2022 16:38:22 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=11
06/15/2022 16:38:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.49 on epoch=12
06/15/2022 16:38:31 - INFO - __main__ - Global step 150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 16:38:33 - INFO - __main__ - Step 160 Global step 160 Train loss 0.55 on epoch=13
06/15/2022 16:38:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=14
06/15/2022 16:38:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.49 on epoch=14
06/15/2022 16:38:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
06/15/2022 16:38:44 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
06/15/2022 16:38:49 - INFO - __main__ - Global step 200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 16:38:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 16:38:55 - INFO - __main__ - Step 220 Global step 220 Train loss 0.49 on epoch=18
06/15/2022 16:38:57 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
06/15/2022 16:39:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.49 on epoch=19
06/15/2022 16:39:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
06/15/2022 16:39:08 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 16:39:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=21
06/15/2022 16:39:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
06/15/2022 16:39:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=23
06/15/2022 16:39:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
06/15/2022 16:39:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
06/15/2022 16:39:27 - INFO - __main__ - Global step 300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 16:39:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=25
06/15/2022 16:39:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.45 on epoch=26
06/15/2022 16:39:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=27
06/15/2022 16:39:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.51 on epoch=28
06/15/2022 16:39:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=29
06/15/2022 16:39:46 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.28625688760498585 on epoch=29
06/15/2022 16:39:46 - INFO - __main__ - Saving model with best Classification-F1: 0.1975867967269197 -> 0.28625688760498585 on epoch=29, global_step=350
06/15/2022 16:39:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.49 on epoch=29
06/15/2022 16:39:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=30
06/15/2022 16:39:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
06/15/2022 16:39:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.47 on epoch=32
06/15/2022 16:39:59 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=33
06/15/2022 16:40:05 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.28172757475083055 on epoch=33
06/15/2022 16:40:08 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=34
06/15/2022 16:40:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=34
06/15/2022 16:40:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
06/15/2022 16:40:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=36
06/15/2022 16:40:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
06/15/2022 16:40:24 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 16:40:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=38
06/15/2022 16:40:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.84 on epoch=39
06/15/2022 16:40:32 - INFO - __main__ - Step 480 Global step 480 Train loss 1.49 on epoch=39
06/15/2022 16:40:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.97 on epoch=40
06/15/2022 16:40:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.50 on epoch=41
06/15/2022 16:40:43 - INFO - __main__ - Global step 500 Train loss 0.86 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 16:40:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.44 on epoch=42
06/15/2022 16:40:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=43
06/15/2022 16:40:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.41 on epoch=44
06/15/2022 16:40:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.49 on epoch=44
06/15/2022 16:40:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
06/15/2022 16:41:01 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 16:41:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
06/15/2022 16:41:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=47
06/15/2022 16:41:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.44 on epoch=48
06/15/2022 16:41:12 - INFO - __main__ - Step 590 Global step 590 Train loss 0.72 on epoch=49
06/15/2022 16:41:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.61 on epoch=49
06/15/2022 16:41:20 - INFO - __main__ - Global step 600 Train loss 0.55 Classification-F1 0.17595815389455882 on epoch=49
06/15/2022 16:41:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.70 on epoch=50
06/15/2022 16:41:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.62 on epoch=51
06/15/2022 16:41:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=52
06/15/2022 16:41:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=53
06/15/2022 16:41:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=54
06/15/2022 16:41:39 - INFO - __main__ - Global step 650 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 16:41:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=54
06/15/2022 16:41:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=55
06/15/2022 16:41:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=56
06/15/2022 16:41:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.44 on epoch=57
06/15/2022 16:41:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
06/15/2022 16:41:58 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 16:42:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=59
06/15/2022 16:42:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.53 on epoch=59
06/15/2022 16:42:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=60
06/15/2022 16:42:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
06/15/2022 16:42:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=62
06/15/2022 16:42:17 - INFO - __main__ - Global step 750 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 16:42:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.49 on epoch=63
06/15/2022 16:42:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=64
06/15/2022 16:42:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=64
06/15/2022 16:42:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.42 on epoch=65
06/15/2022 16:42:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.51 on epoch=66
06/15/2022 16:42:36 - INFO - __main__ - Global step 800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 16:42:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.46 on epoch=67
06/15/2022 16:42:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.76 on epoch=68
06/15/2022 16:42:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.99 on epoch=69
06/15/2022 16:42:47 - INFO - __main__ - Step 840 Global step 840 Train loss 2.08 on epoch=69
06/15/2022 16:42:49 - INFO - __main__ - Step 850 Global step 850 Train loss 3.06 on epoch=70
06/15/2022 16:42:55 - INFO - __main__ - Global step 850 Train loss 1.47 Classification-F1 0.18171428571428572 on epoch=70
06/15/2022 16:42:58 - INFO - __main__ - Step 860 Global step 860 Train loss 3.03 on epoch=71
06/15/2022 16:43:00 - INFO - __main__ - Step 870 Global step 870 Train loss 2.53 on epoch=72
06/15/2022 16:43:03 - INFO - __main__ - Step 880 Global step 880 Train loss 1.81 on epoch=73
06/15/2022 16:43:06 - INFO - __main__ - Step 890 Global step 890 Train loss 1.20 on epoch=74
06/15/2022 16:43:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.87 on epoch=74
06/15/2022 16:43:14 - INFO - __main__ - Global step 900 Train loss 1.89 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 16:43:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.81 on epoch=75
06/15/2022 16:43:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.68 on epoch=76
06/15/2022 16:43:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.62 on epoch=77
06/15/2022 16:43:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.84 on epoch=78
06/15/2022 16:43:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.72 on epoch=79
06/15/2022 16:43:33 - INFO - __main__ - Global step 950 Train loss 0.74 Classification-F1 0.27790849673202617 on epoch=79
06/15/2022 16:43:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=79
06/15/2022 16:43:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.56 on epoch=80
06/15/2022 16:43:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.58 on epoch=81
06/15/2022 16:43:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.68 on epoch=82
06/15/2022 16:43:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.52 on epoch=83
06/15/2022 16:43:52 - INFO - __main__ - Global step 1000 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 16:43:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=84
06/15/2022 16:43:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.59 on epoch=84
06/15/2022 16:44:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.57 on epoch=85
06/15/2022 16:44:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=86
06/15/2022 16:44:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.59 on epoch=87
06/15/2022 16:44:11 - INFO - __main__ - Global step 1050 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 16:44:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.60 on epoch=88
06/15/2022 16:44:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.58 on epoch=89
06/15/2022 16:44:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.65 on epoch=89
06/15/2022 16:44:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.61 on epoch=90
06/15/2022 16:44:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.58 on epoch=91
06/15/2022 16:44:30 - INFO - __main__ - Global step 1100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 16:44:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.50 on epoch=92
06/15/2022 16:44:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.54 on epoch=93
06/15/2022 16:44:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.52 on epoch=94
06/15/2022 16:44:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.51 on epoch=94
06/15/2022 16:44:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.54 on epoch=95
06/15/2022 16:44:49 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 16:44:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=96
06/15/2022 16:44:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=97
06/15/2022 16:44:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=98
06/15/2022 16:45:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.56 on epoch=99
06/15/2022 16:45:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.51 on epoch=99
06/15/2022 16:45:08 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 16:45:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.52 on epoch=100
06/15/2022 16:45:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=101
06/15/2022 16:45:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=102
06/15/2022 16:45:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=103
06/15/2022 16:45:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.50 on epoch=104
06/15/2022 16:45:27 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 16:45:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=104
06/15/2022 16:45:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=105
06/15/2022 16:45:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.52 on epoch=106
06/15/2022 16:45:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.56 on epoch=107
06/15/2022 16:45:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.48 on epoch=108
06/15/2022 16:45:46 - INFO - __main__ - Global step 1300 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 16:45:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=109
06/15/2022 16:45:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=109
06/15/2022 16:45:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.45 on epoch=110
06/15/2022 16:45:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.53 on epoch=111
06/15/2022 16:45:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.51 on epoch=112
06/15/2022 16:46:05 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 16:46:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.49 on epoch=113
06/15/2022 16:46:10 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=114
06/15/2022 16:46:13 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.48 on epoch=114
06/15/2022 16:46:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=115
06/15/2022 16:46:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.51 on epoch=116
06/15/2022 16:46:24 - INFO - __main__ - Global step 1400 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=116
06/15/2022 16:46:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.51 on epoch=117
06/15/2022 16:46:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.51 on epoch=118
06/15/2022 16:46:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.51 on epoch=119
06/15/2022 16:46:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=119
06/15/2022 16:46:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.54 on epoch=120
06/15/2022 16:46:43 - INFO - __main__ - Global step 1450 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=120
06/15/2022 16:46:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.51 on epoch=121
06/15/2022 16:46:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=122
06/15/2022 16:46:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=123
06/15/2022 16:46:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.51 on epoch=124
06/15/2022 16:46:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.50 on epoch=124
06/15/2022 16:47:02 - INFO - __main__ - Global step 1500 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=124
06/15/2022 16:47:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.54 on epoch=125
06/15/2022 16:47:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=126
06/15/2022 16:47:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=127
06/15/2022 16:47:12 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.53 on epoch=128
06/15/2022 16:47:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=129
06/15/2022 16:47:21 - INFO - __main__ - Global step 1550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 16:47:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=129
06/15/2022 16:47:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=130
06/15/2022 16:47:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=131
06/15/2022 16:47:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.53 on epoch=132
06/15/2022 16:47:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.50 on epoch=133
06/15/2022 16:47:40 - INFO - __main__ - Global step 1600 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 16:47:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.51 on epoch=134
06/15/2022 16:47:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=134
06/15/2022 16:47:48 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.50 on epoch=135
06/15/2022 16:47:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=136
06/15/2022 16:47:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=137
06/15/2022 16:47:59 - INFO - __main__ - Global step 1650 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 16:48:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.54 on epoch=138
06/15/2022 16:48:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=139
06/15/2022 16:48:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=139
06/15/2022 16:48:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.55 on epoch=140
06/15/2022 16:48:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=141
06/15/2022 16:48:18 - INFO - __main__ - Global step 1700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=141
06/15/2022 16:48:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=142
06/15/2022 16:48:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=143
06/15/2022 16:48:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.48 on epoch=144
06/15/2022 16:48:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.50 on epoch=144
06/15/2022 16:48:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=145
06/15/2022 16:48:37 - INFO - __main__ - Global step 1750 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=145
06/15/2022 16:48:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=146
06/15/2022 16:48:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.49 on epoch=147
06/15/2022 16:48:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.49 on epoch=148
06/15/2022 16:48:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.49 on epoch=149
06/15/2022 16:48:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=149
06/15/2022 16:48:56 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.2476579799012093 on epoch=149
06/15/2022 16:48:59 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=150
06/15/2022 16:49:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=151
06/15/2022 16:49:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.52 on epoch=152
06/15/2022 16:49:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=153
06/15/2022 16:49:10 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=154
06/15/2022 16:49:15 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.24940655166956796 on epoch=154
06/15/2022 16:49:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=154
06/15/2022 16:49:21 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=155
06/15/2022 16:49:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
06/15/2022 16:49:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=157
06/15/2022 16:49:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.52 on epoch=158
06/15/2022 16:49:35 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.2659659181398311 on epoch=158
06/15/2022 16:49:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.48 on epoch=159
06/15/2022 16:49:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=159
06/15/2022 16:49:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=160
06/15/2022 16:49:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=161
06/15/2022 16:49:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.48 on epoch=162
06/15/2022 16:49:54 - INFO - __main__ - Global step 1950 Train loss 0.45 Classification-F1 0.21035669816157623 on epoch=162
06/15/2022 16:49:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.51 on epoch=163
06/15/2022 16:49:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.47 on epoch=164
06/15/2022 16:50:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
06/15/2022 16:50:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=165
06/15/2022 16:50:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=166
06/15/2022 16:50:13 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.19592763495202523 on epoch=166
06/15/2022 16:50:15 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=167
06/15/2022 16:50:18 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.41 on epoch=168
06/15/2022 16:50:21 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.46 on epoch=169
06/15/2022 16:50:23 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.45 on epoch=169
06/15/2022 16:50:26 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.53 on epoch=170
06/15/2022 16:50:32 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.24645030425963488 on epoch=170
06/15/2022 16:50:35 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.49 on epoch=171
06/15/2022 16:50:37 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=172
06/15/2022 16:50:40 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
06/15/2022 16:50:43 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.49 on epoch=174
06/15/2022 16:50:45 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.46 on epoch=174
06/15/2022 16:50:51 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.25648639095086606 on epoch=174
06/15/2022 16:50:54 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.42 on epoch=175
06/15/2022 16:50:57 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
06/15/2022 16:50:59 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.45 on epoch=177
06/15/2022 16:51:02 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=178
06/15/2022 16:51:05 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.45 on epoch=179
06/15/2022 16:51:11 - INFO - __main__ - Global step 2150 Train loss 0.43 Classification-F1 0.2801077337437476 on epoch=179
06/15/2022 16:51:13 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.49 on epoch=179
06/15/2022 16:51:16 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.48 on epoch=180
06/15/2022 16:51:19 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.48 on epoch=181
06/15/2022 16:51:21 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.52 on epoch=182
06/15/2022 16:51:24 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.48 on epoch=183
06/15/2022 16:51:30 - INFO - __main__ - Global step 2200 Train loss 0.49 Classification-F1 0.26236287253236407 on epoch=183
06/15/2022 16:51:32 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.47 on epoch=184
06/15/2022 16:51:35 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=184
06/15/2022 16:51:38 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.45 on epoch=185
06/15/2022 16:51:40 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.50 on epoch=186
06/15/2022 16:51:43 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.48 on epoch=187
06/15/2022 16:51:49 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.2962788012386501 on epoch=187
06/15/2022 16:51:49 - INFO - __main__ - Saving model with best Classification-F1: 0.28625688760498585 -> 0.2962788012386501 on epoch=187, global_step=2250
06/15/2022 16:51:52 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.48 on epoch=188
06/15/2022 16:51:54 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=189
06/15/2022 16:51:57 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.45 on epoch=189
06/15/2022 16:52:00 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.47 on epoch=190
06/15/2022 16:52:03 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=191
06/15/2022 16:52:09 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.2563333921119144 on epoch=191
06/15/2022 16:52:11 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.46 on epoch=192
06/15/2022 16:52:14 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=193
06/15/2022 16:52:17 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=194
06/15/2022 16:52:19 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.54 on epoch=194
06/15/2022 16:52:22 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.47 on epoch=195
06/15/2022 16:52:28 - INFO - __main__ - Global step 2350 Train loss 0.47 Classification-F1 0.2756734006734007 on epoch=195
06/15/2022 16:52:30 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.42 on epoch=196
06/15/2022 16:52:33 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.41 on epoch=197
06/15/2022 16:52:36 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.44 on epoch=198
06/15/2022 16:52:38 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=199
06/15/2022 16:52:41 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.48 on epoch=199
06/15/2022 16:52:47 - INFO - __main__ - Global step 2400 Train loss 0.44 Classification-F1 0.2828609986504724 on epoch=199
06/15/2022 16:52:50 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.47 on epoch=200
06/15/2022 16:52:52 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.42 on epoch=201
06/15/2022 16:52:55 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=202
06/15/2022 16:52:58 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.48 on epoch=203
06/15/2022 16:53:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=204
06/15/2022 16:53:06 - INFO - __main__ - Global step 2450 Train loss 0.45 Classification-F1 0.2777441199833058 on epoch=204
06/15/2022 16:53:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=204
06/15/2022 16:53:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=205
06/15/2022 16:53:14 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=206
06/15/2022 16:53:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
06/15/2022 16:53:19 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.47 on epoch=208
06/15/2022 16:53:25 - INFO - __main__ - Global step 2500 Train loss 0.45 Classification-F1 0.24628471692677886 on epoch=208
06/15/2022 16:53:28 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.54 on epoch=209
06/15/2022 16:53:31 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.51 on epoch=209
06/15/2022 16:53:33 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.47 on epoch=210
06/15/2022 16:53:36 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=211
06/15/2022 16:53:39 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=212
06/15/2022 16:53:44 - INFO - __main__ - Global step 2550 Train loss 0.48 Classification-F1 0.2784313725490196 on epoch=212
06/15/2022 16:53:47 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=213
06/15/2022 16:53:50 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=214
06/15/2022 16:53:53 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.47 on epoch=214
06/15/2022 16:53:55 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.41 on epoch=215
06/15/2022 16:53:58 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=216
06/15/2022 16:54:04 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.2870952713720009 on epoch=216
06/15/2022 16:54:06 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.46 on epoch=217
06/15/2022 16:54:09 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
06/15/2022 16:54:12 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.45 on epoch=219
06/15/2022 16:54:14 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=219
06/15/2022 16:54:17 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.48 on epoch=220
06/15/2022 16:54:23 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.2543972543972544 on epoch=220
06/15/2022 16:54:25 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.42 on epoch=221
06/15/2022 16:54:28 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=222
06/15/2022 16:54:31 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=223
06/15/2022 16:54:33 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=224
06/15/2022 16:54:36 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=224
06/15/2022 16:54:42 - INFO - __main__ - Global step 2700 Train loss 0.44 Classification-F1 0.27102627650477557 on epoch=224
06/15/2022 16:54:44 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.48 on epoch=225
06/15/2022 16:54:47 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.40 on epoch=226
06/15/2022 16:54:50 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.51 on epoch=227
06/15/2022 16:54:52 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=228
06/15/2022 16:54:55 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.42 on epoch=229
06/15/2022 16:55:01 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.24380032206119162 on epoch=229
06/15/2022 16:55:04 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=229
06/15/2022 16:55:06 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.41 on epoch=230
06/15/2022 16:55:09 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.42 on epoch=231
06/15/2022 16:55:12 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
06/15/2022 16:55:14 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=233
06/15/2022 16:55:20 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.2111111111111111 on epoch=233
06/15/2022 16:55:23 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
06/15/2022 16:55:25 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.41 on epoch=234
06/15/2022 16:55:28 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.41 on epoch=235
06/15/2022 16:55:31 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.43 on epoch=236
06/15/2022 16:55:33 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.49 on epoch=237
06/15/2022 16:55:39 - INFO - __main__ - Global step 2850 Train loss 0.43 Classification-F1 0.1836290071584189 on epoch=237
06/15/2022 16:55:42 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
06/15/2022 16:55:44 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.45 on epoch=239
06/15/2022 16:55:47 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.48 on epoch=239
06/15/2022 16:55:50 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.47 on epoch=240
06/15/2022 16:55:52 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=241
06/15/2022 16:55:58 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=241
06/15/2022 16:56:01 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.43 on epoch=242
06/15/2022 16:56:03 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=243
06/15/2022 16:56:06 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.43 on epoch=244
06/15/2022 16:56:09 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=244
06/15/2022 16:56:11 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.43 on epoch=245
06/15/2022 16:56:17 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=245
06/15/2022 16:56:20 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=246
06/15/2022 16:56:22 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.40 on epoch=247
06/15/2022 16:56:25 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.41 on epoch=248
06/15/2022 16:56:28 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=249
06/15/2022 16:56:30 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=249
06/15/2022 16:56:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:56:32 - INFO - __main__ - Printing 3 examples
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:56:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:56:32 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:56:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:56:32 - INFO - __main__ - Printing 3 examples
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 16:56:32 - INFO - __main__ - ['neutral']
06/15/2022 16:56:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:56:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:56:33 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:56:36 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=249
06/15/2022 16:56:36 - INFO - __main__ - save last model!
06/15/2022 16:56:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 16:56:36 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 16:56:36 - INFO - __main__ - Printing 3 examples
06/15/2022 16:56:36 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 16:56:36 - INFO - __main__ - ['contradiction']
06/15/2022 16:56:36 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 16:56:36 - INFO - __main__ - ['entailment']
06/15/2022 16:56:36 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 16:56:36 - INFO - __main__ - ['contradiction']
06/15/2022 16:56:36 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:56:37 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:56:38 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 16:56:50 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:56:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:56:51 - INFO - __main__ - Starting training!
06/15/2022 16:57:08 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_21_0.2_8_predictions.txt
06/15/2022 16:57:08 - INFO - __main__ - Classification-F1 on test data: 0.1665
06/15/2022 16:57:08 - INFO - __main__ - prefix=anli_64_21, lr=0.2, bsz=8, dev_performance=0.2962788012386501, test_performance=0.16654163540885222
06/15/2022 16:57:08 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.5, bsz=8 ...
06/15/2022 16:57:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:57:09 - INFO - __main__ - Printing 3 examples
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 16:57:09 - INFO - __main__ - ['neutral']
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 16:57:09 - INFO - __main__ - ['neutral']
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 16:57:09 - INFO - __main__ - ['neutral']
06/15/2022 16:57:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:57:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:57:09 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 16:57:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 16:57:09 - INFO - __main__ - Printing 3 examples
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 16:57:09 - INFO - __main__ - ['neutral']
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 16:57:09 - INFO - __main__ - ['neutral']
06/15/2022 16:57:09 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 16:57:10 - INFO - __main__ - ['neutral']
06/15/2022 16:57:10 - INFO - __main__ - Tokenizing Input ...
06/15/2022 16:57:10 - INFO - __main__ - Tokenizing Output ...
06/15/2022 16:57:10 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 16:57:28 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 16:57:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 16:57:29 - INFO - __main__ - Starting training!
06/15/2022 16:57:33 - INFO - __main__ - Step 10 Global step 10 Train loss 0.99 on epoch=0
06/15/2022 16:57:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.64 on epoch=1
06/15/2022 16:57:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=2
06/15/2022 16:57:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=3
06/15/2022 16:57:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.57 on epoch=4
06/15/2022 16:57:49 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 16:57:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 16:57:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.65 on epoch=4
06/15/2022 16:57:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.60 on epoch=5
06/15/2022 16:57:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
06/15/2022 16:58:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.65 on epoch=7
06/15/2022 16:58:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=8
06/15/2022 16:58:08 - INFO - __main__ - Global step 100 Train loss 0.61 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 16:58:11 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=9
06/15/2022 16:58:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.60 on epoch=9
06/15/2022 16:58:16 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=10
06/15/2022 16:58:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.58 on epoch=11
06/15/2022 16:58:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
06/15/2022 16:58:27 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 16:58:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.54 on epoch=13
06/15/2022 16:58:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.49 on epoch=14
06/15/2022 16:58:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.62 on epoch=14
06/15/2022 16:58:38 - INFO - __main__ - Step 190 Global step 190 Train loss 0.54 on epoch=15
06/15/2022 16:58:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.58 on epoch=16
06/15/2022 16:58:46 - INFO - __main__ - Global step 200 Train loss 0.55 Classification-F1 0.21170637740198628 on epoch=16
06/15/2022 16:58:46 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21170637740198628 on epoch=16, global_step=200
06/15/2022 16:58:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
06/15/2022 16:58:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=18
06/15/2022 16:58:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.54 on epoch=19
06/15/2022 16:58:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
06/15/2022 16:58:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.56 on epoch=20
06/15/2022 16:59:05 - INFO - __main__ - Global step 250 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 16:59:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.51 on epoch=21
06/15/2022 16:59:10 - INFO - __main__ - Step 270 Global step 270 Train loss 0.51 on epoch=22
06/15/2022 16:59:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.52 on epoch=23
06/15/2022 16:59:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.55 on epoch=24
06/15/2022 16:59:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=24
06/15/2022 16:59:24 - INFO - __main__ - Global step 300 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 16:59:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.51 on epoch=25
06/15/2022 16:59:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.55 on epoch=26
06/15/2022 16:59:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.49 on epoch=27
06/15/2022 16:59:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.44 on epoch=28
06/15/2022 16:59:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=29
06/15/2022 16:59:43 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.17661691542288557 on epoch=29
06/15/2022 16:59:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.54 on epoch=29
06/15/2022 16:59:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=30
06/15/2022 16:59:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.56 on epoch=31
06/15/2022 16:59:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=32
06/15/2022 16:59:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=33
06/15/2022 17:00:02 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 17:00:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=34
06/15/2022 17:00:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=34
06/15/2022 17:00:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
06/15/2022 17:00:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=36
06/15/2022 17:00:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=37
06/15/2022 17:00:21 - INFO - __main__ - Global step 450 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 17:00:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=38
06/15/2022 17:00:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.51 on epoch=39
06/15/2022 17:00:29 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
06/15/2022 17:00:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=40
06/15/2022 17:00:34 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=41
06/15/2022 17:00:40 - INFO - __main__ - Global step 500 Train loss 0.50 Classification-F1 0.27832954409477084 on epoch=41
06/15/2022 17:00:40 - INFO - __main__ - Saving model with best Classification-F1: 0.21170637740198628 -> 0.27832954409477084 on epoch=41, global_step=500
06/15/2022 17:00:43 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
06/15/2022 17:00:45 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=43
06/15/2022 17:00:48 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=44
06/15/2022 17:00:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.50 on epoch=44
06/15/2022 17:00:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.55 on epoch=45
06/15/2022 17:00:59 - INFO - __main__ - Global step 550 Train loss 0.51 Classification-F1 0.2765128205128205 on epoch=45
06/15/2022 17:01:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=46
06/15/2022 17:01:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.53 on epoch=47
06/15/2022 17:01:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.49 on epoch=48
06/15/2022 17:01:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=49
06/15/2022 17:01:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=49
06/15/2022 17:01:18 - INFO - __main__ - Global step 600 Train loss 0.51 Classification-F1 0.18892001244942422 on epoch=49
06/15/2022 17:01:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.49 on epoch=50
06/15/2022 17:01:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=51
06/15/2022 17:01:26 - INFO - __main__ - Step 630 Global step 630 Train loss 0.54 on epoch=52
06/15/2022 17:01:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 17:01:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.46 on epoch=54
06/15/2022 17:01:37 - INFO - __main__ - Global step 650 Train loss 0.50 Classification-F1 0.16732026143790854 on epoch=54
06/15/2022 17:01:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
06/15/2022 17:01:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=55
06/15/2022 17:01:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.47 on epoch=56
06/15/2022 17:01:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.51 on epoch=57
06/15/2022 17:01:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.44 on epoch=58
06/15/2022 17:01:57 - INFO - __main__ - Global step 700 Train loss 0.48 Classification-F1 0.29284750337381915 on epoch=58
06/15/2022 17:01:57 - INFO - __main__ - Saving model with best Classification-F1: 0.27832954409477084 -> 0.29284750337381915 on epoch=58, global_step=700
06/15/2022 17:01:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.46 on epoch=59
06/15/2022 17:02:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=59
06/15/2022 17:02:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=60
06/15/2022 17:02:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
06/15/2022 17:02:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=62
06/15/2022 17:02:15 - INFO - __main__ - Global step 750 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 17:02:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=63
06/15/2022 17:02:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=64
06/15/2022 17:02:24 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=64
06/15/2022 17:02:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=65
06/15/2022 17:02:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
06/15/2022 17:02:35 - INFO - __main__ - Global step 800 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 17:02:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=67
06/15/2022 17:02:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.41 on epoch=68
06/15/2022 17:02:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=69
06/15/2022 17:02:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=69
06/15/2022 17:02:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=70
06/15/2022 17:02:54 - INFO - __main__ - Global step 850 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 17:02:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=71
06/15/2022 17:03:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.47 on epoch=72
06/15/2022 17:03:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
06/15/2022 17:03:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=74
06/15/2022 17:03:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=74
06/15/2022 17:03:13 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.23772204806687566 on epoch=74
06/15/2022 17:03:16 - INFO - __main__ - Step 910 Global step 910 Train loss 0.53 on epoch=75
06/15/2022 17:03:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=76
06/15/2022 17:03:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.46 on epoch=77
06/15/2022 17:03:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=78
06/15/2022 17:03:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=79
06/15/2022 17:03:32 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.32064777327935223 on epoch=79
06/15/2022 17:03:32 - INFO - __main__ - Saving model with best Classification-F1: 0.29284750337381915 -> 0.32064777327935223 on epoch=79, global_step=950
06/15/2022 17:03:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=79
06/15/2022 17:03:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=80
06/15/2022 17:03:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.49 on epoch=81
06/15/2022 17:03:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=82
06/15/2022 17:03:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
06/15/2022 17:03:52 - INFO - __main__ - Global step 1000 Train loss 0.48 Classification-F1 0.23808032451941 on epoch=83
06/15/2022 17:03:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=84
06/15/2022 17:03:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=84
06/15/2022 17:03:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.51 on epoch=85
06/15/2022 17:04:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.51 on epoch=86
06/15/2022 17:04:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.49 on epoch=87
06/15/2022 17:04:11 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.1824970131421744 on epoch=87
06/15/2022 17:04:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=88
06/15/2022 17:04:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.45 on epoch=89
06/15/2022 17:04:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
06/15/2022 17:04:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.45 on epoch=90
06/15/2022 17:04:24 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.47 on epoch=91
06/15/2022 17:04:27 - INFO - __main__ - Global step 1100 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 17:04:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=92
06/15/2022 17:04:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.44 on epoch=93
06/15/2022 17:04:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=94
06/15/2022 17:04:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=94
06/15/2022 17:04:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.52 on epoch=95
06/15/2022 17:04:47 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.18134226379506133 on epoch=95
06/15/2022 17:04:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.47 on epoch=96
06/15/2022 17:04:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=97
06/15/2022 17:04:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.48 on epoch=98
06/15/2022 17:04:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
06/15/2022 17:05:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.47 on epoch=99
06/15/2022 17:05:06 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.22491039426523296 on epoch=99
06/15/2022 17:05:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=100
06/15/2022 17:05:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=101
06/15/2022 17:05:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.47 on epoch=102
06/15/2022 17:05:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
06/15/2022 17:05:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.48 on epoch=104
06/15/2022 17:05:25 - INFO - __main__ - Global step 1250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 17:05:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=104
06/15/2022 17:05:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=105
06/15/2022 17:05:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=106
06/15/2022 17:05:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=107
06/15/2022 17:05:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 17:05:44 - INFO - __main__ - Global step 1300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 17:05:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=109
06/15/2022 17:05:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.47 on epoch=109
06/15/2022 17:05:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=110
06/15/2022 17:05:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.43 on epoch=111
06/15/2022 17:05:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=112
06/15/2022 17:06:01 - INFO - __main__ - Global step 1350 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 17:06:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=113
06/15/2022 17:06:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=114
06/15/2022 17:06:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=114
06/15/2022 17:06:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.49 on epoch=115
06/15/2022 17:06:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=116
06/15/2022 17:06:21 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.16732026143790854 on epoch=116
06/15/2022 17:06:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.46 on epoch=117
06/15/2022 17:06:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=118
06/15/2022 17:06:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.45 on epoch=119
06/15/2022 17:06:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.46 on epoch=119
06/15/2022 17:06:34 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=120
06/15/2022 17:06:40 - INFO - __main__ - Global step 1450 Train loss 0.46 Classification-F1 0.2623592330101309 on epoch=120
06/15/2022 17:06:43 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=121
06/15/2022 17:06:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=122
06/15/2022 17:06:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=123
06/15/2022 17:06:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.48 on epoch=124
06/15/2022 17:06:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.47 on epoch=124
06/15/2022 17:06:59 - INFO - __main__ - Global step 1500 Train loss 0.48 Classification-F1 0.19631618453721345 on epoch=124
06/15/2022 17:07:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=125
06/15/2022 17:07:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.45 on epoch=126
06/15/2022 17:07:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=127
06/15/2022 17:07:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=128
06/15/2022 17:07:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.48 on epoch=129
06/15/2022 17:07:19 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 17:07:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.44 on epoch=129
06/15/2022 17:07:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.47 on epoch=130
06/15/2022 17:07:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=131
06/15/2022 17:07:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.47 on epoch=132
06/15/2022 17:07:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.45 on epoch=133
06/15/2022 17:07:38 - INFO - __main__ - Global step 1600 Train loss 0.46 Classification-F1 0.2940447180539523 on epoch=133
06/15/2022 17:07:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=134
06/15/2022 17:07:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.49 on epoch=134
06/15/2022 17:07:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.50 on epoch=135
06/15/2022 17:07:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=136
06/15/2022 17:07:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=137
06/15/2022 17:07:57 - INFO - __main__ - Global step 1650 Train loss 0.49 Classification-F1 0.225 on epoch=137
06/15/2022 17:08:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=138
06/15/2022 17:08:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.51 on epoch=139
06/15/2022 17:08:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=139
06/15/2022 17:08:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=140
06/15/2022 17:08:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=141
06/15/2022 17:08:16 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=141
06/15/2022 17:08:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=142
06/15/2022 17:08:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
06/15/2022 17:08:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=144
06/15/2022 17:08:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.47 on epoch=144
06/15/2022 17:08:30 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=145
06/15/2022 17:08:36 - INFO - __main__ - Global step 1750 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=145
06/15/2022 17:08:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=146
06/15/2022 17:08:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=147
06/15/2022 17:08:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=148
06/15/2022 17:08:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.48 on epoch=149
06/15/2022 17:08:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.49 on epoch=149
06/15/2022 17:08:55 - INFO - __main__ - Global step 1800 Train loss 0.46 Classification-F1 0.2805776598880047 on epoch=149
06/15/2022 17:08:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.45 on epoch=150
06/15/2022 17:09:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=151
06/15/2022 17:09:03 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.48 on epoch=152
06/15/2022 17:09:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.40 on epoch=153
06/15/2022 17:09:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=154
06/15/2022 17:09:14 - INFO - __main__ - Global step 1850 Train loss 0.46 Classification-F1 0.2112454737992758 on epoch=154
06/15/2022 17:09:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.49 on epoch=154
06/15/2022 17:09:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=155
06/15/2022 17:09:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.51 on epoch=156
06/15/2022 17:09:25 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=157
06/15/2022 17:09:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=158
06/15/2022 17:09:33 - INFO - __main__ - Global step 1900 Train loss 0.50 Classification-F1 0.23197492163009403 on epoch=158
06/15/2022 17:09:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.49 on epoch=159
06/15/2022 17:09:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.44 on epoch=159
06/15/2022 17:09:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.52 on epoch=160
06/15/2022 17:09:44 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.44 on epoch=161
06/15/2022 17:09:46 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=162
06/15/2022 17:09:52 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.1965714285714286 on epoch=162
06/15/2022 17:09:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
06/15/2022 17:09:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=164
06/15/2022 17:10:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
06/15/2022 17:10:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 17:10:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.51 on epoch=166
06/15/2022 17:10:11 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.25757575757575757 on epoch=166
06/15/2022 17:10:14 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.50 on epoch=167
06/15/2022 17:10:17 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
06/15/2022 17:10:20 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.50 on epoch=169
06/15/2022 17:10:22 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=169
06/15/2022 17:10:25 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.49 on epoch=170
06/15/2022 17:10:31 - INFO - __main__ - Global step 2050 Train loss 0.48 Classification-F1 0.24926792428469235 on epoch=170
06/15/2022 17:10:33 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.47 on epoch=171
06/15/2022 17:10:36 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.43 on epoch=172
06/15/2022 17:10:39 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.50 on epoch=173
06/15/2022 17:10:42 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.46 on epoch=174
06/15/2022 17:10:44 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.43 on epoch=174
06/15/2022 17:10:50 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.32815762668703846 on epoch=174
06/15/2022 17:10:50 - INFO - __main__ - Saving model with best Classification-F1: 0.32064777327935223 -> 0.32815762668703846 on epoch=174, global_step=2100
06/15/2022 17:10:53 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.47 on epoch=175
06/15/2022 17:10:55 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.47 on epoch=176
06/15/2022 17:10:58 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.51 on epoch=177
06/15/2022 17:11:01 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.44 on epoch=178
06/15/2022 17:11:04 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.50 on epoch=179
06/15/2022 17:11:09 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.199241088129977 on epoch=179
06/15/2022 17:11:12 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.47 on epoch=179
06/15/2022 17:11:15 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=180
06/15/2022 17:11:17 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.46 on epoch=181
06/15/2022 17:11:20 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.46 on epoch=182
06/15/2022 17:11:23 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.62 on epoch=183
06/15/2022 17:11:28 - INFO - __main__ - Global step 2200 Train loss 0.49 Classification-F1 0.27486220094279673 on epoch=183
06/15/2022 17:11:31 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.52 on epoch=184
06/15/2022 17:11:34 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=184
06/15/2022 17:11:36 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.46 on epoch=185
06/15/2022 17:11:39 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=186
06/15/2022 17:11:42 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.51 on epoch=187
06/15/2022 17:11:48 - INFO - __main__ - Global step 2250 Train loss 0.48 Classification-F1 0.22648382559774963 on epoch=187
06/15/2022 17:11:50 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.42 on epoch=188
06/15/2022 17:11:53 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.45 on epoch=189
06/15/2022 17:11:56 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=189
06/15/2022 17:11:58 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.51 on epoch=190
06/15/2022 17:12:01 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=191
06/15/2022 17:12:07 - INFO - __main__ - Global step 2300 Train loss 0.45 Classification-F1 0.3599471933804228 on epoch=191
06/15/2022 17:12:07 - INFO - __main__ - Saving model with best Classification-F1: 0.32815762668703846 -> 0.3599471933804228 on epoch=191, global_step=2300
06/15/2022 17:12:09 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=192
06/15/2022 17:12:12 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.44 on epoch=193
06/15/2022 17:12:15 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.48 on epoch=194
06/15/2022 17:12:17 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.45 on epoch=194
06/15/2022 17:12:20 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.46 on epoch=195
06/15/2022 17:12:26 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.29375774278666666 on epoch=195
06/15/2022 17:12:29 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.47 on epoch=196
06/15/2022 17:12:31 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.49 on epoch=197
06/15/2022 17:12:34 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.46 on epoch=198
06/15/2022 17:12:37 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=199
06/15/2022 17:12:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.47 on epoch=199
06/15/2022 17:12:45 - INFO - __main__ - Global step 2400 Train loss 0.47 Classification-F1 0.32463628174781456 on epoch=199
06/15/2022 17:12:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.49 on epoch=200
06/15/2022 17:12:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.46 on epoch=201
06/15/2022 17:12:53 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.48 on epoch=202
06/15/2022 17:12:56 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=203
06/15/2022 17:12:59 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.42 on epoch=204
06/15/2022 17:13:04 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.19597032196582945 on epoch=204
06/15/2022 17:13:07 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.48 on epoch=204
06/15/2022 17:13:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.43 on epoch=205
06/15/2022 17:13:12 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
06/15/2022 17:13:15 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.48 on epoch=207
06/15/2022 17:13:18 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=208
06/15/2022 17:13:23 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.20978024025337574 on epoch=208
06/15/2022 17:13:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.44 on epoch=209
06/15/2022 17:13:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=209
06/15/2022 17:13:31 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=210
06/15/2022 17:13:34 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=211
06/15/2022 17:13:37 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.50 on epoch=212
06/15/2022 17:13:43 - INFO - __main__ - Global step 2550 Train loss 0.46 Classification-F1 0.19597032196582945 on epoch=212
06/15/2022 17:13:45 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=213
06/15/2022 17:13:48 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=214
06/15/2022 17:13:51 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
06/15/2022 17:13:53 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.47 on epoch=215
06/15/2022 17:13:56 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.48 on epoch=216
06/15/2022 17:14:02 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.25067786619510757 on epoch=216
06/15/2022 17:14:04 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=217
06/15/2022 17:14:07 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.44 on epoch=218
06/15/2022 17:14:10 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.47 on epoch=219
06/15/2022 17:14:12 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=219
06/15/2022 17:14:15 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.47 on epoch=220
06/15/2022 17:14:21 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.3640146124222557 on epoch=220
06/15/2022 17:14:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3599471933804228 -> 0.3640146124222557 on epoch=220, global_step=2650
06/15/2022 17:14:24 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.48 on epoch=221
06/15/2022 17:14:26 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=222
06/15/2022 17:14:29 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=223
06/15/2022 17:14:32 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.48 on epoch=224
06/15/2022 17:14:34 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.49 on epoch=224
06/15/2022 17:14:40 - INFO - __main__ - Global step 2700 Train loss 0.47 Classification-F1 0.32032888462463227 on epoch=224
06/15/2022 17:14:43 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.45 on epoch=225
06/15/2022 17:14:45 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.46 on epoch=226
06/15/2022 17:14:48 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=227
06/15/2022 17:14:51 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=228
06/15/2022 17:14:54 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=229
06/15/2022 17:14:59 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.23693925068023314 on epoch=229
06/15/2022 17:15:02 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=229
06/15/2022 17:15:05 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.44 on epoch=230
06/15/2022 17:15:07 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=231
06/15/2022 17:15:10 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.47 on epoch=232
06/15/2022 17:15:13 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
06/15/2022 17:15:18 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.31166352078674353 on epoch=233
06/15/2022 17:15:21 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.42 on epoch=234
06/15/2022 17:15:24 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.50 on epoch=234
06/15/2022 17:15:26 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.42 on epoch=235
06/15/2022 17:15:29 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=236
06/15/2022 17:15:32 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.47 on epoch=237
06/15/2022 17:15:38 - INFO - __main__ - Global step 2850 Train loss 0.45 Classification-F1 0.311675978342645 on epoch=237
06/15/2022 17:15:40 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
06/15/2022 17:15:43 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
06/15/2022 17:15:46 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.44 on epoch=239
06/15/2022 17:15:48 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.45 on epoch=240
06/15/2022 17:15:51 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=241
06/15/2022 17:15:57 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.3933897075715455 on epoch=241
06/15/2022 17:15:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3640146124222557 -> 0.3933897075715455 on epoch=241, global_step=2900
06/15/2022 17:15:59 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.44 on epoch=242
06/15/2022 17:16:02 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=243
06/15/2022 17:16:05 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.47 on epoch=244
06/15/2022 17:16:07 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=244
06/15/2022 17:16:10 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.46 on epoch=245
06/15/2022 17:16:16 - INFO - __main__ - Global step 2950 Train loss 0.45 Classification-F1 0.35109891566584484 on epoch=245
06/15/2022 17:16:19 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.42 on epoch=246
06/15/2022 17:16:21 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.46 on epoch=247
06/15/2022 17:16:24 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.43 on epoch=248
06/15/2022 17:16:27 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.49 on epoch=249
06/15/2022 17:16:29 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.46 on epoch=249
06/15/2022 17:16:31 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:16:31 - INFO - __main__ - Printing 3 examples
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:16:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:16:31 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:16:31 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:16:31 - INFO - __main__ - Printing 3 examples
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:16:31 - INFO - __main__ - ['neutral']
06/15/2022 17:16:31 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:16:31 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:16:32 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:16:35 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.30458636206299755 on epoch=249
06/15/2022 17:16:35 - INFO - __main__ - save last model!
06/15/2022 17:16:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 17:16:35 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 17:16:35 - INFO - __main__ - Printing 3 examples
06/15/2022 17:16:35 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 17:16:35 - INFO - __main__ - ['contradiction']
06/15/2022 17:16:35 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 17:16:35 - INFO - __main__ - ['entailment']
06/15/2022 17:16:35 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 17:16:35 - INFO - __main__ - ['contradiction']
06/15/2022 17:16:35 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:16:36 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:16:37 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 17:16:49 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:16:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:16:50 - INFO - __main__ - Starting training!
06/15/2022 17:17:07 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_42_0.5_8_predictions.txt
06/15/2022 17:17:07 - INFO - __main__ - Classification-F1 on test data: 0.2343
06/15/2022 17:17:08 - INFO - __main__ - prefix=anli_64_42, lr=0.5, bsz=8, dev_performance=0.3933897075715455, test_performance=0.23427697758054902
06/15/2022 17:17:08 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.4, bsz=8 ...
06/15/2022 17:17:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:17:09 - INFO - __main__ - Printing 3 examples
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:17:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:17:09 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:17:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:17:09 - INFO - __main__ - Printing 3 examples
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:17:09 - INFO - __main__ - ['neutral']
06/15/2022 17:17:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:17:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:17:09 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:17:25 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:17:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:17:26 - INFO - __main__ - Starting training!
06/15/2022 17:17:29 - INFO - __main__ - Step 10 Global step 10 Train loss 1.80 on epoch=0
06/15/2022 17:17:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.28 on epoch=1
06/15/2022 17:17:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=2
06/15/2022 17:17:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.77 on epoch=3
06/15/2022 17:17:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.74 on epoch=4
06/15/2022 17:17:44 - INFO - __main__ - Global step 50 Train loss 1.06 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 17:17:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 17:17:46 - INFO - __main__ - Step 60 Global step 60 Train loss 0.80 on epoch=4
06/15/2022 17:17:49 - INFO - __main__ - Step 70 Global step 70 Train loss 0.72 on epoch=5
06/15/2022 17:17:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.64 on epoch=6
06/15/2022 17:17:54 - INFO - __main__ - Step 90 Global step 90 Train loss 0.58 on epoch=7
06/15/2022 17:17:57 - INFO - __main__ - Step 100 Global step 100 Train loss 0.66 on epoch=8
06/15/2022 17:18:00 - INFO - __main__ - Global step 100 Train loss 0.68 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 17:18:03 - INFO - __main__ - Step 110 Global step 110 Train loss 0.56 on epoch=9
06/15/2022 17:18:06 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=9
06/15/2022 17:18:08 - INFO - __main__ - Step 130 Global step 130 Train loss 0.57 on epoch=10
06/15/2022 17:18:11 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=11
06/15/2022 17:18:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=12
06/15/2022 17:18:17 - INFO - __main__ - Global step 150 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 17:18:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.61 on epoch=13
06/15/2022 17:18:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
06/15/2022 17:18:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=14
06/15/2022 17:18:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.62 on epoch=15
06/15/2022 17:18:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.56 on epoch=16
06/15/2022 17:18:34 - INFO - __main__ - Global step 200 Train loss 0.59 Classification-F1 0.21930058967096003 on epoch=16
06/15/2022 17:18:34 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.21930058967096003 on epoch=16, global_step=200
06/15/2022 17:18:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.53 on epoch=17
06/15/2022 17:18:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.56 on epoch=18
06/15/2022 17:18:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=19
06/15/2022 17:18:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.51 on epoch=19
06/15/2022 17:18:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.50 on epoch=20
06/15/2022 17:18:53 - INFO - __main__ - Global step 250 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 17:18:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=21
06/15/2022 17:18:58 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=22
06/15/2022 17:19:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=23
06/15/2022 17:19:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=24
06/15/2022 17:19:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=24
06/15/2022 17:19:11 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 17:19:14 - INFO - __main__ - Step 310 Global step 310 Train loss 0.63 on epoch=25
06/15/2022 17:19:16 - INFO - __main__ - Step 320 Global step 320 Train loss 0.50 on epoch=26
06/15/2022 17:19:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=27
06/15/2022 17:19:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=28
06/15/2022 17:19:24 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=29
06/15/2022 17:19:30 - INFO - __main__ - Global step 350 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 17:19:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=29
06/15/2022 17:19:35 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=30
06/15/2022 17:19:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=31
06/15/2022 17:19:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.52 on epoch=32
06/15/2022 17:19:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.50 on epoch=33
06/15/2022 17:19:48 - INFO - __main__ - Global step 400 Train loss 0.52 Classification-F1 0.28520174421813765 on epoch=33
06/15/2022 17:19:48 - INFO - __main__ - Saving model with best Classification-F1: 0.21930058967096003 -> 0.28520174421813765 on epoch=33, global_step=400
06/15/2022 17:19:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.58 on epoch=34
06/15/2022 17:19:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.51 on epoch=34
06/15/2022 17:19:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=35
06/15/2022 17:19:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.55 on epoch=36
06/15/2022 17:20:02 - INFO - __main__ - Step 450 Global step 450 Train loss 0.54 on epoch=37
06/15/2022 17:20:07 - INFO - __main__ - Global step 450 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 17:20:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=38
06/15/2022 17:20:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=39
06/15/2022 17:20:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
06/15/2022 17:20:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=40
06/15/2022 17:20:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.53 on epoch=41
06/15/2022 17:20:25 - INFO - __main__ - Global step 500 Train loss 0.51 Classification-F1 0.2087619047619048 on epoch=41
06/15/2022 17:20:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.54 on epoch=42
06/15/2022 17:20:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=43
06/15/2022 17:20:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=44
06/15/2022 17:20:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=44
06/15/2022 17:20:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.52 on epoch=45
06/15/2022 17:20:44 - INFO - __main__ - Global step 550 Train loss 0.52 Classification-F1 0.21041954817888428 on epoch=45
06/15/2022 17:20:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.49 on epoch=46
06/15/2022 17:20:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=47
06/15/2022 17:20:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 17:20:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.53 on epoch=49
06/15/2022 17:20:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.50 on epoch=49
06/15/2022 17:21:02 - INFO - __main__ - Global step 600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 17:21:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.55 on epoch=50
06/15/2022 17:21:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=51
06/15/2022 17:21:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=52
06/15/2022 17:21:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.47 on epoch=53
06/15/2022 17:21:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=54
06/15/2022 17:21:21 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 17:21:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=54
06/15/2022 17:21:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=55
06/15/2022 17:21:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
06/15/2022 17:21:31 - INFO - __main__ - Step 690 Global step 690 Train loss 0.47 on epoch=57
06/15/2022 17:21:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.53 on epoch=58
06/15/2022 17:21:39 - INFO - __main__ - Global step 700 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 17:21:42 - INFO - __main__ - Step 710 Global step 710 Train loss 0.54 on epoch=59
06/15/2022 17:21:45 - INFO - __main__ - Step 720 Global step 720 Train loss 0.46 on epoch=59
06/15/2022 17:21:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=60
06/15/2022 17:21:50 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=61
06/15/2022 17:21:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=62
06/15/2022 17:21:58 - INFO - __main__ - Global step 750 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 17:22:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=63
06/15/2022 17:22:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=64
06/15/2022 17:22:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.50 on epoch=64
06/15/2022 17:22:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=65
06/15/2022 17:22:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.48 on epoch=66
06/15/2022 17:22:17 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 17:22:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.48 on epoch=67
06/15/2022 17:22:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.45 on epoch=68
06/15/2022 17:22:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.49 on epoch=69
06/15/2022 17:22:28 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=69
06/15/2022 17:22:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.52 on epoch=70
06/15/2022 17:22:36 - INFO - __main__ - Global step 850 Train loss 0.49 Classification-F1 0.23105727890527486 on epoch=70
06/15/2022 17:22:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.48 on epoch=71
06/15/2022 17:22:42 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=72
06/15/2022 17:22:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
06/15/2022 17:22:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=74
06/15/2022 17:22:50 - INFO - __main__ - Step 900 Global step 900 Train loss 0.49 on epoch=74
06/15/2022 17:22:55 - INFO - __main__ - Global step 900 Train loss 0.48 Classification-F1 0.2322903629536921 on epoch=74
06/15/2022 17:22:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.49 on epoch=75
06/15/2022 17:23:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.47 on epoch=76
06/15/2022 17:23:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=77
06/15/2022 17:23:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
06/15/2022 17:23:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.45 on epoch=79
06/15/2022 17:23:14 - INFO - __main__ - Global step 950 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 17:23:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=79
06/15/2022 17:23:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.51 on epoch=80
06/15/2022 17:23:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.50 on epoch=81
06/15/2022 17:23:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.49 on epoch=82
06/15/2022 17:23:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
06/15/2022 17:23:33 - INFO - __main__ - Global step 1000 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 17:23:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=84
06/15/2022 17:23:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.49 on epoch=84
06/15/2022 17:23:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.55 on epoch=85
06/15/2022 17:23:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.54 on epoch=86
06/15/2022 17:23:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.50 on epoch=87
06/15/2022 17:23:51 - INFO - __main__ - Global step 1050 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 17:23:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.51 on epoch=88
06/15/2022 17:23:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=89
06/15/2022 17:23:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.51 on epoch=89
06/15/2022 17:24:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.48 on epoch=90
06/15/2022 17:24:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.51 on epoch=91
06/15/2022 17:24:10 - INFO - __main__ - Global step 1100 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 17:24:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.49 on epoch=92
06/15/2022 17:24:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.47 on epoch=93
06/15/2022 17:24:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=94
06/15/2022 17:24:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.49 on epoch=94
06/15/2022 17:24:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
06/15/2022 17:24:29 - INFO - __main__ - Global step 1150 Train loss 0.48 Classification-F1 0.20244025702115367 on epoch=95
06/15/2022 17:24:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=96
06/15/2022 17:24:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.47 on epoch=97
06/15/2022 17:24:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=98
06/15/2022 17:24:40 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
06/15/2022 17:24:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=99
06/15/2022 17:24:48 - INFO - __main__ - Global step 1200 Train loss 0.47 Classification-F1 0.2158091197605775 on epoch=99
06/15/2022 17:24:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=100
06/15/2022 17:24:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.49 on epoch=101
06/15/2022 17:24:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.56 on epoch=102
06/15/2022 17:24:59 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.52 on epoch=103
06/15/2022 17:25:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.51 on epoch=104
06/15/2022 17:25:07 - INFO - __main__ - Global step 1250 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 17:25:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.50 on epoch=104
06/15/2022 17:25:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.49 on epoch=105
06/15/2022 17:25:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.46 on epoch=106
06/15/2022 17:25:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.51 on epoch=107
06/15/2022 17:25:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 17:25:26 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 17:25:28 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.44 on epoch=109
06/15/2022 17:25:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=109
06/15/2022 17:25:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
06/15/2022 17:25:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=111
06/15/2022 17:25:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.53 on epoch=112
06/15/2022 17:25:44 - INFO - __main__ - Global step 1350 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 17:25:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.50 on epoch=113
06/15/2022 17:25:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.49 on epoch=114
06/15/2022 17:25:53 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.51 on epoch=114
06/15/2022 17:25:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=115
06/15/2022 17:25:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=116
06/15/2022 17:26:04 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.1647058823529412 on epoch=116
06/15/2022 17:26:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.47 on epoch=117
06/15/2022 17:26:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=118
06/15/2022 17:26:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.52 on epoch=119
06/15/2022 17:26:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=119
06/15/2022 17:26:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.48 on epoch=120
06/15/2022 17:26:22 - INFO - __main__ - Global step 1450 Train loss 0.47 Classification-F1 0.33924641139177586 on epoch=120
06/15/2022 17:26:22 - INFO - __main__ - Saving model with best Classification-F1: 0.28520174421813765 -> 0.33924641139177586 on epoch=120, global_step=1450
06/15/2022 17:26:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.45 on epoch=121
06/15/2022 17:26:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.50 on epoch=122
06/15/2022 17:26:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=123
06/15/2022 17:26:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=124
06/15/2022 17:26:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=124
06/15/2022 17:26:41 - INFO - __main__ - Global step 1500 Train loss 0.46 Classification-F1 0.2201695798398939 on epoch=124
06/15/2022 17:26:44 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.48 on epoch=125
06/15/2022 17:26:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=126
06/15/2022 17:26:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=127
06/15/2022 17:26:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
06/15/2022 17:26:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=129
06/15/2022 17:27:00 - INFO - __main__ - Global step 1550 Train loss 0.48 Classification-F1 0.1647058823529412 on epoch=129
06/15/2022 17:27:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.50 on epoch=129
06/15/2022 17:27:06 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.50 on epoch=130
06/15/2022 17:27:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=131
06/15/2022 17:27:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=132
06/15/2022 17:27:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=133
06/15/2022 17:27:19 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.18906810035842292 on epoch=133
06/15/2022 17:27:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.50 on epoch=134
06/15/2022 17:27:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 17:27:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=135
06/15/2022 17:27:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=136
06/15/2022 17:27:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=137
06/15/2022 17:27:38 - INFO - __main__ - Global step 1650 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 17:27:41 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=138
06/15/2022 17:27:43 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.53 on epoch=139
06/15/2022 17:27:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
06/15/2022 17:27:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=140
06/15/2022 17:27:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.46 on epoch=141
06/15/2022 17:27:57 - INFO - __main__ - Global step 1700 Train loss 0.48 Classification-F1 0.18399830629174121 on epoch=141
06/15/2022 17:28:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=142
06/15/2022 17:28:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
06/15/2022 17:28:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=144
06/15/2022 17:28:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.46 on epoch=144
06/15/2022 17:28:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=145
06/15/2022 17:28:16 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.2662259252932828 on epoch=145
06/15/2022 17:28:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=146
06/15/2022 17:28:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=147
06/15/2022 17:28:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.47 on epoch=148
06/15/2022 17:28:27 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.52 on epoch=149
06/15/2022 17:28:29 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.51 on epoch=149
06/15/2022 17:28:35 - INFO - __main__ - Global step 1800 Train loss 0.49 Classification-F1 0.25870729149417676 on epoch=149
06/15/2022 17:28:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=150
06/15/2022 17:28:41 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=151
06/15/2022 17:28:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=152
06/15/2022 17:28:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=153
06/15/2022 17:28:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.51 on epoch=154
06/15/2022 17:28:54 - INFO - __main__ - Global step 1850 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=154
06/15/2022 17:28:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.50 on epoch=154
06/15/2022 17:28:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.47 on epoch=155
06/15/2022 17:29:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
06/15/2022 17:29:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.46 on epoch=157
06/15/2022 17:29:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.44 on epoch=158
06/15/2022 17:29:13 - INFO - __main__ - Global step 1900 Train loss 0.47 Classification-F1 0.2232598861812345 on epoch=158
06/15/2022 17:29:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=159
06/15/2022 17:29:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=159
06/15/2022 17:29:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=160
06/15/2022 17:29:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=161
06/15/2022 17:29:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.52 on epoch=162
06/15/2022 17:29:32 - INFO - __main__ - Global step 1950 Train loss 0.48 Classification-F1 0.2412121212121212 on epoch=162
06/15/2022 17:29:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=163
06/15/2022 17:29:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.50 on epoch=164
06/15/2022 17:29:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.46 on epoch=164
06/15/2022 17:29:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=165
06/15/2022 17:29:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=166
06/15/2022 17:29:51 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.2153709518825134 on epoch=166
06/15/2022 17:29:54 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.48 on epoch=167
06/15/2022 17:29:56 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.42 on epoch=168
06/15/2022 17:29:59 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.50 on epoch=169
06/15/2022 17:30:02 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=169
06/15/2022 17:30:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.50 on epoch=170
06/15/2022 17:30:10 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.30539021164021163 on epoch=170
06/15/2022 17:30:13 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.50 on epoch=171
06/15/2022 17:30:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.46 on epoch=172
06/15/2022 17:30:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
06/15/2022 17:30:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.44 on epoch=174
06/15/2022 17:30:23 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.44 on epoch=174
06/15/2022 17:30:29 - INFO - __main__ - Global step 2100 Train loss 0.46 Classification-F1 0.2734129478315525 on epoch=174
06/15/2022 17:30:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.49 on epoch=175
06/15/2022 17:30:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.47 on epoch=176
06/15/2022 17:30:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.49 on epoch=177
06/15/2022 17:30:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.46 on epoch=178
06/15/2022 17:30:43 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.47 on epoch=179
06/15/2022 17:30:49 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.2476603824766038 on epoch=179
06/15/2022 17:30:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=179
06/15/2022 17:30:54 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.43 on epoch=180
06/15/2022 17:30:57 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.44 on epoch=181
06/15/2022 17:31:00 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.44 on epoch=182
06/15/2022 17:31:02 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=183
06/15/2022 17:31:08 - INFO - __main__ - Global step 2200 Train loss 0.44 Classification-F1 0.272776346460557 on epoch=183
06/15/2022 17:31:11 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.46 on epoch=184
06/15/2022 17:31:13 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=184
06/15/2022 17:31:16 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.46 on epoch=185
06/15/2022 17:31:19 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=186
06/15/2022 17:31:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.47 on epoch=187
06/15/2022 17:31:27 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.307776949231814 on epoch=187
06/15/2022 17:31:30 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.46 on epoch=188
06/15/2022 17:31:32 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.47 on epoch=189
06/15/2022 17:31:35 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.42 on epoch=189
06/15/2022 17:31:38 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.50 on epoch=190
06/15/2022 17:31:40 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.46 on epoch=191
06/15/2022 17:31:46 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.31510602923646397 on epoch=191
06/15/2022 17:31:49 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.49 on epoch=192
06/15/2022 17:31:51 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.43 on epoch=193
06/15/2022 17:31:54 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.44 on epoch=194
06/15/2022 17:31:57 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=194
06/15/2022 17:32:00 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.46 on epoch=195
06/15/2022 17:32:05 - INFO - __main__ - Global step 2350 Train loss 0.45 Classification-F1 0.2951726715729605 on epoch=195
06/15/2022 17:32:08 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.37 on epoch=196
06/15/2022 17:32:10 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.45 on epoch=197
06/15/2022 17:32:13 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 17:32:16 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.45 on epoch=199
06/15/2022 17:32:19 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.43 on epoch=199
06/15/2022 17:32:24 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.3017608742921429 on epoch=199
06/15/2022 17:32:27 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.48 on epoch=200
06/15/2022 17:32:30 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.50 on epoch=201
06/15/2022 17:32:32 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.45 on epoch=202
06/15/2022 17:32:35 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.46 on epoch=203
06/15/2022 17:32:38 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.40 on epoch=204
06/15/2022 17:32:43 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.30155651561111624 on epoch=204
06/15/2022 17:32:46 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.44 on epoch=204
06/15/2022 17:32:49 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=205
06/15/2022 17:32:51 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.43 on epoch=206
06/15/2022 17:32:54 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
06/15/2022 17:32:57 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=208
06/15/2022 17:33:02 - INFO - __main__ - Global step 2500 Train loss 0.44 Classification-F1 0.37268616829180207 on epoch=208
06/15/2022 17:33:02 - INFO - __main__ - Saving model with best Classification-F1: 0.33924641139177586 -> 0.37268616829180207 on epoch=208, global_step=2500
06/15/2022 17:33:05 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.46 on epoch=209
06/15/2022 17:33:08 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.44 on epoch=209
06/15/2022 17:33:10 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.46 on epoch=210
06/15/2022 17:33:13 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.48 on epoch=211
06/15/2022 17:33:16 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.48 on epoch=212
06/15/2022 17:33:21 - INFO - __main__ - Global step 2550 Train loss 0.47 Classification-F1 0.2741627468597502 on epoch=212
06/15/2022 17:33:24 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.39 on epoch=213
06/15/2022 17:33:27 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.44 on epoch=214
06/15/2022 17:33:29 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.45 on epoch=214
06/15/2022 17:33:32 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.43 on epoch=215
06/15/2022 17:33:35 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.46 on epoch=216
06/15/2022 17:33:40 - INFO - __main__ - Global step 2600 Train loss 0.44 Classification-F1 0.3305787500978448 on epoch=216
06/15/2022 17:33:43 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.49 on epoch=217
06/15/2022 17:33:46 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.46 on epoch=218
06/15/2022 17:33:49 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.43 on epoch=219
06/15/2022 17:33:51 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.42 on epoch=219
06/15/2022 17:33:54 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.46 on epoch=220
06/15/2022 17:34:00 - INFO - __main__ - Global step 2650 Train loss 0.45 Classification-F1 0.38480820547869476 on epoch=220
06/15/2022 17:34:00 - INFO - __main__ - Saving model with best Classification-F1: 0.37268616829180207 -> 0.38480820547869476 on epoch=220, global_step=2650
06/15/2022 17:34:02 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.44 on epoch=221
06/15/2022 17:34:05 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.48 on epoch=222
06/15/2022 17:34:08 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.43 on epoch=223
06/15/2022 17:34:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.45 on epoch=224
06/15/2022 17:34:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.44 on epoch=224
06/15/2022 17:34:19 - INFO - __main__ - Global step 2700 Train loss 0.45 Classification-F1 0.29890716214463836 on epoch=224
06/15/2022 17:34:21 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.48 on epoch=225
06/15/2022 17:34:24 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.47 on epoch=226
06/15/2022 17:34:27 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.44 on epoch=227
06/15/2022 17:34:29 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.43 on epoch=228
06/15/2022 17:34:32 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.43 on epoch=229
06/15/2022 17:34:38 - INFO - __main__ - Global step 2750 Train loss 0.45 Classification-F1 0.26549707602339184 on epoch=229
06/15/2022 17:34:40 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=229
06/15/2022 17:34:43 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.46 on epoch=230
06/15/2022 17:34:46 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=231
06/15/2022 17:34:48 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.45 on epoch=232
06/15/2022 17:34:51 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
06/15/2022 17:34:57 - INFO - __main__ - Global step 2800 Train loss 0.44 Classification-F1 0.29672904094927016 on epoch=233
06/15/2022 17:34:59 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.46 on epoch=234
06/15/2022 17:35:02 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.43 on epoch=234
06/15/2022 17:35:05 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.48 on epoch=235
06/15/2022 17:35:07 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=236
06/15/2022 17:35:10 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.49 on epoch=237
06/15/2022 17:35:16 - INFO - __main__ - Global step 2850 Train loss 0.46 Classification-F1 0.32165861513687594 on epoch=237
06/15/2022 17:35:18 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.43 on epoch=238
06/15/2022 17:35:21 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.48 on epoch=239
06/15/2022 17:35:24 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.47 on epoch=239
06/15/2022 17:35:26 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.44 on epoch=240
06/15/2022 17:35:29 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.46 on epoch=241
06/15/2022 17:35:35 - INFO - __main__ - Global step 2900 Train loss 0.46 Classification-F1 0.38903140203449804 on epoch=241
06/15/2022 17:35:35 - INFO - __main__ - Saving model with best Classification-F1: 0.38480820547869476 -> 0.38903140203449804 on epoch=241, global_step=2900
06/15/2022 17:35:37 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.42 on epoch=242
06/15/2022 17:35:40 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
06/15/2022 17:35:43 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.42 on epoch=244
06/15/2022 17:35:45 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=244
06/15/2022 17:35:48 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.43 on epoch=245
06/15/2022 17:35:54 - INFO - __main__ - Global step 2950 Train loss 0.42 Classification-F1 0.3550209349804492 on epoch=245
06/15/2022 17:35:56 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=246
06/15/2022 17:35:59 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.45 on epoch=247
06/15/2022 17:36:02 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.42 on epoch=248
06/15/2022 17:36:04 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.45 on epoch=249
06/15/2022 17:36:07 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=249
06/15/2022 17:36:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:36:09 - INFO - __main__ - Printing 3 examples
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:36:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:36:09 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:36:09 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:36:09 - INFO - __main__ - Printing 3 examples
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:36:09 - INFO - __main__ - ['neutral']
06/15/2022 17:36:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:36:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:36:09 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:36:13 - INFO - __main__ - Global step 3000 Train loss 0.43 Classification-F1 0.4003117776702682 on epoch=249
06/15/2022 17:36:13 - INFO - __main__ - Saving model with best Classification-F1: 0.38903140203449804 -> 0.4003117776702682 on epoch=249, global_step=3000
06/15/2022 17:36:13 - INFO - __main__ - save last model!
06/15/2022 17:36:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 17:36:13 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 17:36:13 - INFO - __main__ - Printing 3 examples
06/15/2022 17:36:13 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 17:36:13 - INFO - __main__ - ['contradiction']
06/15/2022 17:36:13 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 17:36:13 - INFO - __main__ - ['entailment']
06/15/2022 17:36:13 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 17:36:13 - INFO - __main__ - ['contradiction']
06/15/2022 17:36:13 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:36:13 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:36:15 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 17:36:26 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:36:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:36:26 - INFO - __main__ - Starting training!
06/15/2022 17:36:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_42_0.4_8_predictions.txt
06/15/2022 17:36:44 - INFO - __main__ - Classification-F1 on test data: 0.3197
06/15/2022 17:36:45 - INFO - __main__ - prefix=anli_64_42, lr=0.4, bsz=8, dev_performance=0.4003117776702682, test_performance=0.319713798664001
06/15/2022 17:36:45 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.3, bsz=8 ...
06/15/2022 17:36:46 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:36:46 - INFO - __main__ - Printing 3 examples
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:36:46 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:36:46 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:36:46 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:36:46 - INFO - __main__ - Printing 3 examples
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:36:46 - INFO - __main__ - ['neutral']
06/15/2022 17:36:46 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:36:46 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:36:46 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:37:02 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:37:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:37:02 - INFO - __main__ - Starting training!
06/15/2022 17:37:06 - INFO - __main__ - Step 10 Global step 10 Train loss 1.06 on epoch=0
06/15/2022 17:37:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.77 on epoch=1
06/15/2022 17:37:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=2
06/15/2022 17:37:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=3
06/15/2022 17:37:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=4
06/15/2022 17:37:21 - INFO - __main__ - Global step 50 Train loss 0.74 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 17:37:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 17:37:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.66 on epoch=4
06/15/2022 17:37:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=5
06/15/2022 17:37:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.62 on epoch=6
06/15/2022 17:37:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=7
06/15/2022 17:37:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=8
06/15/2022 17:37:39 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 17:37:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.94 on epoch=9
06/15/2022 17:37:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=9
06/15/2022 17:37:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=10
06/15/2022 17:37:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=11
06/15/2022 17:37:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.62 on epoch=12
06/15/2022 17:37:57 - INFO - __main__ - Global step 150 Train loss 0.77 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 17:38:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=13
06/15/2022 17:38:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.59 on epoch=14
06/15/2022 17:38:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.61 on epoch=14
06/15/2022 17:38:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=15
06/15/2022 17:38:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.52 on epoch=16
06/15/2022 17:38:15 - INFO - __main__ - Global step 200 Train loss 0.56 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 17:38:18 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=17
06/15/2022 17:38:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.53 on epoch=18
06/15/2022 17:38:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=19
06/15/2022 17:38:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.57 on epoch=19
06/15/2022 17:38:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.51 on epoch=20
06/15/2022 17:38:34 - INFO - __main__ - Global step 250 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 17:38:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=21
06/15/2022 17:38:39 - INFO - __main__ - Step 270 Global step 270 Train loss 0.64 on epoch=22
06/15/2022 17:38:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.54 on epoch=23
06/15/2022 17:38:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.58 on epoch=24
06/15/2022 17:38:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.50 on epoch=24
06/15/2022 17:38:52 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.16732026143790854 on epoch=24
06/15/2022 17:38:52 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.16732026143790854 on epoch=24, global_step=300
06/15/2022 17:38:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.60 on epoch=25
06/15/2022 17:38:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.52 on epoch=26
06/15/2022 17:39:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.56 on epoch=27
06/15/2022 17:39:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.52 on epoch=28
06/15/2022 17:39:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.53 on epoch=29
06/15/2022 17:39:10 - INFO - __main__ - Global step 350 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 17:39:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
06/15/2022 17:39:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.53 on epoch=30
06/15/2022 17:39:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.51 on epoch=31
06/15/2022 17:39:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=32
06/15/2022 17:39:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=33
06/15/2022 17:39:28 - INFO - __main__ - Global step 400 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 17:39:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.52 on epoch=34
06/15/2022 17:39:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.52 on epoch=34
06/15/2022 17:39:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=35
06/15/2022 17:39:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.52 on epoch=36
06/15/2022 17:39:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=37
06/15/2022 17:39:47 - INFO - __main__ - Global step 450 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 17:39:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.55 on epoch=38
06/15/2022 17:39:52 - INFO - __main__ - Step 470 Global step 470 Train loss 0.53 on epoch=39
06/15/2022 17:39:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=39
06/15/2022 17:39:58 - INFO - __main__ - Step 490 Global step 490 Train loss 0.55 on epoch=40
06/15/2022 17:40:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.58 on epoch=41
06/15/2022 17:40:06 - INFO - __main__ - Global step 500 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 17:40:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=42
06/15/2022 17:40:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.56 on epoch=43
06/15/2022 17:40:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=44
06/15/2022 17:40:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.56 on epoch=44
06/15/2022 17:40:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=45
06/15/2022 17:40:25 - INFO - __main__ - Global step 550 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 17:40:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.52 on epoch=46
06/15/2022 17:40:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.52 on epoch=47
06/15/2022 17:40:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=48
06/15/2022 17:40:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.51 on epoch=49
06/15/2022 17:40:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=49
06/15/2022 17:40:44 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 17:40:47 - INFO - __main__ - Step 610 Global step 610 Train loss 0.51 on epoch=50
06/15/2022 17:40:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=51
06/15/2022 17:40:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=52
06/15/2022 17:40:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 17:40:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=54
06/15/2022 17:41:03 - INFO - __main__ - Global step 650 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 17:41:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=54
06/15/2022 17:41:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=55
06/15/2022 17:41:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.56 on epoch=56
06/15/2022 17:41:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.48 on epoch=57
06/15/2022 17:41:16 - INFO - __main__ - Step 700 Global step 700 Train loss 0.47 on epoch=58
06/15/2022 17:41:22 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 17:41:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=59
06/15/2022 17:41:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=59
06/15/2022 17:41:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.52 on epoch=60
06/15/2022 17:41:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=61
06/15/2022 17:41:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=62
06/15/2022 17:41:41 - INFO - __main__ - Global step 750 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 17:41:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=63
06/15/2022 17:41:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=64
06/15/2022 17:41:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.53 on epoch=64
06/15/2022 17:41:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.50 on epoch=65
06/15/2022 17:41:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.52 on epoch=66
06/15/2022 17:42:00 - INFO - __main__ - Global step 800 Train loss 0.51 Classification-F1 0.24706953453966088 on epoch=66
06/15/2022 17:42:00 - INFO - __main__ - Saving model with best Classification-F1: 0.16732026143790854 -> 0.24706953453966088 on epoch=66, global_step=800
06/15/2022 17:42:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.50 on epoch=67
06/15/2022 17:42:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.53 on epoch=68
06/15/2022 17:42:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.48 on epoch=69
06/15/2022 17:42:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=69
06/15/2022 17:42:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.49 on epoch=70
06/15/2022 17:42:19 - INFO - __main__ - Global step 850 Train loss 0.50 Classification-F1 0.25416666666666665 on epoch=70
06/15/2022 17:42:19 - INFO - __main__ - Saving model with best Classification-F1: 0.24706953453966088 -> 0.25416666666666665 on epoch=70, global_step=850
06/15/2022 17:42:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=71
06/15/2022 17:42:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.55 on epoch=72
06/15/2022 17:42:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.50 on epoch=73
06/15/2022 17:42:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=74
06/15/2022 17:42:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=74
06/15/2022 17:42:38 - INFO - __main__ - Global step 900 Train loss 0.52 Classification-F1 0.24363788068418857 on epoch=74
06/15/2022 17:42:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.54 on epoch=75
06/15/2022 17:42:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.53 on epoch=76
06/15/2022 17:42:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=77
06/15/2022 17:42:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.47 on epoch=78
06/15/2022 17:42:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.54 on epoch=79
06/15/2022 17:42:57 - INFO - __main__ - Global step 950 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 17:43:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.51 on epoch=79
06/15/2022 17:43:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=80
06/15/2022 17:43:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.51 on epoch=81
06/15/2022 17:43:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.51 on epoch=82
06/15/2022 17:43:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=83
06/15/2022 17:43:16 - INFO - __main__ - Global step 1000 Train loss 0.50 Classification-F1 0.2546482856132909 on epoch=83
06/15/2022 17:43:16 - INFO - __main__ - Saving model with best Classification-F1: 0.25416666666666665 -> 0.2546482856132909 on epoch=83, global_step=1000
06/15/2022 17:43:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=84
06/15/2022 17:43:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=84
06/15/2022 17:43:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=85
06/15/2022 17:43:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=86
06/15/2022 17:43:29 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.53 on epoch=87
06/15/2022 17:43:35 - INFO - __main__ - Global step 1050 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 17:43:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.48 on epoch=88
06/15/2022 17:43:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.54 on epoch=89
06/15/2022 17:43:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.46 on epoch=89
06/15/2022 17:43:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.52 on epoch=90
06/15/2022 17:43:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.52 on epoch=91
06/15/2022 17:43:54 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.2012570882011341 on epoch=91
06/15/2022 17:43:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=92
06/15/2022 17:43:59 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
06/15/2022 17:44:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.46 on epoch=94
06/15/2022 17:44:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.43 on epoch=94
06/15/2022 17:44:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.51 on epoch=95
06/15/2022 17:44:13 - INFO - __main__ - Global step 1150 Train loss 0.47 Classification-F1 0.2638392096746721 on epoch=95
06/15/2022 17:44:13 - INFO - __main__ - Saving model with best Classification-F1: 0.2546482856132909 -> 0.2638392096746721 on epoch=95, global_step=1150
06/15/2022 17:44:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.53 on epoch=96
06/15/2022 17:44:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.46 on epoch=97
06/15/2022 17:44:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.50 on epoch=98
06/15/2022 17:44:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.48 on epoch=99
06/15/2022 17:44:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.46 on epoch=99
06/15/2022 17:44:32 - INFO - __main__ - Global step 1200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=99
06/15/2022 17:44:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.49 on epoch=100
06/15/2022 17:44:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=101
06/15/2022 17:44:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.49 on epoch=102
06/15/2022 17:44:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.46 on epoch=103
06/15/2022 17:44:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.49 on epoch=104
06/15/2022 17:44:51 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 17:44:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=104
06/15/2022 17:44:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.53 on epoch=105
06/15/2022 17:44:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.51 on epoch=106
06/15/2022 17:45:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=107
06/15/2022 17:45:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=108
06/15/2022 17:45:10 - INFO - __main__ - Global step 1300 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 17:45:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=109
06/15/2022 17:45:15 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=109
06/15/2022 17:45:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.50 on epoch=110
06/15/2022 17:45:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=111
06/15/2022 17:45:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.54 on epoch=112
06/15/2022 17:45:29 - INFO - __main__ - Global step 1350 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 17:45:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.46 on epoch=113
06/15/2022 17:45:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=114
06/15/2022 17:45:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=114
06/15/2022 17:45:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=115
06/15/2022 17:45:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=116
06/15/2022 17:45:48 - INFO - __main__ - Global step 1400 Train loss 0.49 Classification-F1 0.1775766716943188 on epoch=116
06/15/2022 17:45:51 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.51 on epoch=117
06/15/2022 17:45:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.48 on epoch=118
06/15/2022 17:45:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.48 on epoch=119
06/15/2022 17:45:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.49 on epoch=119
06/15/2022 17:46:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.50 on epoch=120
06/15/2022 17:46:07 - INFO - __main__ - Global step 1450 Train loss 0.49 Classification-F1 0.2261904761904762 on epoch=120
06/15/2022 17:46:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=121
06/15/2022 17:46:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.54 on epoch=122
06/15/2022 17:46:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=123
06/15/2022 17:46:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=124
06/15/2022 17:46:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.52 on epoch=124
06/15/2022 17:46:27 - INFO - __main__ - Global step 1500 Train loss 0.51 Classification-F1 0.1775766716943188 on epoch=124
06/15/2022 17:46:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=125
06/15/2022 17:46:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=126
06/15/2022 17:46:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.53 on epoch=127
06/15/2022 17:46:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.45 on epoch=128
06/15/2022 17:46:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=129
06/15/2022 17:46:46 - INFO - __main__ - Global step 1550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 17:46:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.51 on epoch=129
06/15/2022 17:46:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.49 on epoch=130
06/15/2022 17:46:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.46 on epoch=131
06/15/2022 17:46:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.50 on epoch=132
06/15/2022 17:46:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.46 on epoch=133
06/15/2022 17:47:05 - INFO - __main__ - Global step 1600 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 17:47:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.53 on epoch=134
06/15/2022 17:47:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.53 on epoch=134
06/15/2022 17:47:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=135
06/15/2022 17:47:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.49 on epoch=136
06/15/2022 17:47:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.49 on epoch=137
06/15/2022 17:47:24 - INFO - __main__ - Global step 1650 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 17:47:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=138
06/15/2022 17:47:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.49 on epoch=139
06/15/2022 17:47:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=139
06/15/2022 17:47:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=140
06/15/2022 17:47:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=141
06/15/2022 17:47:43 - INFO - __main__ - Global step 1700 Train loss 0.47 Classification-F1 0.20284211245473802 on epoch=141
06/15/2022 17:47:46 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=142
06/15/2022 17:47:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.47 on epoch=143
06/15/2022 17:47:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=144
06/15/2022 17:47:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.50 on epoch=144
06/15/2022 17:47:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.47 on epoch=145
06/15/2022 17:48:02 - INFO - __main__ - Global step 1750 Train loss 0.48 Classification-F1 0.17699251303962893 on epoch=145
06/15/2022 17:48:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.53 on epoch=146
06/15/2022 17:48:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.48 on epoch=147
06/15/2022 17:48:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.49 on epoch=148
06/15/2022 17:48:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=149
06/15/2022 17:48:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.43 on epoch=149
06/15/2022 17:48:21 - INFO - __main__ - Global step 1800 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=149
06/15/2022 17:48:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.53 on epoch=150
06/15/2022 17:48:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.49 on epoch=151
06/15/2022 17:48:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=152
06/15/2022 17:48:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=153
06/15/2022 17:48:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.48 on epoch=154
06/15/2022 17:48:40 - INFO - __main__ - Global step 1850 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=154
06/15/2022 17:48:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.48 on epoch=154
06/15/2022 17:48:45 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.48 on epoch=155
06/15/2022 17:48:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=156
06/15/2022 17:48:51 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=157
06/15/2022 17:48:53 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
06/15/2022 17:48:59 - INFO - __main__ - Global step 1900 Train loss 0.48 Classification-F1 0.1647058823529412 on epoch=158
06/15/2022 17:49:02 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.51 on epoch=159
06/15/2022 17:49:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=159
06/15/2022 17:49:07 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.48 on epoch=160
06/15/2022 17:49:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.46 on epoch=161
06/15/2022 17:49:12 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.54 on epoch=162
06/15/2022 17:49:18 - INFO - __main__ - Global step 1950 Train loss 0.49 Classification-F1 0.178743961352657 on epoch=162
06/15/2022 17:49:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.46 on epoch=163
06/15/2022 17:49:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=164
06/15/2022 17:49:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.41 on epoch=164
06/15/2022 17:49:29 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.53 on epoch=165
06/15/2022 17:49:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.49 on epoch=166
06/15/2022 17:49:36 - INFO - __main__ - Global step 2000 Train loss 0.47 Classification-F1 0.20370370370370372 on epoch=166
06/15/2022 17:49:39 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.53 on epoch=167
06/15/2022 17:49:42 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
06/15/2022 17:49:44 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.49 on epoch=169
06/15/2022 17:49:47 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.47 on epoch=169
06/15/2022 17:49:50 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.49 on epoch=170
06/15/2022 17:49:55 - INFO - __main__ - Global step 2050 Train loss 0.49 Classification-F1 0.3157546082949309 on epoch=170
06/15/2022 17:49:55 - INFO - __main__ - Saving model with best Classification-F1: 0.2638392096746721 -> 0.3157546082949309 on epoch=170, global_step=2050
06/15/2022 17:49:58 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.51 on epoch=171
06/15/2022 17:50:01 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.48 on epoch=172
06/15/2022 17:50:04 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
06/15/2022 17:50:06 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
06/15/2022 17:50:09 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.48 on epoch=174
06/15/2022 17:50:15 - INFO - __main__ - Global step 2100 Train loss 0.47 Classification-F1 0.2888888888888889 on epoch=174
06/15/2022 17:50:18 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.53 on epoch=175
06/15/2022 17:50:21 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=176
06/15/2022 17:50:24 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.47 on epoch=177
06/15/2022 17:50:26 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.47 on epoch=178
06/15/2022 17:50:29 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.47 on epoch=179
06/15/2022 17:50:35 - INFO - __main__ - Global step 2150 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=179
06/15/2022 17:50:37 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.44 on epoch=179
06/15/2022 17:50:40 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.48 on epoch=180
06/15/2022 17:50:43 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.48 on epoch=181
06/15/2022 17:50:46 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.48 on epoch=182
06/15/2022 17:50:49 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.44 on epoch=183
06/15/2022 17:50:54 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.1990221455277538 on epoch=183
06/15/2022 17:50:57 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.47 on epoch=184
06/15/2022 17:51:00 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.44 on epoch=184
06/15/2022 17:51:03 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.50 on epoch=185
06/15/2022 17:51:05 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.45 on epoch=186
06/15/2022 17:51:08 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=187
06/15/2022 17:51:14 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.29752456538170824 on epoch=187
06/15/2022 17:51:17 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=188
06/15/2022 17:51:20 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.52 on epoch=189
06/15/2022 17:51:22 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.46 on epoch=189
06/15/2022 17:51:25 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.45 on epoch=190
06/15/2022 17:51:28 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.51 on epoch=191
06/15/2022 17:51:34 - INFO - __main__ - Global step 2300 Train loss 0.49 Classification-F1 0.2476318970121146 on epoch=191
06/15/2022 17:51:36 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.47 on epoch=192
06/15/2022 17:51:39 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.49 on epoch=193
06/15/2022 17:51:42 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=194
06/15/2022 17:51:45 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.44 on epoch=194
06/15/2022 17:51:48 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.50 on epoch=195
06/15/2022 17:51:53 - INFO - __main__ - Global step 2350 Train loss 0.47 Classification-F1 0.36970828101847136 on epoch=195
06/15/2022 17:51:54 - INFO - __main__ - Saving model with best Classification-F1: 0.3157546082949309 -> 0.36970828101847136 on epoch=195, global_step=2350
06/15/2022 17:51:56 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.48 on epoch=196
06/15/2022 17:51:59 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.53 on epoch=197
06/15/2022 17:52:02 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.47 on epoch=198
06/15/2022 17:52:05 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.47 on epoch=199
06/15/2022 17:52:07 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.51 on epoch=199
06/15/2022 17:52:13 - INFO - __main__ - Global step 2400 Train loss 0.49 Classification-F1 0.2558272763981425 on epoch=199
06/15/2022 17:52:16 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.54 on epoch=200
06/15/2022 17:52:19 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.54 on epoch=201
06/15/2022 17:52:22 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=202
06/15/2022 17:52:24 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.41 on epoch=203
06/15/2022 17:52:27 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.45 on epoch=204
06/15/2022 17:52:33 - INFO - __main__ - Global step 2450 Train loss 0.48 Classification-F1 0.27939590075512405 on epoch=204
06/15/2022 17:52:36 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.48 on epoch=204
06/15/2022 17:52:39 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.46 on epoch=205
06/15/2022 17:52:41 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=206
06/15/2022 17:52:44 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.43 on epoch=207
06/15/2022 17:52:47 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.48 on epoch=208
06/15/2022 17:52:53 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.3097867771130542 on epoch=208
06/15/2022 17:52:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.43 on epoch=209
06/15/2022 17:52:58 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.49 on epoch=209
06/15/2022 17:53:01 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=210
06/15/2022 17:53:04 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.48 on epoch=211
06/15/2022 17:53:07 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.44 on epoch=212
06/15/2022 17:53:12 - INFO - __main__ - Global step 2550 Train loss 0.47 Classification-F1 0.19654293065586081 on epoch=212
06/15/2022 17:53:15 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.48 on epoch=213
06/15/2022 17:53:18 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.46 on epoch=214
06/15/2022 17:53:21 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.48 on epoch=214
06/15/2022 17:53:23 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.42 on epoch=215
06/15/2022 17:53:26 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=216
06/15/2022 17:53:32 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.2664999739542637 on epoch=216
06/15/2022 17:53:35 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.49 on epoch=217
06/15/2022 17:53:38 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=218
06/15/2022 17:53:40 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.47 on epoch=219
06/15/2022 17:53:43 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.48 on epoch=219
06/15/2022 17:53:46 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.44 on epoch=220
06/15/2022 17:53:52 - INFO - __main__ - Global step 2650 Train loss 0.46 Classification-F1 0.275196501611596 on epoch=220
06/15/2022 17:53:54 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.46 on epoch=221
06/15/2022 17:53:57 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.44 on epoch=222
06/15/2022 17:54:00 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.48 on epoch=223
06/15/2022 17:54:03 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=224
06/15/2022 17:54:05 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=224
06/15/2022 17:54:11 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.32498181289102285 on epoch=224
06/15/2022 17:54:14 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.47 on epoch=225
06/15/2022 17:54:17 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.43 on epoch=226
06/15/2022 17:54:19 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.50 on epoch=227
06/15/2022 17:54:22 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.48 on epoch=228
06/15/2022 17:54:25 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.44 on epoch=229
06/15/2022 17:54:31 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.2443024919319959 on epoch=229
06/15/2022 17:54:33 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.45 on epoch=229
06/15/2022 17:54:36 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.42 on epoch=230
06/15/2022 17:54:39 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=231
06/15/2022 17:54:42 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
06/15/2022 17:54:45 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.49 on epoch=233
06/15/2022 17:54:50 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.17676767676767677 on epoch=233
06/15/2022 17:54:53 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
06/15/2022 17:54:56 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.44 on epoch=234
06/15/2022 17:54:58 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.47 on epoch=235
06/15/2022 17:55:01 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.48 on epoch=236
06/15/2022 17:55:04 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=237
06/15/2022 17:55:09 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.23062795790068522 on epoch=237
06/15/2022 17:55:12 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
06/15/2022 17:55:15 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.46 on epoch=239
06/15/2022 17:55:18 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.46 on epoch=239
06/15/2022 17:55:21 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.43 on epoch=240
06/15/2022 17:55:23 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.45 on epoch=241
06/15/2022 17:55:29 - INFO - __main__ - Global step 2900 Train loss 0.45 Classification-F1 0.3535343895145384 on epoch=241
06/15/2022 17:55:32 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.46 on epoch=242
06/15/2022 17:55:35 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.42 on epoch=243
06/15/2022 17:55:38 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
06/15/2022 17:55:40 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.43 on epoch=244
06/15/2022 17:55:43 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.44 on epoch=245
06/15/2022 17:55:49 - INFO - __main__ - Global step 2950 Train loss 0.43 Classification-F1 0.33860578929492574 on epoch=245
06/15/2022 17:55:52 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.47 on epoch=246
06/15/2022 17:55:55 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.47 on epoch=247
06/15/2022 17:55:57 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=248
06/15/2022 17:56:00 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.41 on epoch=249
06/15/2022 17:56:03 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.44 on epoch=249
06/15/2022 17:56:04 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:56:04 - INFO - __main__ - Printing 3 examples
06/15/2022 17:56:04 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:56:04 - INFO - __main__ - ['neutral']
06/15/2022 17:56:04 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:56:04 - INFO - __main__ - ['neutral']
06/15/2022 17:56:04 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:56:04 - INFO - __main__ - ['neutral']
06/15/2022 17:56:04 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:56:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:56:05 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:56:05 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:56:05 - INFO - __main__ - Printing 3 examples
06/15/2022 17:56:05 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:56:05 - INFO - __main__ - ['neutral']
06/15/2022 17:56:05 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:56:05 - INFO - __main__ - ['neutral']
06/15/2022 17:56:05 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:56:05 - INFO - __main__ - ['neutral']
06/15/2022 17:56:05 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:56:05 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:56:05 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:56:09 - INFO - __main__ - Global step 3000 Train loss 0.45 Classification-F1 0.3599587141253808 on epoch=249
06/15/2022 17:56:09 - INFO - __main__ - save last model!
06/15/2022 17:56:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 17:56:09 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 17:56:09 - INFO - __main__ - Printing 3 examples
06/15/2022 17:56:09 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 17:56:09 - INFO - __main__ - ['contradiction']
06/15/2022 17:56:09 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 17:56:09 - INFO - __main__ - ['entailment']
06/15/2022 17:56:09 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 17:56:09 - INFO - __main__ - ['contradiction']
06/15/2022 17:56:09 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:56:09 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:56:10 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 17:56:26 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:56:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:56:27 - INFO - __main__ - Starting training!
06/15/2022 17:56:41 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_42_0.3_8_predictions.txt
06/15/2022 17:56:41 - INFO - __main__ - Classification-F1 on test data: 0.2798
06/15/2022 17:56:41 - INFO - __main__ - prefix=anli_64_42, lr=0.3, bsz=8, dev_performance=0.36970828101847136, test_performance=0.27981936918078587
06/15/2022 17:56:41 - INFO - __main__ - Running ... prefix=anli_64_42, lr=0.2, bsz=8 ...
06/15/2022 17:56:42 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:56:42 - INFO - __main__ - Printing 3 examples
06/15/2022 17:56:42 - INFO - __main__ -  [anli] premise: The Other One is the third solo album by former Fleetwood Mac guitarist Bob Welch. The track "Future Games" was first released on the Fleetwood Mac album of the same name in 1971. Members of Welch's backing band also make songwriting contributions here though the majority of tracks are Welch's own. [SEP] hypothesis: The Other One is an album by the group TOOL, for which Bob Welch did most of the backing vocals.
06/15/2022 17:56:42 - INFO - __main__ - ['neutral']
06/15/2022 17:56:42 - INFO - __main__ -  [anli] premise: The Living and the Dead is a British supernatural horror television miniseries created by Ashley Pharoah and Matthew Graham. The plot revolves around Nathan Appleby (played by Colin Morgan) and his wife, Charlotte Appleby (played by Charlotte Spencer), whose farm is believed to be at the centre of numerous supernatural occurrences. [SEP] hypothesis: The Living and the Dead was renewed for 7 seasons.
06/15/2022 17:56:42 - INFO - __main__ - ['neutral']
06/15/2022 17:56:42 - INFO - __main__ -  [anli] premise: Katie Liu Leung (born 8 August 1987) is a Scottish film, television, and stage actress. She played Cho Chang, the first love interest for lead character Harry Potter in the Harry Potter film series. In 2012, Leung made her stage debut in the play "Wild Swans". Leung has an interest in painting and photography and studied art and design at the University of the Arts, London. [SEP] hypothesis: Katie Liu Leung graduated with honors after studying art and design at the University of the Arts, London. 
06/15/2022 17:56:42 - INFO - __main__ - ['neutral']
06/15/2022 17:56:42 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:56:42 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:56:43 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 17:56:43 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 17:56:43 - INFO - __main__ - Printing 3 examples
06/15/2022 17:56:43 - INFO - __main__ -  [anli] premise: Blood for Irina is a 2012 vampire film written and directed by "Fangoria" editor and film critic Chris Alexander, who also helped to create the film's score. The movie released on November 2, 2012 in Belgium. A sequel, entitled "Queen of Blood", released in 2014. A third Irina film called BLood Dynasty is due out from Castle Films this Fall. [SEP] hypothesis: The Blood Dynasty movie will focus on vampires.
06/15/2022 17:56:43 - INFO - __main__ - ['neutral']
06/15/2022 17:56:43 - INFO - __main__ -  [anli] premise: Sportivo Atlético Club is an Argentine sports club, from the city of Las Parejas, in the Santa Fe Province. Although many sports are practised in the club, it is mostly known for its football and basketball teams. The football squad currently plays in the Torneo Argentino B, the regionalised 4th division of the Argentine football league system). [SEP] hypothesis: Sportivio Athletico Club has produced many pro football players. 
06/15/2022 17:56:43 - INFO - __main__ - ['neutral']
06/15/2022 17:56:43 - INFO - __main__ -  [anli] premise: Hikari is the second full-length album by British metalcore band Oceans Ate Alaska, released on July 28th 2017 through Fearless Records. It is the first album released by the band without original vocalist James Harrison, and instead it was recorded with Jake Noakes, who joined the band shortly after Harrison's departure. [SEP] hypothesis: Oceans Ate Alaska has released six albums. 
06/15/2022 17:56:43 - INFO - __main__ - ['neutral']
06/15/2022 17:56:43 - INFO - __main__ - Tokenizing Input ...
06/15/2022 17:56:43 - INFO - __main__ - Tokenizing Output ...
06/15/2022 17:56:43 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 17:57:00 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 17:57:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 17:57:01 - INFO - __main__ - Starting training!
06/15/2022 17:57:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.26 on epoch=0
06/15/2022 17:57:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.75 on epoch=1
06/15/2022 17:57:10 - INFO - __main__ - Step 30 Global step 30 Train loss 0.77 on epoch=2
06/15/2022 17:57:13 - INFO - __main__ - Step 40 Global step 40 Train loss 0.72 on epoch=3
06/15/2022 17:57:16 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=4
06/15/2022 17:57:20 - INFO - __main__ - Global step 50 Train loss 0.81 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 17:57:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 17:57:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=4
06/15/2022 17:57:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.59 on epoch=5
06/15/2022 17:57:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.68 on epoch=6
06/15/2022 17:57:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.60 on epoch=7
06/15/2022 17:57:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.55 on epoch=8
06/15/2022 17:57:37 - INFO - __main__ - Global step 100 Train loss 0.60 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 17:57:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.58 on epoch=9
06/15/2022 17:57:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.62 on epoch=9
06/15/2022 17:57:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.58 on epoch=10
06/15/2022 17:57:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.55 on epoch=11
06/15/2022 17:57:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.59 on epoch=12
06/15/2022 17:57:57 - INFO - __main__ - Global step 150 Train loss 0.59 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 17:58:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=13
06/15/2022 17:58:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.62 on epoch=14
06/15/2022 17:58:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.59 on epoch=14
06/15/2022 17:58:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.58 on epoch=15
06/15/2022 17:58:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.59 on epoch=16
06/15/2022 17:58:17 - INFO - __main__ - Global step 200 Train loss 0.58 Classification-F1 0.20547504025764896 on epoch=16
06/15/2022 17:58:17 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.20547504025764896 on epoch=16, global_step=200
06/15/2022 17:58:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=17
06/15/2022 17:58:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.16 on epoch=18
06/15/2022 17:58:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.81 on epoch=19
06/15/2022 17:58:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=19
06/15/2022 17:58:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.62 on epoch=20
06/15/2022 17:58:37 - INFO - __main__ - Global step 250 Train loss 0.82 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 17:58:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.60 on epoch=21
06/15/2022 17:58:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.57 on epoch=22
06/15/2022 17:58:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.53 on epoch=23
06/15/2022 17:58:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.57 on epoch=24
06/15/2022 17:58:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.56 on epoch=24
06/15/2022 17:58:56 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 17:58:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.55 on epoch=25
06/15/2022 17:59:02 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
06/15/2022 17:59:05 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=27
06/15/2022 17:59:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=28
06/15/2022 17:59:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.58 on epoch=29
06/15/2022 17:59:17 - INFO - __main__ - Global step 350 Train loss 0.53 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 17:59:20 - INFO - __main__ - Step 360 Global step 360 Train loss 0.55 on epoch=29
06/15/2022 17:59:23 - INFO - __main__ - Step 370 Global step 370 Train loss 0.55 on epoch=30
06/15/2022 17:59:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=31
06/15/2022 17:59:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.57 on epoch=32
06/15/2022 17:59:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.52 on epoch=33
06/15/2022 17:59:38 - INFO - __main__ - Global step 400 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 17:59:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.53 on epoch=34
06/15/2022 17:59:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.55 on epoch=34
06/15/2022 17:59:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=35
06/15/2022 17:59:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=36
06/15/2022 17:59:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.56 on epoch=37
06/15/2022 17:59:59 - INFO - __main__ - Global step 450 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 18:00:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=38
06/15/2022 18:00:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.55 on epoch=39
06/15/2022 18:00:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.56 on epoch=39
06/15/2022 18:00:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.62 on epoch=40
06/15/2022 18:00:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=41
06/15/2022 18:00:20 - INFO - __main__ - Global step 500 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 18:00:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=42
06/15/2022 18:00:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=43
06/15/2022 18:00:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=44
06/15/2022 18:00:32 - INFO - __main__ - Step 540 Global step 540 Train loss 0.58 on epoch=44
06/15/2022 18:00:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=45
06/15/2022 18:00:41 - INFO - __main__ - Global step 550 Train loss 0.53 Classification-F1 0.2622874446773818 on epoch=45
06/15/2022 18:00:41 - INFO - __main__ - Saving model with best Classification-F1: 0.20547504025764896 -> 0.2622874446773818 on epoch=45, global_step=550
06/15/2022 18:00:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.54 on epoch=46
06/15/2022 18:00:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.50 on epoch=47
06/15/2022 18:00:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.52 on epoch=48
06/15/2022 18:00:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.50 on epoch=49
06/15/2022 18:00:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.55 on epoch=49
06/15/2022 18:01:01 - INFO - __main__ - Global step 600 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=49
06/15/2022 18:01:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.58 on epoch=50
06/15/2022 18:01:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.50 on epoch=51
06/15/2022 18:01:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.53 on epoch=52
06/15/2022 18:01:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 18:01:15 - INFO - __main__ - Step 650 Global step 650 Train loss 0.62 on epoch=54
06/15/2022 18:01:21 - INFO - __main__ - Global step 650 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=54
06/15/2022 18:01:24 - INFO - __main__ - Step 660 Global step 660 Train loss 0.51 on epoch=54
06/15/2022 18:01:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.52 on epoch=55
06/15/2022 18:01:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.49 on epoch=56
06/15/2022 18:01:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.50 on epoch=57
06/15/2022 18:01:34 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=58
06/15/2022 18:01:40 - INFO - __main__ - Global step 700 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=58
06/15/2022 18:01:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=59
06/15/2022 18:01:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=59
06/15/2022 18:01:48 - INFO - __main__ - Step 730 Global step 730 Train loss 0.55 on epoch=60
06/15/2022 18:01:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.56 on epoch=61
06/15/2022 18:01:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.51 on epoch=62
06/15/2022 18:01:59 - INFO - __main__ - Global step 750 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 18:02:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.50 on epoch=63
06/15/2022 18:02:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.52 on epoch=64
06/15/2022 18:02:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.49 on epoch=64
06/15/2022 18:02:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.53 on epoch=65
06/15/2022 18:02:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.54 on epoch=66
06/15/2022 18:02:19 - INFO - __main__ - Global step 800 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 18:02:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.54 on epoch=67
06/15/2022 18:02:24 - INFO - __main__ - Step 820 Global step 820 Train loss 0.49 on epoch=68
06/15/2022 18:02:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.55 on epoch=69
06/15/2022 18:02:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=69
06/15/2022 18:02:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.55 on epoch=70
06/15/2022 18:02:39 - INFO - __main__ - Global step 850 Train loss 0.52 Classification-F1 0.2660968660968661 on epoch=70
06/15/2022 18:02:39 - INFO - __main__ - Saving model with best Classification-F1: 0.2622874446773818 -> 0.2660968660968661 on epoch=70, global_step=850
06/15/2022 18:02:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=71
06/15/2022 18:02:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=72
06/15/2022 18:02:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.53 on epoch=73
06/15/2022 18:02:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.57 on epoch=74
06/15/2022 18:02:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=74
06/15/2022 18:03:00 - INFO - __main__ - Global step 900 Train loss 0.53 Classification-F1 0.18627450980392157 on epoch=74
06/15/2022 18:03:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.56 on epoch=75
06/15/2022 18:03:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.52 on epoch=76
06/15/2022 18:03:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=77
06/15/2022 18:03:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.49 on epoch=78
06/15/2022 18:03:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.52 on epoch=79
06/15/2022 18:03:21 - INFO - __main__ - Global step 950 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 18:03:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.50 on epoch=79
06/15/2022 18:03:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=80
06/15/2022 18:03:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.56 on epoch=81
06/15/2022 18:03:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.48 on epoch=82
06/15/2022 18:03:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.53 on epoch=83
06/15/2022 18:03:42 - INFO - __main__ - Global step 1000 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 18:03:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.55 on epoch=84
06/15/2022 18:03:48 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=84
06/15/2022 18:03:51 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=85
06/15/2022 18:03:54 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.57 on epoch=86
06/15/2022 18:03:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=87
06/15/2022 18:04:04 - INFO - __main__ - Global step 1050 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 18:04:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.49 on epoch=88
06/15/2022 18:04:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.53 on epoch=89
06/15/2022 18:04:13 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.53 on epoch=89
06/15/2022 18:04:16 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.60 on epoch=90
06/15/2022 18:04:19 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.50 on epoch=91
06/15/2022 18:04:24 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.30586850803869653 on epoch=91
06/15/2022 18:04:24 - INFO - __main__ - Saving model with best Classification-F1: 0.2660968660968661 -> 0.30586850803869653 on epoch=91, global_step=1100
06/15/2022 18:04:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.51 on epoch=92
06/15/2022 18:04:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.49 on epoch=93
06/15/2022 18:04:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.49 on epoch=94
06/15/2022 18:04:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=94
06/15/2022 18:04:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.53 on epoch=95
06/15/2022 18:04:44 - INFO - __main__ - Global step 1150 Train loss 0.52 Classification-F1 0.2780337267634885 on epoch=95
06/15/2022 18:04:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=96
06/15/2022 18:04:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=97
06/15/2022 18:04:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.47 on epoch=98
06/15/2022 18:04:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.53 on epoch=99
06/15/2022 18:04:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.50 on epoch=99
06/15/2022 18:05:03 - INFO - __main__ - Global step 1200 Train loss 0.50 Classification-F1 0.1775766716943188 on epoch=99
06/15/2022 18:05:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.48 on epoch=100
06/15/2022 18:05:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.50 on epoch=101
06/15/2022 18:05:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.51 on epoch=102
06/15/2022 18:05:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.49 on epoch=103
06/15/2022 18:05:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.46 on epoch=104
06/15/2022 18:05:22 - INFO - __main__ - Global step 1250 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 18:05:25 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=104
06/15/2022 18:05:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.50 on epoch=105
06/15/2022 18:05:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.49 on epoch=106
06/15/2022 18:05:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.50 on epoch=107
06/15/2022 18:05:36 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.53 on epoch=108
06/15/2022 18:05:42 - INFO - __main__ - Global step 1300 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 18:05:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=109
06/15/2022 18:05:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=109
06/15/2022 18:05:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.56 on epoch=110
06/15/2022 18:05:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=111
06/15/2022 18:05:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=112
06/15/2022 18:06:01 - INFO - __main__ - Global step 1350 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=112
06/15/2022 18:06:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.48 on epoch=113
06/15/2022 18:06:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.46 on epoch=114
06/15/2022 18:06:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.49 on epoch=114
06/15/2022 18:06:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=115
06/15/2022 18:06:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.48 on epoch=116
06/15/2022 18:06:20 - INFO - __main__ - Global step 1400 Train loss 0.48 Classification-F1 0.26802341048151107 on epoch=116
06/15/2022 18:06:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.50 on epoch=117
06/15/2022 18:06:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.47 on epoch=118
06/15/2022 18:06:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.57 on epoch=119
06/15/2022 18:06:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.51 on epoch=119
06/15/2022 18:06:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.55 on epoch=120
06/15/2022 18:06:41 - INFO - __main__ - Global step 1450 Train loss 0.52 Classification-F1 0.1775766716943188 on epoch=120
06/15/2022 18:06:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.52 on epoch=121
06/15/2022 18:06:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.49 on epoch=122
06/15/2022 18:06:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.47 on epoch=123
06/15/2022 18:06:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=124
06/15/2022 18:06:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.62 on epoch=124
06/15/2022 18:07:04 - INFO - __main__ - Global step 1500 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=124
06/15/2022 18:07:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.51 on epoch=125
06/15/2022 18:07:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.51 on epoch=126
06/15/2022 18:07:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=127
06/15/2022 18:07:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.50 on epoch=128
06/15/2022 18:07:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.50 on epoch=129
06/15/2022 18:07:28 - INFO - __main__ - Global step 1550 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 18:07:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.52 on epoch=129
06/15/2022 18:07:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.53 on epoch=130
06/15/2022 18:07:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=131
06/15/2022 18:07:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.49 on epoch=132
06/15/2022 18:07:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=133
06/15/2022 18:07:51 - INFO - __main__ - Global step 1600 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 18:07:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=134
06/15/2022 18:07:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.51 on epoch=134
06/15/2022 18:08:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.51 on epoch=135
06/15/2022 18:08:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.50 on epoch=136
06/15/2022 18:08:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=137
06/15/2022 18:08:13 - INFO - __main__ - Global step 1650 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=137
06/15/2022 18:08:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.50 on epoch=138
06/15/2022 18:08:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=139
06/15/2022 18:08:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.53 on epoch=139
06/15/2022 18:08:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.51 on epoch=140
06/15/2022 18:08:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.52 on epoch=141
06/15/2022 18:08:35 - INFO - __main__ - Global step 1700 Train loss 0.52 Classification-F1 0.21593478088295695 on epoch=141
06/15/2022 18:08:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.44 on epoch=142
06/15/2022 18:08:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=143
06/15/2022 18:08:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.54 on epoch=144
06/15/2022 18:08:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.49 on epoch=144
06/15/2022 18:08:50 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.54 on epoch=145
06/15/2022 18:08:57 - INFO - __main__ - Global step 1750 Train loss 0.50 Classification-F1 0.1775766716943188 on epoch=145
06/15/2022 18:09:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.53 on epoch=146
06/15/2022 18:09:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.50 on epoch=147
06/15/2022 18:09:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.54 on epoch=148
06/15/2022 18:09:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.48 on epoch=149
06/15/2022 18:09:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=149
06/15/2022 18:09:18 - INFO - __main__ - Global step 1800 Train loss 0.50 Classification-F1 0.27331577792407286 on epoch=149
06/15/2022 18:09:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.55 on epoch=150
06/15/2022 18:09:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.47 on epoch=151
06/15/2022 18:09:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=152
06/15/2022 18:09:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.48 on epoch=153
06/15/2022 18:09:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.50 on epoch=154
06/15/2022 18:09:40 - INFO - __main__ - Global step 1850 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=154
06/15/2022 18:09:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.53 on epoch=154
06/15/2022 18:09:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.53 on epoch=155
06/15/2022 18:09:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=156
06/15/2022 18:09:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.53 on epoch=157
06/15/2022 18:09:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=158
06/15/2022 18:10:01 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=158
06/15/2022 18:10:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.53 on epoch=159
06/15/2022 18:10:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.52 on epoch=159
06/15/2022 18:10:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.50 on epoch=160
06/15/2022 18:10:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.49 on epoch=161
06/15/2022 18:10:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.45 on epoch=162
06/15/2022 18:10:21 - INFO - __main__ - Global step 1950 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=162
06/15/2022 18:10:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
06/15/2022 18:10:26 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.47 on epoch=164
06/15/2022 18:10:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.50 on epoch=164
06/15/2022 18:10:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.46 on epoch=165
06/15/2022 18:10:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=166
06/15/2022 18:10:41 - INFO - __main__ - Global step 2000 Train loss 0.46 Classification-F1 0.20102851799906496 on epoch=166
06/15/2022 18:10:44 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.43 on epoch=167
06/15/2022 18:10:47 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.47 on epoch=168
06/15/2022 18:10:50 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.48 on epoch=169
06/15/2022 18:10:53 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.53 on epoch=169
06/15/2022 18:10:56 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.45 on epoch=170
06/15/2022 18:11:01 - INFO - __main__ - Global step 2050 Train loss 0.47 Classification-F1 0.25697331688632086 on epoch=170
06/15/2022 18:11:04 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.49 on epoch=171
06/15/2022 18:11:07 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.48 on epoch=172
06/15/2022 18:11:10 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.47 on epoch=173
06/15/2022 18:11:14 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.49 on epoch=174
06/15/2022 18:11:17 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.47 on epoch=174
06/15/2022 18:11:23 - INFO - __main__ - Global step 2100 Train loss 0.48 Classification-F1 0.19504870724382917 on epoch=174
06/15/2022 18:11:26 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.54 on epoch=175
06/15/2022 18:11:29 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.55 on epoch=176
06/15/2022 18:11:32 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.53 on epoch=177
06/15/2022 18:11:35 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.46 on epoch=178
06/15/2022 18:11:38 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.49 on epoch=179
06/15/2022 18:11:45 - INFO - __main__ - Global step 2150 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=179
06/15/2022 18:11:48 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.54 on epoch=179
06/15/2022 18:11:51 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.51 on epoch=180
06/15/2022 18:11:54 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.47 on epoch=181
06/15/2022 18:11:57 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.49 on epoch=182
06/15/2022 18:12:00 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.45 on epoch=183
06/15/2022 18:12:06 - INFO - __main__ - Global step 2200 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=183
06/15/2022 18:12:09 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.45 on epoch=184
06/15/2022 18:12:12 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.46 on epoch=184
06/15/2022 18:12:15 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.44 on epoch=185
06/15/2022 18:12:18 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.48 on epoch=186
06/15/2022 18:12:21 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.50 on epoch=187
06/15/2022 18:12:28 - INFO - __main__ - Global step 2250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=187
06/15/2022 18:12:31 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.43 on epoch=188
06/15/2022 18:12:34 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.48 on epoch=189
06/15/2022 18:12:37 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=189
06/15/2022 18:12:40 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.50 on epoch=190
06/15/2022 18:12:43 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.45 on epoch=191
06/15/2022 18:12:49 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=191
06/15/2022 18:12:52 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.52 on epoch=192
06/15/2022 18:12:55 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.45 on epoch=193
06/15/2022 18:12:59 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.45 on epoch=194
06/15/2022 18:13:02 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.49 on epoch=194
06/15/2022 18:13:05 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.49 on epoch=195
06/15/2022 18:13:11 - INFO - __main__ - Global step 2350 Train loss 0.48 Classification-F1 0.2498274672187716 on epoch=195
06/15/2022 18:13:14 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.45 on epoch=196
06/15/2022 18:13:17 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.46 on epoch=197
06/15/2022 18:13:20 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.51 on epoch=198
06/15/2022 18:13:23 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.51 on epoch=199
06/15/2022 18:13:26 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.49 on epoch=199
06/15/2022 18:13:31 - INFO - __main__ - Global step 2400 Train loss 0.48 Classification-F1 0.2569444444444444 on epoch=199
06/15/2022 18:13:34 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.49 on epoch=200
06/15/2022 18:13:37 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.50 on epoch=201
06/15/2022 18:13:40 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.46 on epoch=202
06/15/2022 18:13:42 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.48 on epoch=203
06/15/2022 18:13:45 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.54 on epoch=204
06/15/2022 18:13:51 - INFO - __main__ - Global step 2450 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=204
06/15/2022 18:13:54 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.48 on epoch=204
06/15/2022 18:13:57 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.44 on epoch=205
06/15/2022 18:14:00 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.46 on epoch=206
06/15/2022 18:14:03 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.46 on epoch=207
06/15/2022 18:14:06 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.46 on epoch=208
06/15/2022 18:14:12 - INFO - __main__ - Global step 2500 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=208
06/15/2022 18:14:15 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.48 on epoch=209
06/15/2022 18:14:18 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.45 on epoch=209
06/15/2022 18:14:21 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.49 on epoch=210
06/15/2022 18:14:24 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.52 on epoch=211
06/15/2022 18:14:27 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.46 on epoch=212
06/15/2022 18:14:33 - INFO - __main__ - Global step 2550 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=212
06/15/2022 18:14:36 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.47 on epoch=213
06/15/2022 18:14:38 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.51 on epoch=214
06/15/2022 18:14:41 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.45 on epoch=214
06/15/2022 18:14:44 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.45 on epoch=215
06/15/2022 18:14:47 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=216
06/15/2022 18:14:53 - INFO - __main__ - Global step 2600 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=216
06/15/2022 18:14:55 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.54 on epoch=217
06/15/2022 18:14:58 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.47 on epoch=218
06/15/2022 18:15:01 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.45 on epoch=219
06/15/2022 18:15:04 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.44 on epoch=219
06/15/2022 18:15:07 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.49 on epoch=220
06/15/2022 18:15:13 - INFO - __main__ - Global step 2650 Train loss 0.48 Classification-F1 0.3039013775481523 on epoch=220
06/15/2022 18:15:15 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.53 on epoch=221
06/15/2022 18:15:18 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.46 on epoch=222
06/15/2022 18:15:21 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.49 on epoch=223
06/15/2022 18:15:23 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.47 on epoch=224
06/15/2022 18:15:26 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=224
06/15/2022 18:15:32 - INFO - __main__ - Global step 2700 Train loss 0.48 Classification-F1 0.16732026143790854 on epoch=224
06/15/2022 18:15:34 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.47 on epoch=225
06/15/2022 18:15:37 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.48 on epoch=226
06/15/2022 18:15:40 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.46 on epoch=227
06/15/2022 18:15:42 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=228
06/15/2022 18:15:45 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.46 on epoch=229
06/15/2022 18:15:51 - INFO - __main__ - Global step 2750 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=229
06/15/2022 18:15:53 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.43 on epoch=229
06/15/2022 18:15:56 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.51 on epoch=230
06/15/2022 18:15:59 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=231
06/15/2022 18:16:01 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
06/15/2022 18:16:04 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
06/15/2022 18:16:10 - INFO - __main__ - Global step 2800 Train loss 0.46 Classification-F1 0.29582643868358155 on epoch=233
06/15/2022 18:16:12 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.51 on epoch=234
06/15/2022 18:16:15 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.39 on epoch=234
06/15/2022 18:16:18 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.48 on epoch=235
06/15/2022 18:16:20 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.49 on epoch=236
06/15/2022 18:16:23 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.52 on epoch=237
06/15/2022 18:16:29 - INFO - __main__ - Global step 2850 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=237
06/15/2022 18:16:31 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
06/15/2022 18:16:34 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.49 on epoch=239
06/15/2022 18:16:37 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.48 on epoch=239
06/15/2022 18:16:39 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.50 on epoch=240
06/15/2022 18:16:42 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.51 on epoch=241
06/15/2022 18:16:48 - INFO - __main__ - Global step 2900 Train loss 0.49 Classification-F1 0.2722988701527281 on epoch=241
06/15/2022 18:16:51 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.54 on epoch=242
06/15/2022 18:16:53 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.45 on epoch=243
06/15/2022 18:16:56 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.46 on epoch=244
06/15/2022 18:16:58 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.46 on epoch=244
06/15/2022 18:17:01 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.48 on epoch=245
06/15/2022 18:17:07 - INFO - __main__ - Global step 2950 Train loss 0.48 Classification-F1 0.2451805373153688 on epoch=245
06/15/2022 18:17:10 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=246
06/15/2022 18:17:13 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.49 on epoch=247
06/15/2022 18:17:16 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
06/15/2022 18:17:19 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.50 on epoch=249
06/15/2022 18:17:22 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.46 on epoch=249
06/15/2022 18:17:24 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:17:24 - INFO - __main__ - Printing 3 examples
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:17:24 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:17:24 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 18:17:24 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:17:24 - INFO - __main__ - Printing 3 examples
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 18:17:24 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:24 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:17:24 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:17:25 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 18:17:28 - INFO - __main__ - Global step 3000 Train loss 0.47 Classification-F1 0.17752380952380953 on epoch=249
06/15/2022 18:17:28 - INFO - __main__ - save last model!
06/15/2022 18:17:28 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 18:17:28 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 18:17:28 - INFO - __main__ - Printing 3 examples
06/15/2022 18:17:28 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 18:17:28 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:28 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 18:17:28 - INFO - __main__ - ['entailment']
06/15/2022 18:17:28 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 18:17:28 - INFO - __main__ - ['contradiction']
06/15/2022 18:17:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:17:29 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:17:30 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 18:17:42 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 18:17:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 18:17:43 - INFO - __main__ - Starting training!
06/15/2022 18:18:05 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_42_0.2_8_predictions.txt
06/15/2022 18:18:05 - INFO - __main__ - Classification-F1 on test data: 0.1864
06/15/2022 18:18:05 - INFO - __main__ - prefix=anli_64_42, lr=0.2, bsz=8, dev_performance=0.30586850803869653, test_performance=0.18641374973800043
06/15/2022 18:18:05 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.5, bsz=8 ...
06/15/2022 18:18:06 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:18:06 - INFO - __main__ - Printing 3 examples
06/15/2022 18:18:06 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 18:18:06 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:06 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 18:18:06 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:06 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 18:18:06 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:06 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:18:06 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:18:07 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 18:18:07 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:18:07 - INFO - __main__ - Printing 3 examples
06/15/2022 18:18:07 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 18:18:07 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:07 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 18:18:07 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:07 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 18:18:07 - INFO - __main__ - ['contradiction']
06/15/2022 18:18:07 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:18:07 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:18:07 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 18:18:23 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 18:18:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 18:18:24 - INFO - __main__ - Starting training!
06/15/2022 18:18:28 - INFO - __main__ - Step 10 Global step 10 Train loss 0.93 on epoch=0
06/15/2022 18:18:31 - INFO - __main__ - Step 20 Global step 20 Train loss 0.69 on epoch=1
06/15/2022 18:18:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=2
06/15/2022 18:18:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.63 on epoch=3
06/15/2022 18:18:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=4
06/15/2022 18:18:46 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 18:18:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 18:18:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=4
06/15/2022 18:18:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.54 on epoch=5
06/15/2022 18:18:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.56 on epoch=6
06/15/2022 18:18:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
06/15/2022 18:19:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=8
06/15/2022 18:19:07 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 18:19:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.49 on epoch=9
06/15/2022 18:19:13 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=9
06/15/2022 18:19:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=10
06/15/2022 18:19:18 - INFO - __main__ - Step 140 Global step 140 Train loss 0.46 on epoch=11
06/15/2022 18:19:21 - INFO - __main__ - Step 150 Global step 150 Train loss 0.51 on epoch=12
06/15/2022 18:19:25 - INFO - __main__ - Global step 150 Train loss 0.50 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 18:19:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.58 on epoch=13
06/15/2022 18:19:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.55 on epoch=14
06/15/2022 18:19:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.47 on epoch=14
06/15/2022 18:19:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.48 on epoch=15
06/15/2022 18:19:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.46 on epoch=16
06/15/2022 18:19:46 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 18:19:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.46 on epoch=17
06/15/2022 18:19:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.52 on epoch=18
06/15/2022 18:19:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=19
06/15/2022 18:19:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=19
06/15/2022 18:20:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=20
06/15/2022 18:20:06 - INFO - __main__ - Global step 250 Train loss 0.48 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 18:20:09 - INFO - __main__ - Step 260 Global step 260 Train loss 0.53 on epoch=21
06/15/2022 18:20:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.43 on epoch=22
06/15/2022 18:20:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.55 on epoch=23
06/15/2022 18:20:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.47 on epoch=24
06/15/2022 18:20:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.47 on epoch=24
06/15/2022 18:20:27 - INFO - __main__ - Global step 300 Train loss 0.49 Classification-F1 0.25379867046533716 on epoch=24
06/15/2022 18:20:27 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.25379867046533716 on epoch=24, global_step=300
06/15/2022 18:20:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=25
06/15/2022 18:20:32 - INFO - __main__ - Step 320 Global step 320 Train loss 0.49 on epoch=26
06/15/2022 18:20:35 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=27
06/15/2022 18:20:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.48 on epoch=28
06/15/2022 18:20:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.42 on epoch=29
06/15/2022 18:20:47 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 18:20:50 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=29
06/15/2022 18:20:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.46 on epoch=30
06/15/2022 18:20:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=31
06/15/2022 18:20:58 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=32
06/15/2022 18:21:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.65 on epoch=33
06/15/2022 18:21:08 - INFO - __main__ - Global step 400 Train loss 0.49 Classification-F1 0.23080241587575492 on epoch=33
06/15/2022 18:21:11 - INFO - __main__ - Step 410 Global step 410 Train loss 3.00 on epoch=34
06/15/2022 18:21:14 - INFO - __main__ - Step 420 Global step 420 Train loss 0.75 on epoch=34
06/15/2022 18:21:17 - INFO - __main__ - Step 430 Global step 430 Train loss 0.41 on epoch=35
06/15/2022 18:21:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=36
06/15/2022 18:21:23 - INFO - __main__ - Step 450 Global step 450 Train loss 0.42 on epoch=37
06/15/2022 18:21:29 - INFO - __main__ - Global step 450 Train loss 1.00 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 18:21:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=38
06/15/2022 18:21:35 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=39
06/15/2022 18:21:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
06/15/2022 18:21:41 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=40
06/15/2022 18:21:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=41
06/15/2022 18:21:50 - INFO - __main__ - Global step 500 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 18:21:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=42
06/15/2022 18:21:56 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=43
06/15/2022 18:21:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
06/15/2022 18:22:02 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=44
06/15/2022 18:22:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
06/15/2022 18:22:11 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.18134226379506133 on epoch=45
06/15/2022 18:22:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.48 on epoch=46
06/15/2022 18:22:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=47
06/15/2022 18:22:20 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=48
06/15/2022 18:22:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.44 on epoch=49
06/15/2022 18:22:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
06/15/2022 18:22:32 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16732026143790854 on epoch=49
06/15/2022 18:22:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.47 on epoch=50
06/15/2022 18:22:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.45 on epoch=51
06/15/2022 18:22:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=52
06/15/2022 18:22:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=53
06/15/2022 18:22:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
06/15/2022 18:22:52 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.18134226379506133 on epoch=54
06/15/2022 18:22:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=54
06/15/2022 18:22:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.46 on epoch=55
06/15/2022 18:23:01 - INFO - __main__ - Step 680 Global step 680 Train loss 0.46 on epoch=56
06/15/2022 18:23:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=57
06/15/2022 18:23:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=58
06/15/2022 18:23:13 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.18290721535133936 on epoch=58
06/15/2022 18:23:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.40 on epoch=59
06/15/2022 18:23:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.47 on epoch=59
06/15/2022 18:23:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
06/15/2022 18:23:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.43 on epoch=61
06/15/2022 18:23:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.43 on epoch=62
06/15/2022 18:23:33 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 18:23:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
06/15/2022 18:23:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=64
06/15/2022 18:23:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.43 on epoch=64
06/15/2022 18:23:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.44 on epoch=65
06/15/2022 18:23:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.41 on epoch=66
06/15/2022 18:23:54 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.21930058967096003 on epoch=66
06/15/2022 18:23:56 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=67
06/15/2022 18:23:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=68
06/15/2022 18:24:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=69
06/15/2022 18:24:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.46 on epoch=69
06/15/2022 18:24:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=70
06/15/2022 18:24:14 - INFO - __main__ - Global step 850 Train loss 0.42 Classification-F1 0.22424951267056534 on epoch=70
06/15/2022 18:24:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=71
06/15/2022 18:24:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=72
06/15/2022 18:24:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=73
06/15/2022 18:24:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=74
06/15/2022 18:24:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=74
06/15/2022 18:24:33 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.20779407535231523 on epoch=74
06/15/2022 18:24:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=75
06/15/2022 18:24:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=76
06/15/2022 18:24:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=77
06/15/2022 18:24:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 18:24:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.46 on epoch=79
06/15/2022 18:24:53 - INFO - __main__ - Global step 950 Train loss 0.44 Classification-F1 0.2400793650793651 on epoch=79
06/15/2022 18:24:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.43 on epoch=79
06/15/2022 18:24:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.40 on epoch=80
06/15/2022 18:25:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=81
06/15/2022 18:25:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=82
06/15/2022 18:25:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=83
06/15/2022 18:25:13 - INFO - __main__ - Global step 1000 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=83
06/15/2022 18:25:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=84
06/15/2022 18:25:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.47 on epoch=84
06/15/2022 18:25:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.41 on epoch=85
06/15/2022 18:25:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
06/15/2022 18:25:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=87
06/15/2022 18:25:34 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=87
06/15/2022 18:25:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=88
06/15/2022 18:25:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=89
06/15/2022 18:25:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.40 on epoch=89
06/15/2022 18:25:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.43 on epoch=90
06/15/2022 18:25:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=91
06/15/2022 18:25:55 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.26912241682883886 on epoch=91
06/15/2022 18:25:55 - INFO - __main__ - Saving model with best Classification-F1: 0.25379867046533716 -> 0.26912241682883886 on epoch=91, global_step=1100
06/15/2022 18:25:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
06/15/2022 18:26:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=93
06/15/2022 18:26:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=94
06/15/2022 18:26:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=94
06/15/2022 18:26:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.44 on epoch=95
06/15/2022 18:26:16 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.17332310778125185 on epoch=95
06/15/2022 18:26:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
06/15/2022 18:26:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.42 on epoch=97
06/15/2022 18:26:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=98
06/15/2022 18:26:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=99
06/15/2022 18:26:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.39 on epoch=99
06/15/2022 18:26:35 - INFO - __main__ - Global step 1200 Train loss 0.43 Classification-F1 0.2498906587859242 on epoch=99
06/15/2022 18:26:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=100
06/15/2022 18:26:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=101
06/15/2022 18:26:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.40 on epoch=102
06/15/2022 18:26:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=103
06/15/2022 18:26:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.45 on epoch=104
06/15/2022 18:26:55 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.1647058823529412 on epoch=104
06/15/2022 18:26:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.42 on epoch=104
06/15/2022 18:27:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.40 on epoch=105
06/15/2022 18:27:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=106
06/15/2022 18:27:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.41 on epoch=107
06/15/2022 18:27:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
06/15/2022 18:27:14 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=108
06/15/2022 18:27:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.43 on epoch=109
06/15/2022 18:27:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.42 on epoch=109
06/15/2022 18:27:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
06/15/2022 18:27:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=111
06/15/2022 18:27:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=112
06/15/2022 18:27:33 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.21043083900226758 on epoch=112
06/15/2022 18:27:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
06/15/2022 18:27:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.44 on epoch=114
06/15/2022 18:27:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=114
06/15/2022 18:27:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.37 on epoch=115
06/15/2022 18:27:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.41 on epoch=116
06/15/2022 18:27:54 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.32089998825152427 on epoch=116
06/15/2022 18:27:54 - INFO - __main__ - Saving model with best Classification-F1: 0.26912241682883886 -> 0.32089998825152427 on epoch=116, global_step=1400
06/15/2022 18:27:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
06/15/2022 18:27:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=118
06/15/2022 18:28:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=119
06/15/2022 18:28:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.43 on epoch=119
06/15/2022 18:28:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
06/15/2022 18:28:14 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.21294567943134038 on epoch=120
06/15/2022 18:28:17 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=121
06/15/2022 18:28:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.40 on epoch=122
06/15/2022 18:28:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.46 on epoch=123
06/15/2022 18:28:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.46 on epoch=124
06/15/2022 18:28:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=124
06/15/2022 18:28:35 - INFO - __main__ - Global step 1500 Train loss 0.44 Classification-F1 0.1647058823529412 on epoch=124
06/15/2022 18:28:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=125
06/15/2022 18:28:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=126
06/15/2022 18:28:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
06/15/2022 18:28:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.46 on epoch=128
06/15/2022 18:28:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=129
06/15/2022 18:28:56 - INFO - __main__ - Global step 1550 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=129
06/15/2022 18:28:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
06/15/2022 18:29:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.44 on epoch=130
06/15/2022 18:29:05 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=131
06/15/2022 18:29:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=132
06/15/2022 18:29:11 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=133
06/15/2022 18:29:17 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=133
06/15/2022 18:29:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
06/15/2022 18:29:24 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 18:29:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=135
06/15/2022 18:29:30 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.45 on epoch=136
06/15/2022 18:29:33 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.41 on epoch=137
06/15/2022 18:29:39 - INFO - __main__ - Global step 1650 Train loss 0.43 Classification-F1 0.24070252891213587 on epoch=137
06/15/2022 18:29:42 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.43 on epoch=138
06/15/2022 18:29:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=139
06/15/2022 18:29:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
06/15/2022 18:29:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=140
06/15/2022 18:29:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.43 on epoch=141
06/15/2022 18:30:00 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.23365079365079364 on epoch=141
06/15/2022 18:30:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.42 on epoch=142
06/15/2022 18:30:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=143
06/15/2022 18:30:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=144
06/15/2022 18:30:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=144
06/15/2022 18:30:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
06/15/2022 18:30:20 - INFO - __main__ - Global step 1750 Train loss 0.43 Classification-F1 0.22053960241492984 on epoch=145
06/15/2022 18:30:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=146
06/15/2022 18:30:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=147
06/15/2022 18:30:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=148
06/15/2022 18:30:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=149
06/15/2022 18:30:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=149
06/15/2022 18:30:41 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.256969696969697 on epoch=149
06/15/2022 18:30:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.43 on epoch=150
06/15/2022 18:30:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
06/15/2022 18:30:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=152
06/15/2022 18:30:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.47 on epoch=153
06/15/2022 18:30:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=154
06/15/2022 18:31:00 - INFO - __main__ - Global step 1850 Train loss 0.45 Classification-F1 0.1754553408096715 on epoch=154
06/15/2022 18:31:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
06/15/2022 18:31:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.40 on epoch=155
06/15/2022 18:31:08 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=156
06/15/2022 18:31:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.45 on epoch=157
06/15/2022 18:31:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=158
06/15/2022 18:31:19 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=158
06/15/2022 18:31:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.45 on epoch=159
06/15/2022 18:31:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.38 on epoch=159
06/15/2022 18:31:27 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.43 on epoch=160
06/15/2022 18:31:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=161
06/15/2022 18:31:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=162
06/15/2022 18:31:38 - INFO - __main__ - Global step 1950 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=162
06/15/2022 18:31:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.43 on epoch=163
06/15/2022 18:31:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.42 on epoch=164
06/15/2022 18:31:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.39 on epoch=164
06/15/2022 18:31:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 18:31:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
06/15/2022 18:31:58 - INFO - __main__ - Global step 2000 Train loss 0.41 Classification-F1 0.2504767959313414 on epoch=166
06/15/2022 18:32:01 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
06/15/2022 18:32:04 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
06/15/2022 18:32:07 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
06/15/2022 18:32:09 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
06/15/2022 18:32:12 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.36 on epoch=170
06/15/2022 18:32:18 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.23611111111111108 on epoch=170
06/15/2022 18:32:21 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.41 on epoch=171
06/15/2022 18:32:24 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.41 on epoch=172
06/15/2022 18:32:27 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.44 on epoch=173
06/15/2022 18:32:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.39 on epoch=174
06/15/2022 18:32:32 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=174
06/15/2022 18:32:38 - INFO - __main__ - Global step 2100 Train loss 0.41 Classification-F1 0.27971626766807495 on epoch=174
06/15/2022 18:32:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.37 on epoch=175
06/15/2022 18:32:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.42 on epoch=176
06/15/2022 18:32:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.42 on epoch=177
06/15/2022 18:32:49 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=178
06/15/2022 18:32:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.43 on epoch=179
06/15/2022 18:32:58 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=179
06/15/2022 18:33:01 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.42 on epoch=179
06/15/2022 18:33:04 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.44 on epoch=180
06/15/2022 18:33:07 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.41 on epoch=181
06/15/2022 18:33:10 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.46 on epoch=182
06/15/2022 18:33:12 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
06/15/2022 18:33:18 - INFO - __main__ - Global step 2200 Train loss 0.43 Classification-F1 0.1939047619047619 on epoch=183
06/15/2022 18:33:21 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 18:33:24 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
06/15/2022 18:33:26 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.37 on epoch=185
06/15/2022 18:33:29 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.44 on epoch=186
06/15/2022 18:33:32 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.38 on epoch=187
06/15/2022 18:33:38 - INFO - __main__ - Global step 2250 Train loss 0.40 Classification-F1 0.2832581130453471 on epoch=187
06/15/2022 18:33:41 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.49 on epoch=188
06/15/2022 18:33:44 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=189
06/15/2022 18:33:48 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.41 on epoch=189
06/15/2022 18:33:51 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.39 on epoch=190
06/15/2022 18:33:55 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=191
06/15/2022 18:34:00 - INFO - __main__ - Global step 2300 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=191
06/15/2022 18:34:03 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.40 on epoch=192
06/15/2022 18:34:07 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.39 on epoch=193
06/15/2022 18:34:10 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.43 on epoch=194
06/15/2022 18:34:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.43 on epoch=194
06/15/2022 18:34:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.40 on epoch=195
06/15/2022 18:34:23 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.26801203607538554 on epoch=195
06/15/2022 18:34:26 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.40 on epoch=196
06/15/2022 18:34:28 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.42 on epoch=197
06/15/2022 18:34:31 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.42 on epoch=198
06/15/2022 18:34:34 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=199
06/15/2022 18:34:37 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.42 on epoch=199
06/15/2022 18:34:43 - INFO - __main__ - Global step 2400 Train loss 0.42 Classification-F1 0.1881810228266921 on epoch=199
06/15/2022 18:34:45 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.43 on epoch=200
06/15/2022 18:34:48 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.44 on epoch=201
06/15/2022 18:34:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.40 on epoch=202
06/15/2022 18:34:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.45 on epoch=203
06/15/2022 18:34:56 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.47 on epoch=204
06/15/2022 18:35:02 - INFO - __main__ - Global step 2450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=204
06/15/2022 18:35:05 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.41 on epoch=204
06/15/2022 18:35:08 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.40 on epoch=205
06/15/2022 18:35:11 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.43 on epoch=206
06/15/2022 18:35:14 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.41 on epoch=207
06/15/2022 18:35:17 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.40 on epoch=208
06/15/2022 18:35:23 - INFO - __main__ - Global step 2500 Train loss 0.41 Classification-F1 0.2654215400736597 on epoch=208
06/15/2022 18:35:26 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
06/15/2022 18:35:29 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.42 on epoch=209
06/15/2022 18:35:32 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.43 on epoch=210
06/15/2022 18:35:35 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.44 on epoch=211
06/15/2022 18:35:38 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
06/15/2022 18:35:45 - INFO - __main__ - Global step 2550 Train loss 0.41 Classification-F1 0.24559091194376448 on epoch=212
06/15/2022 18:35:48 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.44 on epoch=213
06/15/2022 18:35:51 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.39 on epoch=214
06/15/2022 18:35:54 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.42 on epoch=214
06/15/2022 18:35:57 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.39 on epoch=215
06/15/2022 18:36:00 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.45 on epoch=216
06/15/2022 18:36:06 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.29178736077679407 on epoch=216
06/15/2022 18:36:09 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.39 on epoch=217
06/15/2022 18:36:12 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=218
06/15/2022 18:36:15 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.39 on epoch=219
06/15/2022 18:36:18 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.40 on epoch=219
06/15/2022 18:36:21 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
06/15/2022 18:36:28 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.24570977816413922 on epoch=220
06/15/2022 18:36:30 - INFO - __main__ - Step 2660 Global step 2660 Train loss 1.08 on epoch=221
06/15/2022 18:36:33 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.54 on epoch=222
06/15/2022 18:36:36 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.54 on epoch=223
06/15/2022 18:36:39 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.49 on epoch=224
06/15/2022 18:36:43 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.48 on epoch=224
06/15/2022 18:36:49 - INFO - __main__ - Global step 2700 Train loss 0.63 Classification-F1 0.2839217120907262 on epoch=224
06/15/2022 18:36:52 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.40 on epoch=225
06/15/2022 18:36:55 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.37 on epoch=226
06/15/2022 18:36:58 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
06/15/2022 18:37:01 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.40 on epoch=228
06/15/2022 18:37:04 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.40 on epoch=229
06/15/2022 18:37:11 - INFO - __main__ - Global step 2750 Train loss 0.40 Classification-F1 0.20861049519586103 on epoch=229
06/15/2022 18:37:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.41 on epoch=229
06/15/2022 18:37:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.41 on epoch=230
06/15/2022 18:37:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.37 on epoch=231
06/15/2022 18:37:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.37 on epoch=232
06/15/2022 18:37:26 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.47 on epoch=233
06/15/2022 18:37:32 - INFO - __main__ - Global step 2800 Train loss 0.41 Classification-F1 0.29351763093592437 on epoch=233
06/15/2022 18:37:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
06/15/2022 18:37:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.38 on epoch=234
06/15/2022 18:37:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.39 on epoch=235
06/15/2022 18:37:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.41 on epoch=236
06/15/2022 18:37:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=237
06/15/2022 18:37:54 - INFO - __main__ - Global step 2850 Train loss 0.40 Classification-F1 0.3330563701531443 on epoch=237
06/15/2022 18:37:54 - INFO - __main__ - Saving model with best Classification-F1: 0.32089998825152427 -> 0.3330563701531443 on epoch=237, global_step=2850
06/15/2022 18:37:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.39 on epoch=238
06/15/2022 18:37:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.42 on epoch=239
06/15/2022 18:38:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.40 on epoch=239
06/15/2022 18:38:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.37 on epoch=240
06/15/2022 18:38:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.42 on epoch=241
06/15/2022 18:38:14 - INFO - __main__ - Global step 2900 Train loss 0.40 Classification-F1 0.27762060316164033 on epoch=241
06/15/2022 18:38:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.39 on epoch=242
06/15/2022 18:38:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.40 on epoch=243
06/15/2022 18:38:23 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.41 on epoch=244
06/15/2022 18:38:26 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.40 on epoch=244
06/15/2022 18:38:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.36 on epoch=245
06/15/2022 18:38:36 - INFO - __main__ - Global step 2950 Train loss 0.39 Classification-F1 0.2596128299141211 on epoch=245
06/15/2022 18:38:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.46 on epoch=246
06/15/2022 18:38:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=247
06/15/2022 18:38:46 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.39 on epoch=248
06/15/2022 18:38:49 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.43 on epoch=249
06/15/2022 18:38:53 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.40 on epoch=249
06/15/2022 18:38:54 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:38:54 - INFO - __main__ - Printing 3 examples
06/15/2022 18:38:54 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 18:38:54 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:54 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 18:38:54 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:54 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 18:38:54 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:54 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:38:54 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:38:55 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 18:38:55 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:38:55 - INFO - __main__ - Printing 3 examples
06/15/2022 18:38:55 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 18:38:55 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:55 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 18:38:55 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:55 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 18:38:55 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:55 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:38:55 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:38:55 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 18:38:59 - INFO - __main__ - Global step 3000 Train loss 0.41 Classification-F1 0.2593010853880419 on epoch=249
06/15/2022 18:38:59 - INFO - __main__ - save last model!
06/15/2022 18:38:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 18:38:59 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 18:38:59 - INFO - __main__ - Printing 3 examples
06/15/2022 18:38:59 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 18:38:59 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:59 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 18:38:59 - INFO - __main__ - ['entailment']
06/15/2022 18:38:59 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 18:38:59 - INFO - __main__ - ['contradiction']
06/15/2022 18:38:59 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:38:59 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:39:00 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 18:39:15 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 18:39:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 18:39:16 - INFO - __main__ - Starting training!
06/15/2022 18:39:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_87_0.5_8_predictions.txt
06/15/2022 18:39:30 - INFO - __main__ - Classification-F1 on test data: 0.2325
06/15/2022 18:39:31 - INFO - __main__ - prefix=anli_64_87, lr=0.5, bsz=8, dev_performance=0.3330563701531443, test_performance=0.23251665025871351
06/15/2022 18:39:31 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.4, bsz=8 ...
06/15/2022 18:39:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:39:32 - INFO - __main__ - Printing 3 examples
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:39:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:39:32 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 18:39:32 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 18:39:32 - INFO - __main__ - Printing 3 examples
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 18:39:32 - INFO - __main__ - ['contradiction']
06/15/2022 18:39:32 - INFO - __main__ - Tokenizing Input ...
06/15/2022 18:39:32 - INFO - __main__ - Tokenizing Output ...
06/15/2022 18:39:33 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 18:39:52 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 18:39:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 18:39:53 - INFO - __main__ - Starting training!
06/15/2022 18:39:57 - INFO - __main__ - Step 10 Global step 10 Train loss 1.00 on epoch=0
06/15/2022 18:40:00 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=1
06/15/2022 18:40:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.62 on epoch=2
06/15/2022 18:40:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=3
06/15/2022 18:40:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=4
06/15/2022 18:40:12 - INFO - __main__ - Global step 50 Train loss 0.71 Classification-F1 0.1647058823529412 on epoch=4
06/15/2022 18:40:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.1647058823529412 on epoch=4, global_step=50
06/15/2022 18:40:15 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=4
06/15/2022 18:40:18 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=5
06/15/2022 18:40:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=6
06/15/2022 18:40:24 - INFO - __main__ - Step 90 Global step 90 Train loss 0.51 on epoch=7
06/15/2022 18:40:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=8
06/15/2022 18:40:31 - INFO - __main__ - Global step 100 Train loss 0.55 Classification-F1 0.16666666666666666 on epoch=8
06/15/2022 18:40:31 - INFO - __main__ - Saving model with best Classification-F1: 0.1647058823529412 -> 0.16666666666666666 on epoch=8, global_step=100
06/15/2022 18:40:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
06/15/2022 18:40:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.63 on epoch=9
06/15/2022 18:40:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.77 on epoch=10
06/15/2022 18:40:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.50 on epoch=11
06/15/2022 18:40:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=12
06/15/2022 18:40:51 - INFO - __main__ - Global step 150 Train loss 0.58 Classification-F1 0.2131519274376417 on epoch=12
06/15/2022 18:40:51 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.2131519274376417 on epoch=12, global_step=150
06/15/2022 18:40:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=13
06/15/2022 18:40:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=14
06/15/2022 18:40:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=14
06/15/2022 18:41:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.47 on epoch=15
06/15/2022 18:41:04 - INFO - __main__ - Step 200 Global step 200 Train loss 0.47 on epoch=16
06/15/2022 18:41:10 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.18055555555555555 on epoch=16
06/15/2022 18:41:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=17
06/15/2022 18:41:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=18
06/15/2022 18:41:19 - INFO - __main__ - Step 230 Global step 230 Train loss 0.48 on epoch=19
06/15/2022 18:41:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=19
06/15/2022 18:41:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=20
06/15/2022 18:41:30 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 18:41:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.43 on epoch=21
06/15/2022 18:41:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=22
06/15/2022 18:41:38 - INFO - __main__ - Step 280 Global step 280 Train loss 0.46 on epoch=23
06/15/2022 18:41:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=24
06/15/2022 18:41:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.49 on epoch=24
06/15/2022 18:41:50 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.2788200958932667 on epoch=24
06/15/2022 18:41:50 - INFO - __main__ - Saving model with best Classification-F1: 0.2131519274376417 -> 0.2788200958932667 on epoch=24, global_step=300
06/15/2022 18:41:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.46 on epoch=25
06/15/2022 18:41:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.53 on epoch=26
06/15/2022 18:41:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.50 on epoch=27
06/15/2022 18:42:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
06/15/2022 18:42:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.47 on epoch=29
06/15/2022 18:42:10 - INFO - __main__ - Global step 350 Train loss 0.49 Classification-F1 0.16666666666666666 on epoch=29
06/15/2022 18:42:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=29
06/15/2022 18:42:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.44 on epoch=30
06/15/2022 18:42:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.50 on epoch=31
06/15/2022 18:42:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.48 on epoch=32
06/15/2022 18:42:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.47 on epoch=33
06/15/2022 18:42:31 - INFO - __main__ - Global step 400 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 18:42:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=34
06/15/2022 18:42:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.44 on epoch=34
06/15/2022 18:42:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.44 on epoch=35
06/15/2022 18:42:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=36
06/15/2022 18:42:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.48 on epoch=37
06/15/2022 18:42:51 - INFO - __main__ - Global step 450 Train loss 0.45 Classification-F1 0.21872450693411394 on epoch=37
06/15/2022 18:42:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.43 on epoch=38
06/15/2022 18:42:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=39
06/15/2022 18:43:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=39
06/15/2022 18:43:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=40
06/15/2022 18:43:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=41
06/15/2022 18:43:11 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=41
06/15/2022 18:43:14 - INFO - __main__ - Step 510 Global step 510 Train loss 0.49 on epoch=42
06/15/2022 18:43:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=43
06/15/2022 18:43:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=44
06/15/2022 18:43:23 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=44
06/15/2022 18:43:26 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=45
06/15/2022 18:43:31 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 18:43:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.46 on epoch=46
06/15/2022 18:43:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.46 on epoch=47
06/15/2022 18:43:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 18:43:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.48 on epoch=49
06/15/2022 18:43:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=49
06/15/2022 18:43:51 - INFO - __main__ - Global step 600 Train loss 0.46 Classification-F1 0.33148995107563844 on epoch=49
06/15/2022 18:43:52 - INFO - __main__ - Saving model with best Classification-F1: 0.2788200958932667 -> 0.33148995107563844 on epoch=49, global_step=600
06/15/2022 18:43:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
06/15/2022 18:43:57 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=51
06/15/2022 18:44:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=52
06/15/2022 18:44:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=53
06/15/2022 18:44:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=54
06/15/2022 18:44:12 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.178080012725682 on epoch=54
06/15/2022 18:44:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=54
06/15/2022 18:44:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=55
06/15/2022 18:44:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=56
06/15/2022 18:44:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=57
06/15/2022 18:44:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=58
06/15/2022 18:44:32 - INFO - __main__ - Global step 700 Train loss 0.47 Classification-F1 0.27225116792546816 on epoch=58
06/15/2022 18:44:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.45 on epoch=59
06/15/2022 18:44:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.39 on epoch=59
06/15/2022 18:44:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.44 on epoch=60
06/15/2022 18:44:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.45 on epoch=61
06/15/2022 18:44:46 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=62
06/15/2022 18:44:52 - INFO - __main__ - Global step 750 Train loss 0.43 Classification-F1 0.2915244011303261 on epoch=62
06/15/2022 18:44:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
06/15/2022 18:44:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=64
06/15/2022 18:45:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=64
06/15/2022 18:45:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.43 on epoch=65
06/15/2022 18:45:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
06/15/2022 18:45:13 - INFO - __main__ - Global step 800 Train loss 0.43 Classification-F1 0.24890705728723103 on epoch=66
06/15/2022 18:45:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=67
06/15/2022 18:45:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=68
06/15/2022 18:45:21 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=69
06/15/2022 18:45:24 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=69
06/15/2022 18:45:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 18:45:34 - INFO - __main__ - Global step 850 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 18:45:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.42 on epoch=71
06/15/2022 18:45:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=72
06/15/2022 18:45:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.46 on epoch=73
06/15/2022 18:45:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.45 on epoch=74
06/15/2022 18:45:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.48 on epoch=74
06/15/2022 18:45:56 - INFO - __main__ - Global step 900 Train loss 0.45 Classification-F1 0.2904754267078338 on epoch=74
06/15/2022 18:45:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=75
06/15/2022 18:46:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.42 on epoch=76
06/15/2022 18:46:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.44 on epoch=77
06/15/2022 18:46:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 18:46:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
06/15/2022 18:46:19 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.3085261621846988 on epoch=79
06/15/2022 18:46:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.46 on epoch=79
06/15/2022 18:46:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
06/15/2022 18:46:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=81
06/15/2022 18:46:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
06/15/2022 18:46:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
06/15/2022 18:46:40 - INFO - __main__ - Global step 1000 Train loss 0.45 Classification-F1 0.37750393081761 on epoch=83
06/15/2022 18:46:41 - INFO - __main__ - Saving model with best Classification-F1: 0.33148995107563844 -> 0.37750393081761 on epoch=83, global_step=1000
06/15/2022 18:46:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=84
06/15/2022 18:46:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=84
06/15/2022 18:46:50 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=85
06/15/2022 18:46:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.45 on epoch=86
06/15/2022 18:46:56 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=87
06/15/2022 18:47:02 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.27033605186915294 on epoch=87
06/15/2022 18:47:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.45 on epoch=88
06/15/2022 18:47:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
06/15/2022 18:47:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=89
06/15/2022 18:47:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.47 on epoch=90
06/15/2022 18:47:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=91
06/15/2022 18:47:24 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.2856155185938601 on epoch=91
06/15/2022 18:47:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
06/15/2022 18:47:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=93
06/15/2022 18:47:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.43 on epoch=94
06/15/2022 18:47:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=94
06/15/2022 18:47:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.38 on epoch=95
06/15/2022 18:47:46 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 18:47:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
06/15/2022 18:47:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=97
06/15/2022 18:47:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.45 on epoch=98
06/15/2022 18:47:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=99
06/15/2022 18:48:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=99
06/15/2022 18:48:08 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.32232594771568573 on epoch=99
06/15/2022 18:48:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=100
06/15/2022 18:48:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.46 on epoch=101
06/15/2022 18:48:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
06/15/2022 18:48:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=103
06/15/2022 18:48:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=104
06/15/2022 18:48:30 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=104
06/15/2022 18:48:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=104
06/15/2022 18:48:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.41 on epoch=105
06/15/2022 18:48:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=106
06/15/2022 18:48:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=107
06/15/2022 18:48:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.44 on epoch=108
06/15/2022 18:48:52 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.24091966776077542 on epoch=108
06/15/2022 18:48:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.45 on epoch=109
06/15/2022 18:48:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.49 on epoch=109
06/15/2022 18:49:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.44 on epoch=110
06/15/2022 18:49:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
06/15/2022 18:49:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=112
06/15/2022 18:49:14 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.29928603854566466 on epoch=112
06/15/2022 18:49:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
06/15/2022 18:49:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=114
06/15/2022 18:49:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.41 on epoch=114
06/15/2022 18:49:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=115
06/15/2022 18:49:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.44 on epoch=116
06/15/2022 18:49:36 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.35345268542199487 on epoch=116
06/15/2022 18:49:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
06/15/2022 18:49:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=118
06/15/2022 18:49:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.43 on epoch=119
06/15/2022 18:49:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.41 on epoch=119
06/15/2022 18:49:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.43 on epoch=120
06/15/2022 18:49:57 - INFO - __main__ - Global step 1450 Train loss 0.41 Classification-F1 0.16666666666666666 on epoch=120
06/15/2022 18:50:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=121
06/15/2022 18:50:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.66 on epoch=122
06/15/2022 18:50:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.19 on epoch=123
06/15/2022 18:50:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.86 on epoch=124
06/15/2022 18:50:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.45 on epoch=124
06/15/2022 18:50:19 - INFO - __main__ - Global step 1500 Train loss 1.52 Classification-F1 0.16732026143790854 on epoch=124
06/15/2022 18:50:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.79 on epoch=125
06/15/2022 18:50:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.00 on epoch=126
06/15/2022 18:50:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.52 on epoch=127
06/15/2022 18:50:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.10 on epoch=128
06/15/2022 18:50:34 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.88 on epoch=129
06/15/2022 18:50:41 - INFO - __main__ - Global step 1550 Train loss 0.86 Classification-F1 0.20789909678798568 on epoch=129
06/15/2022 18:50:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.49 on epoch=129
06/15/2022 18:50:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.24 on epoch=130
06/15/2022 18:50:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.34 on epoch=131
06/15/2022 18:50:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.93 on epoch=132
06/15/2022 18:50:56 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.18 on epoch=133
06/15/2022 18:51:03 - INFO - __main__ - Global step 1600 Train loss 1.64 Classification-F1 0.2392874337987482 on epoch=133
06/15/2022 18:51:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.28 on epoch=134
06/15/2022 18:51:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.01 on epoch=134
06/15/2022 18:51:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.80 on epoch=135
06/15/2022 18:51:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.73 on epoch=136
06/15/2022 18:51:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.70 on epoch=137
06/15/2022 18:51:24 - INFO - __main__ - Global step 1650 Train loss 0.91 Classification-F1 0.23097662321185017 on epoch=137
06/15/2022 18:51:27 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.59 on epoch=138
06/15/2022 18:51:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.73 on epoch=139
06/15/2022 18:51:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.63 on epoch=139
06/15/2022 18:51:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.59 on epoch=140
06/15/2022 18:51:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=141
06/15/2022 18:51:46 - INFO - __main__ - Global step 1700 Train loss 0.62 Classification-F1 0.2982008644800407 on epoch=141
06/15/2022 18:51:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.54 on epoch=142
06/15/2022 18:51:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.55 on epoch=143
06/15/2022 18:51:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.58 on epoch=144
06/15/2022 18:51:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.55 on epoch=144
06/15/2022 18:52:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=145
06/15/2022 18:52:05 - INFO - __main__ - Global step 1750 Train loss 0.56 Classification-F1 0.26339346263393465 on epoch=145
06/15/2022 18:52:08 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.68 on epoch=146
06/15/2022 18:52:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.51 on epoch=147
06/15/2022 18:52:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.58 on epoch=148
06/15/2022 18:52:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.54 on epoch=149
06/15/2022 18:52:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.52 on epoch=149
06/15/2022 18:52:25 - INFO - __main__ - Global step 1800 Train loss 0.57 Classification-F1 0.21296296296296294 on epoch=149
06/15/2022 18:52:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.51 on epoch=150
06/15/2022 18:52:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.48 on epoch=151
06/15/2022 18:52:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=152
06/15/2022 18:52:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.52 on epoch=153
06/15/2022 18:52:40 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.55 on epoch=154
06/15/2022 18:52:46 - INFO - __main__ - Global step 1850 Train loss 0.51 Classification-F1 0.30431594385082755 on epoch=154
06/15/2022 18:52:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.60 on epoch=154
06/15/2022 18:52:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.49 on epoch=155
06/15/2022 18:52:55 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.48 on epoch=156
06/15/2022 18:52:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.51 on epoch=157
06/15/2022 18:53:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.50 on epoch=158
06/15/2022 18:53:08 - INFO - __main__ - Global step 1900 Train loss 0.51 Classification-F1 0.2955440088175091 on epoch=158
06/15/2022 18:53:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=159
06/15/2022 18:53:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=159
06/15/2022 18:53:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=160
06/15/2022 18:53:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=161
06/15/2022 18:53:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=162
06/15/2022 18:53:29 - INFO - __main__ - Global step 1950 Train loss 0.51 Classification-F1 0.2759311119228323 on epoch=162
06/15/2022 18:53:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.58 on epoch=163
06/15/2022 18:53:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.48 on epoch=164
06/15/2022 18:53:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=164
06/15/2022 18:53:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=165
06/15/2022 18:53:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.52 on epoch=166
06/15/2022 18:53:49 - INFO - __main__ - Global step 2000 Train loss 0.52 Classification-F1 0.29210256410256413 on epoch=166
06/15/2022 18:53:52 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.46 on epoch=167
06/15/2022 18:53:55 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.49 on epoch=168
06/15/2022 18:53:58 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.48 on epoch=169
06/15/2022 18:54:01 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.48 on epoch=169
06/15/2022 18:54:04 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.53 on epoch=170
06/15/2022 18:54:09 - INFO - __main__ - Global step 2050 Train loss 0.49 Classification-F1 0.29467839932956214 on epoch=170
06/15/2022 18:54:12 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.50 on epoch=171
06/15/2022 18:54:15 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.45 on epoch=172
06/15/2022 18:54:18 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.51 on epoch=173
06/15/2022 18:54:21 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.48 on epoch=174
06/15/2022 18:54:24 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.50 on epoch=174
06/15/2022 18:54:30 - INFO - __main__ - Global step 2100 Train loss 0.49 Classification-F1 0.2796389227963892 on epoch=174
06/15/2022 18:54:32 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.46 on epoch=175
06/15/2022 18:54:35 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.51 on epoch=176
06/15/2022 18:54:38 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.46 on epoch=177
06/15/2022 18:54:41 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.48 on epoch=178
06/15/2022 18:54:44 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.56 on epoch=179
06/15/2022 18:54:49 - INFO - __main__ - Global step 2150 Train loss 0.49 Classification-F1 0.2731249910934405 on epoch=179
06/15/2022 18:54:52 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.48 on epoch=179
06/15/2022 18:54:55 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.47 on epoch=180
06/15/2022 18:54:58 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.47 on epoch=181
06/15/2022 18:55:01 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.43 on epoch=182
06/15/2022 18:55:04 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.46 on epoch=183
06/15/2022 18:55:10 - INFO - __main__ - Global step 2200 Train loss 0.46 Classification-F1 0.2477925199944524 on epoch=183
06/15/2022 18:55:13 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.47 on epoch=184
06/15/2022 18:55:16 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.45 on epoch=184
06/15/2022 18:55:19 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.48 on epoch=185
06/15/2022 18:55:22 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.47 on epoch=186
06/15/2022 18:55:24 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.47 on epoch=187
06/15/2022 18:55:30 - INFO - __main__ - Global step 2250 Train loss 0.47 Classification-F1 0.23809523809523814 on epoch=187
06/15/2022 18:55:33 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=188
06/15/2022 18:55:36 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.44 on epoch=189
06/15/2022 18:55:39 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.44 on epoch=189
06/15/2022 18:55:42 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.49 on epoch=190
06/15/2022 18:55:45 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.49 on epoch=191
06/15/2022 18:55:52 - INFO - __main__ - Global step 2300 Train loss 0.46 Classification-F1 0.27110729940918615 on epoch=191
06/15/2022 18:55:55 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.44 on epoch=192
06/15/2022 18:55:58 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.51 on epoch=193
06/15/2022 18:56:01 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.46 on epoch=194
06/15/2022 18:56:04 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.47 on epoch=194
06/15/2022 18:56:08 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.42 on epoch=195
06/15/2022 18:56:15 - INFO - __main__ - Global step 2350 Train loss 0.46 Classification-F1 0.16724187517975267 on epoch=195
06/15/2022 18:56:19 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.54 on epoch=196
06/15/2022 18:56:22 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.44 on epoch=197
06/15/2022 18:56:25 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.47 on epoch=198
06/15/2022 18:56:29 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.42 on epoch=199
06/15/2022 18:56:33 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.44 on epoch=199
06/15/2022 18:56:40 - INFO - __main__ - Global step 2400 Train loss 0.46 Classification-F1 0.22076390669059218 on epoch=199
06/15/2022 18:56:44 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.39 on epoch=200
06/15/2022 18:56:47 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.47 on epoch=201
06/15/2022 18:56:51 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.42 on epoch=202
06/15/2022 18:56:54 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.51 on epoch=203
06/15/2022 18:56:57 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.49 on epoch=204
06/15/2022 18:57:03 - INFO - __main__ - Global step 2450 Train loss 0.46 Classification-F1 0.16129032258064516 on epoch=204
06/15/2022 18:57:06 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.45 on epoch=204
06/15/2022 18:57:10 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.45 on epoch=205
06/15/2022 18:57:13 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.50 on epoch=206
06/15/2022 18:57:17 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.44 on epoch=207
06/15/2022 18:57:20 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.50 on epoch=208
06/15/2022 18:57:28 - INFO - __main__ - Global step 2500 Train loss 0.47 Classification-F1 0.20857142857142855 on epoch=208
06/15/2022 18:57:31 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.40 on epoch=209
06/15/2022 18:57:35 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.46 on epoch=209
06/15/2022 18:57:38 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.40 on epoch=210
06/15/2022 18:57:41 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.45 on epoch=211
06/15/2022 18:57:43 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.43 on epoch=212
06/15/2022 18:57:49 - INFO - __main__ - Global step 2550 Train loss 0.43 Classification-F1 0.16840607210626182 on epoch=212
06/15/2022 18:57:52 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.45 on epoch=213
06/15/2022 18:57:55 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.46 on epoch=214
06/15/2022 18:57:58 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.44 on epoch=214
06/15/2022 18:58:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.47 on epoch=215
06/15/2022 18:58:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.44 on epoch=216
06/15/2022 18:58:09 - INFO - __main__ - Global step 2600 Train loss 0.45 Classification-F1 0.1993620414673046 on epoch=216
06/15/2022 18:58:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.43 on epoch=217
06/15/2022 18:58:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.41 on epoch=218
06/15/2022 18:58:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.49 on epoch=219
06/15/2022 18:58:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.46 on epoch=219
06/15/2022 18:58:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
06/15/2022 18:58:30 - INFO - __main__ - Global step 2650 Train loss 0.44 Classification-F1 0.16266666666666665 on epoch=220
06/15/2022 18:58:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.45 on epoch=221
06/15/2022 18:58:36 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.47 on epoch=222
06/15/2022 18:58:39 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.46 on epoch=223
06/15/2022 18:58:42 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.44 on epoch=224
06/15/2022 18:58:45 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.46 on epoch=224
06/15/2022 18:58:51 - INFO - __main__ - Global step 2700 Train loss 0.46 Classification-F1 0.23872188965415306 on epoch=224
06/15/2022 18:58:54 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.39 on epoch=225
06/15/2022 18:58:57 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.41 on epoch=226
06/15/2022 18:59:00 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.43 on epoch=227
06/15/2022 18:59:03 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.44 on epoch=228
06/15/2022 18:59:07 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.45 on epoch=229
06/15/2022 18:59:13 - INFO - __main__ - Global step 2750 Train loss 0.43 Classification-F1 0.19143162280932122 on epoch=229
06/15/2022 18:59:16 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.50 on epoch=229
06/15/2022 18:59:19 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.43 on epoch=230
06/15/2022 18:59:22 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.45 on epoch=231
06/15/2022 18:59:25 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.46 on epoch=232
06/15/2022 18:59:28 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.43 on epoch=233
06/15/2022 18:59:35 - INFO - __main__ - Global step 2800 Train loss 0.45 Classification-F1 0.22623853211009173 on epoch=233
06/15/2022 18:59:37 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.44 on epoch=234
06/15/2022 18:59:40 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.49 on epoch=234
06/15/2022 18:59:43 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.40 on epoch=235
06/15/2022 18:59:46 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=236
06/15/2022 18:59:49 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.42 on epoch=237
06/15/2022 18:59:54 - INFO - __main__ - Global step 2850 Train loss 0.44 Classification-F1 0.23760964912280702 on epoch=237
06/15/2022 18:59:57 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
06/15/2022 19:00:00 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.46 on epoch=239
06/15/2022 19:00:03 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.42 on epoch=239
06/15/2022 19:00:06 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.39 on epoch=240
06/15/2022 19:00:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.48 on epoch=241
06/15/2022 19:00:14 - INFO - __main__ - Global step 2900 Train loss 0.44 Classification-F1 0.18402724823344008 on epoch=241
06/15/2022 19:00:17 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.46 on epoch=242
06/15/2022 19:00:20 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.47 on epoch=243
06/15/2022 19:00:22 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.46 on epoch=244
06/15/2022 19:00:25 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.39 on epoch=244
06/15/2022 19:00:28 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.43 on epoch=245
06/15/2022 19:00:34 - INFO - __main__ - Global step 2950 Train loss 0.44 Classification-F1 0.23156878963330577 on epoch=245
06/15/2022 19:00:36 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.43 on epoch=246
06/15/2022 19:00:39 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.43 on epoch=247
06/15/2022 19:00:42 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.45 on epoch=248
06/15/2022 19:00:45 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.44 on epoch=249
06/15/2022 19:00:48 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=249
06/15/2022 19:00:49 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:00:49 - INFO - __main__ - Printing 3 examples
06/15/2022 19:00:49 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 19:00:49 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:49 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 19:00:49 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:49 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 19:00:49 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:49 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:00:49 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:00:50 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 19:00:50 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:00:50 - INFO - __main__ - Printing 3 examples
06/15/2022 19:00:50 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 19:00:50 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:50 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 19:00:50 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:50 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 19:00:50 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:50 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:00:50 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:00:50 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 19:00:53 - INFO - __main__ - Global step 3000 Train loss 0.44 Classification-F1 0.22481426224814263 on epoch=249
06/15/2022 19:00:54 - INFO - __main__ - save last model!
06/15/2022 19:00:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 19:00:54 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 19:00:54 - INFO - __main__ - Printing 3 examples
06/15/2022 19:00:54 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 19:00:54 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:54 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 19:00:54 - INFO - __main__ - ['entailment']
06/15/2022 19:00:54 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 19:00:54 - INFO - __main__ - ['contradiction']
06/15/2022 19:00:54 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:00:54 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:00:55 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 19:01:08 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 19:01:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 19:01:09 - INFO - __main__ - Starting training!
06/15/2022 19:01:26 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_87_0.4_8_predictions.txt
06/15/2022 19:01:26 - INFO - __main__ - Classification-F1 on test data: 0.2326
06/15/2022 19:01:26 - INFO - __main__ - prefix=anli_64_87, lr=0.4, bsz=8, dev_performance=0.37750393081761, test_performance=0.23257642036297618
06/15/2022 19:01:26 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.3, bsz=8 ...
06/15/2022 19:01:27 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:01:27 - INFO - __main__ - Printing 3 examples
06/15/2022 19:01:27 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 19:01:27 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:27 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 19:01:27 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:27 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 19:01:27 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:27 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:01:28 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:01:28 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 19:01:28 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:01:28 - INFO - __main__ - Printing 3 examples
06/15/2022 19:01:28 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 19:01:28 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:28 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 19:01:28 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:28 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 19:01:28 - INFO - __main__ - ['contradiction']
06/15/2022 19:01:28 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:01:28 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:01:28 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 19:01:50 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 19:01:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 19:01:51 - INFO - __main__ - Starting training!
06/15/2022 19:01:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.02 on epoch=0
06/15/2022 19:01:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=1
06/15/2022 19:02:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=2
06/15/2022 19:02:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.66 on epoch=3
06/15/2022 19:02:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=4
06/15/2022 19:02:12 - INFO - __main__ - Global step 50 Train loss 0.70 Classification-F1 0.16666666666666666 on epoch=4
06/15/2022 19:02:12 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.16666666666666666 on epoch=4, global_step=50
06/15/2022 19:02:14 - INFO - __main__ - Step 60 Global step 60 Train loss 0.53 on epoch=4
06/15/2022 19:02:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=5
06/15/2022 19:02:20 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=6
06/15/2022 19:02:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.46 on epoch=7
06/15/2022 19:02:25 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=8
06/15/2022 19:02:32 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.27930402930402926 on epoch=8
06/15/2022 19:02:32 - INFO - __main__ - Saving model with best Classification-F1: 0.16666666666666666 -> 0.27930402930402926 on epoch=8, global_step=100
06/15/2022 19:02:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=9
06/15/2022 19:02:37 - INFO - __main__ - Step 120 Global step 120 Train loss 0.56 on epoch=9
06/15/2022 19:02:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.53 on epoch=10
06/15/2022 19:02:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.45 on epoch=11
06/15/2022 19:02:46 - INFO - __main__ - Step 150 Global step 150 Train loss 0.55 on epoch=12
06/15/2022 19:02:52 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 19:02:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.49 on epoch=13
06/15/2022 19:02:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=14
06/15/2022 19:03:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
06/15/2022 19:03:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.45 on epoch=15
06/15/2022 19:03:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.48 on epoch=16
06/15/2022 19:03:11 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 19:03:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=17
06/15/2022 19:03:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=18
06/15/2022 19:03:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=19
06/15/2022 19:03:22 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=19
06/15/2022 19:03:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.45 on epoch=20
06/15/2022 19:03:31 - INFO - __main__ - Global step 250 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 19:03:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=21
06/15/2022 19:03:36 - INFO - __main__ - Step 270 Global step 270 Train loss 0.49 on epoch=22
06/15/2022 19:03:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=23
06/15/2022 19:03:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=24
06/15/2022 19:03:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
06/15/2022 19:03:50 - INFO - __main__ - Global step 300 Train loss 0.48 Classification-F1 0.18627450980392157 on epoch=24
06/15/2022 19:03:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.44 on epoch=25
06/15/2022 19:03:56 - INFO - __main__ - Step 320 Global step 320 Train loss 0.51 on epoch=26
06/15/2022 19:03:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=27
06/15/2022 19:04:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.46 on epoch=28
06/15/2022 19:04:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.48 on epoch=29
06/15/2022 19:04:10 - INFO - __main__ - Global step 350 Train loss 0.47 Classification-F1 0.19233868014355818 on epoch=29
06/15/2022 19:04:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.51 on epoch=29
06/15/2022 19:04:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.42 on epoch=30
06/15/2022 19:04:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.43 on epoch=31
06/15/2022 19:04:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.43 on epoch=32
06/15/2022 19:04:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.49 on epoch=33
06/15/2022 19:04:31 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.28807678110003687 on epoch=33
06/15/2022 19:04:31 - INFO - __main__ - Saving model with best Classification-F1: 0.27930402930402926 -> 0.28807678110003687 on epoch=33, global_step=400
06/15/2022 19:04:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.44 on epoch=34
06/15/2022 19:04:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.45 on epoch=34
06/15/2022 19:04:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
06/15/2022 19:04:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.44 on epoch=36
06/15/2022 19:04:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=37
06/15/2022 19:04:51 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=37
06/15/2022 19:04:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=38
06/15/2022 19:04:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.39 on epoch=39
06/15/2022 19:04:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
06/15/2022 19:05:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=40
06/15/2022 19:05:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.48 on epoch=41
06/15/2022 19:05:10 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.1647058823529412 on epoch=41
06/15/2022 19:05:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.43 on epoch=42
06/15/2022 19:05:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.49 on epoch=43
06/15/2022 19:05:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.47 on epoch=44
06/15/2022 19:05:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.47 on epoch=44
06/15/2022 19:05:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=45
06/15/2022 19:05:31 - INFO - __main__ - Global step 550 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 19:05:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.42 on epoch=46
06/15/2022 19:05:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=47
06/15/2022 19:05:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.51 on epoch=48
06/15/2022 19:05:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=49
06/15/2022 19:05:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=49
06/15/2022 19:05:50 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.32285618952285616 on epoch=49
06/15/2022 19:05:50 - INFO - __main__ - Saving model with best Classification-F1: 0.28807678110003687 -> 0.32285618952285616 on epoch=49, global_step=600
06/15/2022 19:05:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
06/15/2022 19:05:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.44 on epoch=51
06/15/2022 19:05:59 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=52
06/15/2022 19:06:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=53
06/15/2022 19:06:05 - INFO - __main__ - Step 650 Global step 650 Train loss 0.45 on epoch=54
06/15/2022 19:06:12 - INFO - __main__ - Global step 650 Train loss 0.43 Classification-F1 0.27956989247311825 on epoch=54
06/15/2022 19:06:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.45 on epoch=54
06/15/2022 19:06:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.42 on epoch=55
06/15/2022 19:06:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=56
06/15/2022 19:06:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=57
06/15/2022 19:06:25 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=58
06/15/2022 19:06:31 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.17595815389455882 on epoch=58
06/15/2022 19:06:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=59
06/15/2022 19:06:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.43 on epoch=59
06/15/2022 19:06:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
06/15/2022 19:06:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.42 on epoch=61
06/15/2022 19:06:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.42 on epoch=62
06/15/2022 19:06:50 - INFO - __main__ - Global step 750 Train loss 0.42 Classification-F1 0.16666666666666666 on epoch=62
06/15/2022 19:06:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.45 on epoch=63
06/15/2022 19:06:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.44 on epoch=64
06/15/2022 19:06:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=64
06/15/2022 19:07:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.46 on epoch=65
06/15/2022 19:07:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.47 on epoch=66
06/15/2022 19:07:10 - INFO - __main__ - Global step 800 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=66
06/15/2022 19:07:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=67
06/15/2022 19:07:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.44 on epoch=68
06/15/2022 19:07:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=69
06/15/2022 19:07:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.44 on epoch=69
06/15/2022 19:07:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 19:07:30 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 19:07:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=71
06/15/2022 19:07:35 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=72
06/15/2022 19:07:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.48 on epoch=73
06/15/2022 19:07:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=74
06/15/2022 19:07:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.44 on epoch=74
06/15/2022 19:07:49 - INFO - __main__ - Global step 900 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=74
06/15/2022 19:07:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.46 on epoch=75
06/15/2022 19:07:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
06/15/2022 19:07:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=77
06/15/2022 19:08:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.44 on epoch=78
06/15/2022 19:08:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=79
06/15/2022 19:08:10 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=79
06/15/2022 19:08:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.42 on epoch=79
06/15/2022 19:08:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.43 on epoch=80
06/15/2022 19:08:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.41 on epoch=81
06/15/2022 19:08:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.41 on epoch=82
06/15/2022 19:08:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.47 on epoch=83
06/15/2022 19:08:30 - INFO - __main__ - Global step 1000 Train loss 0.43 Classification-F1 0.1936601420405335 on epoch=83
06/15/2022 19:08:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.42 on epoch=84
06/15/2022 19:08:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=84
06/15/2022 19:08:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.49 on epoch=85
06/15/2022 19:08:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=86
06/15/2022 19:08:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.41 on epoch=87
06/15/2022 19:08:50 - INFO - __main__ - Global step 1050 Train loss 0.44 Classification-F1 0.24576404030040044 on epoch=87
06/15/2022 19:08:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.60 on epoch=88
06/15/2022 19:08:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.51 on epoch=89
06/15/2022 19:08:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=89
06/15/2022 19:09:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.49 on epoch=90
06/15/2022 19:09:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
06/15/2022 19:09:10 - INFO - __main__ - Global step 1100 Train loss 0.51 Classification-F1 0.2855793417742835 on epoch=91
06/15/2022 19:09:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.42 on epoch=92
06/15/2022 19:09:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.45 on epoch=93
06/15/2022 19:09:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=94
06/15/2022 19:09:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=94
06/15/2022 19:09:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
06/15/2022 19:09:30 - INFO - __main__ - Global step 1150 Train loss 0.44 Classification-F1 0.24324042175592442 on epoch=95
06/15/2022 19:09:33 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=96
06/15/2022 19:09:36 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
06/15/2022 19:09:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.44 on epoch=98
06/15/2022 19:09:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.47 on epoch=99
06/15/2022 19:09:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.44 on epoch=99
06/15/2022 19:09:50 - INFO - __main__ - Global step 1200 Train loss 0.44 Classification-F1 0.2483979424696112 on epoch=99
06/15/2022 19:09:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.38 on epoch=100
06/15/2022 19:09:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.45 on epoch=101
06/15/2022 19:09:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.39 on epoch=102
06/15/2022 19:10:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.42 on epoch=103
06/15/2022 19:10:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.42 on epoch=104
06/15/2022 19:10:10 - INFO - __main__ - Global step 1250 Train loss 0.41 Classification-F1 0.1871132633844498 on epoch=104
06/15/2022 19:10:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.44 on epoch=104
06/15/2022 19:10:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=105
06/15/2022 19:10:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=106
06/15/2022 19:10:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=107
06/15/2022 19:10:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.33 on epoch=108
06/15/2022 19:10:30 - INFO - __main__ - Global step 1300 Train loss 0.62 Classification-F1 0.29924242424242425 on epoch=108
06/15/2022 19:10:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.30 on epoch=109
06/15/2022 19:10:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.48 on epoch=109
06/15/2022 19:10:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=110
06/15/2022 19:10:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.41 on epoch=111
06/15/2022 19:10:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.39 on epoch=112
06/15/2022 19:10:50 - INFO - __main__ - Global step 1350 Train loss 0.60 Classification-F1 0.23090909090909092 on epoch=112
06/15/2022 19:10:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.44 on epoch=113
06/15/2022 19:10:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=114
06/15/2022 19:10:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=114
06/15/2022 19:11:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=115
06/15/2022 19:11:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=116
06/15/2022 19:11:11 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.2241833670405099 on epoch=116
06/15/2022 19:11:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.42 on epoch=117
06/15/2022 19:11:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.42 on epoch=118
06/15/2022 19:11:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.44 on epoch=119
06/15/2022 19:11:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.40 on epoch=119
06/15/2022 19:11:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=120
06/15/2022 19:11:32 - INFO - __main__ - Global step 1450 Train loss 0.42 Classification-F1 0.21064258101295139 on epoch=120
06/15/2022 19:11:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.46 on epoch=121
06/15/2022 19:11:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
06/15/2022 19:11:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.41 on epoch=123
06/15/2022 19:11:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.44 on epoch=124
06/15/2022 19:11:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.40 on epoch=124
06/15/2022 19:11:54 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.20915032679738563 on epoch=124
06/15/2022 19:11:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.41 on epoch=125
06/15/2022 19:12:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.42 on epoch=126
06/15/2022 19:12:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=127
06/15/2022 19:12:06 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=128
06/15/2022 19:12:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=129
06/15/2022 19:12:16 - INFO - __main__ - Global step 1550 Train loss 0.43 Classification-F1 0.21872450693411394 on epoch=129
06/15/2022 19:12:19 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.43 on epoch=129
06/15/2022 19:12:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=130
06/15/2022 19:12:25 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.44 on epoch=131
06/15/2022 19:12:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=132
06/15/2022 19:12:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=133
06/15/2022 19:12:37 - INFO - __main__ - Global step 1600 Train loss 0.42 Classification-F1 0.21159783062161033 on epoch=133
06/15/2022 19:12:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=134
06/15/2022 19:12:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=134
06/15/2022 19:12:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.43 on epoch=135
06/15/2022 19:12:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=136
06/15/2022 19:12:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.42 on epoch=137
06/15/2022 19:12:59 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.21312782354504387 on epoch=137
06/15/2022 19:13:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=138
06/15/2022 19:13:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=139
06/15/2022 19:13:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=139
06/15/2022 19:13:10 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=140
06/15/2022 19:13:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=141
06/15/2022 19:13:19 - INFO - __main__ - Global step 1700 Train loss 0.43 Classification-F1 0.20641836981706263 on epoch=141
06/15/2022 19:13:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=142
06/15/2022 19:13:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=143
06/15/2022 19:13:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.45 on epoch=144
06/15/2022 19:13:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=144
06/15/2022 19:13:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.41 on epoch=145
06/15/2022 19:13:39 - INFO - __main__ - Global step 1750 Train loss 0.42 Classification-F1 0.20707070707070704 on epoch=145
06/15/2022 19:13:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.45 on epoch=146
06/15/2022 19:13:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
06/15/2022 19:13:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=148
06/15/2022 19:13:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
06/15/2022 19:13:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.48 on epoch=149
06/15/2022 19:14:01 - INFO - __main__ - Global step 1800 Train loss 0.43 Classification-F1 0.1926530612244898 on epoch=149
06/15/2022 19:14:04 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=150
06/15/2022 19:14:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=151
06/15/2022 19:14:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.40 on epoch=152
06/15/2022 19:14:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=153
06/15/2022 19:14:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=154
06/15/2022 19:14:23 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.2135339408066681 on epoch=154
06/15/2022 19:14:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=154
06/15/2022 19:14:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.43 on epoch=155
06/15/2022 19:14:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.47 on epoch=156
06/15/2022 19:14:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=157
06/15/2022 19:14:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=158
06/15/2022 19:14:44 - INFO - __main__ - Global step 1900 Train loss 0.43 Classification-F1 0.21753246753246758 on epoch=158
06/15/2022 19:14:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=159
06/15/2022 19:14:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.46 on epoch=159
06/15/2022 19:14:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=160
06/15/2022 19:14:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=161
06/15/2022 19:14:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=162
06/15/2022 19:15:05 - INFO - __main__ - Global step 1950 Train loss 0.43 Classification-F1 0.23824456103458783 on epoch=162
06/15/2022 19:15:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=163
06/15/2022 19:15:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=164
06/15/2022 19:15:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.42 on epoch=164
06/15/2022 19:15:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=165
06/15/2022 19:15:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.42 on epoch=166
06/15/2022 19:15:26 - INFO - __main__ - Global step 2000 Train loss 0.43 Classification-F1 0.21232927279438907 on epoch=166
06/15/2022 19:15:29 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.42 on epoch=167
06/15/2022 19:15:32 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.44 on epoch=168
06/15/2022 19:15:35 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.40 on epoch=169
06/15/2022 19:15:38 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.42 on epoch=169
06/15/2022 19:15:41 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.40 on epoch=170
06/15/2022 19:15:48 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.2201695798398939 on epoch=170
06/15/2022 19:15:51 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.44 on epoch=171
06/15/2022 19:15:53 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
06/15/2022 19:15:56 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.45 on epoch=173
06/15/2022 19:15:59 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.41 on epoch=174
06/15/2022 19:16:02 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.40 on epoch=174
06/15/2022 19:16:08 - INFO - __main__ - Global step 2100 Train loss 0.42 Classification-F1 0.22435897435897437 on epoch=174
06/15/2022 19:16:10 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.44 on epoch=175
06/15/2022 19:16:13 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.40 on epoch=176
06/15/2022 19:16:16 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
06/15/2022 19:16:19 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.43 on epoch=178
06/15/2022 19:16:21 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.42 on epoch=179
06/15/2022 19:16:27 - INFO - __main__ - Global step 2150 Train loss 0.42 Classification-F1 0.2188025345920083 on epoch=179
06/15/2022 19:16:30 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.43 on epoch=179
06/15/2022 19:16:32 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.40 on epoch=180
06/15/2022 19:16:35 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.42 on epoch=181
06/15/2022 19:16:38 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.41 on epoch=182
06/15/2022 19:16:40 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.43 on epoch=183
06/15/2022 19:16:46 - INFO - __main__ - Global step 2200 Train loss 0.42 Classification-F1 0.22316079559070215 on epoch=183
06/15/2022 19:16:49 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 19:16:52 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.43 on epoch=184
06/15/2022 19:16:54 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.39 on epoch=185
06/15/2022 19:16:57 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.42 on epoch=186
06/15/2022 19:17:00 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.37 on epoch=187
06/15/2022 19:17:06 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.2294454882690177 on epoch=187
06/15/2022 19:17:09 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.44 on epoch=188
06/15/2022 19:17:11 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.42 on epoch=189
06/15/2022 19:17:14 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.40 on epoch=189
06/15/2022 19:17:17 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.41 on epoch=190
06/15/2022 19:17:20 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.42 on epoch=191
06/15/2022 19:17:26 - INFO - __main__ - Global step 2300 Train loss 0.42 Classification-F1 0.2280673659487015 on epoch=191
06/15/2022 19:17:29 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.43 on epoch=192
06/15/2022 19:17:31 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.42 on epoch=193
06/15/2022 19:17:34 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.42 on epoch=194
06/15/2022 19:17:37 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.38 on epoch=194
06/15/2022 19:17:40 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
06/15/2022 19:17:47 - INFO - __main__ - Global step 2350 Train loss 0.41 Classification-F1 0.2326331216414265 on epoch=195
06/15/2022 19:17:50 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.44 on epoch=196
06/15/2022 19:17:53 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.39 on epoch=197
06/15/2022 19:17:56 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.41 on epoch=198
06/15/2022 19:17:59 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.41 on epoch=199
06/15/2022 19:18:02 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.41 on epoch=199
06/15/2022 19:18:09 - INFO - __main__ - Global step 2400 Train loss 0.41 Classification-F1 0.2067670561098266 on epoch=199
06/15/2022 19:18:12 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.40 on epoch=200
06/15/2022 19:18:15 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.43 on epoch=201
06/15/2022 19:18:18 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.39 on epoch=202
06/15/2022 19:18:21 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.38 on epoch=203
06/15/2022 19:18:24 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.38 on epoch=204
06/15/2022 19:18:31 - INFO - __main__ - Global step 2450 Train loss 0.40 Classification-F1 0.2517361111111111 on epoch=204
06/15/2022 19:18:34 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=204
06/15/2022 19:18:37 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.42 on epoch=205
06/15/2022 19:18:40 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.44 on epoch=206
06/15/2022 19:18:43 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.42 on epoch=207
06/15/2022 19:18:46 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.45 on epoch=208
06/15/2022 19:18:53 - INFO - __main__ - Global step 2500 Train loss 0.43 Classification-F1 0.2130011210585319 on epoch=208
06/15/2022 19:18:56 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
06/15/2022 19:18:59 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.38 on epoch=209
06/15/2022 19:19:02 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.42 on epoch=210
06/15/2022 19:19:05 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.42 on epoch=211
06/15/2022 19:19:08 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.39 on epoch=212
06/15/2022 19:19:15 - INFO - __main__ - Global step 2550 Train loss 0.40 Classification-F1 0.21368985314175148 on epoch=212
06/15/2022 19:19:18 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.40 on epoch=213
06/15/2022 19:19:21 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.38 on epoch=214
06/15/2022 19:19:24 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.43 on epoch=214
06/15/2022 19:19:27 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.44 on epoch=215
06/15/2022 19:19:30 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.43 on epoch=216
06/15/2022 19:19:36 - INFO - __main__ - Global step 2600 Train loss 0.42 Classification-F1 0.23063492063492064 on epoch=216
06/15/2022 19:19:39 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.37 on epoch=217
06/15/2022 19:19:42 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.43 on epoch=218
06/15/2022 19:19:45 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.40 on epoch=219
06/15/2022 19:19:48 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=219
06/15/2022 19:19:52 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.42 on epoch=220
06/15/2022 19:19:58 - INFO - __main__ - Global step 2650 Train loss 0.40 Classification-F1 0.2520847419427541 on epoch=220
06/15/2022 19:20:01 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.41 on epoch=221
06/15/2022 19:20:04 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.40 on epoch=222
06/15/2022 19:20:07 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.42 on epoch=223
06/15/2022 19:20:10 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.38 on epoch=224
06/15/2022 19:20:13 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.42 on epoch=224
06/15/2022 19:20:20 - INFO - __main__ - Global step 2700 Train loss 0.41 Classification-F1 0.24623296446597662 on epoch=224
06/15/2022 19:20:23 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.41 on epoch=225
06/15/2022 19:20:26 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.42 on epoch=226
06/15/2022 19:20:29 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.41 on epoch=227
06/15/2022 19:20:32 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.45 on epoch=228
06/15/2022 19:20:35 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.36 on epoch=229
06/15/2022 19:20:41 - INFO - __main__ - Global step 2750 Train loss 0.41 Classification-F1 0.2056863909228841 on epoch=229
06/15/2022 19:20:44 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.45 on epoch=229
06/15/2022 19:20:47 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.38 on epoch=230
06/15/2022 19:20:49 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.46 on epoch=231
06/15/2022 19:20:52 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=232
06/15/2022 19:20:55 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.42 on epoch=233
06/15/2022 19:21:01 - INFO - __main__ - Global step 2800 Train loss 0.42 Classification-F1 0.216620644312952 on epoch=233
06/15/2022 19:21:04 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.41 on epoch=234
06/15/2022 19:21:06 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.40 on epoch=234
06/15/2022 19:21:09 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.38 on epoch=235
06/15/2022 19:21:12 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.45 on epoch=236
06/15/2022 19:21:15 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.43 on epoch=237
06/15/2022 19:21:21 - INFO - __main__ - Global step 2850 Train loss 0.42 Classification-F1 0.2665364642473241 on epoch=237
06/15/2022 19:21:23 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.44 on epoch=238
06/15/2022 19:21:26 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.44 on epoch=239
06/15/2022 19:21:29 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.43 on epoch=239
06/15/2022 19:21:32 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.40 on epoch=240
06/15/2022 19:21:34 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.43 on epoch=241
06/15/2022 19:21:40 - INFO - __main__ - Global step 2900 Train loss 0.43 Classification-F1 0.2982778382202164 on epoch=241
06/15/2022 19:21:43 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.40 on epoch=242
06/15/2022 19:21:46 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.43 on epoch=243
06/15/2022 19:21:49 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.39 on epoch=244
06/15/2022 19:21:51 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.44 on epoch=244
06/15/2022 19:21:54 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.40 on epoch=245
06/15/2022 19:22:00 - INFO - __main__ - Global step 2950 Train loss 0.41 Classification-F1 0.1847258936513857 on epoch=245
06/15/2022 19:22:02 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.44 on epoch=246
06/15/2022 19:22:05 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.39 on epoch=247
06/15/2022 19:22:08 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.44 on epoch=248
06/15/2022 19:22:11 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.42 on epoch=249
06/15/2022 19:22:14 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.43 on epoch=249
06/15/2022 19:22:15 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:22:15 - INFO - __main__ - Printing 3 examples
06/15/2022 19:22:15 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 19:22:15 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:15 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 19:22:15 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:15 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 19:22:15 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:15 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:22:16 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:22:16 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 19:22:16 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:22:16 - INFO - __main__ - Printing 3 examples
06/15/2022 19:22:16 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 19:22:16 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:16 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 19:22:16 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:16 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 19:22:16 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:16 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:22:16 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:22:16 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 19:22:21 - INFO - __main__ - Global step 3000 Train loss 0.42 Classification-F1 0.2902183920612665 on epoch=249
06/15/2022 19:22:21 - INFO - __main__ - save last model!
06/15/2022 19:22:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 19:22:21 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 19:22:21 - INFO - __main__ - Printing 3 examples
06/15/2022 19:22:21 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 19:22:21 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:21 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 19:22:21 - INFO - __main__ - ['entailment']
06/15/2022 19:22:21 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 19:22:21 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:21 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:22:22 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:22:23 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 19:22:36 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 19:22:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 19:22:37 - INFO - __main__ - Starting training!
06/15/2022 19:22:58 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_87_0.3_8_predictions.txt
06/15/2022 19:22:58 - INFO - __main__ - Classification-F1 on test data: 0.2930
06/15/2022 19:22:59 - INFO - __main__ - prefix=anli_64_87, lr=0.3, bsz=8, dev_performance=0.32285618952285616, test_performance=0.2930408058928955
06/15/2022 19:22:59 - INFO - __main__ - Running ... prefix=anli_64_87, lr=0.2, bsz=8 ...
06/15/2022 19:22:59 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:22:59 - INFO - __main__ - Printing 3 examples
06/15/2022 19:22:59 - INFO - __main__ -  [anli] premise: "I Love Rock 'n' Roll" is a rock song written in 1975 by Alan Merrill of the Arrows, who recorded the first released version. The song was later made famous by Joan Jett & the Blackhearts in 1982. Alan Merrill has played the song live in Europe, Japan and most often in his home town New York City. [SEP] hypothesis: "I Love Rock 'n' Roll" is a rock song written in 1975 Joan Jett & the Blackhearts.
06/15/2022 19:22:59 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:59 - INFO - __main__ -  [anli] premise: Binani Industries Ltd is an Indian business group based in Mumbai. It is a 143-year old business conglomerate and belongs to the legendary Braj Binani Group. The business portfolio of Binani Industries includes sectors like cement, zinc, glass-fiber, and downstream composite products. [SEP] hypothesis: Binani industries was founded in America over 143 years ago. 
06/15/2022 19:22:59 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:59 - INFO - __main__ -  [anli] premise: Konec agenta W4C prostřednictvím psa pana Foustky (English: The End of Agent W4C ) is a 1967 Czechoslovak film parodying the James Bond secret agent genre. Directed by Václav Vorlíček based on the story by Oldřich Daněk. Runtime 87 min. Mono. Produced by Filmové Studio Barrandov and distributed by Central Office of Film Distribution, Prague. [SEP] hypothesis: Konec agenta W4C prostřednictvím psa pana Foustky (magic: The End of Agent W4C ) is a 1967 Czechoslovak film 
06/15/2022 19:22:59 - INFO - __main__ - ['contradiction']
06/15/2022 19:22:59 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:23:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:23:00 - INFO - __main__ - Loaded 192 examples from train data
06/15/2022 19:23:00 - INFO - __main__ - Start tokenizing ... 192 instances
06/15/2022 19:23:00 - INFO - __main__ - Printing 3 examples
06/15/2022 19:23:00 - INFO - __main__ -  [anli] premise: Argentinean label Dias De Garage released the first-ever tribute to the rock legend Kyuss in July 2004. Titled "Listen Without Distraction", the collection was named after the instructions Kyuss used to give music fans on their albums. This CD contains songs from Argentinean bands and features liner notes by ex-Kyuss bass player Scott Reeder. [SEP] hypothesis: Scott Reeder did not contribute to the tribute collection titled "Listen Without Distraction."
06/15/2022 19:23:00 - INFO - __main__ - ['contradiction']
06/15/2022 19:23:00 - INFO - __main__ -  [anli] premise: Patrick Brontë ( , "commonly" ; 17 March 1777 – 7 June 1861) was an Irish priest and author who spent most of his adult life in England. He was the father of the writers Charlotte, Emily, and Anne Brontë, and of Branwell Brontë, his only son. Patrick outlived his wife, the former Maria Branwell, by forty years by which time all of their children had died as well. [SEP] hypothesis: Patrick Brontë was born during 1777.
06/15/2022 19:23:00 - INFO - __main__ - ['contradiction']
06/15/2022 19:23:00 - INFO - __main__ -  [anli] premise: Talking Smack is an American talk show produced by WWE and currently airing on their subscription based streaming service, the WWE Network. The show is primarily hosted by Renee Young, who is usually joined by a co-host. Past hosts have included Daniel Bryan, Shane McMahon, The Miz and Jerry Lawler. [SEP] hypothesis: Talking Smack is available on a broadcast network
06/15/2022 19:23:00 - INFO - __main__ - ['contradiction']
06/15/2022 19:23:00 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:23:00 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:23:00 - INFO - __main__ - Loaded 192 examples from dev data
06/15/2022 19:23:21 - INFO - __main__ - load prompt embedding from ckpt
06/15/2022 19:23:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
06/15/2022 19:23:22 - INFO - __main__ - Starting training!
06/15/2022 19:23:26 - INFO - __main__ - Step 10 Global step 10 Train loss 1.08 on epoch=0
06/15/2022 19:23:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=1
06/15/2022 19:23:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=2
06/15/2022 19:23:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.68 on epoch=3
06/15/2022 19:23:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=4
06/15/2022 19:23:41 - INFO - __main__ - Global step 50 Train loss 0.72 Classification-F1 0.20198457235494272 on epoch=4
06/15/2022 19:23:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.20198457235494272 on epoch=4, global_step=50
06/15/2022 19:23:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=4
06/15/2022 19:23:48 - INFO - __main__ - Step 70 Global step 70 Train loss 0.55 on epoch=5
06/15/2022 19:23:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.53 on epoch=6
06/15/2022 19:23:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=7
06/15/2022 19:23:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.54 on epoch=8
06/15/2022 19:24:02 - INFO - __main__ - Global step 100 Train loss 0.54 Classification-F1 0.25777777777777783 on epoch=8
06/15/2022 19:24:02 - INFO - __main__ - Saving model with best Classification-F1: 0.20198457235494272 -> 0.25777777777777783 on epoch=8, global_step=100
06/15/2022 19:24:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.54 on epoch=9
06/15/2022 19:24:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=9
06/15/2022 19:24:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=10
06/15/2022 19:24:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=11
06/15/2022 19:24:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=12
06/15/2022 19:24:22 - INFO - __main__ - Global step 150 Train loss 0.52 Classification-F1 0.16666666666666666 on epoch=12
06/15/2022 19:24:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.50 on epoch=13
06/15/2022 19:24:28 - INFO - __main__ - Step 170 Global step 170 Train loss 0.56 on epoch=14
06/15/2022 19:24:31 - INFO - __main__ - Step 180 Global step 180 Train loss 0.48 on epoch=14
06/15/2022 19:24:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.50 on epoch=15
06/15/2022 19:24:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=16
06/15/2022 19:24:42 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.16666666666666666 on epoch=16
06/15/2022 19:24:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.40 on epoch=17
06/15/2022 19:24:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.54 on epoch=18
06/15/2022 19:24:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.44 on epoch=19
06/15/2022 19:24:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.47 on epoch=19
06/15/2022 19:24:56 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=20
06/15/2022 19:25:02 - INFO - __main__ - Global step 250 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=20
06/15/2022 19:25:05 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=21
06/15/2022 19:25:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.44 on epoch=22
06/15/2022 19:25:11 - INFO - __main__ - Step 280 Global step 280 Train loss 0.51 on epoch=23
06/15/2022 19:25:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.46 on epoch=24
06/15/2022 19:25:16 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=24
06/15/2022 19:25:23 - INFO - __main__ - Global step 300 Train loss 0.47 Classification-F1 0.16666666666666666 on epoch=24
06/15/2022 19:25:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=25
06/15/2022 19:25:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.44 on epoch=26
06/15/2022 19:25:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.43 on epoch=27
06/15/2022 19:25:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.47 on epoch=28
06/15/2022 19:25:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.45 on epoch=29
06/15/2022 19:25:45 - INFO - __main__ - Global step 350 Train loss 0.45 Classification-F1 0.2499560317653584 on epoch=29
06/15/2022 19:25:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.44 on epoch=29
06/15/2022 19:25:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.41 on epoch=30
06/15/2022 19:25:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=31
06/15/2022 19:25:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.45 on epoch=32
06/15/2022 19:26:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=33
06/15/2022 19:26:06 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=33
06/15/2022 19:26:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=34
06/15/2022 19:26:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=34
06/15/2022 19:26:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.45 on epoch=35
06/15/2022 19:26:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=36
06/15/2022 19:26:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=37
06/15/2022 19:26:27 - INFO - __main__ - Global step 450 Train loss 0.44 Classification-F1 0.1647058823529412 on epoch=37
06/15/2022 19:26:30 - INFO - __main__ - Step 460 Global step 460 Train loss 0.51 on epoch=38
06/15/2022 19:26:33 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=39
06/15/2022 19:26:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.48 on epoch=39
06/15/2022 19:26:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.42 on epoch=40
06/15/2022 19:26:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.38 on epoch=41
06/15/2022 19:26:49 - INFO - __main__ - Global step 500 Train loss 0.45 Classification-F1 0.16535433070866143 on epoch=41
06/15/2022 19:26:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=42
06/15/2022 19:26:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.46 on epoch=43
06/15/2022 19:26:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.46 on epoch=44
06/15/2022 19:27:01 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=44
06/15/2022 19:27:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.43 on epoch=45
06/15/2022 19:27:10 - INFO - __main__ - Global step 550 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=45
06/15/2022 19:27:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=46
06/15/2022 19:27:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=47
06/15/2022 19:27:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=48
06/15/2022 19:27:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.43 on epoch=49
06/15/2022 19:27:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=49
06/15/2022 19:27:32 - INFO - __main__ - Global step 600 Train loss 0.44 Classification-F1 0.16535433070866143 on epoch=49
06/15/2022 19:27:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=50
06/15/2022 19:27:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.46 on epoch=51
06/15/2022 19:27:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.42 on epoch=52
06/15/2022 19:27:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=53
06/15/2022 19:27:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.43 on epoch=54
06/15/2022 19:27:53 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.19203963544266356 on epoch=54
06/15/2022 19:27:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.42 on epoch=54
06/15/2022 19:27:59 - INFO - __main__ - Step 670 Global step 670 Train loss 0.39 on epoch=55
06/15/2022 19:28:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=56
06/15/2022 19:28:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.42 on epoch=57
06/15/2022 19:28:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.46 on epoch=58
06/15/2022 19:28:13 - INFO - __main__ - Global step 700 Train loss 0.43 Classification-F1 0.27636363636363637 on epoch=58
06/15/2022 19:28:13 - INFO - __main__ - Saving model with best Classification-F1: 0.25777777777777783 -> 0.27636363636363637 on epoch=58, global_step=700
06/15/2022 19:28:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=59
06/15/2022 19:28:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=59
06/15/2022 19:28:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=60
06/15/2022 19:28:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.46 on epoch=61
06/15/2022 19:28:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.45 on epoch=62
06/15/2022 19:28:33 - INFO - __main__ - Global step 750 Train loss 0.45 Classification-F1 0.19126328217237307 on epoch=62
06/15/2022 19:28:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.46 on epoch=63
06/15/2022 19:28:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.47 on epoch=64
06/15/2022 19:28:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.46 on epoch=64
06/15/2022 19:28:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=65
06/15/2022 19:28:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.45 on epoch=66
06/15/2022 19:28:53 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.30303546634087275 on epoch=66
06/15/2022 19:28:53 - INFO - __main__ - Saving model with best Classification-F1: 0.27636363636363637 -> 0.30303546634087275 on epoch=66, global_step=800
06/15/2022 19:28:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=67
06/15/2022 19:28:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.46 on epoch=68
06/15/2022 19:29:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.46 on epoch=69
06/15/2022 19:29:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.45 on epoch=69
06/15/2022 19:29:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.44 on epoch=70
06/15/2022 19:29:12 - INFO - __main__ - Global step 850 Train loss 0.45 Classification-F1 0.16666666666666666 on epoch=70
06/15/2022 19:29:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.44 on epoch=71
06/15/2022 19:29:18 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=72
06/15/2022 19:29:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=73
06/15/2022 19:29:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.41 on epoch=74
06/15/2022 19:29:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.41 on epoch=74
06/15/2022 19:29:32 - INFO - __main__ - Global step 900 Train loss 0.43 Classification-F1 0.3872218446686532 on epoch=74
06/15/2022 19:29:32 - INFO - __main__ - Saving model with best Classification-F1: 0.30303546634087275 -> 0.3872218446686532 on epoch=74, global_step=900
06/15/2022 19:29:35 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=75
06/15/2022 19:29:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=76
06/15/2022 19:29:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.48 on epoch=77
06/15/2022 19:29:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=78
06/15/2022 19:29:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.41 on epoch=79
06/15/2022 19:29:51 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.27069616155874504 on epoch=79
06/15/2022 19:29:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.41 on epoch=79
06/15/2022 19:29:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=80
06/15/2022 19:30:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.44 on epoch=81
06/15/2022 19:30:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=82
06/15/2022 19:30:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=83
06/15/2022 19:30:11 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.1885434487640847 on epoch=83
06/15/2022 19:30:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.44 on epoch=84
06/15/2022 19:30:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=84
06/15/2022 19:30:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.40 on epoch=85
06/15/2022 19:30:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=86
06/15/2022 19:30:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=87
06/15/2022 19:30:30 - INFO - __main__ - Global step 1050 Train loss 0.43 Classification-F1 0.2278787878787879 on epoch=87
06/15/2022 19:30:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.46 on epoch=88
06/15/2022 19:30:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.44 on epoch=89
06/15/2022 19:30:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=89
06/15/2022 19:30:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.42 on epoch=90
06/15/2022 19:30:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.44 on epoch=91
06/15/2022 19:30:50 - INFO - __main__ - Global step 1100 Train loss 0.44 Classification-F1 0.16666666666666666 on epoch=91
06/15/2022 19:30:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.45 on epoch=92
06/15/2022 19:30:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.43 on epoch=93
06/15/2022 19:30:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.48 on epoch=94
06/15/2022 19:31:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=94
06/15/2022 19:31:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.46 on epoch=95
06/15/2022 19:31:09 - INFO - __main__ - Global step 1150 Train loss 0.46 Classification-F1 0.16666666666666666 on epoch=95
06/15/2022 19:31:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.45 on epoch=96
06/15/2022 19:31:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.43 on epoch=97
06/15/2022 19:31:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.42 on epoch=98
06/15/2022 19:31:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.40 on epoch=99
06/15/2022 19:31:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.41 on epoch=99
06/15/2022 19:31:29 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.3012724238349131 on epoch=99
06/15/2022 19:31:32 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=100
06/15/2022 19:31:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=101
06/15/2022 19:31:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=102
06/15/2022 19:31:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=103
06/15/2022 19:31:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=104
06/15/2022 19:31:49 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.24692881913830597 on epoch=104
06/15/2022 19:31:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.43 on epoch=104
06/15/2022 19:31:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=105
06/15/2022 19:31:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.37 on epoch=106
06/15/2022 19:32:00 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=107
06/15/2022 19:32:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.42 on epoch=108
06/15/2022 19:32:08 - INFO - __main__ - Global step 1300 Train loss 0.42 Classification-F1 0.2378586099516332 on epoch=108
06/15/2022 19:32:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.41 on epoch=109
06/15/2022 19:32:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.45 on epoch=109
06/15/2022 19:32:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=110
06/15/2022 19:32:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.42 on epoch=111
06/15/2022 19:32:22 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.40 on epoch=112
06/15/2022 19:32:26 - INFO - __main__ - Global step 1350 Train loss 0.43 Classification-F1 0.1843809523809524 on epoch=112
06/15/2022 19:32:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.42 on epoch=113
06/15/2022 19:32:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.42 on epoch=114
06/15/2022 19:32:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=114
06/15/2022 19:32:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=115
06/15/2022 19:32:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.43 on epoch=116
06/15/2022 19:32:45 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.32119259186928356 on epoch=116
06/15/2022 19:32:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=117
06/15/2022 19:32:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=118
06/15/2022 19:32:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=119
06/15/2022 19:32:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.42 on epoch=119
06/15/2022 19:32:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.44 on epoch=120
06/15/2022 19:33:05 - INFO - __main__ - Global step 1450 Train loss 0.43 Classification-F1 0.16666666666666666 on epoch=120
06/15/2022 19:33:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=121
06/15/2022 19:33:11 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=122
06/15/2022 19:33:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.44 on epoch=123
06/15/2022 19:33:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.42 on epoch=124
06/15/2022 19:33:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.43 on epoch=124
06/15/2022 19:33:27 - INFO - __main__ - Global step 1500 Train loss 0.43 Classification-F1 0.2797075210868314 on epoch=124
06/15/2022 19:33:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=125
06/15/2022 19:33:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.40 on epoch=126
06/15/2022 19:33:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.44 on epoch=127
06/15/2022 19:33:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=128
06/15/2022 19:33:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.39 on epoch=129
06/15/2022 19:33:48 - INFO - __main__ - Global step 1550 Train loss 0.41 Classification-F1 0.25405103079521685 on epoch=129
06/15/2022 19:33:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=129
06/15/2022 19:33:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=130
06/15/2022 19:33:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.43 on epoch=131
06/15/2022 19:34:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.38 on epoch=132
06/15/2022 19:34:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=133
06/15/2022 19:34:08 - INFO - __main__ - Global step 1600 Train loss 0.41 Classification-F1 0.2732343310686215 on epoch=133
06/15/2022 19:34:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.43 on epoch=134
06/15/2022 19:34:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=134
06/15/2022 19:34:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.35 on epoch=135
06/15/2022 19:34:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=136
06/15/2022 19:34:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.45 on epoch=137
06/15/2022 19:34:29 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.2733990147783251 on epoch=137
06/15/2022 19:34:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.46 on epoch=138
06/15/2022 19:34:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=139
06/15/2022 19:34:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=139
06/15/2022 19:34:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.46 on epoch=140
06/15/2022 19:34:43 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.45 on epoch=141
06/15/2022 19:34:49 - INFO - __main__ - Global step 1700 Train loss 0.44 Classification-F1 0.33541740382454194 on epoch=141
06/15/2022 19:34:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.40 on epoch=142
06/15/2022 19:34:55 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=143
06/15/2022 19:34:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.42 on epoch=144
06/15/2022 19:35:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=144
06/15/2022 19:35:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.39 on epoch=145
06/15/2022 19:35:10 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.22795160617438467 on epoch=145
06/15/2022 19:35:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.40 on epoch=146
06/15/2022 19:35:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.42 on epoch=147
06/15/2022 19:35:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.43 on epoch=148
06/15/2022 19:35:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=149
06/15/2022 19:35:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=149
06/15/2022 19:35:31 - INFO - __main__ - Global step 1800 Train loss 0.41 Classification-F1 0.2960213608117047 on epoch=149
06/15/2022 19:35:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=150
06/15/2022 19:35:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.42 on epoch=151
06/15/2022 19:35:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=152
06/15/2022 19:35:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.45 on epoch=153
06/15/2022 19:35:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=154
06/15/2022 19:35:52 - INFO - __main__ - Global step 1850 Train loss 0.42 Classification-F1 0.3915308387358077 on epoch=154
06/15/2022 19:35:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3872218446686532 -> 0.3915308387358077 on epoch=154, global_step=1850
06/15/2022 19:35:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.41 on epoch=154
06/15/2022 19:35:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.39 on epoch=155
06/15/2022 19:36:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=156
06/15/2022 19:36:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=157
06/15/2022 19:36:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.42 on epoch=158
06/15/2022 19:36:13 - INFO - __main__ - Global step 1900 Train loss 0.42 Classification-F1 0.31217391304347825 on epoch=158
06/15/2022 19:36:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=159
06/15/2022 19:36:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=159
06/15/2022 19:36:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=160
06/15/2022 19:36:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.45 on epoch=161
06/15/2022 19:36:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.37 on epoch=162
06/15/2022 19:36:35 - INFO - __main__ - Global step 1950 Train loss 0.40 Classification-F1 0.2753804461121534 on epoch=162
06/15/2022 19:36:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.42 on epoch=163
06/15/2022 19:36:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=164
06/15/2022 19:36:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.37 on epoch=164
06/15/2022 19:36:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.37 on epoch=165
06/15/2022 19:36:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=166
06/15/2022 19:36:56 - INFO - __main__ - Global step 2000 Train loss 0.39 Classification-F1 0.2976737412421125 on epoch=166
06/15/2022 19:36:59 - INFO - __main__ - Step 2010 Global step 2010 Train loss 0.38 on epoch=167
06/15/2022 19:37:02 - INFO - __main__ - Step 2020 Global step 2020 Train loss 0.39 on epoch=168
06/15/2022 19:37:05 - INFO - __main__ - Step 2030 Global step 2030 Train loss 0.44 on epoch=169
06/15/2022 19:37:08 - INFO - __main__ - Step 2040 Global step 2040 Train loss 0.41 on epoch=169
06/15/2022 19:37:11 - INFO - __main__ - Step 2050 Global step 2050 Train loss 0.42 on epoch=170
06/15/2022 19:37:17 - INFO - __main__ - Global step 2050 Train loss 0.41 Classification-F1 0.20613085931432948 on epoch=170
06/15/2022 19:37:19 - INFO - __main__ - Step 2060 Global step 2060 Train loss 0.46 on epoch=171
06/15/2022 19:37:22 - INFO - __main__ - Step 2070 Global step 2070 Train loss 0.42 on epoch=172
06/15/2022 19:37:25 - INFO - __main__ - Step 2080 Global step 2080 Train loss 0.40 on epoch=173
06/15/2022 19:37:29 - INFO - __main__ - Step 2090 Global step 2090 Train loss 0.45 on epoch=174
06/15/2022 19:37:31 - INFO - __main__ - Step 2100 Global step 2100 Train loss 0.42 on epoch=174
06/15/2022 19:37:38 - INFO - __main__ - Global step 2100 Train loss 0.43 Classification-F1 0.34626646060041505 on epoch=174
06/15/2022 19:37:41 - INFO - __main__ - Step 2110 Global step 2110 Train loss 0.40 on epoch=175
06/15/2022 19:37:44 - INFO - __main__ - Step 2120 Global step 2120 Train loss 0.43 on epoch=176
06/15/2022 19:37:47 - INFO - __main__ - Step 2130 Global step 2130 Train loss 0.41 on epoch=177
06/15/2022 19:37:50 - INFO - __main__ - Step 2140 Global step 2140 Train loss 0.42 on epoch=178
06/15/2022 19:37:52 - INFO - __main__ - Step 2150 Global step 2150 Train loss 0.40 on epoch=179
06/15/2022 19:37:59 - INFO - __main__ - Global step 2150 Train loss 0.41 Classification-F1 0.3235978106444428 on epoch=179
06/15/2022 19:38:02 - INFO - __main__ - Step 2160 Global step 2160 Train loss 0.36 on epoch=179
06/15/2022 19:38:05 - INFO - __main__ - Step 2170 Global step 2170 Train loss 0.39 on epoch=180
06/15/2022 19:38:08 - INFO - __main__ - Step 2180 Global step 2180 Train loss 0.43 on epoch=181
06/15/2022 19:38:11 - INFO - __main__ - Step 2190 Global step 2190 Train loss 0.42 on epoch=182
06/15/2022 19:38:14 - INFO - __main__ - Step 2200 Global step 2200 Train loss 0.42 on epoch=183
06/15/2022 19:38:20 - INFO - __main__ - Global step 2200 Train loss 0.40 Classification-F1 0.37507985650400505 on epoch=183
06/15/2022 19:38:23 - INFO - __main__ - Step 2210 Global step 2210 Train loss 0.41 on epoch=184
06/15/2022 19:38:26 - INFO - __main__ - Step 2220 Global step 2220 Train loss 0.40 on epoch=184
06/15/2022 19:38:29 - INFO - __main__ - Step 2230 Global step 2230 Train loss 0.41 on epoch=185
06/15/2022 19:38:32 - INFO - __main__ - Step 2240 Global step 2240 Train loss 0.43 on epoch=186
06/15/2022 19:38:35 - INFO - __main__ - Step 2250 Global step 2250 Train loss 0.41 on epoch=187
06/15/2022 19:38:42 - INFO - __main__ - Global step 2250 Train loss 0.41 Classification-F1 0.3449589954493466 on epoch=187
06/15/2022 19:38:45 - INFO - __main__ - Step 2260 Global step 2260 Train loss 0.40 on epoch=188
06/15/2022 19:38:48 - INFO - __main__ - Step 2270 Global step 2270 Train loss 0.38 on epoch=189
06/15/2022 19:38:51 - INFO - __main__ - Step 2280 Global step 2280 Train loss 0.43 on epoch=189
06/15/2022 19:38:53 - INFO - __main__ - Step 2290 Global step 2290 Train loss 0.38 on epoch=190
06/15/2022 19:38:56 - INFO - __main__ - Step 2300 Global step 2300 Train loss 0.36 on epoch=191
06/15/2022 19:39:03 - INFO - __main__ - Global step 2300 Train loss 0.39 Classification-F1 0.30587301587301585 on epoch=191
06/15/2022 19:39:06 - INFO - __main__ - Step 2310 Global step 2310 Train loss 0.36 on epoch=192
06/15/2022 19:39:09 - INFO - __main__ - Step 2320 Global step 2320 Train loss 0.41 on epoch=193
06/15/2022 19:39:12 - INFO - __main__ - Step 2330 Global step 2330 Train loss 0.40 on epoch=194
06/15/2022 19:39:14 - INFO - __main__ - Step 2340 Global step 2340 Train loss 0.41 on epoch=194
06/15/2022 19:39:17 - INFO - __main__ - Step 2350 Global step 2350 Train loss 0.39 on epoch=195
06/15/2022 19:39:24 - INFO - __main__ - Global step 2350 Train loss 0.39 Classification-F1 0.2514705882352941 on epoch=195
06/15/2022 19:39:27 - INFO - __main__ - Step 2360 Global step 2360 Train loss 0.38 on epoch=196
06/15/2022 19:39:30 - INFO - __main__ - Step 2370 Global step 2370 Train loss 0.37 on epoch=197
06/15/2022 19:39:33 - INFO - __main__ - Step 2380 Global step 2380 Train loss 0.38 on epoch=198
06/15/2022 19:39:36 - INFO - __main__ - Step 2390 Global step 2390 Train loss 0.38 on epoch=199
06/15/2022 19:39:39 - INFO - __main__ - Step 2400 Global step 2400 Train loss 0.37 on epoch=199
06/15/2022 19:39:45 - INFO - __main__ - Global step 2400 Train loss 0.38 Classification-F1 0.2998059034644401 on epoch=199
06/15/2022 19:39:48 - INFO - __main__ - Step 2410 Global step 2410 Train loss 0.41 on epoch=200
06/15/2022 19:39:51 - INFO - __main__ - Step 2420 Global step 2420 Train loss 0.38 on epoch=201
06/15/2022 19:39:54 - INFO - __main__ - Step 2430 Global step 2430 Train loss 0.35 on epoch=202
06/15/2022 19:39:57 - INFO - __main__ - Step 2440 Global step 2440 Train loss 0.43 on epoch=203
06/15/2022 19:40:00 - INFO - __main__ - Step 2450 Global step 2450 Train loss 0.44 on epoch=204
06/15/2022 19:40:06 - INFO - __main__ - Global step 2450 Train loss 0.40 Classification-F1 0.28339244460887075 on epoch=204
06/15/2022 19:40:09 - INFO - __main__ - Step 2460 Global step 2460 Train loss 0.40 on epoch=204
06/15/2022 19:40:12 - INFO - __main__ - Step 2470 Global step 2470 Train loss 0.41 on epoch=205
06/15/2022 19:40:15 - INFO - __main__ - Step 2480 Global step 2480 Train loss 0.42 on epoch=206
06/15/2022 19:40:18 - INFO - __main__ - Step 2490 Global step 2490 Train loss 0.40 on epoch=207
06/15/2022 19:40:21 - INFO - __main__ - Step 2500 Global step 2500 Train loss 0.37 on epoch=208
06/15/2022 19:40:27 - INFO - __main__ - Global step 2500 Train loss 0.40 Classification-F1 0.3438558558558558 on epoch=208
06/15/2022 19:40:30 - INFO - __main__ - Step 2510 Global step 2510 Train loss 0.39 on epoch=209
06/15/2022 19:40:33 - INFO - __main__ - Step 2520 Global step 2520 Train loss 0.37 on epoch=209
06/15/2022 19:40:36 - INFO - __main__ - Step 2530 Global step 2530 Train loss 0.35 on epoch=210
06/15/2022 19:40:39 - INFO - __main__ - Step 2540 Global step 2540 Train loss 0.38 on epoch=211
06/15/2022 19:40:42 - INFO - __main__ - Step 2550 Global step 2550 Train loss 0.41 on epoch=212
06/15/2022 19:40:48 - INFO - __main__ - Global step 2550 Train loss 0.38 Classification-F1 0.3335710041592395 on epoch=212
06/15/2022 19:40:51 - INFO - __main__ - Step 2560 Global step 2560 Train loss 0.41 on epoch=213
06/15/2022 19:40:54 - INFO - __main__ - Step 2570 Global step 2570 Train loss 0.37 on epoch=214
06/15/2022 19:40:57 - INFO - __main__ - Step 2580 Global step 2580 Train loss 0.35 on epoch=214
06/15/2022 19:41:00 - INFO - __main__ - Step 2590 Global step 2590 Train loss 0.33 on epoch=215
06/15/2022 19:41:03 - INFO - __main__ - Step 2600 Global step 2600 Train loss 0.41 on epoch=216
06/15/2022 19:41:09 - INFO - __main__ - Global step 2600 Train loss 0.38 Classification-F1 0.2541910331384016 on epoch=216
06/15/2022 19:41:12 - INFO - __main__ - Step 2610 Global step 2610 Train loss 0.35 on epoch=217
06/15/2022 19:41:15 - INFO - __main__ - Step 2620 Global step 2620 Train loss 0.42 on epoch=218
06/15/2022 19:41:18 - INFO - __main__ - Step 2630 Global step 2630 Train loss 0.38 on epoch=219
06/15/2022 19:41:21 - INFO - __main__ - Step 2640 Global step 2640 Train loss 0.39 on epoch=219
06/15/2022 19:41:24 - INFO - __main__ - Step 2650 Global step 2650 Train loss 0.33 on epoch=220
06/15/2022 19:41:30 - INFO - __main__ - Global step 2650 Train loss 0.37 Classification-F1 0.3149292513160198 on epoch=220
06/15/2022 19:41:33 - INFO - __main__ - Step 2660 Global step 2660 Train loss 0.37 on epoch=221
06/15/2022 19:41:35 - INFO - __main__ - Step 2670 Global step 2670 Train loss 0.34 on epoch=222
06/15/2022 19:41:38 - INFO - __main__ - Step 2680 Global step 2680 Train loss 0.41 on epoch=223
06/15/2022 19:41:41 - INFO - __main__ - Step 2690 Global step 2690 Train loss 0.37 on epoch=224
06/15/2022 19:41:44 - INFO - __main__ - Step 2700 Global step 2700 Train loss 0.38 on epoch=224
06/15/2022 19:41:50 - INFO - __main__ - Global step 2700 Train loss 0.37 Classification-F1 0.28089139189148277 on epoch=224
06/15/2022 19:41:53 - INFO - __main__ - Step 2710 Global step 2710 Train loss 0.38 on epoch=225
06/15/2022 19:41:56 - INFO - __main__ - Step 2720 Global step 2720 Train loss 0.35 on epoch=226
06/15/2022 19:41:59 - INFO - __main__ - Step 2730 Global step 2730 Train loss 0.40 on epoch=227
06/15/2022 19:42:02 - INFO - __main__ - Step 2740 Global step 2740 Train loss 0.39 on epoch=228
06/15/2022 19:42:05 - INFO - __main__ - Step 2750 Global step 2750 Train loss 0.35 on epoch=229
06/15/2022 19:42:11 - INFO - __main__ - Global step 2750 Train loss 0.37 Classification-F1 0.28439897698209715 on epoch=229
06/15/2022 19:42:14 - INFO - __main__ - Step 2760 Global step 2760 Train loss 0.39 on epoch=229
06/15/2022 19:42:17 - INFO - __main__ - Step 2770 Global step 2770 Train loss 0.36 on epoch=230
06/15/2022 19:42:20 - INFO - __main__ - Step 2780 Global step 2780 Train loss 0.36 on epoch=231
06/15/2022 19:42:23 - INFO - __main__ - Step 2790 Global step 2790 Train loss 0.39 on epoch=232
06/15/2022 19:42:27 - INFO - __main__ - Step 2800 Global step 2800 Train loss 0.32 on epoch=233
06/15/2022 19:42:32 - INFO - __main__ - Global step 2800 Train loss 0.36 Classification-F1 0.31114302048847936 on epoch=233
06/15/2022 19:42:35 - INFO - __main__ - Step 2810 Global step 2810 Train loss 0.40 on epoch=234
06/15/2022 19:42:38 - INFO - __main__ - Step 2820 Global step 2820 Train loss 0.36 on epoch=234
06/15/2022 19:42:41 - INFO - __main__ - Step 2830 Global step 2830 Train loss 0.35 on epoch=235
06/15/2022 19:42:44 - INFO - __main__ - Step 2840 Global step 2840 Train loss 0.42 on epoch=236
06/15/2022 19:42:47 - INFO - __main__ - Step 2850 Global step 2850 Train loss 0.35 on epoch=237
06/15/2022 19:42:53 - INFO - __main__ - Global step 2850 Train loss 0.38 Classification-F1 0.29961685823754786 on epoch=237
06/15/2022 19:42:56 - INFO - __main__ - Step 2860 Global step 2860 Train loss 0.35 on epoch=238
06/15/2022 19:42:59 - INFO - __main__ - Step 2870 Global step 2870 Train loss 0.39 on epoch=239
06/15/2022 19:43:02 - INFO - __main__ - Step 2880 Global step 2880 Train loss 0.39 on epoch=239
06/15/2022 19:43:05 - INFO - __main__ - Step 2890 Global step 2890 Train loss 0.36 on epoch=240
06/15/2022 19:43:08 - INFO - __main__ - Step 2900 Global step 2900 Train loss 0.38 on epoch=241
06/15/2022 19:43:15 - INFO - __main__ - Global step 2900 Train loss 0.38 Classification-F1 0.30281054317751566 on epoch=241
06/15/2022 19:43:18 - INFO - __main__ - Step 2910 Global step 2910 Train loss 0.37 on epoch=242
06/15/2022 19:43:21 - INFO - __main__ - Step 2920 Global step 2920 Train loss 0.38 on epoch=243
06/15/2022 19:43:24 - INFO - __main__ - Step 2930 Global step 2930 Train loss 0.35 on epoch=244
06/15/2022 19:43:27 - INFO - __main__ - Step 2940 Global step 2940 Train loss 0.34 on epoch=244
06/15/2022 19:43:30 - INFO - __main__ - Step 2950 Global step 2950 Train loss 0.33 on epoch=245
06/15/2022 19:43:36 - INFO - __main__ - Global step 2950 Train loss 0.36 Classification-F1 0.30516474266474264 on epoch=245
06/15/2022 19:43:39 - INFO - __main__ - Step 2960 Global step 2960 Train loss 0.36 on epoch=246
06/15/2022 19:43:42 - INFO - __main__ - Step 2970 Global step 2970 Train loss 0.33 on epoch=247
06/15/2022 19:43:45 - INFO - __main__ - Step 2980 Global step 2980 Train loss 0.37 on epoch=248
06/15/2022 19:43:48 - INFO - __main__ - Step 2990 Global step 2990 Train loss 0.37 on epoch=249
06/15/2022 19:43:51 - INFO - __main__ - Step 3000 Global step 3000 Train loss 0.37 on epoch=249
06/15/2022 19:43:58 - INFO - __main__ - Global step 3000 Train loss 0.36 Classification-F1 0.28525577393951423 on epoch=249
06/15/2022 19:43:58 - INFO - __main__ - save last model!
06/15/2022 19:43:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/15/2022 19:43:58 - INFO - __main__ - Start tokenizing ... 1000 instances
06/15/2022 19:43:58 - INFO - __main__ - Printing 3 examples
06/15/2022 19:43:58 - INFO - __main__ -  [anli] premise: Linguistics is the scientific study of language, and involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini, who wrote a formal description of the Sanskrit language in his "Aṣṭādhyāyī ". [SEP] hypothesis: Form and meaning are the only aspects of language linguistics is concerned with.
06/15/2022 19:43:58 - INFO - __main__ - ['contradiction']
06/15/2022 19:43:58 - INFO - __main__ -  [anli] premise: Franco Zeffirelli, KBE Grande Ufficiale OMRI (] ; born 12 February 1923) is an Italian director and producer of operas, films and television. He is also a former senator (1994–2001) for the Italian centre-right "Forza Italia" party. Recently, Italian researchers have found that he is one of the few distant relatives of Leonardo da Vinci. [SEP] hypothesis: Franco Zeffirelli had a political career
06/15/2022 19:43:58 - INFO - __main__ - ['entailment']
06/15/2022 19:43:58 - INFO - __main__ -  [anli] premise: Eme 15 is the self-titled debut studio album by Mexican-Argentine pop band, Eme 15. The album was released in Mexico and Latin America on June 26, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV. [SEP] hypothesis: Eme 15 was released in Mexico and Latin America on June 27, 2012 through Warner Music México, and features songs from the Nickelodeon Latin America and Televisa musical television series, Miss XV.
06/15/2022 19:43:58 - INFO - __main__ - ['contradiction']
06/15/2022 19:43:58 - INFO - __main__ - Tokenizing Input ...
06/15/2022 19:43:59 - INFO - __main__ - Tokenizing Output ...
06/15/2022 19:44:00 - INFO - __main__ - Loaded 1000 examples from test data
06/15/2022 19:44:33 - INFO - __main__ - Saved prediction in models/T5-large-maml-cls2cls-3e-5-2-5000-5e-1-down64shot/singletask-anli/anli_64_87_0.2_8_predictions.txt
06/15/2022 19:44:33 - INFO - __main__ - Classification-F1 on test data: 0.2658
06/15/2022 19:44:34 - INFO - __main__ - prefix=anli_64_87, lr=0.2, bsz=8, dev_performance=0.3915308387358077, test_performance=0.26577891915157875
