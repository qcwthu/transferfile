05/21/2022 21:29:36 - INFO - __main__ - Namespace(task_dir='data_128/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:29:36 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb
05/21/2022 21:29:36 - INFO - __main__ - Namespace(task_dir='data_128/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
05/21/2022 21:29:36 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb
05/21/2022 21:29:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
05/21/2022 21:29:38 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
05/21/2022 21:29:38 - INFO - __main__ - args.device: cuda:0
05/21/2022 21:29:38 - INFO - __main__ - Using 2 gpus
05/21/2022 21:29:38 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_128_100', 'superglue-cb_128_13', 'superglue-cb_128_21', 'superglue-cb_128_42', 'superglue-cb_128_87']
05/21/2022 21:29:38 - INFO - __main__ - args.device: cuda:1
05/21/2022 21:29:38 - INFO - __main__ - Using 2 gpus
05/21/2022 21:29:38 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_128_100', 'superglue-cb_128_13', 'superglue-cb_128_21', 'superglue-cb_128_42', 'superglue-cb_128_87']
05/21/2022 21:29:43 - INFO - __main__ - Running ... prefix=superglue-cb_128_100, lr=0.0005, bsz=8 ...
06/03/2022 06:54:37 - INFO - __main__ - Namespace(task_dir='data_128/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/03/2022 06:54:37 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb
06/03/2022 06:54:37 - INFO - __main__ - Namespace(task_dir='data_128/superglue-cb/', task_name='superglue-cb', identifier='T5-large-ft-cls2cls-down128shot', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='None', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=32, learning_rate=3e-05, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=300.0, warmup_steps=50, total_steps=1000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.0005, 0.0003, 0.0002, 0.0001], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, model='google/t5-v1_1-large', cuda='2,3')
06/03/2022 06:54:37 - INFO - __main__ - models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb
06/03/2022 06:54:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/03/2022 06:54:39 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/03/2022 06:54:39 - INFO - __main__ - args.device: cuda:0
06/03/2022 06:54:39 - INFO - __main__ - Using 2 gpus
06/03/2022 06:54:39 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_128_100', 'superglue-cb_128_13', 'superglue-cb_128_21', 'superglue-cb_128_42', 'superglue-cb_128_87']
06/03/2022 06:54:39 - INFO - __main__ - args.device: cuda:1
06/03/2022 06:54:39 - INFO - __main__ - Using 2 gpus
06/03/2022 06:54:39 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_128_100', 'superglue-cb_128_13', 'superglue-cb_128_21', 'superglue-cb_128_42', 'superglue-cb_128_87']
06/03/2022 06:54:44 - INFO - __main__ - Running ... prefix=superglue-cb_128_100, lr=0.0005, bsz=8 ...
06/03/2022 06:54:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 06:54:45 - INFO - __main__ - Printing 3 examples
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 06:54:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 06:54:45 - INFO - __main__ - Printing 3 examples
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 06:54:45 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 06:54:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 06:54:45 - INFO - __main__ - Printing 3 examples
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 06:54:45 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 06:54:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 06:54:45 - INFO - __main__ - Printing 3 examples
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 06:54:45 - INFO - __main__ - ['contradiction']
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 06:54:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 06:54:45 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 06:54:45 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 06:54:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 06:54:58 - INFO - __main__ - Starting training!
06/03/2022 06:54:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 06:54:59 - INFO - __main__ - Starting training!
06/03/2022 06:55:04 - INFO - __main__ - Step 10 Global step 10 Train loss 22.559036 on epoch=1
06/03/2022 06:55:08 - INFO - __main__ - Step 20 Global step 20 Train loss 20.012617 on epoch=2
06/03/2022 06:55:13 - INFO - __main__ - Step 30 Global step 30 Train loss 10.752123 on epoch=3
06/03/2022 06:55:18 - INFO - __main__ - Step 40 Global step 40 Train loss 9.639382 on epoch=4
06/03/2022 06:55:23 - INFO - __main__ - Step 50 Global step 50 Train loss 8.234222 on epoch=6
06/03/2022 06:55:27 - INFO - __main__ - Global step 50 Train loss 14.239475 ACC 0.016129032258064516 on epoch=6
06/03/2022 06:55:33 - INFO - __main__ - Step 60 Global step 60 Train loss 7.233557 on epoch=7
06/03/2022 06:55:38 - INFO - __main__ - Step 70 Global step 70 Train loss 5.747434 on epoch=8
06/03/2022 06:55:43 - INFO - __main__ - Step 80 Global step 80 Train loss 4.559546 on epoch=9
06/03/2022 06:55:48 - INFO - __main__ - Step 90 Global step 90 Train loss 3.994498 on epoch=11
06/03/2022 06:55:53 - INFO - __main__ - Step 100 Global step 100 Train loss 2.459037 on epoch=12
06/03/2022 06:55:55 - INFO - __main__ - Global step 100 Train loss 4.798815 ACC 0.47580645161290325 on epoch=12
06/03/2022 06:56:02 - INFO - __main__ - Step 110 Global step 110 Train loss 2.045641 on epoch=13
06/03/2022 06:56:07 - INFO - __main__ - Step 120 Global step 120 Train loss 1.908962 on epoch=14
06/03/2022 06:56:12 - INFO - __main__ - Step 130 Global step 130 Train loss 1.573313 on epoch=16
06/03/2022 06:56:17 - INFO - __main__ - Step 140 Global step 140 Train loss 1.865729 on epoch=17
06/03/2022 06:56:22 - INFO - __main__ - Step 150 Global step 150 Train loss 1.302210 on epoch=18
06/03/2022 06:56:24 - INFO - __main__ - Global step 150 Train loss 1.739171 ACC 0.47580645161290325 on epoch=18
06/03/2022 06:56:29 - INFO - __main__ - Step 160 Global step 160 Train loss 1.423105 on epoch=19
06/03/2022 06:56:34 - INFO - __main__ - Step 170 Global step 170 Train loss 1.192866 on epoch=21
06/03/2022 06:56:39 - INFO - __main__ - Step 180 Global step 180 Train loss 1.226342 on epoch=22
06/03/2022 06:56:44 - INFO - __main__ - Step 190 Global step 190 Train loss 1.276851 on epoch=23
06/03/2022 06:56:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.204482 on epoch=24
06/03/2022 06:56:51 - INFO - __main__ - Global step 200 Train loss 1.264729 ACC 0.47580645161290325 on epoch=24
06/03/2022 06:56:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.707190 on epoch=26
06/03/2022 06:57:02 - INFO - __main__ - Step 220 Global step 220 Train loss 1.205361 on epoch=27
06/03/2022 06:57:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.947754 on epoch=28
06/03/2022 06:57:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.948606 on epoch=29
06/03/2022 06:57:17 - INFO - __main__ - Step 250 Global step 250 Train loss 1.114638 on epoch=31
06/03/2022 06:57:19 - INFO - __main__ - Global step 250 Train loss 0.984710 ACC 0.47580645161290325 on epoch=31
06/03/2022 06:57:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.708927 on epoch=32
06/03/2022 06:57:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.567007 on epoch=33
06/03/2022 06:57:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.592182 on epoch=34
06/03/2022 06:57:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.600341 on epoch=36
06/03/2022 06:57:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.793914 on epoch=37
06/03/2022 06:57:45 - INFO - __main__ - Global step 300 Train loss 0.652474 ACC 0.47580645161290325 on epoch=37
06/03/2022 06:57:50 - INFO - __main__ - Step 310 Global step 310 Train loss 0.577726 on epoch=38
06/03/2022 06:57:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.430377 on epoch=39
06/03/2022 06:58:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.537978 on epoch=41
06/03/2022 06:58:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.480015 on epoch=42
06/03/2022 06:58:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.501277 on epoch=43
06/03/2022 06:58:13 - INFO - __main__ - Global step 350 Train loss 0.505475 ACC 0.6370967741935484 on epoch=43
06/03/2022 06:58:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.580732 on epoch=44
06/03/2022 06:58:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.331061 on epoch=46
06/03/2022 06:58:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.427409 on epoch=47
06/03/2022 06:58:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.335173 on epoch=48
06/03/2022 06:58:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.455279 on epoch=49
06/03/2022 06:58:42 - INFO - __main__ - Global step 400 Train loss 0.425931 ACC 0.532258064516129 on epoch=49
06/03/2022 06:58:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.366167 on epoch=51
06/03/2022 06:58:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.518052 on epoch=52
06/03/2022 06:58:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.386773 on epoch=53
06/03/2022 06:59:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.341741 on epoch=54
06/03/2022 06:59:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.372618 on epoch=56
06/03/2022 06:59:10 - INFO - __main__ - Global step 450 Train loss 0.397070 ACC 0.47580645161290325 on epoch=56
06/03/2022 06:59:15 - INFO - __main__ - Step 460 Global step 460 Train loss 0.401773 on epoch=57
06/03/2022 06:59:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.468860 on epoch=58
06/03/2022 06:59:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.359597 on epoch=59
06/03/2022 06:59:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.430807 on epoch=61
06/03/2022 06:59:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.363677 on epoch=62
06/03/2022 06:59:39 - INFO - __main__ - Global step 500 Train loss 0.404943 ACC 0.47580645161290325 on epoch=62
06/03/2022 06:59:44 - INFO - __main__ - Step 510 Global step 510 Train loss 0.382575 on epoch=63
06/03/2022 06:59:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.375111 on epoch=64
06/03/2022 06:59:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.419063 on epoch=66
06/03/2022 06:59:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.411189 on epoch=67
06/03/2022 07:00:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.332037 on epoch=68
06/03/2022 07:00:07 - INFO - __main__ - Global step 550 Train loss 0.383995 ACC 0.4596774193548387 on epoch=68
06/03/2022 07:00:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.343961 on epoch=69
06/03/2022 07:00:18 - INFO - __main__ - Step 570 Global step 570 Train loss 0.307947 on epoch=71
06/03/2022 07:00:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.278628 on epoch=72
06/03/2022 07:00:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.321096 on epoch=73
06/03/2022 07:00:33 - INFO - __main__ - Step 600 Global step 600 Train loss 0.344429 on epoch=74
06/03/2022 07:00:35 - INFO - __main__ - Global step 600 Train loss 0.319212 ACC 0.47580645161290325 on epoch=74
06/03/2022 07:00:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.303593 on epoch=76
06/03/2022 07:00:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.385612 on epoch=77
06/03/2022 07:00:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.346281 on epoch=78
06/03/2022 07:00:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.377479 on epoch=79
06/03/2022 07:01:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.404610 on epoch=81
06/03/2022 07:01:03 - INFO - __main__ - Global step 650 Train loss 0.363515 ACC 0.47580645161290325 on epoch=81
06/03/2022 07:01:08 - INFO - __main__ - Step 660 Global step 660 Train loss 0.309994 on epoch=82
06/03/2022 07:01:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.296186 on epoch=83
06/03/2022 07:01:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.334795 on epoch=84
06/03/2022 07:01:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.327266 on epoch=86
06/03/2022 07:01:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.296358 on epoch=87
06/03/2022 07:01:32 - INFO - __main__ - Global step 700 Train loss 0.312920 ACC 0.5806451612903226 on epoch=87
06/03/2022 07:01:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.337084 on epoch=88
06/03/2022 07:01:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.272430 on epoch=89
06/03/2022 07:01:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.249824 on epoch=91
06/03/2022 07:01:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.269403 on epoch=92
06/03/2022 07:01:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.308168 on epoch=93
06/03/2022 07:02:00 - INFO - __main__ - Global step 750 Train loss 0.287382 ACC 0.47580645161290325 on epoch=93
06/03/2022 07:02:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.256576 on epoch=94
06/03/2022 07:02:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.278051 on epoch=96
06/03/2022 07:02:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.293568 on epoch=97
06/03/2022 07:02:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.231502 on epoch=98
06/03/2022 07:02:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.313250 on epoch=99
06/03/2022 07:02:28 - INFO - __main__ - Global step 800 Train loss 0.274589 ACC 0.47580645161290325 on epoch=99
06/03/2022 07:02:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.276910 on epoch=101
06/03/2022 07:02:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.261658 on epoch=102
06/03/2022 07:02:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.304525 on epoch=103
06/03/2022 07:02:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.269428 on epoch=104
06/03/2022 07:02:53 - INFO - __main__ - Step 850 Global step 850 Train loss 1.374838 on epoch=106
06/03/2022 07:02:56 - INFO - __main__ - Global step 850 Train loss 0.497472 ACC 0.5887096774193549 on epoch=106
06/03/2022 07:03:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.601278 on epoch=107
06/03/2022 07:03:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.262987 on epoch=108
06/03/2022 07:03:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.293282 on epoch=109
06/03/2022 07:03:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.340568 on epoch=111
06/03/2022 07:03:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.264881 on epoch=112
06/03/2022 07:03:25 - INFO - __main__ - Global step 900 Train loss 0.352599 ACC 0.47580645161290325 on epoch=112
06/03/2022 07:03:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.271431 on epoch=113
06/03/2022 07:03:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.261608 on epoch=114
06/03/2022 07:03:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.304473 on epoch=116
06/03/2022 07:03:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.244322 on epoch=117
06/03/2022 07:03:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.276520 on epoch=118
06/03/2022 07:03:53 - INFO - __main__ - Global step 950 Train loss 0.271671 ACC 0.4596774193548387 on epoch=118
06/03/2022 07:03:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.278437 on epoch=119
06/03/2022 07:04:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.270584 on epoch=121
06/03/2022 07:04:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.275731 on epoch=122
06/03/2022 07:04:13 - INFO - __main__ - Step 990 Global step 990 Train loss 0.241045 on epoch=123
06/03/2022 07:04:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.256912 on epoch=124
06/03/2022 07:04:20 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:04:20 - INFO - __main__ - Printing 3 examples
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:04:20 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:04:20 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:04:20 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:04:20 - INFO - __main__ - Printing 3 examples
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:04:20 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:20 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:04:20 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:04:20 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:04:21 - INFO - __main__ - Global step 1000 Train loss 0.264542 ACC 0.49193548387096775 on epoch=124
06/03/2022 07:04:21 - INFO - __main__ - save last model!
06/03/2022 07:04:28 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:04:29 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:04:29 - INFO - __main__ - Printing 3 examples
06/03/2022 07:04:29 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:04:29 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:29 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:04:29 - INFO - __main__ - ['neutral']
06/03/2022 07:04:29 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:04:29 - INFO - __main__ - ['entailment']
06/03/2022 07:04:29 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:04:29 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:04:29 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:04:30 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_100_0.0005_8_predictions.txt
06/03/2022 07:04:30 - INFO - __main__ - ACC on test data: 0.5893
06/03/2022 07:04:30 - INFO - __main__ - prefix=superglue-cb_128_100, lr=0.0005, bsz=8, dev_performance=0.6370967741935484, test_performance=0.5892857142857143
06/03/2022 07:04:30 - INFO - __main__ - Running ... prefix=superglue-cb_128_100, lr=0.0003, bsz=8 ...
06/03/2022 07:04:31 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:04:31 - INFO - __main__ - Printing 3 examples
06/03/2022 07:04:31 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:04:31 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:31 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:04:31 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:31 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:04:31 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:31 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:04:31 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:04:32 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:04:32 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:04:32 - INFO - __main__ - Printing 3 examples
06/03/2022 07:04:32 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:04:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:32 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:04:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:32 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:04:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:04:32 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:04:32 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:04:32 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:04:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:04:33 - INFO - __main__ - Starting training!
06/03/2022 07:04:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:04:44 - INFO - __main__ - Starting training!
06/03/2022 07:04:49 - INFO - __main__ - Step 10 Global step 10 Train loss 22.973104 on epoch=1
06/03/2022 07:04:54 - INFO - __main__ - Step 20 Global step 20 Train loss 14.029772 on epoch=2
06/03/2022 07:04:59 - INFO - __main__ - Step 30 Global step 30 Train loss 10.210480 on epoch=3
06/03/2022 07:05:04 - INFO - __main__ - Step 40 Global step 40 Train loss 9.822154 on epoch=4
06/03/2022 07:05:10 - INFO - __main__ - Step 50 Global step 50 Train loss 9.107738 on epoch=6
06/03/2022 07:05:12 - INFO - __main__ - Global step 50 Train loss 13.228650 ACC 0.0 on epoch=6
06/03/2022 07:05:18 - INFO - __main__ - Step 60 Global step 60 Train loss 8.447577 on epoch=7
06/03/2022 07:05:23 - INFO - __main__ - Step 70 Global step 70 Train loss 8.210900 on epoch=8
06/03/2022 07:05:28 - INFO - __main__ - Step 80 Global step 80 Train loss 7.271407 on epoch=9
06/03/2022 07:05:33 - INFO - __main__ - Step 90 Global step 90 Train loss 7.042633 on epoch=11
06/03/2022 07:05:39 - INFO - __main__ - Step 100 Global step 100 Train loss 5.654704 on epoch=12
06/03/2022 07:05:41 - INFO - __main__ - Global step 100 Train loss 7.325444 ACC 0.0 on epoch=12
06/03/2022 07:05:46 - INFO - __main__ - Step 110 Global step 110 Train loss 5.314733 on epoch=13
06/03/2022 07:05:51 - INFO - __main__ - Step 120 Global step 120 Train loss 4.093137 on epoch=14
06/03/2022 07:05:57 - INFO - __main__ - Step 130 Global step 130 Train loss 3.576308 on epoch=16
06/03/2022 07:06:02 - INFO - __main__ - Step 140 Global step 140 Train loss 2.759709 on epoch=17
06/03/2022 07:06:07 - INFO - __main__ - Step 150 Global step 150 Train loss 2.635735 on epoch=18
06/03/2022 07:06:10 - INFO - __main__ - Global step 150 Train loss 3.675924 ACC 0.4435483870967742 on epoch=18
06/03/2022 07:06:16 - INFO - __main__ - Step 160 Global step 160 Train loss 0.662426 on epoch=19
06/03/2022 07:06:21 - INFO - __main__ - Step 170 Global step 170 Train loss 0.459920 on epoch=21
06/03/2022 07:06:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.288880 on epoch=22
06/03/2022 07:06:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.338688 on epoch=23
06/03/2022 07:06:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.785919 on epoch=24
06/03/2022 07:06:39 - INFO - __main__ - Global step 200 Train loss 0.507167 ACC 0.47580645161290325 on epoch=24
06/03/2022 07:06:45 - INFO - __main__ - Step 210 Global step 210 Train loss 0.367040 on epoch=26
06/03/2022 07:06:50 - INFO - __main__ - Step 220 Global step 220 Train loss 2.412280 on epoch=27
06/03/2022 07:06:55 - INFO - __main__ - Step 230 Global step 230 Train loss 1.263329 on epoch=28
06/03/2022 07:07:00 - INFO - __main__ - Step 240 Global step 240 Train loss 1.877981 on epoch=29
06/03/2022 07:07:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.450080 on epoch=31
06/03/2022 07:07:08 - INFO - __main__ - Global step 250 Train loss 1.474142 ACC 0.47580645161290325 on epoch=31
06/03/2022 07:07:13 - INFO - __main__ - Step 260 Global step 260 Train loss 1.871324 on epoch=32
06/03/2022 07:07:18 - INFO - __main__ - Step 270 Global step 270 Train loss 1.471998 on epoch=33
06/03/2022 07:07:23 - INFO - __main__ - Step 280 Global step 280 Train loss 1.747933 on epoch=34
06/03/2022 07:07:28 - INFO - __main__ - Step 290 Global step 290 Train loss 1.830297 on epoch=36
06/03/2022 07:07:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.500675 on epoch=37
06/03/2022 07:07:36 - INFO - __main__ - Global step 300 Train loss 1.684445 ACC 0.47580645161290325 on epoch=37
06/03/2022 07:07:41 - INFO - __main__ - Step 310 Global step 310 Train loss 1.765668 on epoch=38
06/03/2022 07:07:46 - INFO - __main__ - Step 320 Global step 320 Train loss 1.252451 on epoch=39
06/03/2022 07:07:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.834906 on epoch=41
06/03/2022 07:07:57 - INFO - __main__ - Step 340 Global step 340 Train loss 1.412586 on epoch=42
06/03/2022 07:08:02 - INFO - __main__ - Step 350 Global step 350 Train loss 1.025472 on epoch=43
06/03/2022 07:08:04 - INFO - __main__ - Global step 350 Train loss 1.458217 ACC 0.47580645161290325 on epoch=43
06/03/2022 07:08:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.268130 on epoch=44
06/03/2022 07:08:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.086894 on epoch=46
06/03/2022 07:08:20 - INFO - __main__ - Step 380 Global step 380 Train loss 1.055958 on epoch=47
06/03/2022 07:08:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.814653 on epoch=48
06/03/2022 07:08:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.992489 on epoch=49
06/03/2022 07:08:33 - INFO - __main__ - Global step 400 Train loss 1.043625 ACC 0.47580645161290325 on epoch=49
06/03/2022 07:08:38 - INFO - __main__ - Step 410 Global step 410 Train loss 1.053148 on epoch=51
06/03/2022 07:08:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.753468 on epoch=52
06/03/2022 07:08:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.808123 on epoch=53
06/03/2022 07:08:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.756334 on epoch=54
06/03/2022 07:08:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.692676 on epoch=56
06/03/2022 07:09:01 - INFO - __main__ - Global step 450 Train loss 0.812750 ACC 0.47580645161290325 on epoch=56
06/03/2022 07:09:07 - INFO - __main__ - Step 460 Global step 460 Train loss 0.668970 on epoch=57
06/03/2022 07:09:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.765288 on epoch=58
06/03/2022 07:09:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.707023 on epoch=59
06/03/2022 07:09:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.776943 on epoch=61
06/03/2022 07:09:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.742074 on epoch=62
06/03/2022 07:09:30 - INFO - __main__ - Global step 500 Train loss 0.732060 ACC 0.47580645161290325 on epoch=62
06/03/2022 07:09:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.686873 on epoch=63
06/03/2022 07:09:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.555297 on epoch=64
06/03/2022 07:09:45 - INFO - __main__ - Step 530 Global step 530 Train loss 0.806684 on epoch=66
06/03/2022 07:09:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.551259 on epoch=67
06/03/2022 07:09:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.515607 on epoch=68
06/03/2022 07:09:58 - INFO - __main__ - Global step 550 Train loss 0.623144 ACC 0.47580645161290325 on epoch=68
06/03/2022 07:10:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.544123 on epoch=69
06/03/2022 07:10:08 - INFO - __main__ - Step 570 Global step 570 Train loss 0.507441 on epoch=71
06/03/2022 07:10:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.426774 on epoch=72
06/03/2022 07:10:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.473007 on epoch=73
06/03/2022 07:10:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.565134 on epoch=74
06/03/2022 07:10:27 - INFO - __main__ - Global step 600 Train loss 0.503296 ACC 0.6290322580645161 on epoch=74
06/03/2022 07:10:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.563495 on epoch=76
06/03/2022 07:10:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.558660 on epoch=77
06/03/2022 07:10:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.432651 on epoch=78
06/03/2022 07:10:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.524799 on epoch=79
06/03/2022 07:10:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.367245 on epoch=81
06/03/2022 07:10:56 - INFO - __main__ - Global step 650 Train loss 0.489370 ACC 0.47580645161290325 on epoch=81
06/03/2022 07:11:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.334127 on epoch=82
06/03/2022 07:11:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.387187 on epoch=83
06/03/2022 07:11:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.401984 on epoch=84
06/03/2022 07:11:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.299982 on epoch=86
06/03/2022 07:11:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.359434 on epoch=87
06/03/2022 07:11:24 - INFO - __main__ - Global step 700 Train loss 0.356543 ACC 0.717741935483871 on epoch=87
06/03/2022 07:11:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.533629 on epoch=88
06/03/2022 07:11:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.422104 on epoch=89
06/03/2022 07:11:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.339861 on epoch=91
06/03/2022 07:11:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.324257 on epoch=92
06/03/2022 07:11:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.474348 on epoch=93
06/03/2022 07:11:53 - INFO - __main__ - Global step 750 Train loss 0.418840 ACC 0.4596774193548387 on epoch=93
06/03/2022 07:11:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.361707 on epoch=94
06/03/2022 07:12:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.471605 on epoch=96
06/03/2022 07:12:09 - INFO - __main__ - Step 780 Global step 780 Train loss 0.307682 on epoch=97
06/03/2022 07:12:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.414660 on epoch=98
06/03/2022 07:12:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.408983 on epoch=99
06/03/2022 07:12:22 - INFO - __main__ - Global step 800 Train loss 0.392928 ACC 0.4596774193548387 on epoch=99
06/03/2022 07:12:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.308708 on epoch=101
06/03/2022 07:12:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.318395 on epoch=102
06/03/2022 07:12:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.372301 on epoch=103
06/03/2022 07:12:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.392188 on epoch=104
06/03/2022 07:12:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.461121 on epoch=106
06/03/2022 07:12:50 - INFO - __main__ - Global step 850 Train loss 0.370543 ACC 0.47580645161290325 on epoch=106
06/03/2022 07:12:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.346276 on epoch=107
06/03/2022 07:13:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.370811 on epoch=108
06/03/2022 07:13:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.344667 on epoch=109
06/03/2022 07:13:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.334523 on epoch=111
06/03/2022 07:13:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.327735 on epoch=112
06/03/2022 07:13:19 - INFO - __main__ - Global step 900 Train loss 0.344802 ACC 0.717741935483871 on epoch=112
06/03/2022 07:13:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.366841 on epoch=113
06/03/2022 07:13:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.310415 on epoch=114
06/03/2022 07:13:34 - INFO - __main__ - Step 930 Global step 930 Train loss 0.415923 on epoch=116
06/03/2022 07:13:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.327398 on epoch=117
06/03/2022 07:13:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.329848 on epoch=118
06/03/2022 07:13:47 - INFO - __main__ - Global step 950 Train loss 0.350085 ACC 0.5725806451612904 on epoch=118
06/03/2022 07:13:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.370233 on epoch=119
06/03/2022 07:13:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.301324 on epoch=121
06/03/2022 07:14:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.276349 on epoch=122
06/03/2022 07:14:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.364325 on epoch=123
06/03/2022 07:14:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.336614 on epoch=124
06/03/2022 07:14:14 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:14:14 - INFO - __main__ - Printing 3 examples
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:14:14 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:14:14 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:14:14 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:14:14 - INFO - __main__ - Printing 3 examples
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:14:14 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:14 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:14:15 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:14:15 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:14:16 - INFO - __main__ - Global step 1000 Train loss 0.329769 ACC 0.5161290322580645 on epoch=124
06/03/2022 07:14:16 - INFO - __main__ - save last model!
06/03/2022 07:14:22 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:14:23 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:14:23 - INFO - __main__ - Printing 3 examples
06/03/2022 07:14:23 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:14:23 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:23 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:14:23 - INFO - __main__ - ['neutral']
06/03/2022 07:14:23 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:14:23 - INFO - __main__ - ['entailment']
06/03/2022 07:14:23 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:14:23 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:14:23 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:14:25 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_100_0.0003_8_predictions.txt
06/03/2022 07:14:25 - INFO - __main__ - ACC on test data: 0.5357
06/03/2022 07:14:25 - INFO - __main__ - prefix=superglue-cb_128_100, lr=0.0003, bsz=8, dev_performance=0.717741935483871, test_performance=0.5357142857142857
06/03/2022 07:14:25 - INFO - __main__ - Running ... prefix=superglue-cb_128_100, lr=0.0002, bsz=8 ...
06/03/2022 07:14:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:14:25 - INFO - __main__ - Starting training!
06/03/2022 07:14:26 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:14:26 - INFO - __main__ - Printing 3 examples
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:14:26 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:14:26 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:14:26 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:14:26 - INFO - __main__ - Printing 3 examples
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:14:26 - INFO - __main__ - ['contradiction']
06/03/2022 07:14:26 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:14:26 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:14:26 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:14:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:14:39 - INFO - __main__ - Starting training!
06/03/2022 07:14:43 - INFO - __main__ - Step 10 Global step 10 Train loss 25.099497 on epoch=1
06/03/2022 07:14:48 - INFO - __main__ - Step 20 Global step 20 Train loss 17.154640 on epoch=2
06/03/2022 07:14:54 - INFO - __main__ - Step 30 Global step 30 Train loss 10.802287 on epoch=3
06/03/2022 07:14:59 - INFO - __main__ - Step 40 Global step 40 Train loss 9.931071 on epoch=4
06/03/2022 07:15:04 - INFO - __main__ - Step 50 Global step 50 Train loss 10.255579 on epoch=6
06/03/2022 07:15:07 - INFO - __main__ - Global step 50 Train loss 14.648616 ACC 0.0 on epoch=6
06/03/2022 07:15:13 - INFO - __main__ - Step 60 Global step 60 Train loss 9.450377 on epoch=7
06/03/2022 07:15:18 - INFO - __main__ - Step 70 Global step 70 Train loss 8.821327 on epoch=8
06/03/2022 07:15:23 - INFO - __main__ - Step 80 Global step 80 Train loss 8.247540 on epoch=9
06/03/2022 07:15:28 - INFO - __main__ - Step 90 Global step 90 Train loss 8.540140 on epoch=11
06/03/2022 07:15:33 - INFO - __main__ - Step 100 Global step 100 Train loss 7.838397 on epoch=12
06/03/2022 07:15:36 - INFO - __main__ - Global step 100 Train loss 8.579556 ACC 0.0 on epoch=12
06/03/2022 07:15:41 - INFO - __main__ - Step 110 Global step 110 Train loss 7.254228 on epoch=13
06/03/2022 07:15:46 - INFO - __main__ - Step 120 Global step 120 Train loss 7.018319 on epoch=14
06/03/2022 07:15:51 - INFO - __main__ - Step 130 Global step 130 Train loss 7.111701 on epoch=16
06/03/2022 07:15:57 - INFO - __main__ - Step 140 Global step 140 Train loss 5.600738 on epoch=17
06/03/2022 07:16:02 - INFO - __main__ - Step 150 Global step 150 Train loss 4.916512 on epoch=18
06/03/2022 07:16:05 - INFO - __main__ - Global step 150 Train loss 6.380300 ACC 0.0 on epoch=18
06/03/2022 07:16:10 - INFO - __main__ - Step 160 Global step 160 Train loss 4.566308 on epoch=19
06/03/2022 07:16:15 - INFO - __main__ - Step 170 Global step 170 Train loss 3.954665 on epoch=21
06/03/2022 07:16:20 - INFO - __main__ - Step 180 Global step 180 Train loss 3.091656 on epoch=22
06/03/2022 07:16:25 - INFO - __main__ - Step 190 Global step 190 Train loss 2.300255 on epoch=23
06/03/2022 07:16:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.837492 on epoch=24
06/03/2022 07:17:17 - INFO - __main__ - Global step 200 Train loss 3.150075 ACC 0.0 on epoch=24
06/03/2022 07:17:22 - INFO - __main__ - Step 210 Global step 210 Train loss 1.246421 on epoch=26
06/03/2022 07:17:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.442794 on epoch=27
06/03/2022 07:17:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.502820 on epoch=28
06/03/2022 07:17:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.307786 on epoch=29
06/03/2022 07:17:42 - INFO - __main__ - Step 250 Global step 250 Train loss 0.302848 on epoch=31
06/03/2022 07:17:44 - INFO - __main__ - Global step 250 Train loss 0.560534 ACC 0.47580645161290325 on epoch=31
06/03/2022 07:17:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.273925 on epoch=32
06/03/2022 07:17:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.282685 on epoch=33
06/03/2022 07:18:01 - INFO - __main__ - Step 280 Global step 280 Train loss 0.192334 on epoch=34
06/03/2022 07:18:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.238955 on epoch=36
06/03/2022 07:18:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.216318 on epoch=37
06/03/2022 07:18:13 - INFO - __main__ - Global step 300 Train loss 0.240843 ACC 0.7983870967741935 on epoch=37
06/03/2022 07:18:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.212994 on epoch=38
06/03/2022 07:18:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.168192 on epoch=39
06/03/2022 07:18:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.104782 on epoch=41
06/03/2022 07:18:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.093084 on epoch=42
06/03/2022 07:18:40 - INFO - __main__ - Step 350 Global step 350 Train loss 0.082446 on epoch=43
06/03/2022 07:18:43 - INFO - __main__ - Global step 350 Train loss 0.132300 ACC 0.6532258064516129 on epoch=43
06/03/2022 07:18:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.065808 on epoch=44
06/03/2022 07:18:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.093384 on epoch=46
06/03/2022 07:18:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.047640 on epoch=47
06/03/2022 07:19:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.032460 on epoch=48
06/03/2022 07:19:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.022222 on epoch=49
06/03/2022 07:19:11 - INFO - __main__ - Global step 400 Train loss 0.052303 ACC 0.6048387096774194 on epoch=49
06/03/2022 07:19:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.067154 on epoch=51
06/03/2022 07:19:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.024252 on epoch=52
06/03/2022 07:19:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.011294 on epoch=53
06/03/2022 07:19:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.005708 on epoch=54
06/03/2022 07:19:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.008117 on epoch=56
06/03/2022 07:19:39 - INFO - __main__ - Global step 450 Train loss 0.023305 ACC 0.6451612903225806 on epoch=56
06/03/2022 07:19:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.007843 on epoch=57
06/03/2022 07:19:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.003995 on epoch=58
06/03/2022 07:19:55 - INFO - __main__ - Step 480 Global step 480 Train loss 0.004587 on epoch=59
06/03/2022 07:20:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.003438 on epoch=61
06/03/2022 07:20:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.002113 on epoch=62
06/03/2022 07:20:08 - INFO - __main__ - Global step 500 Train loss 0.004395 ACC 0.7661290322580645 on epoch=62
06/03/2022 07:20:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.210963 on epoch=63
06/03/2022 07:20:18 - INFO - __main__ - Step 520 Global step 520 Train loss 0.003199 on epoch=64
06/03/2022 07:20:23 - INFO - __main__ - Step 530 Global step 530 Train loss 1.698853 on epoch=66
06/03/2022 07:20:28 - INFO - __main__ - Step 540 Global step 540 Train loss 2.471931 on epoch=67
06/03/2022 07:20:33 - INFO - __main__ - Step 550 Global step 550 Train loss 1.119758 on epoch=68
06/03/2022 07:20:36 - INFO - __main__ - Global step 550 Train loss 1.100941 ACC 0.47580645161290325 on epoch=68
06/03/2022 07:20:41 - INFO - __main__ - Step 560 Global step 560 Train loss 1.195081 on epoch=69
06/03/2022 07:20:46 - INFO - __main__ - Step 570 Global step 570 Train loss 1.524765 on epoch=71
06/03/2022 07:20:51 - INFO - __main__ - Step 580 Global step 580 Train loss 1.648808 on epoch=72
06/03/2022 07:20:57 - INFO - __main__ - Step 590 Global step 590 Train loss 1.612222 on epoch=73
06/03/2022 07:21:02 - INFO - __main__ - Step 600 Global step 600 Train loss 1.196219 on epoch=74
06/03/2022 07:21:05 - INFO - __main__ - Global step 600 Train loss 1.435419 ACC 0.8870967741935484 on epoch=74
06/03/2022 07:21:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.244099 on epoch=76
06/03/2022 07:21:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.001073 on epoch=77
06/03/2022 07:21:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.004188 on epoch=78
06/03/2022 07:21:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.007869 on epoch=79
06/03/2022 07:21:32 - INFO - __main__ - Step 650 Global step 650 Train loss 0.046367 on epoch=81
06/03/2022 07:21:34 - INFO - __main__ - Global step 650 Train loss 0.060719 ACC 0.7903225806451613 on epoch=81
06/03/2022 07:21:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.002796 on epoch=82
06/03/2022 07:21:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.002060 on epoch=83
06/03/2022 07:21:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000322 on epoch=84
06/03/2022 07:21:55 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000340 on epoch=86
06/03/2022 07:22:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000756 on epoch=87
06/03/2022 07:22:03 - INFO - __main__ - Global step 700 Train loss 0.001255 ACC 0.8629032258064516 on epoch=87
06/03/2022 07:22:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.001598 on epoch=88
06/03/2022 07:22:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000811 on epoch=89
06/03/2022 07:22:18 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000181 on epoch=91
06/03/2022 07:22:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000266 on epoch=92
06/03/2022 07:22:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000783 on epoch=93
06/03/2022 07:22:31 - INFO - __main__ - Global step 750 Train loss 0.000728 ACC 0.8709677419354839 on epoch=93
06/03/2022 07:22:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.025802 on epoch=94
06/03/2022 07:22:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.036290 on epoch=96
06/03/2022 07:22:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.001402 on epoch=97
06/03/2022 07:22:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000538 on epoch=98
06/03/2022 07:22:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.004673 on epoch=99
06/03/2022 07:23:00 - INFO - __main__ - Global step 800 Train loss 0.013741 ACC 0.8467741935483871 on epoch=99
06/03/2022 07:23:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000381 on epoch=101
06/03/2022 07:23:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.001082 on epoch=102
06/03/2022 07:23:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000228 on epoch=103
06/03/2022 07:23:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000230 on epoch=104
06/03/2022 07:23:26 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000161 on epoch=106
06/03/2022 07:23:28 - INFO - __main__ - Global step 850 Train loss 0.000416 ACC 0.8548387096774194 on epoch=106
06/03/2022 07:23:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000234 on epoch=107
06/03/2022 07:23:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000110 on epoch=108
06/03/2022 07:23:44 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000175 on epoch=109
06/03/2022 07:23:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000104 on epoch=111
06/03/2022 07:23:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000091 on epoch=112
06/03/2022 07:23:56 - INFO - __main__ - Global step 900 Train loss 0.000143 ACC 0.8467741935483871 on epoch=112
06/03/2022 07:24:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000100 on epoch=113
06/03/2022 07:24:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000073 on epoch=114
06/03/2022 07:24:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000329 on epoch=116
06/03/2022 07:24:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000172 on epoch=117
06/03/2022 07:24:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000099 on epoch=118
06/03/2022 07:24:25 - INFO - __main__ - Global step 950 Train loss 0.000154 ACC 0.8709677419354839 on epoch=118
06/03/2022 07:24:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000039 on epoch=119
06/03/2022 07:24:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000173 on epoch=121
06/03/2022 07:24:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000072 on epoch=122
06/03/2022 07:24:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000052 on epoch=123
06/03/2022 07:24:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000068 on epoch=124
06/03/2022 07:24:52 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:24:52 - INFO - __main__ - Printing 3 examples
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:24:52 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:24:52 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:24:52 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:24:52 - INFO - __main__ - Printing 3 examples
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:24:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:24:52 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:24:52 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:24:52 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:24:53 - INFO - __main__ - Global step 1000 Train loss 0.000081 ACC 0.8709677419354839 on epoch=124
06/03/2022 07:24:53 - INFO - __main__ - save last model!
06/03/2022 07:25:01 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:25:01 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:25:01 - INFO - __main__ - Printing 3 examples
06/03/2022 07:25:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:25:01 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:25:01 - INFO - __main__ - ['neutral']
06/03/2022 07:25:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:25:01 - INFO - __main__ - ['entailment']
06/03/2022 07:25:01 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:25:01 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:25:01 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:25:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_100_0.0002_8_predictions.txt
06/03/2022 07:25:03 - INFO - __main__ - ACC on test data: 0.7857
06/03/2022 07:25:03 - INFO - __main__ - prefix=superglue-cb_128_100, lr=0.0002, bsz=8, dev_performance=0.8870967741935484, test_performance=0.7857142857142857
06/03/2022 07:25:03 - INFO - __main__ - Running ... prefix=superglue-cb_128_100, lr=0.0001, bsz=8 ...
06/03/2022 07:25:04 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:25:04 - INFO - __main__ - Printing 3 examples
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: If there are spirits at work at the time, they come only from yourself, not from the fume of the incense. Why should spirits aid living beings? What arrogance is it that drives people to believe they can have power over them? [SEP] hypothesis: people can have power over spirits
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: ``Ely,'' I said (that was her name and the first time I 'd ever used it), ``I want to be free.'' She looked stunned. I don't think she 'd considered this. [SEP] hypothesis: Ely had considered him wanting to be free
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: B: you know, sometimes I would go over, but you know, it wouldn't hit me in a big way because I knew that, uh, I would have it covered in that respect. A: Right.  Right. That's good. I don't think we've gone that far, to pay it you know, in advance before we spend it, [SEP] hypothesis: they've gone that far
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:25:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:25:04 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:25:04 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:25:04 - INFO - __main__ - Printing 3 examples
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, uh, I don't understand, I guess, why the schools seem to have such a high dropout rate in the big cities. B: Uh, well, I don't pretend to understand that either. Uh, but I'm not quite sure that it's the kind of thing that ought to be blamed on the schools. But then, again, I'm not quite sure where the blame ought to be put. Uh, because the dropout rate is, in those areas, w-, it's high in those areas where also there's poverty and crime. And they all seem to go together. And it seems like if you could eliminate one of the parts of that circle, where you have the dropout rate and crime and, you know, general poverty kind of conditions, that things ought to get better. So, uh, the other two a-, they're all three social issues and could be addressed by the government in any ways. And clearly, to me, is a kind of government thing to fix but it's just like, I don't expect them to know which part is best to fix just like I don't know. it's a complicated issue. I still don't think I would blame it directly on the school. [SEP] hypothesis: he would blame it directly on the school
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: He pulled occasionally, his arms tiring. Conker slowed a little, but the branches were coming too fast, he had to lean right forward and couldn't use his hands. He remembered the gate at the end of the track had time to hope it was open because he didn't think Conker could jump it. [SEP] hypothesis: Conker could jump the gate
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. Well, that's the guy that counts. A: Yes. But, maybe we'll get your guy. B: Oh, I don't think Jim Kelly is about to be swayed away from the Bills any time. [SEP] hypothesis: Jim Kelly is about to be swayed away from the Bills any time
06/03/2022 07:25:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:25:04 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:25:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:25:04 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:25:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:25:05 - INFO - __main__ - Starting training!
06/03/2022 07:25:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:25:16 - INFO - __main__ - Starting training!
06/03/2022 07:25:20 - INFO - __main__ - Step 10 Global step 10 Train loss 24.231777 on epoch=1
06/03/2022 07:25:25 - INFO - __main__ - Step 20 Global step 20 Train loss 21.126701 on epoch=2
06/03/2022 07:25:30 - INFO - __main__ - Step 30 Global step 30 Train loss 18.193405 on epoch=3
06/03/2022 07:25:35 - INFO - __main__ - Step 40 Global step 40 Train loss 13.651970 on epoch=4
06/03/2022 07:25:40 - INFO - __main__ - Step 50 Global step 50 Train loss 13.017268 on epoch=6
06/03/2022 07:26:22 - INFO - __main__ - Global step 50 Train loss 18.044226 ACC 0.24193548387096775 on epoch=6
06/03/2022 07:26:27 - INFO - __main__ - Step 60 Global step 60 Train loss 12.147565 on epoch=7
06/03/2022 07:26:32 - INFO - __main__ - Step 70 Global step 70 Train loss 10.959861 on epoch=8
06/03/2022 07:26:38 - INFO - __main__ - Step 80 Global step 80 Train loss 10.403440 on epoch=9
06/03/2022 07:26:43 - INFO - __main__ - Step 90 Global step 90 Train loss 10.391004 on epoch=11
06/03/2022 07:26:48 - INFO - __main__ - Step 100 Global step 100 Train loss 10.210409 on epoch=12
06/03/2022 07:27:10 - INFO - __main__ - Global step 100 Train loss 10.822454 ACC 0.1935483870967742 on epoch=12
06/03/2022 07:27:15 - INFO - __main__ - Step 110 Global step 110 Train loss 9.320464 on epoch=13
06/03/2022 07:27:20 - INFO - __main__ - Step 120 Global step 120 Train loss 9.601412 on epoch=14
06/03/2022 07:27:25 - INFO - __main__ - Step 130 Global step 130 Train loss 9.923457 on epoch=16
06/03/2022 07:27:30 - INFO - __main__ - Step 140 Global step 140 Train loss 9.228578 on epoch=17
06/03/2022 07:27:35 - INFO - __main__ - Step 150 Global step 150 Train loss 8.714931 on epoch=18
06/03/2022 07:27:50 - INFO - __main__ - Global step 150 Train loss 9.357769 ACC 0.08064516129032258 on epoch=18
06/03/2022 07:27:55 - INFO - __main__ - Step 160 Global step 160 Train loss 8.472965 on epoch=19
06/03/2022 07:28:01 - INFO - __main__ - Step 170 Global step 170 Train loss 8.703803 on epoch=21
06/03/2022 07:28:06 - INFO - __main__ - Step 180 Global step 180 Train loss 8.502951 on epoch=22
06/03/2022 07:28:11 - INFO - __main__ - Step 190 Global step 190 Train loss 8.187166 on epoch=23
06/03/2022 07:28:16 - INFO - __main__ - Step 200 Global step 200 Train loss 7.838297 on epoch=24
06/03/2022 07:28:22 - INFO - __main__ - Global step 200 Train loss 8.341037 ACC 0.03225806451612903 on epoch=24
06/03/2022 07:28:27 - INFO - __main__ - Step 210 Global step 210 Train loss 8.150396 on epoch=26
06/03/2022 07:28:32 - INFO - __main__ - Step 220 Global step 220 Train loss 7.555964 on epoch=27
06/03/2022 07:28:37 - INFO - __main__ - Step 230 Global step 230 Train loss 6.765883 on epoch=28
06/03/2022 07:28:42 - INFO - __main__ - Step 240 Global step 240 Train loss 6.932023 on epoch=29
06/03/2022 07:28:47 - INFO - __main__ - Step 250 Global step 250 Train loss 7.246702 on epoch=31
06/03/2022 07:28:52 - INFO - __main__ - Global step 250 Train loss 7.330194 ACC 0.008064516129032258 on epoch=31
06/03/2022 07:28:57 - INFO - __main__ - Step 260 Global step 260 Train loss 6.470877 on epoch=32
06/03/2022 07:29:02 - INFO - __main__ - Step 270 Global step 270 Train loss 6.702569 on epoch=33
06/03/2022 07:29:07 - INFO - __main__ - Step 280 Global step 280 Train loss 6.387399 on epoch=34
06/03/2022 07:29:12 - INFO - __main__ - Step 290 Global step 290 Train loss 6.162778 on epoch=36
06/03/2022 07:29:17 - INFO - __main__ - Step 300 Global step 300 Train loss 5.826097 on epoch=37
06/03/2022 07:29:21 - INFO - __main__ - Global step 300 Train loss 6.309944 ACC 0.008064516129032258 on epoch=37
06/03/2022 07:29:26 - INFO - __main__ - Step 310 Global step 310 Train loss 5.091109 on epoch=38
06/03/2022 07:29:31 - INFO - __main__ - Step 320 Global step 320 Train loss 5.258821 on epoch=39
06/03/2022 07:29:36 - INFO - __main__ - Step 330 Global step 330 Train loss 5.181485 on epoch=41
06/03/2022 07:29:41 - INFO - __main__ - Step 340 Global step 340 Train loss 4.654605 on epoch=42
06/03/2022 07:29:46 - INFO - __main__ - Step 350 Global step 350 Train loss 4.195127 on epoch=43
06/03/2022 07:29:50 - INFO - __main__ - Global step 350 Train loss 4.876229 ACC 0.016129032258064516 on epoch=43
06/03/2022 07:29:55 - INFO - __main__ - Step 360 Global step 360 Train loss 3.523424 on epoch=44
06/03/2022 07:30:00 - INFO - __main__ - Step 370 Global step 370 Train loss 3.835982 on epoch=46
06/03/2022 07:30:05 - INFO - __main__ - Step 380 Global step 380 Train loss 3.250716 on epoch=47
06/03/2022 07:30:10 - INFO - __main__ - Step 390 Global step 390 Train loss 3.027138 on epoch=48
06/03/2022 07:30:15 - INFO - __main__ - Step 400 Global step 400 Train loss 2.645780 on epoch=49
06/03/2022 07:30:18 - INFO - __main__ - Global step 400 Train loss 3.256608 ACC 0.04838709677419355 on epoch=49
06/03/2022 07:30:23 - INFO - __main__ - Step 410 Global step 410 Train loss 3.092836 on epoch=51
06/03/2022 07:30:28 - INFO - __main__ - Step 420 Global step 420 Train loss 2.438065 on epoch=52
06/03/2022 07:30:33 - INFO - __main__ - Step 430 Global step 430 Train loss 2.142162 on epoch=53
06/03/2022 07:30:38 - INFO - __main__ - Step 440 Global step 440 Train loss 2.410382 on epoch=54
06/03/2022 07:30:43 - INFO - __main__ - Step 450 Global step 450 Train loss 1.753350 on epoch=56
06/03/2022 07:30:46 - INFO - __main__ - Global step 450 Train loss 2.367359 ACC 0.47580645161290325 on epoch=56
06/03/2022 07:30:52 - INFO - __main__ - Step 460 Global step 460 Train loss 2.056157 on epoch=57
06/03/2022 07:30:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.858159 on epoch=58
06/03/2022 07:31:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.959977 on epoch=59
06/03/2022 07:31:07 - INFO - __main__ - Step 490 Global step 490 Train loss 2.244118 on epoch=61
06/03/2022 07:31:12 - INFO - __main__ - Step 500 Global step 500 Train loss 1.956058 on epoch=62
06/03/2022 07:31:14 - INFO - __main__ - Global step 500 Train loss 2.014894 ACC 0.47580645161290325 on epoch=62
06/03/2022 07:31:19 - INFO - __main__ - Step 510 Global step 510 Train loss 2.008865 on epoch=63
06/03/2022 07:31:24 - INFO - __main__ - Step 520 Global step 520 Train loss 1.967590 on epoch=64
06/03/2022 07:31:29 - INFO - __main__ - Step 530 Global step 530 Train loss 1.630532 on epoch=66
06/03/2022 07:31:34 - INFO - __main__ - Step 540 Global step 540 Train loss 1.909821 on epoch=67
06/03/2022 07:31:39 - INFO - __main__ - Step 550 Global step 550 Train loss 2.551348 on epoch=68
06/03/2022 07:31:41 - INFO - __main__ - Global step 550 Train loss 2.013631 ACC 0.47580645161290325 on epoch=68
06/03/2022 07:31:46 - INFO - __main__ - Step 560 Global step 560 Train loss 1.940137 on epoch=69
06/03/2022 07:31:51 - INFO - __main__ - Step 570 Global step 570 Train loss 1.502983 on epoch=71
06/03/2022 07:31:57 - INFO - __main__ - Step 580 Global step 580 Train loss 1.710896 on epoch=72
06/03/2022 07:32:02 - INFO - __main__ - Step 590 Global step 590 Train loss 1.852151 on epoch=73
06/03/2022 07:32:07 - INFO - __main__ - Step 600 Global step 600 Train loss 1.899075 on epoch=74
06/03/2022 07:32:09 - INFO - __main__ - Global step 600 Train loss 1.781048 ACC 0.47580645161290325 on epoch=74
06/03/2022 07:32:14 - INFO - __main__ - Step 610 Global step 610 Train loss 1.555207 on epoch=76
06/03/2022 07:32:19 - INFO - __main__ - Step 620 Global step 620 Train loss 1.502452 on epoch=77
06/03/2022 07:32:24 - INFO - __main__ - Step 630 Global step 630 Train loss 1.450462 on epoch=78
06/03/2022 07:32:29 - INFO - __main__ - Step 640 Global step 640 Train loss 1.567219 on epoch=79
06/03/2022 07:32:34 - INFO - __main__ - Step 650 Global step 650 Train loss 1.152763 on epoch=81
06/03/2022 07:32:36 - INFO - __main__ - Global step 650 Train loss 1.445621 ACC 0.47580645161290325 on epoch=81
06/03/2022 07:32:41 - INFO - __main__ - Step 660 Global step 660 Train loss 1.387184 on epoch=82
06/03/2022 07:32:46 - INFO - __main__ - Step 670 Global step 670 Train loss 1.532940 on epoch=83
06/03/2022 07:32:51 - INFO - __main__ - Step 680 Global step 680 Train loss 1.518395 on epoch=84
06/03/2022 07:32:56 - INFO - __main__ - Step 690 Global step 690 Train loss 1.428994 on epoch=86
06/03/2022 07:33:01 - INFO - __main__ - Step 700 Global step 700 Train loss 1.399635 on epoch=87
06/03/2022 07:33:03 - INFO - __main__ - Global step 700 Train loss 1.453430 ACC 0.47580645161290325 on epoch=87
06/03/2022 07:33:08 - INFO - __main__ - Step 710 Global step 710 Train loss 1.350941 on epoch=88
06/03/2022 07:33:13 - INFO - __main__ - Step 720 Global step 720 Train loss 1.128482 on epoch=89
06/03/2022 07:33:18 - INFO - __main__ - Step 730 Global step 730 Train loss 1.576894 on epoch=91
06/03/2022 07:33:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.989414 on epoch=92
06/03/2022 07:33:29 - INFO - __main__ - Step 750 Global step 750 Train loss 1.460327 on epoch=93
06/03/2022 07:33:31 - INFO - __main__ - Global step 750 Train loss 1.301211 ACC 0.47580645161290325 on epoch=93
06/03/2022 07:33:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.996552 on epoch=94
06/03/2022 07:33:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.228765 on epoch=96
06/03/2022 07:33:46 - INFO - __main__ - Step 780 Global step 780 Train loss 1.065396 on epoch=97
06/03/2022 07:33:51 - INFO - __main__ - Step 790 Global step 790 Train loss 1.134739 on epoch=98
06/03/2022 07:33:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.954007 on epoch=99
06/03/2022 07:33:59 - INFO - __main__ - Global step 800 Train loss 1.075892 ACC 0.5887096774193549 on epoch=99
06/03/2022 07:34:05 - INFO - __main__ - Step 810 Global step 810 Train loss 1.140284 on epoch=101
06/03/2022 07:34:10 - INFO - __main__ - Step 820 Global step 820 Train loss 1.254648 on epoch=102
06/03/2022 07:34:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.931798 on epoch=103
06/03/2022 07:34:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.569727 on epoch=104
06/03/2022 07:34:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.969285 on epoch=106
06/03/2022 07:34:28 - INFO - __main__ - Global step 850 Train loss 0.973148 ACC 0.5161290322580645 on epoch=106
06/03/2022 07:34:33 - INFO - __main__ - Step 860 Global step 860 Train loss 1.049351 on epoch=107
06/03/2022 07:34:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.876796 on epoch=108
06/03/2022 07:34:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.833582 on epoch=109
06/03/2022 07:34:48 - INFO - __main__ - Step 890 Global step 890 Train loss 0.769236 on epoch=111
06/03/2022 07:34:53 - INFO - __main__ - Step 900 Global step 900 Train loss 1.090364 on epoch=112
06/03/2022 07:34:55 - INFO - __main__ - Global step 900 Train loss 0.923866 ACC 0.5967741935483871 on epoch=112
06/03/2022 07:35:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.839266 on epoch=113
06/03/2022 07:35:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.664420 on epoch=114
06/03/2022 07:35:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.695099 on epoch=116
06/03/2022 07:35:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.220005 on epoch=117
06/03/2022 07:35:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.308894 on epoch=118
06/03/2022 07:35:24 - INFO - __main__ - Global step 950 Train loss 0.545537 ACC 0.8225806451612904 on epoch=118
06/03/2022 07:35:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.247613 on epoch=119
06/03/2022 07:35:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.139376 on epoch=121
06/03/2022 07:35:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.143844 on epoch=122
06/03/2022 07:35:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.132136 on epoch=123
06/03/2022 07:35:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.080202 on epoch=124
06/03/2022 07:35:52 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:35:52 - INFO - __main__ - Printing 3 examples
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:35:52 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:35:52 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:35:52 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:35:52 - INFO - __main__ - Printing 3 examples
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:35:52 - INFO - __main__ - ['contradiction']
06/03/2022 07:35:52 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:35:52 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:35:52 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:35:53 - INFO - __main__ - Global step 1000 Train loss 0.148634 ACC 0.8306451612903226 on epoch=124
06/03/2022 07:35:54 - INFO - __main__ - save last model!
06/03/2022 07:36:01 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:36:01 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:36:01 - INFO - __main__ - Printing 3 examples
06/03/2022 07:36:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:36:01 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:36:01 - INFO - __main__ - ['neutral']
06/03/2022 07:36:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:36:01 - INFO - __main__ - ['entailment']
06/03/2022 07:36:01 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:36:01 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:36:02 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:36:03 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_100_0.0001_8_predictions.txt
06/03/2022 07:36:03 - INFO - __main__ - ACC on test data: 0.8036
06/03/2022 07:36:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:36:03 - INFO - __main__ - Starting training!
06/03/2022 07:36:03 - INFO - __main__ - prefix=superglue-cb_128_100, lr=0.0001, bsz=8, dev_performance=0.8306451612903226, test_performance=0.8035714285714286
06/03/2022 07:36:03 - INFO - __main__ - Running ... prefix=superglue-cb_128_13, lr=0.0005, bsz=8 ...
06/03/2022 07:36:04 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:36:04 - INFO - __main__ - Printing 3 examples
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:36:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:36:04 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:36:04 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:36:04 - INFO - __main__ - Printing 3 examples
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:36:04 - INFO - __main__ - ['contradiction']
06/03/2022 07:36:04 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:36:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:36:05 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:36:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:36:16 - INFO - __main__ - Starting training!
06/03/2022 07:36:20 - INFO - __main__ - Step 10 Global step 10 Train loss 23.926846 on epoch=1
06/03/2022 07:36:25 - INFO - __main__ - Step 20 Global step 20 Train loss 15.233805 on epoch=2
06/03/2022 07:36:30 - INFO - __main__ - Step 30 Global step 30 Train loss 11.587193 on epoch=3
06/03/2022 07:36:35 - INFO - __main__ - Step 40 Global step 40 Train loss 8.651285 on epoch=4
06/03/2022 07:36:40 - INFO - __main__ - Step 50 Global step 50 Train loss 8.623663 on epoch=6
06/03/2022 07:36:43 - INFO - __main__ - Global step 50 Train loss 13.604558 ACC 0.0 on epoch=6
06/03/2022 07:36:49 - INFO - __main__ - Step 60 Global step 60 Train loss 7.193881 on epoch=7
06/03/2022 07:36:54 - INFO - __main__ - Step 70 Global step 70 Train loss 6.485606 on epoch=8
06/03/2022 07:36:59 - INFO - __main__ - Step 80 Global step 80 Train loss 4.241903 on epoch=9
06/03/2022 07:37:04 - INFO - __main__ - Step 90 Global step 90 Train loss 3.684474 on epoch=11
06/03/2022 07:37:09 - INFO - __main__ - Step 100 Global step 100 Train loss 2.457803 on epoch=12
06/03/2022 07:37:11 - INFO - __main__ - Global step 100 Train loss 4.812734 ACC 0.47580645161290325 on epoch=12
06/03/2022 07:37:17 - INFO - __main__ - Step 110 Global step 110 Train loss 2.336235 on epoch=13
06/03/2022 07:37:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.863811 on epoch=14
06/03/2022 07:37:28 - INFO - __main__ - Step 130 Global step 130 Train loss 1.880794 on epoch=16
06/03/2022 07:37:32 - INFO - __main__ - Step 140 Global step 140 Train loss 1.931259 on epoch=17
06/03/2022 07:37:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.990599 on epoch=18
06/03/2022 07:37:40 - INFO - __main__ - Global step 150 Train loss 2.000539 ACC 0.47580645161290325 on epoch=18
06/03/2022 07:37:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.931195 on epoch=19
06/03/2022 07:37:50 - INFO - __main__ - Step 170 Global step 170 Train loss 1.558583 on epoch=21
06/03/2022 07:37:55 - INFO - __main__ - Step 180 Global step 180 Train loss 1.157432 on epoch=22
06/03/2022 07:38:00 - INFO - __main__ - Step 190 Global step 190 Train loss 1.528635 on epoch=23
06/03/2022 07:38:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.267485 on epoch=24
06/03/2022 07:38:07 - INFO - __main__ - Global step 200 Train loss 1.488666 ACC 0.47580645161290325 on epoch=24
06/03/2022 07:38:12 - INFO - __main__ - Step 210 Global step 210 Train loss 1.107496 on epoch=26
06/03/2022 07:38:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.943231 on epoch=27
06/03/2022 07:38:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.278533 on epoch=28
06/03/2022 07:38:28 - INFO - __main__ - Step 240 Global step 240 Train loss 1.016329 on epoch=29
06/03/2022 07:38:33 - INFO - __main__ - Step 250 Global step 250 Train loss 1.002374 on epoch=31
06/03/2022 07:38:36 - INFO - __main__ - Global step 250 Train loss 1.069593 ACC 0.47580645161290325 on epoch=31
06/03/2022 07:38:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.798735 on epoch=32
06/03/2022 07:38:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.948920 on epoch=33
06/03/2022 07:38:51 - INFO - __main__ - Step 280 Global step 280 Train loss 0.834697 on epoch=34
06/03/2022 07:38:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.707511 on epoch=36
06/03/2022 07:39:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.758673 on epoch=37
06/03/2022 07:39:04 - INFO - __main__ - Global step 300 Train loss 0.809707 ACC 0.47580645161290325 on epoch=37
06/03/2022 07:39:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.576410 on epoch=38
06/03/2022 07:39:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.658437 on epoch=39
06/03/2022 07:39:19 - INFO - __main__ - Step 330 Global step 330 Train loss 0.689244 on epoch=41
06/03/2022 07:39:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.971732 on epoch=42
06/03/2022 07:39:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.724896 on epoch=43
06/03/2022 07:39:31 - INFO - __main__ - Global step 350 Train loss 0.724144 ACC 0.47580645161290325 on epoch=43
06/03/2022 07:39:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.560073 on epoch=44
06/03/2022 07:39:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.407999 on epoch=46
06/03/2022 07:39:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.425151 on epoch=47
06/03/2022 07:39:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.527451 on epoch=48
06/03/2022 07:39:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.538987 on epoch=49
06/03/2022 07:39:59 - INFO - __main__ - Global step 400 Train loss 0.491932 ACC 0.4596774193548387 on epoch=49
06/03/2022 07:40:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.491646 on epoch=51
06/03/2022 07:40:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.434370 on epoch=52
06/03/2022 07:40:14 - INFO - __main__ - Step 430 Global step 430 Train loss 0.554716 on epoch=53
06/03/2022 07:40:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.338324 on epoch=54
06/03/2022 07:40:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.498151 on epoch=56
06/03/2022 07:40:27 - INFO - __main__ - Global step 450 Train loss 0.463441 ACC 0.47580645161290325 on epoch=56
06/03/2022 07:40:32 - INFO - __main__ - Step 460 Global step 460 Train loss 0.469834 on epoch=57
06/03/2022 07:40:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.467623 on epoch=58
06/03/2022 07:40:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.402267 on epoch=59
06/03/2022 07:40:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.471615 on epoch=61
06/03/2022 07:40:52 - INFO - __main__ - Step 500 Global step 500 Train loss 0.466272 on epoch=62
06/03/2022 07:40:54 - INFO - __main__ - Global step 500 Train loss 0.455522 ACC 0.47580645161290325 on epoch=62
06/03/2022 07:40:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.390004 on epoch=63
06/03/2022 07:41:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.423174 on epoch=64
06/03/2022 07:41:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.355080 on epoch=66
06/03/2022 07:41:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.396787 on epoch=67
06/03/2022 07:41:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.348692 on epoch=68
06/03/2022 07:41:22 - INFO - __main__ - Global step 550 Train loss 0.382748 ACC 0.4596774193548387 on epoch=68
06/03/2022 07:41:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.398349 on epoch=69
06/03/2022 07:41:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.595263 on epoch=71
06/03/2022 07:41:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.363220 on epoch=72
06/03/2022 07:41:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.444216 on epoch=73
06/03/2022 07:41:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.439266 on epoch=74
06/03/2022 07:41:50 - INFO - __main__ - Global step 600 Train loss 0.448063 ACC 0.4838709677419355 on epoch=74
06/03/2022 07:41:56 - INFO - __main__ - Step 610 Global step 610 Train loss 0.351630 on epoch=76
06/03/2022 07:42:01 - INFO - __main__ - Step 620 Global step 620 Train loss 0.413367 on epoch=77
06/03/2022 07:42:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.346496 on epoch=78
06/03/2022 07:42:11 - INFO - __main__ - Step 640 Global step 640 Train loss 0.354793 on epoch=79
06/03/2022 07:42:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.338844 on epoch=81
06/03/2022 07:42:18 - INFO - __main__ - Global step 650 Train loss 0.361026 ACC 0.5161290322580645 on epoch=81
06/03/2022 07:42:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.395712 on epoch=82
06/03/2022 07:42:30 - INFO - __main__ - Step 670 Global step 670 Train loss 1.145103 on epoch=83
06/03/2022 07:42:35 - INFO - __main__ - Step 680 Global step 680 Train loss 0.378608 on epoch=84
06/03/2022 07:42:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.317750 on epoch=86
06/03/2022 07:42:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.365249 on epoch=87
06/03/2022 07:42:47 - INFO - __main__ - Global step 700 Train loss 0.520484 ACC 0.4596774193548387 on epoch=87
06/03/2022 07:42:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.302365 on epoch=88
06/03/2022 07:42:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.356034 on epoch=89
06/03/2022 07:43:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.303600 on epoch=91
06/03/2022 07:43:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.300790 on epoch=92
06/03/2022 07:43:12 - INFO - __main__ - Step 750 Global step 750 Train loss 0.335482 on epoch=93
06/03/2022 07:43:15 - INFO - __main__ - Global step 750 Train loss 0.319654 ACC 0.4596774193548387 on epoch=93
06/03/2022 07:43:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.302406 on epoch=94
06/03/2022 07:43:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.280773 on epoch=96
06/03/2022 07:43:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.281075 on epoch=97
06/03/2022 07:43:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.306032 on epoch=98
06/03/2022 07:43:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.279824 on epoch=99
06/03/2022 07:43:43 - INFO - __main__ - Global step 800 Train loss 0.290022 ACC 0.4838709677419355 on epoch=99
06/03/2022 07:43:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.268841 on epoch=101
06/03/2022 07:43:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.303251 on epoch=102
06/03/2022 07:43:58 - INFO - __main__ - Step 830 Global step 830 Train loss 0.327642 on epoch=103
06/03/2022 07:44:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.239833 on epoch=104
06/03/2022 07:44:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.279774 on epoch=106
06/03/2022 07:44:10 - INFO - __main__ - Global step 850 Train loss 0.283868 ACC 0.49193548387096775 on epoch=106
06/03/2022 07:44:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.309138 on epoch=107
06/03/2022 07:44:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.308186 on epoch=108
06/03/2022 07:44:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.287848 on epoch=109
06/03/2022 07:44:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.252000 on epoch=111
06/03/2022 07:44:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.277687 on epoch=112
06/03/2022 07:44:38 - INFO - __main__ - Global step 900 Train loss 0.286972 ACC 0.5887096774193549 on epoch=112
06/03/2022 07:44:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.303827 on epoch=113
06/03/2022 07:44:49 - INFO - __main__ - Step 920 Global step 920 Train loss 0.272963 on epoch=114
06/03/2022 07:44:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.300842 on epoch=116
06/03/2022 07:44:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.306279 on epoch=117
06/03/2022 07:45:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.297525 on epoch=118
06/03/2022 07:45:07 - INFO - __main__ - Global step 950 Train loss 0.296287 ACC 0.43548387096774194 on epoch=118
06/03/2022 07:45:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.243128 on epoch=119
06/03/2022 07:45:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.263061 on epoch=121
06/03/2022 07:45:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.296369 on epoch=122
06/03/2022 07:45:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.320856 on epoch=123
06/03/2022 07:45:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.292148 on epoch=124
06/03/2022 07:45:34 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:45:34 - INFO - __main__ - Printing 3 examples
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:45:34 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:45:34 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:45:34 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:45:34 - INFO - __main__ - Printing 3 examples
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:45:34 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:34 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:45:34 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:45:34 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:45:35 - INFO - __main__ - Global step 1000 Train loss 0.283112 ACC 0.5725806451612904 on epoch=124
06/03/2022 07:45:35 - INFO - __main__ - save last model!
06/03/2022 07:45:42 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:45:42 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:45:42 - INFO - __main__ - Printing 3 examples
06/03/2022 07:45:42 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:45:42 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:42 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:45:42 - INFO - __main__ - ['neutral']
06/03/2022 07:45:42 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:45:42 - INFO - __main__ - ['entailment']
06/03/2022 07:45:42 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:45:42 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:45:43 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:45:44 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_13_0.0005_8_predictions.txt
06/03/2022 07:45:44 - INFO - __main__ - ACC on test data: 0.4286
06/03/2022 07:45:44 - INFO - __main__ - prefix=superglue-cb_128_13, lr=0.0005, bsz=8, dev_performance=0.5887096774193549, test_performance=0.42857142857142855
06/03/2022 07:45:44 - INFO - __main__ - Running ... prefix=superglue-cb_128_13, lr=0.0003, bsz=8 ...
06/03/2022 07:45:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:45:44 - INFO - __main__ - Starting training!
06/03/2022 07:45:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:45:45 - INFO - __main__ - Printing 3 examples
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:45:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:45:45 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:45:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:45:45 - INFO - __main__ - Printing 3 examples
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:45:45 - INFO - __main__ - ['contradiction']
06/03/2022 07:45:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:45:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:45:45 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:45:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:45:58 - INFO - __main__ - Starting training!
06/03/2022 07:46:02 - INFO - __main__ - Step 10 Global step 10 Train loss 24.083292 on epoch=1
06/03/2022 07:46:07 - INFO - __main__ - Step 20 Global step 20 Train loss 17.209011 on epoch=2
06/03/2022 07:46:12 - INFO - __main__ - Step 30 Global step 30 Train loss 11.657287 on epoch=3
06/03/2022 07:46:17 - INFO - __main__ - Step 40 Global step 40 Train loss 10.573317 on epoch=4
06/03/2022 07:46:22 - INFO - __main__ - Step 50 Global step 50 Train loss 10.281352 on epoch=6
06/03/2022 07:46:28 - INFO - __main__ - Global step 50 Train loss 14.760852 ACC 0.07258064516129033 on epoch=6
06/03/2022 07:46:34 - INFO - __main__ - Step 60 Global step 60 Train loss 8.707924 on epoch=7
06/03/2022 07:46:39 - INFO - __main__ - Step 70 Global step 70 Train loss 8.914007 on epoch=8
06/03/2022 07:46:44 - INFO - __main__ - Step 80 Global step 80 Train loss 7.347797 on epoch=9
06/03/2022 07:46:49 - INFO - __main__ - Step 90 Global step 90 Train loss 7.073831 on epoch=11
06/03/2022 07:46:54 - INFO - __main__ - Step 100 Global step 100 Train loss 6.345932 on epoch=12
06/03/2022 07:46:58 - INFO - __main__ - Global step 100 Train loss 7.677898 ACC 0.024193548387096774 on epoch=12
06/03/2022 07:47:03 - INFO - __main__ - Step 110 Global step 110 Train loss 5.402643 on epoch=13
06/03/2022 07:47:08 - INFO - __main__ - Step 120 Global step 120 Train loss 4.352673 on epoch=14
06/03/2022 07:47:13 - INFO - __main__ - Step 130 Global step 130 Train loss 3.185335 on epoch=16
06/03/2022 07:47:18 - INFO - __main__ - Step 140 Global step 140 Train loss 2.132898 on epoch=17
06/03/2022 07:47:23 - INFO - __main__ - Step 150 Global step 150 Train loss 2.325430 on epoch=18
06/03/2022 07:47:26 - INFO - __main__ - Global step 150 Train loss 3.479796 ACC 0.18548387096774194 on epoch=18
06/03/2022 07:47:31 - INFO - __main__ - Step 160 Global step 160 Train loss 1.528546 on epoch=19
06/03/2022 07:47:36 - INFO - __main__ - Step 170 Global step 170 Train loss 0.570195 on epoch=21
06/03/2022 07:47:42 - INFO - __main__ - Step 180 Global step 180 Train loss 1.935539 on epoch=22
06/03/2022 07:47:47 - INFO - __main__ - Step 190 Global step 190 Train loss 2.266387 on epoch=23
06/03/2022 07:47:52 - INFO - __main__ - Step 200 Global step 200 Train loss 2.180594 on epoch=24
06/03/2022 07:47:54 - INFO - __main__ - Global step 200 Train loss 1.696252 ACC 0.47580645161290325 on epoch=24
06/03/2022 07:48:00 - INFO - __main__ - Step 210 Global step 210 Train loss 2.244689 on epoch=26
06/03/2022 07:48:05 - INFO - __main__ - Step 220 Global step 220 Train loss 2.167370 on epoch=27
06/03/2022 07:48:10 - INFO - __main__ - Step 230 Global step 230 Train loss 1.904533 on epoch=28
06/03/2022 07:48:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.956015 on epoch=29
06/03/2022 07:48:20 - INFO - __main__ - Step 250 Global step 250 Train loss 2.197417 on epoch=31
06/03/2022 07:48:22 - INFO - __main__ - Global step 250 Train loss 2.094005 ACC 0.47580645161290325 on epoch=31
06/03/2022 07:48:27 - INFO - __main__ - Step 260 Global step 260 Train loss 1.617746 on epoch=32
06/03/2022 07:48:33 - INFO - __main__ - Step 270 Global step 270 Train loss 2.007510 on epoch=33
06/03/2022 07:48:38 - INFO - __main__ - Step 280 Global step 280 Train loss 1.775291 on epoch=34
06/03/2022 07:48:43 - INFO - __main__ - Step 290 Global step 290 Train loss 1.541332 on epoch=36
06/03/2022 07:48:48 - INFO - __main__ - Step 300 Global step 300 Train loss 1.239647 on epoch=37
06/03/2022 07:48:50 - INFO - __main__ - Global step 300 Train loss 1.636305 ACC 0.47580645161290325 on epoch=37
06/03/2022 07:48:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.732623 on epoch=38
06/03/2022 07:49:00 - INFO - __main__ - Step 320 Global step 320 Train loss 1.237734 on epoch=39
06/03/2022 07:49:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.285923 on epoch=41
06/03/2022 07:49:10 - INFO - __main__ - Step 340 Global step 340 Train loss 1.540169 on epoch=42
06/03/2022 07:49:15 - INFO - __main__ - Step 350 Global step 350 Train loss 1.283251 on epoch=43
06/03/2022 07:49:18 - INFO - __main__ - Global step 350 Train loss 1.415940 ACC 0.47580645161290325 on epoch=43
06/03/2022 07:49:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.840503 on epoch=44
06/03/2022 07:49:28 - INFO - __main__ - Step 370 Global step 370 Train loss 1.180943 on epoch=46
06/03/2022 07:49:33 - INFO - __main__ - Step 380 Global step 380 Train loss 1.181323 on epoch=47
06/03/2022 07:49:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.913305 on epoch=48
06/03/2022 07:49:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.947831 on epoch=49
06/03/2022 07:49:45 - INFO - __main__ - Global step 400 Train loss 1.012781 ACC 0.47580645161290325 on epoch=49
06/03/2022 07:49:50 - INFO - __main__ - Step 410 Global step 410 Train loss 1.042215 on epoch=51
06/03/2022 07:49:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.846952 on epoch=52
06/03/2022 07:50:00 - INFO - __main__ - Step 430 Global step 430 Train loss 0.845408 on epoch=53
06/03/2022 07:50:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.741156 on epoch=54
06/03/2022 07:50:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.840275 on epoch=56
06/03/2022 07:50:13 - INFO - __main__ - Global step 450 Train loss 0.863201 ACC 0.47580645161290325 on epoch=56
06/03/2022 07:50:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.661407 on epoch=57
06/03/2022 07:50:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.694209 on epoch=58
06/03/2022 07:50:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.451779 on epoch=59
06/03/2022 07:50:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.621317 on epoch=61
06/03/2022 07:50:38 - INFO - __main__ - Step 500 Global step 500 Train loss 0.618343 on epoch=62
06/03/2022 07:50:40 - INFO - __main__ - Global step 500 Train loss 0.609411 ACC 0.47580645161290325 on epoch=62
06/03/2022 07:50:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.697389 on epoch=63
06/03/2022 07:50:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.585452 on epoch=64
06/03/2022 07:50:55 - INFO - __main__ - Step 530 Global step 530 Train loss 0.428348 on epoch=66
06/03/2022 07:51:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.421226 on epoch=67
06/03/2022 07:51:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.552252 on epoch=68
06/03/2022 07:51:08 - INFO - __main__ - Global step 550 Train loss 0.536933 ACC 0.6693548387096774 on epoch=68
06/03/2022 07:51:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.458390 on epoch=69
06/03/2022 07:51:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.372364 on epoch=71
06/03/2022 07:51:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.384969 on epoch=72
06/03/2022 07:51:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.437201 on epoch=73
06/03/2022 07:51:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.401270 on epoch=74
06/03/2022 07:51:36 - INFO - __main__ - Global step 600 Train loss 0.410839 ACC 0.7338709677419355 on epoch=74
06/03/2022 07:51:42 - INFO - __main__ - Step 610 Global step 610 Train loss 0.447646 on epoch=76
06/03/2022 07:51:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.376873 on epoch=77
06/03/2022 07:51:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.455108 on epoch=78
06/03/2022 07:51:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.375137 on epoch=79
06/03/2022 07:52:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.450788 on epoch=81
06/03/2022 07:52:04 - INFO - __main__ - Global step 650 Train loss 0.421111 ACC 0.8145161290322581 on epoch=81
06/03/2022 07:52:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.278247 on epoch=82
06/03/2022 07:52:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.273212 on epoch=83
06/03/2022 07:52:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.257276 on epoch=84
06/03/2022 07:52:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.193870 on epoch=86
06/03/2022 07:52:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.264204 on epoch=87
06/03/2022 07:52:32 - INFO - __main__ - Global step 700 Train loss 0.253362 ACC 0.45161290322580644 on epoch=87
06/03/2022 07:52:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.346325 on epoch=88
06/03/2022 07:52:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.165540 on epoch=89
06/03/2022 07:52:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.153795 on epoch=91
06/03/2022 07:52:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.133959 on epoch=92
06/03/2022 07:52:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.089230 on epoch=93
06/03/2022 07:53:00 - INFO - __main__ - Global step 750 Train loss 0.177770 ACC 0.782258064516129 on epoch=93
06/03/2022 07:53:05 - INFO - __main__ - Step 760 Global step 760 Train loss 0.121015 on epoch=94
06/03/2022 07:53:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.122227 on epoch=96
06/03/2022 07:53:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.212364 on epoch=97
06/03/2022 07:53:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.350014 on epoch=98
06/03/2022 07:53:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.192132 on epoch=99
06/03/2022 07:53:29 - INFO - __main__ - Global step 800 Train loss 0.199551 ACC 0.7903225806451613 on epoch=99
06/03/2022 07:53:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.211625 on epoch=101
06/03/2022 07:53:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.404286 on epoch=102
06/03/2022 07:53:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.327575 on epoch=103
06/03/2022 07:53:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.483224 on epoch=104
06/03/2022 07:53:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.559372 on epoch=106
06/03/2022 07:53:57 - INFO - __main__ - Global step 850 Train loss 0.397216 ACC 0.47580645161290325 on epoch=106
06/03/2022 07:54:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.312657 on epoch=107
06/03/2022 07:54:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.423507 on epoch=108
06/03/2022 07:54:12 - INFO - __main__ - Step 880 Global step 880 Train loss 0.351058 on epoch=109
06/03/2022 07:54:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.400489 on epoch=111
06/03/2022 07:54:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.407892 on epoch=112
06/03/2022 07:54:25 - INFO - __main__ - Global step 900 Train loss 0.379121 ACC 0.5645161290322581 on epoch=112
06/03/2022 07:54:30 - INFO - __main__ - Step 910 Global step 910 Train loss 0.402166 on epoch=113
06/03/2022 07:54:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.372658 on epoch=114
06/03/2022 07:54:41 - INFO - __main__ - Step 930 Global step 930 Train loss 0.328652 on epoch=116
06/03/2022 07:54:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.340203 on epoch=117
06/03/2022 07:54:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.404993 on epoch=118
06/03/2022 07:54:53 - INFO - __main__ - Global step 950 Train loss 0.369734 ACC 0.4838709677419355 on epoch=118
06/03/2022 07:54:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.460152 on epoch=119
06/03/2022 07:55:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.333905 on epoch=121
06/03/2022 07:55:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.313144 on epoch=122
06/03/2022 07:55:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.375418 on epoch=123
06/03/2022 07:55:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.314100 on epoch=124
06/03/2022 07:55:21 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:55:21 - INFO - __main__ - Printing 3 examples
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:55:21 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:55:21 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:55:21 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:55:21 - INFO - __main__ - Printing 3 examples
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:55:21 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:21 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:55:21 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:55:21 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:55:21 - INFO - __main__ - Global step 1000 Train loss 0.359344 ACC 0.46774193548387094 on epoch=124
06/03/2022 07:55:21 - INFO - __main__ - save last model!
06/03/2022 07:55:29 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 07:55:29 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 07:55:29 - INFO - __main__ - Printing 3 examples
06/03/2022 07:55:29 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 07:55:29 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:29 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 07:55:29 - INFO - __main__ - ['neutral']
06/03/2022 07:55:29 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 07:55:29 - INFO - __main__ - ['entailment']
06/03/2022 07:55:29 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:55:29 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:55:29 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 07:55:31 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_13_0.0003_8_predictions.txt
06/03/2022 07:55:31 - INFO - __main__ - ACC on test data: 0.7143
06/03/2022 07:55:31 - INFO - __main__ - prefix=superglue-cb_128_13, lr=0.0003, bsz=8, dev_performance=0.8145161290322581, test_performance=0.7142857142857143
06/03/2022 07:55:31 - INFO - __main__ - Running ... prefix=superglue-cb_128_13, lr=0.0002, bsz=8 ...
06/03/2022 07:55:32 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:55:32 - INFO - __main__ - Printing 3 examples
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:55:32 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:55:32 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 07:55:32 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 07:55:32 - INFO - __main__ - Printing 3 examples
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 07:55:32 - INFO - __main__ - ['contradiction']
06/03/2022 07:55:32 - INFO - __main__ - Tokenizing Input ...
06/03/2022 07:55:32 - INFO - __main__ - Tokenizing Output ...
06/03/2022 07:55:32 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 07:55:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:55:34 - INFO - __main__ - Starting training!
06/03/2022 07:55:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 07:55:44 - INFO - __main__ - Starting training!
06/03/2022 07:55:48 - INFO - __main__ - Step 10 Global step 10 Train loss 24.788416 on epoch=1
06/03/2022 07:55:53 - INFO - __main__ - Step 20 Global step 20 Train loss 19.549515 on epoch=2
06/03/2022 07:55:58 - INFO - __main__ - Step 30 Global step 30 Train loss 11.957376 on epoch=3
06/03/2022 07:56:03 - INFO - __main__ - Step 40 Global step 40 Train loss 10.217874 on epoch=4
06/03/2022 07:56:08 - INFO - __main__ - Step 50 Global step 50 Train loss 10.418954 on epoch=6
06/03/2022 07:56:11 - INFO - __main__ - Global step 50 Train loss 15.386427 ACC 0.0 on epoch=6
06/03/2022 07:56:17 - INFO - __main__ - Step 60 Global step 60 Train loss 9.850640 on epoch=7
06/03/2022 07:56:22 - INFO - __main__ - Step 70 Global step 70 Train loss 9.786032 on epoch=8
06/03/2022 07:56:27 - INFO - __main__ - Step 80 Global step 80 Train loss 8.809801 on epoch=9
06/03/2022 07:56:32 - INFO - __main__ - Step 90 Global step 90 Train loss 8.774114 on epoch=11
06/03/2022 07:56:37 - INFO - __main__ - Step 100 Global step 100 Train loss 8.236298 on epoch=12
06/03/2022 07:56:40 - INFO - __main__ - Global step 100 Train loss 9.091377 ACC 0.0 on epoch=12
06/03/2022 07:56:45 - INFO - __main__ - Step 110 Global step 110 Train loss 7.998052 on epoch=13
06/03/2022 07:56:50 - INFO - __main__ - Step 120 Global step 120 Train loss 7.132204 on epoch=14
06/03/2022 07:56:55 - INFO - __main__ - Step 130 Global step 130 Train loss 7.282286 on epoch=16
06/03/2022 07:57:00 - INFO - __main__ - Step 140 Global step 140 Train loss 6.686330 on epoch=17
06/03/2022 07:57:05 - INFO - __main__ - Step 150 Global step 150 Train loss 5.814450 on epoch=18
06/03/2022 07:57:08 - INFO - __main__ - Global step 150 Train loss 6.982664 ACC 0.0 on epoch=18
06/03/2022 07:57:13 - INFO - __main__ - Step 160 Global step 160 Train loss 4.946856 on epoch=19
06/03/2022 07:57:18 - INFO - __main__ - Step 170 Global step 170 Train loss 4.358299 on epoch=21
06/03/2022 07:57:23 - INFO - __main__ - Step 180 Global step 180 Train loss 3.142915 on epoch=22
06/03/2022 07:57:29 - INFO - __main__ - Step 190 Global step 190 Train loss 2.711755 on epoch=23
06/03/2022 07:57:34 - INFO - __main__ - Step 200 Global step 200 Train loss 2.298094 on epoch=24
06/03/2022 07:57:36 - INFO - __main__ - Global step 200 Train loss 3.491583 ACC 0.47580645161290325 on epoch=24
06/03/2022 07:57:42 - INFO - __main__ - Step 210 Global step 210 Train loss 2.363657 on epoch=26
06/03/2022 07:57:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.753947 on epoch=27
06/03/2022 07:57:52 - INFO - __main__ - Step 230 Global step 230 Train loss 2.206842 on epoch=28
06/03/2022 07:57:58 - INFO - __main__ - Step 240 Global step 240 Train loss 1.716594 on epoch=29
06/03/2022 07:58:03 - INFO - __main__ - Step 250 Global step 250 Train loss 1.619413 on epoch=31
06/03/2022 07:58:05 - INFO - __main__ - Global step 250 Train loss 1.932090 ACC 0.47580645161290325 on epoch=31
06/03/2022 07:58:10 - INFO - __main__ - Step 260 Global step 260 Train loss 2.218828 on epoch=32
06/03/2022 07:58:15 - INFO - __main__ - Step 270 Global step 270 Train loss 2.043590 on epoch=33
06/03/2022 07:58:20 - INFO - __main__ - Step 280 Global step 280 Train loss 1.838775 on epoch=34
06/03/2022 07:58:25 - INFO - __main__ - Step 290 Global step 290 Train loss 2.099858 on epoch=36
06/03/2022 07:58:31 - INFO - __main__ - Step 300 Global step 300 Train loss 1.846023 on epoch=37
06/03/2022 07:58:33 - INFO - __main__ - Global step 300 Train loss 2.009415 ACC 0.47580645161290325 on epoch=37
06/03/2022 07:58:38 - INFO - __main__ - Step 310 Global step 310 Train loss 1.571383 on epoch=38
06/03/2022 07:58:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.777021 on epoch=39
06/03/2022 07:58:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.509802 on epoch=41
06/03/2022 07:58:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.442854 on epoch=42
06/03/2022 07:58:59 - INFO - __main__ - Step 350 Global step 350 Train loss 1.586901 on epoch=43
06/03/2022 07:59:01 - INFO - __main__ - Global step 350 Train loss 1.577592 ACC 0.47580645161290325 on epoch=43
06/03/2022 07:59:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.457833 on epoch=44
06/03/2022 07:59:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.435305 on epoch=46
06/03/2022 07:59:16 - INFO - __main__ - Step 380 Global step 380 Train loss 1.409274 on epoch=47
06/03/2022 07:59:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.393141 on epoch=48
06/03/2022 07:59:27 - INFO - __main__ - Step 400 Global step 400 Train loss 1.356033 on epoch=49
06/03/2022 07:59:29 - INFO - __main__ - Global step 400 Train loss 1.410317 ACC 0.47580645161290325 on epoch=49
06/03/2022 07:59:34 - INFO - __main__ - Step 410 Global step 410 Train loss 1.058499 on epoch=51
06/03/2022 07:59:39 - INFO - __main__ - Step 420 Global step 420 Train loss 1.191570 on epoch=52
06/03/2022 07:59:44 - INFO - __main__ - Step 430 Global step 430 Train loss 1.461194 on epoch=53
06/03/2022 07:59:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.958376 on epoch=54
06/03/2022 07:59:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.953159 on epoch=56
06/03/2022 07:59:57 - INFO - __main__ - Global step 450 Train loss 1.124560 ACC 0.47580645161290325 on epoch=56
06/03/2022 08:00:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.753836 on epoch=57
06/03/2022 08:00:07 - INFO - __main__ - Step 470 Global step 470 Train loss 1.145326 on epoch=58
06/03/2022 08:00:13 - INFO - __main__ - Step 480 Global step 480 Train loss 1.018648 on epoch=59
06/03/2022 08:00:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.829830 on epoch=61
06/03/2022 08:00:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.689735 on epoch=62
06/03/2022 08:00:25 - INFO - __main__ - Global step 500 Train loss 0.887475 ACC 0.47580645161290325 on epoch=62
06/03/2022 08:00:31 - INFO - __main__ - Step 510 Global step 510 Train loss 0.917725 on epoch=63
06/03/2022 08:00:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.707192 on epoch=64
06/03/2022 08:00:41 - INFO - __main__ - Step 530 Global step 530 Train loss 0.774937 on epoch=66
06/03/2022 08:00:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.624720 on epoch=67
06/03/2022 08:00:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.627798 on epoch=68
06/03/2022 08:00:54 - INFO - __main__ - Global step 550 Train loss 0.730475 ACC 0.45161290322580644 on epoch=68
06/03/2022 08:00:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.578280 on epoch=69
06/03/2022 08:01:04 - INFO - __main__ - Step 570 Global step 570 Train loss 0.433291 on epoch=71
06/03/2022 08:01:10 - INFO - __main__ - Step 580 Global step 580 Train loss 0.413077 on epoch=72
06/03/2022 08:01:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.303445 on epoch=73
06/03/2022 08:01:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.311597 on epoch=74
06/03/2022 08:01:23 - INFO - __main__ - Global step 600 Train loss 0.407938 ACC 0.6451612903225806 on epoch=74
06/03/2022 08:01:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.290641 on epoch=76
06/03/2022 08:01:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.306831 on epoch=77
06/03/2022 08:01:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.327634 on epoch=78
06/03/2022 08:01:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.259616 on epoch=79
06/03/2022 08:01:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.251262 on epoch=81
06/03/2022 08:01:52 - INFO - __main__ - Global step 650 Train loss 0.287197 ACC 0.8225806451612904 on epoch=81
06/03/2022 08:01:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.274706 on epoch=82
06/03/2022 08:02:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.272748 on epoch=83
06/03/2022 08:02:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.235328 on epoch=84
06/03/2022 08:02:13 - INFO - __main__ - Step 690 Global step 690 Train loss 0.167979 on epoch=86
06/03/2022 08:02:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.197928 on epoch=87
06/03/2022 08:02:21 - INFO - __main__ - Global step 700 Train loss 0.229738 ACC 0.8145161290322581 on epoch=87
06/03/2022 08:02:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.209356 on epoch=88
06/03/2022 08:02:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.165873 on epoch=89
06/03/2022 08:02:37 - INFO - __main__ - Step 730 Global step 730 Train loss 0.152723 on epoch=91
06/03/2022 08:02:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.221258 on epoch=92
06/03/2022 08:02:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.187581 on epoch=93
06/03/2022 08:02:50 - INFO - __main__ - Global step 750 Train loss 0.187358 ACC 0.8306451612903226 on epoch=93
06/03/2022 08:02:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.162801 on epoch=94
06/03/2022 08:03:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.173589 on epoch=96
06/03/2022 08:03:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.173574 on epoch=97
06/03/2022 08:03:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.153624 on epoch=98
06/03/2022 08:03:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.195337 on epoch=99
06/03/2022 08:03:19 - INFO - __main__ - Global step 800 Train loss 0.171785 ACC 0.6693548387096774 on epoch=99
06/03/2022 08:03:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.233871 on epoch=101
06/03/2022 08:03:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.257560 on epoch=102
06/03/2022 08:03:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.276635 on epoch=103
06/03/2022 08:03:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.153831 on epoch=104
06/03/2022 08:03:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.071613 on epoch=106
06/03/2022 08:03:47 - INFO - __main__ - Global step 850 Train loss 0.198702 ACC 0.8709677419354839 on epoch=106
06/03/2022 08:03:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.086510 on epoch=107
06/03/2022 08:03:58 - INFO - __main__ - Step 870 Global step 870 Train loss 0.102709 on epoch=108
06/03/2022 08:04:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.081735 on epoch=109
06/03/2022 08:04:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.056080 on epoch=111
06/03/2022 08:04:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.071417 on epoch=112
06/03/2022 08:04:17 - INFO - __main__ - Global step 900 Train loss 0.079690 ACC 0.8951612903225806 on epoch=112
06/03/2022 08:04:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.064623 on epoch=113
06/03/2022 08:04:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.076432 on epoch=114
06/03/2022 08:04:33 - INFO - __main__ - Step 930 Global step 930 Train loss 0.071578 on epoch=116
06/03/2022 08:04:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.039324 on epoch=117
06/03/2022 08:04:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.042401 on epoch=118
06/03/2022 08:04:46 - INFO - __main__ - Global step 950 Train loss 0.058872 ACC 0.8870967741935484 on epoch=118
06/03/2022 08:04:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.086055 on epoch=119
06/03/2022 08:04:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.172976 on epoch=121
06/03/2022 08:05:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.169753 on epoch=122
06/03/2022 08:05:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.125543 on epoch=123
06/03/2022 08:05:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.045474 on epoch=124
06/03/2022 08:05:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:05:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:05:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:05:13 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:05:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:05:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 08:05:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:05:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:05:14 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:05:15 - INFO - __main__ - Global step 1000 Train loss 0.119960 ACC 0.8387096774193549 on epoch=124
06/03/2022 08:05:15 - INFO - __main__ - save last model!
06/03/2022 08:05:22 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:05:22 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:05:22 - INFO - __main__ - Printing 3 examples
06/03/2022 08:05:22 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:05:22 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:22 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:05:22 - INFO - __main__ - ['neutral']
06/03/2022 08:05:22 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:05:22 - INFO - __main__ - ['entailment']
06/03/2022 08:05:22 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:05:22 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:05:22 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:05:24 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_13_0.0002_8_predictions.txt
06/03/2022 08:05:24 - INFO - __main__ - ACC on test data: 0.8571
06/03/2022 08:05:24 - INFO - __main__ - prefix=superglue-cb_128_13, lr=0.0002, bsz=8, dev_performance=0.8951612903225806, test_performance=0.8571428571428571
06/03/2022 08:05:24 - INFO - __main__ - Running ... prefix=superglue-cb_128_13, lr=0.0001, bsz=8 ...
06/03/2022 08:05:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:05:24 - INFO - __main__ - Starting training!
06/03/2022 08:05:25 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:05:25 - INFO - __main__ - Printing 3 examples
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: B: That might be kind of tough, huh. A: It really would, yes, yes, and like I said, my sister's still in it, and I really don't think my mother'd want to be there, either. [SEP] hypothesis: his mother would want to be there
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: A: Big time there, sure is. B: It surely is. A: I don't think I'd go to work without a bulletproof vest on myself. [SEP] hypothesis: he would go to work without a bulletproof vest
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: B: Right, you know, like In packaging A: Yeah. B: and, uh, you know, just goodness. A: Yeah, I don't think they do the packaging at this plant, [SEP] hypothesis: they do the packaging at this plant
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:05:25 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:05:25 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:05:25 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:05:25 - INFO - __main__ - Printing 3 examples
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: A: or you know, it doesn't seem that it's going to make much of a difference. B: Uh-huh. It, I mean, I don't know, I don't think George Bush will make the American people happy with ninety-seven cents a week. [SEP] hypothesis: George Bush will make American people happy with ninety-seven cents a week
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: A: How did Radio Shack work? B: If you go in and buy anything they want your phone number. And I don't think they're going to call me and ask me how it's functioning, [SEP] hypothesis: they're going to call him
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ -  [superglue-cb] premise: B: That's true. A: So. B: Uh, the other argument is that the death penalty is a deterrent and I really don't, uh, agree with that. I don't think anyone who would commit uh, a crime that would get them the death penalty would stop at the moment and say, well, I was about to kill and dismember this person but, oh, if they catch me they're going to kill me so I better not do it. I just, don't think uh, that it works that way. A: Yeah.  I don't think it's done. [SEP] hypothesis: it's done
06/03/2022 08:05:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:05:25 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:05:25 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:05:25 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:05:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:05:36 - INFO - __main__ - Starting training!
06/03/2022 08:05:41 - INFO - __main__ - Step 10 Global step 10 Train loss 24.607368 on epoch=1
06/03/2022 08:05:45 - INFO - __main__ - Step 20 Global step 20 Train loss 20.505728 on epoch=2
06/03/2022 08:05:50 - INFO - __main__ - Step 30 Global step 30 Train loss 17.028555 on epoch=3
06/03/2022 08:05:55 - INFO - __main__ - Step 40 Global step 40 Train loss 12.678663 on epoch=4
06/03/2022 08:06:01 - INFO - __main__ - Step 50 Global step 50 Train loss 13.364202 on epoch=6
06/03/2022 08:06:36 - INFO - __main__ - Global step 50 Train loss 17.636904 ACC 0.0 on epoch=6
06/03/2022 08:06:42 - INFO - __main__ - Step 60 Global step 60 Train loss 11.566096 on epoch=7
06/03/2022 08:06:47 - INFO - __main__ - Step 70 Global step 70 Train loss 11.577010 on epoch=8
06/03/2022 08:06:52 - INFO - __main__ - Step 80 Global step 80 Train loss 11.101495 on epoch=9
06/03/2022 08:06:57 - INFO - __main__ - Step 90 Global step 90 Train loss 10.820273 on epoch=11
06/03/2022 08:07:02 - INFO - __main__ - Step 100 Global step 100 Train loss 10.397871 on epoch=12
06/03/2022 08:07:09 - INFO - __main__ - Global step 100 Train loss 11.092548 ACC 0.0 on epoch=12
06/03/2022 08:07:14 - INFO - __main__ - Step 110 Global step 110 Train loss 10.444590 on epoch=13
06/03/2022 08:07:19 - INFO - __main__ - Step 120 Global step 120 Train loss 9.922241 on epoch=14
06/03/2022 08:07:24 - INFO - __main__ - Step 130 Global step 130 Train loss 10.079541 on epoch=16
06/03/2022 08:07:29 - INFO - __main__ - Step 140 Global step 140 Train loss 9.405789 on epoch=17
06/03/2022 08:07:34 - INFO - __main__ - Step 150 Global step 150 Train loss 9.616470 on epoch=18
06/03/2022 08:07:37 - INFO - __main__ - Global step 150 Train loss 9.893726 ACC 0.0 on epoch=18
06/03/2022 08:07:42 - INFO - __main__ - Step 160 Global step 160 Train loss 9.087235 on epoch=19
06/03/2022 08:07:47 - INFO - __main__ - Step 170 Global step 170 Train loss 8.594198 on epoch=21
06/03/2022 08:07:52 - INFO - __main__ - Step 180 Global step 180 Train loss 8.260463 on epoch=22
06/03/2022 08:07:57 - INFO - __main__ - Step 190 Global step 190 Train loss 8.348281 on epoch=23
06/03/2022 08:08:02 - INFO - __main__ - Step 200 Global step 200 Train loss 7.365047 on epoch=24
06/03/2022 08:08:05 - INFO - __main__ - Global step 200 Train loss 8.331045 ACC 0.0 on epoch=24
06/03/2022 08:08:10 - INFO - __main__ - Step 210 Global step 210 Train loss 8.625444 on epoch=26
06/03/2022 08:08:15 - INFO - __main__ - Step 220 Global step 220 Train loss 7.587167 on epoch=27
06/03/2022 08:08:20 - INFO - __main__ - Step 230 Global step 230 Train loss 7.322383 on epoch=28
06/03/2022 08:08:25 - INFO - __main__ - Step 240 Global step 240 Train loss 7.191175 on epoch=29
06/03/2022 08:08:30 - INFO - __main__ - Step 250 Global step 250 Train loss 7.504829 on epoch=31
06/03/2022 08:08:33 - INFO - __main__ - Global step 250 Train loss 7.646200 ACC 0.0 on epoch=31
06/03/2022 08:08:38 - INFO - __main__ - Step 260 Global step 260 Train loss 6.396407 on epoch=32
06/03/2022 08:08:43 - INFO - __main__ - Step 270 Global step 270 Train loss 6.898458 on epoch=33
06/03/2022 08:08:48 - INFO - __main__ - Step 280 Global step 280 Train loss 5.665018 on epoch=34
06/03/2022 08:08:53 - INFO - __main__ - Step 290 Global step 290 Train loss 5.945082 on epoch=36
06/03/2022 08:08:58 - INFO - __main__ - Step 300 Global step 300 Train loss 5.829198 on epoch=37
06/03/2022 08:09:01 - INFO - __main__ - Global step 300 Train loss 6.146832 ACC 0.0 on epoch=37
06/03/2022 08:09:06 - INFO - __main__ - Step 310 Global step 310 Train loss 5.871534 on epoch=38
06/03/2022 08:09:11 - INFO - __main__ - Step 320 Global step 320 Train loss 4.784076 on epoch=39
06/03/2022 08:09:16 - INFO - __main__ - Step 330 Global step 330 Train loss 4.348081 on epoch=41
06/03/2022 08:09:21 - INFO - __main__ - Step 340 Global step 340 Train loss 4.409740 on epoch=42
06/03/2022 08:09:26 - INFO - __main__ - Step 350 Global step 350 Train loss 4.160282 on epoch=43
06/03/2022 08:09:29 - INFO - __main__ - Global step 350 Train loss 4.714743 ACC 0.0 on epoch=43
06/03/2022 08:09:34 - INFO - __main__ - Step 360 Global step 360 Train loss 3.313893 on epoch=44
06/03/2022 08:09:39 - INFO - __main__ - Step 370 Global step 370 Train loss 3.170299 on epoch=46
06/03/2022 08:09:44 - INFO - __main__ - Step 380 Global step 380 Train loss 2.318376 on epoch=47
06/03/2022 08:09:49 - INFO - __main__ - Step 390 Global step 390 Train loss 2.492686 on epoch=48
06/03/2022 08:09:54 - INFO - __main__ - Step 400 Global step 400 Train loss 2.687062 on epoch=49
06/03/2022 08:09:56 - INFO - __main__ - Global step 400 Train loss 2.796463 ACC 0.6532258064516129 on epoch=49
06/03/2022 08:10:02 - INFO - __main__ - Step 410 Global step 410 Train loss 1.917627 on epoch=51
06/03/2022 08:10:07 - INFO - __main__ - Step 420 Global step 420 Train loss 2.832703 on epoch=52
06/03/2022 08:10:13 - INFO - __main__ - Step 430 Global step 430 Train loss 2.122375 on epoch=53
06/03/2022 08:10:18 - INFO - __main__ - Step 440 Global step 440 Train loss 2.157593 on epoch=54
06/03/2022 08:10:23 - INFO - __main__ - Step 450 Global step 450 Train loss 2.332954 on epoch=56
06/03/2022 08:10:25 - INFO - __main__ - Global step 450 Train loss 2.272650 ACC 0.47580645161290325 on epoch=56
06/03/2022 08:10:30 - INFO - __main__ - Step 460 Global step 460 Train loss 2.316171 on epoch=57
06/03/2022 08:10:35 - INFO - __main__ - Step 470 Global step 470 Train loss 2.280462 on epoch=58
06/03/2022 08:10:40 - INFO - __main__ - Step 480 Global step 480 Train loss 1.896505 on epoch=59
06/03/2022 08:10:45 - INFO - __main__ - Step 490 Global step 490 Train loss 1.804166 on epoch=61
06/03/2022 08:10:50 - INFO - __main__ - Step 500 Global step 500 Train loss 2.082220 on epoch=62
06/03/2022 08:10:53 - INFO - __main__ - Global step 500 Train loss 2.075905 ACC 0.47580645161290325 on epoch=62
06/03/2022 08:10:58 - INFO - __main__ - Step 510 Global step 510 Train loss 1.917197 on epoch=63
06/03/2022 08:11:03 - INFO - __main__ - Step 520 Global step 520 Train loss 1.726191 on epoch=64
06/03/2022 08:11:08 - INFO - __main__ - Step 530 Global step 530 Train loss 1.936637 on epoch=66
06/03/2022 08:11:13 - INFO - __main__ - Step 540 Global step 540 Train loss 1.832560 on epoch=67
06/03/2022 08:11:18 - INFO - __main__ - Step 550 Global step 550 Train loss 2.134588 on epoch=68
06/03/2022 08:11:20 - INFO - __main__ - Global step 550 Train loss 1.909435 ACC 0.47580645161290325 on epoch=68
06/03/2022 08:11:25 - INFO - __main__ - Step 560 Global step 560 Train loss 2.270387 on epoch=69
06/03/2022 08:11:31 - INFO - __main__ - Step 570 Global step 570 Train loss 1.875325 on epoch=71
06/03/2022 08:11:36 - INFO - __main__ - Step 580 Global step 580 Train loss 1.533285 on epoch=72
06/03/2022 08:11:41 - INFO - __main__ - Step 590 Global step 590 Train loss 1.634809 on epoch=73
06/03/2022 08:11:46 - INFO - __main__ - Step 600 Global step 600 Train loss 1.886404 on epoch=74
06/03/2022 08:11:48 - INFO - __main__ - Global step 600 Train loss 1.840042 ACC 0.47580645161290325 on epoch=74
06/03/2022 08:11:53 - INFO - __main__ - Step 610 Global step 610 Train loss 2.173250 on epoch=76
06/03/2022 08:11:58 - INFO - __main__ - Step 620 Global step 620 Train loss 1.898429 on epoch=77
06/03/2022 08:12:03 - INFO - __main__ - Step 630 Global step 630 Train loss 1.827595 on epoch=78
06/03/2022 08:12:08 - INFO - __main__ - Step 640 Global step 640 Train loss 1.603324 on epoch=79
06/03/2022 08:12:13 - INFO - __main__ - Step 650 Global step 650 Train loss 1.464844 on epoch=81
06/03/2022 08:12:16 - INFO - __main__ - Global step 650 Train loss 1.793488 ACC 0.47580645161290325 on epoch=81
06/03/2022 08:12:21 - INFO - __main__ - Step 660 Global step 660 Train loss 1.872491 on epoch=82
06/03/2022 08:12:26 - INFO - __main__ - Step 670 Global step 670 Train loss 1.628415 on epoch=83
06/03/2022 08:12:31 - INFO - __main__ - Step 680 Global step 680 Train loss 1.914516 on epoch=84
06/03/2022 08:12:36 - INFO - __main__ - Step 690 Global step 690 Train loss 1.594026 on epoch=86
06/03/2022 08:12:41 - INFO - __main__ - Step 700 Global step 700 Train loss 1.819297 on epoch=87
06/03/2022 08:12:44 - INFO - __main__ - Global step 700 Train loss 1.765749 ACC 0.47580645161290325 on epoch=87
06/03/2022 08:12:49 - INFO - __main__ - Step 710 Global step 710 Train loss 1.369782 on epoch=88
06/03/2022 08:12:54 - INFO - __main__ - Step 720 Global step 720 Train loss 1.252681 on epoch=89
06/03/2022 08:12:59 - INFO - __main__ - Step 730 Global step 730 Train loss 1.336399 on epoch=91
06/03/2022 08:13:04 - INFO - __main__ - Step 740 Global step 740 Train loss 1.492745 on epoch=92
06/03/2022 08:13:09 - INFO - __main__ - Step 750 Global step 750 Train loss 1.374477 on epoch=93
06/03/2022 08:13:12 - INFO - __main__ - Global step 750 Train loss 1.365217 ACC 0.47580645161290325 on epoch=93
06/03/2022 08:13:17 - INFO - __main__ - Step 760 Global step 760 Train loss 1.617149 on epoch=94
06/03/2022 08:13:22 - INFO - __main__ - Step 770 Global step 770 Train loss 1.039446 on epoch=96
06/03/2022 08:13:27 - INFO - __main__ - Step 780 Global step 780 Train loss 1.194562 on epoch=97
06/03/2022 08:13:32 - INFO - __main__ - Step 790 Global step 790 Train loss 1.395646 on epoch=98
06/03/2022 08:13:37 - INFO - __main__ - Step 800 Global step 800 Train loss 1.130329 on epoch=99
06/03/2022 08:13:39 - INFO - __main__ - Global step 800 Train loss 1.275426 ACC 0.47580645161290325 on epoch=99
06/03/2022 08:13:44 - INFO - __main__ - Step 810 Global step 810 Train loss 1.292642 on epoch=101
06/03/2022 08:13:50 - INFO - __main__ - Step 820 Global step 820 Train loss 1.063340 on epoch=102
06/03/2022 08:13:55 - INFO - __main__ - Step 830 Global step 830 Train loss 1.086429 on epoch=103
06/03/2022 08:14:00 - INFO - __main__ - Step 840 Global step 840 Train loss 1.121661 on epoch=104
06/03/2022 08:14:05 - INFO - __main__ - Step 850 Global step 850 Train loss 1.085485 on epoch=106
06/03/2022 08:14:07 - INFO - __main__ - Global step 850 Train loss 1.129912 ACC 0.47580645161290325 on epoch=106
06/03/2022 08:14:12 - INFO - __main__ - Step 860 Global step 860 Train loss 1.032889 on epoch=107
06/03/2022 08:14:17 - INFO - __main__ - Step 870 Global step 870 Train loss 1.028477 on epoch=108
06/03/2022 08:14:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.779372 on epoch=109
06/03/2022 08:14:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.869636 on epoch=111
06/03/2022 08:14:32 - INFO - __main__ - Step 900 Global step 900 Train loss 1.094911 on epoch=112
06/03/2022 08:14:35 - INFO - __main__ - Global step 900 Train loss 0.961057 ACC 0.47580645161290325 on epoch=112
06/03/2022 08:14:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.868636 on epoch=113
06/03/2022 08:14:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.849462 on epoch=114
06/03/2022 08:14:50 - INFO - __main__ - Step 930 Global step 930 Train loss 1.350861 on epoch=116
06/03/2022 08:14:55 - INFO - __main__ - Step 940 Global step 940 Train loss 1.006073 on epoch=117
06/03/2022 08:15:00 - INFO - __main__ - Step 950 Global step 950 Train loss 0.992228 on epoch=118
06/03/2022 08:15:02 - INFO - __main__ - Global step 950 Train loss 1.013452 ACC 0.47580645161290325 on epoch=118
06/03/2022 08:15:07 - INFO - __main__ - Step 960 Global step 960 Train loss 1.051669 on epoch=119
06/03/2022 08:15:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.877936 on epoch=121
06/03/2022 08:15:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.637438 on epoch=122
06/03/2022 08:15:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.711791 on epoch=123
06/03/2022 08:15:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.775812 on epoch=124
06/03/2022 08:15:29 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:15:29 - INFO - __main__ - Printing 3 examples
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:15:29 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:15:29 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:15:29 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:15:29 - INFO - __main__ - Printing 3 examples
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:15:29 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:29 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:15:29 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:15:29 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:15:30 - INFO - __main__ - Global step 1000 Train loss 0.810929 ACC 0.7258064516129032 on epoch=124
06/03/2022 08:15:31 - INFO - __main__ - save last model!
06/03/2022 08:15:37 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:15:38 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:15:38 - INFO - __main__ - Printing 3 examples
06/03/2022 08:15:38 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:15:38 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:38 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:15:38 - INFO - __main__ - ['neutral']
06/03/2022 08:15:38 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:15:38 - INFO - __main__ - ['entailment']
06/03/2022 08:15:38 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:15:38 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:15:38 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:15:40 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_13_0.0001_8_predictions.txt
06/03/2022 08:15:40 - INFO - __main__ - ACC on test data: 0.6429
06/03/2022 08:15:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:15:40 - INFO - __main__ - Starting training!
06/03/2022 08:15:40 - INFO - __main__ - prefix=superglue-cb_128_13, lr=0.0001, bsz=8, dev_performance=0.7258064516129032, test_performance=0.6428571428571429
06/03/2022 08:15:40 - INFO - __main__ - Running ... prefix=superglue-cb_128_21, lr=0.0005, bsz=8 ...
06/03/2022 08:15:41 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:15:41 - INFO - __main__ - Printing 3 examples
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:15:41 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:15:41 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:15:41 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:15:41 - INFO - __main__ - Printing 3 examples
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:15:41 - INFO - __main__ - ['contradiction']
06/03/2022 08:15:41 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:15:41 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:15:41 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:15:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:15:52 - INFO - __main__ - Starting training!
06/03/2022 08:15:56 - INFO - __main__ - Step 10 Global step 10 Train loss 23.874655 on epoch=1
06/03/2022 08:16:01 - INFO - __main__ - Step 20 Global step 20 Train loss 15.406659 on epoch=2
06/03/2022 08:16:06 - INFO - __main__ - Step 30 Global step 30 Train loss 12.266238 on epoch=3
06/03/2022 08:16:11 - INFO - __main__ - Step 40 Global step 40 Train loss 10.462611 on epoch=4
06/03/2022 08:16:16 - INFO - __main__ - Step 50 Global step 50 Train loss 8.949786 on epoch=6
06/03/2022 08:16:19 - INFO - __main__ - Global step 50 Train loss 14.191991 ACC 0.0 on epoch=6
06/03/2022 08:16:25 - INFO - __main__ - Step 60 Global step 60 Train loss 11.301470 on epoch=7
06/03/2022 08:16:30 - INFO - __main__ - Step 70 Global step 70 Train loss 7.189618 on epoch=8
06/03/2022 08:16:35 - INFO - __main__ - Step 80 Global step 80 Train loss 5.827911 on epoch=9
06/03/2022 08:16:40 - INFO - __main__ - Step 90 Global step 90 Train loss 5.124539 on epoch=11
06/03/2022 08:16:45 - INFO - __main__ - Step 100 Global step 100 Train loss 2.975307 on epoch=12
06/03/2022 08:16:48 - INFO - __main__ - Global step 100 Train loss 6.483769 ACC 0.49193548387096775 on epoch=12
06/03/2022 08:16:54 - INFO - __main__ - Step 110 Global step 110 Train loss 2.623512 on epoch=13
06/03/2022 08:16:59 - INFO - __main__ - Step 120 Global step 120 Train loss 2.872869 on epoch=14
06/03/2022 08:17:04 - INFO - __main__ - Step 130 Global step 130 Train loss 2.117042 on epoch=16
06/03/2022 08:17:09 - INFO - __main__ - Step 140 Global step 140 Train loss 1.693300 on epoch=17
06/03/2022 08:17:14 - INFO - __main__ - Step 150 Global step 150 Train loss 1.690577 on epoch=18
06/03/2022 08:17:16 - INFO - __main__ - Global step 150 Train loss 2.199460 ACC 0.47580645161290325 on epoch=18
06/03/2022 08:17:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.803957 on epoch=19
06/03/2022 08:17:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.304479 on epoch=21
06/03/2022 08:17:31 - INFO - __main__ - Step 180 Global step 180 Train loss 1.564224 on epoch=22
06/03/2022 08:17:37 - INFO - __main__ - Step 190 Global step 190 Train loss 1.583654 on epoch=23
06/03/2022 08:17:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.349805 on epoch=24
06/03/2022 08:17:44 - INFO - __main__ - Global step 200 Train loss 1.521224 ACC 0.47580645161290325 on epoch=24
06/03/2022 08:17:49 - INFO - __main__ - Step 210 Global step 210 Train loss 1.092494 on epoch=26
06/03/2022 08:17:54 - INFO - __main__ - Step 220 Global step 220 Train loss 0.915746 on epoch=27
06/03/2022 08:17:59 - INFO - __main__ - Step 230 Global step 230 Train loss 1.006014 on epoch=28
06/03/2022 08:18:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.910482 on epoch=29
06/03/2022 08:18:09 - INFO - __main__ - Step 250 Global step 250 Train loss 1.075431 on epoch=31
06/03/2022 08:18:12 - INFO - __main__ - Global step 250 Train loss 1.000033 ACC 0.47580645161290325 on epoch=31
06/03/2022 08:18:17 - INFO - __main__ - Step 260 Global step 260 Train loss 1.091398 on epoch=32
06/03/2022 08:18:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.983317 on epoch=33
06/03/2022 08:18:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.679483 on epoch=34
06/03/2022 08:18:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.765694 on epoch=36
06/03/2022 08:18:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.742914 on epoch=37
06/03/2022 08:18:39 - INFO - __main__ - Global step 300 Train loss 0.852561 ACC 0.47580645161290325 on epoch=37
06/03/2022 08:18:44 - INFO - __main__ - Step 310 Global step 310 Train loss 0.689113 on epoch=38
06/03/2022 08:18:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.659336 on epoch=39
06/03/2022 08:18:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.595592 on epoch=41
06/03/2022 08:18:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.553788 on epoch=42
06/03/2022 08:19:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.475221 on epoch=43
06/03/2022 08:19:07 - INFO - __main__ - Global step 350 Train loss 0.594610 ACC 0.4596774193548387 on epoch=43
06/03/2022 08:19:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.379525 on epoch=44
06/03/2022 08:19:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.414526 on epoch=46
06/03/2022 08:19:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.562799 on epoch=47
06/03/2022 08:19:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.392955 on epoch=48
06/03/2022 08:19:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.502884 on epoch=49
06/03/2022 08:19:35 - INFO - __main__ - Global step 400 Train loss 0.450538 ACC 0.4838709677419355 on epoch=49
06/03/2022 08:19:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.419467 on epoch=51
06/03/2022 08:19:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.460217 on epoch=52
06/03/2022 08:19:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.408989 on epoch=53
06/03/2022 08:19:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.461609 on epoch=54
06/03/2022 08:20:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.364849 on epoch=56
06/03/2022 08:20:03 - INFO - __main__ - Global step 450 Train loss 0.423026 ACC 0.47580645161290325 on epoch=56
06/03/2022 08:20:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.470146 on epoch=57
06/03/2022 08:20:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.347722 on epoch=58
06/03/2022 08:20:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.310819 on epoch=59
06/03/2022 08:20:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.320119 on epoch=61
06/03/2022 08:20:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.381114 on epoch=62
06/03/2022 08:20:31 - INFO - __main__ - Global step 500 Train loss 0.365984 ACC 0.47580645161290325 on epoch=62
06/03/2022 08:20:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.318211 on epoch=63
06/03/2022 08:20:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.349905 on epoch=64
06/03/2022 08:20:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.348048 on epoch=66
06/03/2022 08:20:51 - INFO - __main__ - Step 540 Global step 540 Train loss 0.363452 on epoch=67
06/03/2022 08:20:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.420235 on epoch=68
06/03/2022 08:20:59 - INFO - __main__ - Global step 550 Train loss 0.359970 ACC 0.43548387096774194 on epoch=68
06/03/2022 08:21:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.380705 on epoch=69
06/03/2022 08:21:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.413628 on epoch=71
06/03/2022 08:21:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.359908 on epoch=72
06/03/2022 08:21:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.389292 on epoch=73
06/03/2022 08:21:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.293530 on epoch=74
06/03/2022 08:21:27 - INFO - __main__ - Global step 600 Train loss 0.367413 ACC 0.46774193548387094 on epoch=74
06/03/2022 08:21:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.265726 on epoch=76
06/03/2022 08:21:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.326735 on epoch=77
06/03/2022 08:21:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.296302 on epoch=78
06/03/2022 08:21:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.393413 on epoch=79
06/03/2022 08:21:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.354542 on epoch=81
06/03/2022 08:21:55 - INFO - __main__ - Global step 650 Train loss 0.327343 ACC 0.4596774193548387 on epoch=81
06/03/2022 08:22:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.330299 on epoch=82
06/03/2022 08:22:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.320780 on epoch=83
06/03/2022 08:22:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.311627 on epoch=84
06/03/2022 08:22:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.323489 on epoch=86
06/03/2022 08:22:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.258021 on epoch=87
06/03/2022 08:22:23 - INFO - __main__ - Global step 700 Train loss 0.308843 ACC 0.5241935483870968 on epoch=87
06/03/2022 08:22:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.308735 on epoch=88
06/03/2022 08:22:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.319279 on epoch=89
06/03/2022 08:22:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.328326 on epoch=91
06/03/2022 08:22:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.311609 on epoch=92
06/03/2022 08:22:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.313508 on epoch=93
06/03/2022 08:22:52 - INFO - __main__ - Global step 750 Train loss 0.316291 ACC 0.4274193548387097 on epoch=93
06/03/2022 08:22:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.325163 on epoch=94
06/03/2022 08:23:02 - INFO - __main__ - Step 770 Global step 770 Train loss 0.278631 on epoch=96
06/03/2022 08:23:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.262026 on epoch=97
06/03/2022 08:23:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.278984 on epoch=98
06/03/2022 08:23:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.274483 on epoch=99
06/03/2022 08:23:20 - INFO - __main__ - Global step 800 Train loss 0.283858 ACC 0.5967741935483871 on epoch=99
06/03/2022 08:23:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.281514 on epoch=101
06/03/2022 08:23:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.397353 on epoch=102
06/03/2022 08:23:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.268318 on epoch=103
06/03/2022 08:23:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.269417 on epoch=104
06/03/2022 08:23:46 - INFO - __main__ - Step 850 Global step 850 Train loss 0.273157 on epoch=106
06/03/2022 08:23:49 - INFO - __main__ - Global step 850 Train loss 0.297952 ACC 0.5161290322580645 on epoch=106
06/03/2022 08:23:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.285841 on epoch=107
06/03/2022 08:23:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.275268 on epoch=108
06/03/2022 08:24:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.280061 on epoch=109
06/03/2022 08:24:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.264735 on epoch=111
06/03/2022 08:24:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.401686 on epoch=112
06/03/2022 08:24:17 - INFO - __main__ - Global step 900 Train loss 0.301518 ACC 0.4596774193548387 on epoch=112
06/03/2022 08:24:22 - INFO - __main__ - Step 910 Global step 910 Train loss 0.258764 on epoch=113
06/03/2022 08:24:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.242260 on epoch=114
06/03/2022 08:24:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.226438 on epoch=116
06/03/2022 08:24:37 - INFO - __main__ - Step 940 Global step 940 Train loss 0.239023 on epoch=117
06/03/2022 08:24:42 - INFO - __main__ - Step 950 Global step 950 Train loss 0.228965 on epoch=118
06/03/2022 08:24:45 - INFO - __main__ - Global step 950 Train loss 0.239090 ACC 0.4596774193548387 on epoch=118
06/03/2022 08:24:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.270108 on epoch=119
06/03/2022 08:24:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.229346 on epoch=121
06/03/2022 08:25:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.265572 on epoch=122
06/03/2022 08:25:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.222093 on epoch=123
06/03/2022 08:25:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.236100 on epoch=124
06/03/2022 08:25:11 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:25:11 - INFO - __main__ - Printing 3 examples
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:25:11 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:25:11 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:25:11 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:25:11 - INFO - __main__ - Printing 3 examples
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:25:11 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:11 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:25:11 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:25:12 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:25:12 - INFO - __main__ - Global step 1000 Train loss 0.244644 ACC 0.6693548387096774 on epoch=124
06/03/2022 08:25:13 - INFO - __main__ - save last model!
06/03/2022 08:25:20 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:25:21 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:25:21 - INFO - __main__ - Printing 3 examples
06/03/2022 08:25:21 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:25:21 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:21 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:25:21 - INFO - __main__ - ['neutral']
06/03/2022 08:25:21 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:25:21 - INFO - __main__ - ['entailment']
06/03/2022 08:25:21 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:25:21 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:25:21 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:25:22 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_21_0.0005_8_predictions.txt
06/03/2022 08:25:22 - INFO - __main__ - ACC on test data: 0.5893
06/03/2022 08:25:23 - INFO - __main__ - prefix=superglue-cb_128_21, lr=0.0005, bsz=8, dev_performance=0.6693548387096774, test_performance=0.5892857142857143
06/03/2022 08:25:23 - INFO - __main__ - Running ... prefix=superglue-cb_128_21, lr=0.0003, bsz=8 ...
06/03/2022 08:25:24 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:25:24 - INFO - __main__ - Printing 3 examples
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:25:24 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:25:24 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:25:24 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:25:24 - INFO - __main__ - Printing 3 examples
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:25:24 - INFO - __main__ - ['contradiction']
06/03/2022 08:25:24 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:25:24 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:25:24 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:25:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:25:25 - INFO - __main__ - Starting training!
06/03/2022 08:25:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:25:35 - INFO - __main__ - Starting training!
06/03/2022 08:25:39 - INFO - __main__ - Step 10 Global step 10 Train loss 24.378876 on epoch=1
06/03/2022 08:25:44 - INFO - __main__ - Step 20 Global step 20 Train loss 18.684557 on epoch=2
06/03/2022 08:25:49 - INFO - __main__ - Step 30 Global step 30 Train loss 12.474428 on epoch=3
06/03/2022 08:25:55 - INFO - __main__ - Step 40 Global step 40 Train loss 10.384687 on epoch=4
06/03/2022 08:26:00 - INFO - __main__ - Step 50 Global step 50 Train loss 10.099058 on epoch=6
06/03/2022 08:26:03 - INFO - __main__ - Global step 50 Train loss 15.204320 ACC 0.0 on epoch=6
06/03/2022 08:26:09 - INFO - __main__ - Step 60 Global step 60 Train loss 8.135351 on epoch=7
06/03/2022 08:26:14 - INFO - __main__ - Step 70 Global step 70 Train loss 7.919575 on epoch=8
06/03/2022 08:26:19 - INFO - __main__ - Step 80 Global step 80 Train loss 7.451427 on epoch=9
06/03/2022 08:26:24 - INFO - __main__ - Step 90 Global step 90 Train loss 7.709949 on epoch=11
06/03/2022 08:26:29 - INFO - __main__ - Step 100 Global step 100 Train loss 6.723031 on epoch=12
06/03/2022 08:26:31 - INFO - __main__ - Global step 100 Train loss 7.587866 ACC 0.07258064516129033 on epoch=12
06/03/2022 08:26:37 - INFO - __main__ - Step 110 Global step 110 Train loss 5.598063 on epoch=13
06/03/2022 08:26:42 - INFO - __main__ - Step 120 Global step 120 Train loss 5.130462 on epoch=14
06/03/2022 08:26:47 - INFO - __main__ - Step 130 Global step 130 Train loss 3.992083 on epoch=16
06/03/2022 08:26:53 - INFO - __main__ - Step 140 Global step 140 Train loss 3.527524 on epoch=17
06/03/2022 08:26:58 - INFO - __main__ - Step 150 Global step 150 Train loss 2.590894 on epoch=18
06/03/2022 08:27:00 - INFO - __main__ - Global step 150 Train loss 4.167805 ACC 0.5483870967741935 on epoch=18
06/03/2022 08:27:06 - INFO - __main__ - Step 160 Global step 160 Train loss 2.183604 on epoch=19
06/03/2022 08:27:11 - INFO - __main__ - Step 170 Global step 170 Train loss 1.821442 on epoch=21
06/03/2022 08:27:16 - INFO - __main__ - Step 180 Global step 180 Train loss 2.348990 on epoch=22
06/03/2022 08:27:21 - INFO - __main__ - Step 190 Global step 190 Train loss 1.963084 on epoch=23
06/03/2022 08:27:26 - INFO - __main__ - Step 200 Global step 200 Train loss 2.152923 on epoch=24
06/03/2022 08:27:29 - INFO - __main__ - Global step 200 Train loss 2.094008 ACC 0.47580645161290325 on epoch=24
06/03/2022 08:27:34 - INFO - __main__ - Step 210 Global step 210 Train loss 2.251923 on epoch=26
06/03/2022 08:27:39 - INFO - __main__ - Step 220 Global step 220 Train loss 1.627972 on epoch=27
06/03/2022 08:27:44 - INFO - __main__ - Step 230 Global step 230 Train loss 1.379807 on epoch=28
06/03/2022 08:27:49 - INFO - __main__ - Step 240 Global step 240 Train loss 1.265141 on epoch=29
06/03/2022 08:27:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.520476 on epoch=31
06/03/2022 08:27:57 - INFO - __main__ - Global step 250 Train loss 1.609064 ACC 0.47580645161290325 on epoch=31
06/03/2022 08:28:02 - INFO - __main__ - Step 260 Global step 260 Train loss 1.752662 on epoch=32
06/03/2022 08:28:07 - INFO - __main__ - Step 270 Global step 270 Train loss 1.021111 on epoch=33
06/03/2022 08:28:12 - INFO - __main__ - Step 280 Global step 280 Train loss 1.183899 on epoch=34
06/03/2022 08:28:18 - INFO - __main__ - Step 290 Global step 290 Train loss 1.126634 on epoch=36
06/03/2022 08:28:23 - INFO - __main__ - Step 300 Global step 300 Train loss 1.047745 on epoch=37
06/03/2022 08:28:25 - INFO - __main__ - Global step 300 Train loss 1.226410 ACC 0.47580645161290325 on epoch=37
06/03/2022 08:28:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.154686 on epoch=38
06/03/2022 08:28:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.162452 on epoch=39
06/03/2022 08:28:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.693476 on epoch=41
06/03/2022 08:28:46 - INFO - __main__ - Step 340 Global step 340 Train loss 0.450177 on epoch=42
06/03/2022 08:28:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.204948 on epoch=43
06/03/2022 08:28:54 - INFO - __main__ - Global step 350 Train loss 0.733148 ACC 0.75 on epoch=43
06/03/2022 08:28:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.212334 on epoch=44
06/03/2022 08:29:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.182910 on epoch=46
06/03/2022 08:29:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.127522 on epoch=47
06/03/2022 08:29:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.145643 on epoch=48
06/03/2022 08:29:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.091603 on epoch=49
06/03/2022 08:29:22 - INFO - __main__ - Global step 400 Train loss 0.152002 ACC 0.6290322580645161 on epoch=49
06/03/2022 08:29:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.058520 on epoch=51
06/03/2022 08:29:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.125100 on epoch=52
06/03/2022 08:29:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.066697 on epoch=53
06/03/2022 08:29:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.027890 on epoch=54
06/03/2022 08:29:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.054792 on epoch=56
06/03/2022 08:29:51 - INFO - __main__ - Global step 450 Train loss 0.066600 ACC 0.7338709677419355 on epoch=56
06/03/2022 08:29:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.015029 on epoch=57
06/03/2022 08:30:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.027299 on epoch=58
06/03/2022 08:30:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.120286 on epoch=59
06/03/2022 08:30:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.116535 on epoch=61
06/03/2022 08:30:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.102026 on epoch=62
06/03/2022 08:30:20 - INFO - __main__ - Global step 500 Train loss 0.076235 ACC 0.7741935483870968 on epoch=62
06/03/2022 08:30:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.137120 on epoch=63
06/03/2022 08:30:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.059463 on epoch=64
06/03/2022 08:30:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.013146 on epoch=66
06/03/2022 08:30:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002435 on epoch=67
06/03/2022 08:30:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.002415 on epoch=68
06/03/2022 08:30:49 - INFO - __main__ - Global step 550 Train loss 0.042916 ACC 0.782258064516129 on epoch=68
06/03/2022 08:30:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.001585 on epoch=69
06/03/2022 08:31:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002450 on epoch=71
06/03/2022 08:31:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.026243 on epoch=72
06/03/2022 08:31:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.002418 on epoch=73
06/03/2022 08:31:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.001946 on epoch=74
06/03/2022 08:31:18 - INFO - __main__ - Global step 600 Train loss 0.006928 ACC 0.8064516129032258 on epoch=74
06/03/2022 08:31:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.004460 on epoch=76
06/03/2022 08:31:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000885 on epoch=77
06/03/2022 08:31:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.000540 on epoch=78
06/03/2022 08:31:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001721 on epoch=79
06/03/2022 08:31:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000661 on epoch=81
06/03/2022 08:31:47 - INFO - __main__ - Global step 650 Train loss 0.001653 ACC 0.8145161290322581 on epoch=81
06/03/2022 08:31:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.000363 on epoch=82
06/03/2022 08:31:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.007182 on epoch=83
06/03/2022 08:32:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.000904 on epoch=84
06/03/2022 08:32:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000527 on epoch=86
06/03/2022 08:32:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.019486 on epoch=87
06/03/2022 08:32:16 - INFO - __main__ - Global step 700 Train loss 0.005692 ACC 0.8306451612903226 on epoch=87
06/03/2022 08:32:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.012099 on epoch=88
06/03/2022 08:32:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.010690 on epoch=89
06/03/2022 08:32:32 - INFO - __main__ - Step 730 Global step 730 Train loss 0.005336 on epoch=91
06/03/2022 08:32:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.002522 on epoch=92
06/03/2022 08:32:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000349 on epoch=93
06/03/2022 08:32:46 - INFO - __main__ - Global step 750 Train loss 0.006199 ACC 0.8467741935483871 on epoch=93
06/03/2022 08:32:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000496 on epoch=94
06/03/2022 08:32:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000116 on epoch=96
06/03/2022 08:33:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.003932 on epoch=97
06/03/2022 08:33:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.001634 on epoch=98
06/03/2022 08:33:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000228 on epoch=99
06/03/2022 08:33:15 - INFO - __main__ - Global step 800 Train loss 0.001281 ACC 0.8387096774193549 on epoch=99
06/03/2022 08:33:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000153 on epoch=101
06/03/2022 08:33:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.041335 on epoch=102
06/03/2022 08:33:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.002792 on epoch=103
06/03/2022 08:33:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.026138 on epoch=104
06/03/2022 08:33:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000429 on epoch=106
06/03/2022 08:33:44 - INFO - __main__ - Global step 850 Train loss 0.014169 ACC 0.8387096774193549 on epoch=106
06/03/2022 08:33:49 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000270 on epoch=107
06/03/2022 08:33:54 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000571 on epoch=108
06/03/2022 08:33:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000882 on epoch=109
06/03/2022 08:34:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.024951 on epoch=111
06/03/2022 08:34:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.001277 on epoch=112
06/03/2022 08:34:12 - INFO - __main__ - Global step 900 Train loss 0.005590 ACC 0.8225806451612904 on epoch=112
06/03/2022 08:34:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.001519 on epoch=113
06/03/2022 08:34:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000418 on epoch=114
06/03/2022 08:34:28 - INFO - __main__ - Step 930 Global step 930 Train loss 0.039641 on epoch=116
06/03/2022 08:34:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.204561 on epoch=117
06/03/2022 08:34:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.263436 on epoch=118
06/03/2022 08:34:41 - INFO - __main__ - Global step 950 Train loss 0.101915 ACC 0.8387096774193549 on epoch=118
06/03/2022 08:34:46 - INFO - __main__ - Step 960 Global step 960 Train loss 0.007206 on epoch=119
06/03/2022 08:34:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.718397 on epoch=121
06/03/2022 08:34:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.542331 on epoch=122
06/03/2022 08:35:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.406341 on epoch=123
06/03/2022 08:35:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.190795 on epoch=124
06/03/2022 08:35:08 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:35:08 - INFO - __main__ - Printing 3 examples
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:35:08 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:35:08 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:35:08 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:35:08 - INFO - __main__ - Printing 3 examples
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:35:08 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:08 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:35:08 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:35:08 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:35:09 - INFO - __main__ - Global step 1000 Train loss 0.373014 ACC 0.8306451612903226 on epoch=124
06/03/2022 08:35:09 - INFO - __main__ - save last model!
06/03/2022 08:35:16 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:35:17 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:35:17 - INFO - __main__ - Printing 3 examples
06/03/2022 08:35:17 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:35:17 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:17 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:35:17 - INFO - __main__ - ['neutral']
06/03/2022 08:35:17 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:35:17 - INFO - __main__ - ['entailment']
06/03/2022 08:35:17 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:35:17 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:35:17 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:35:19 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_21_0.0003_8_predictions.txt
06/03/2022 08:35:19 - INFO - __main__ - ACC on test data: 0.8571
06/03/2022 08:35:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:35:19 - INFO - __main__ - Starting training!
06/03/2022 08:35:19 - INFO - __main__ - prefix=superglue-cb_128_21, lr=0.0003, bsz=8, dev_performance=0.8467741935483871, test_performance=0.8571428571428571
06/03/2022 08:35:19 - INFO - __main__ - Running ... prefix=superglue-cb_128_21, lr=0.0002, bsz=8 ...
06/03/2022 08:35:20 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:35:20 - INFO - __main__ - Printing 3 examples
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:35:20 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:35:20 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:35:20 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:35:20 - INFO - __main__ - Printing 3 examples
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:35:20 - INFO - __main__ - ['contradiction']
06/03/2022 08:35:20 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:35:20 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:35:20 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:35:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:35:33 - INFO - __main__ - Starting training!
06/03/2022 08:35:37 - INFO - __main__ - Step 10 Global step 10 Train loss 24.501060 on epoch=1
06/03/2022 08:35:42 - INFO - __main__ - Step 20 Global step 20 Train loss 20.652985 on epoch=2
06/03/2022 08:35:47 - INFO - __main__ - Step 30 Global step 30 Train loss 13.184896 on epoch=3
06/03/2022 08:35:52 - INFO - __main__ - Step 40 Global step 40 Train loss 11.848318 on epoch=4
06/03/2022 08:35:57 - INFO - __main__ - Step 50 Global step 50 Train loss 12.145315 on epoch=6
06/03/2022 08:36:02 - INFO - __main__ - Global step 50 Train loss 16.466515 ACC 0.008064516129032258 on epoch=6
06/03/2022 08:36:08 - INFO - __main__ - Step 60 Global step 60 Train loss 10.315600 on epoch=7
06/03/2022 08:36:13 - INFO - __main__ - Step 70 Global step 70 Train loss 10.194040 on epoch=8
06/03/2022 08:36:18 - INFO - __main__ - Step 80 Global step 80 Train loss 9.091605 on epoch=9
06/03/2022 08:36:23 - INFO - __main__ - Step 90 Global step 90 Train loss 9.522047 on epoch=11
06/03/2022 08:36:28 - INFO - __main__ - Step 100 Global step 100 Train loss 8.131006 on epoch=12
06/03/2022 08:36:31 - INFO - __main__ - Global step 100 Train loss 9.450860 ACC 0.0 on epoch=12
06/03/2022 08:36:36 - INFO - __main__ - Step 110 Global step 110 Train loss 7.738425 on epoch=13
06/03/2022 08:36:41 - INFO - __main__ - Step 120 Global step 120 Train loss 7.625731 on epoch=14
06/03/2022 08:36:46 - INFO - __main__ - Step 130 Global step 130 Train loss 7.811281 on epoch=16
06/03/2022 08:36:52 - INFO - __main__ - Step 140 Global step 140 Train loss 6.580444 on epoch=17
06/03/2022 08:36:57 - INFO - __main__ - Step 150 Global step 150 Train loss 6.274472 on epoch=18
06/03/2022 08:37:00 - INFO - __main__ - Global step 150 Train loss 7.206070 ACC 0.0 on epoch=18
06/03/2022 08:37:05 - INFO - __main__ - Step 160 Global step 160 Train loss 6.515752 on epoch=19
06/03/2022 08:37:10 - INFO - __main__ - Step 170 Global step 170 Train loss 5.852921 on epoch=21
06/03/2022 08:37:15 - INFO - __main__ - Step 180 Global step 180 Train loss 4.935926 on epoch=22
06/03/2022 08:37:20 - INFO - __main__ - Step 190 Global step 190 Train loss 4.321943 on epoch=23
06/03/2022 08:37:25 - INFO - __main__ - Step 200 Global step 200 Train loss 4.047030 on epoch=24
06/03/2022 08:37:28 - INFO - __main__ - Global step 200 Train loss 5.134714 ACC 0.0 on epoch=24
06/03/2022 08:37:33 - INFO - __main__ - Step 210 Global step 210 Train loss 4.326356 on epoch=26
06/03/2022 08:37:38 - INFO - __main__ - Step 220 Global step 220 Train loss 3.335590 on epoch=27
06/03/2022 08:37:43 - INFO - __main__ - Step 230 Global step 230 Train loss 2.953431 on epoch=28
06/03/2022 08:37:49 - INFO - __main__ - Step 240 Global step 240 Train loss 2.466353 on epoch=29
06/03/2022 08:37:54 - INFO - __main__ - Step 250 Global step 250 Train loss 2.169413 on epoch=31
06/03/2022 08:37:56 - INFO - __main__ - Global step 250 Train loss 3.050229 ACC 0.47580645161290325 on epoch=31
06/03/2022 08:38:02 - INFO - __main__ - Step 260 Global step 260 Train loss 2.285699 on epoch=32
06/03/2022 08:38:07 - INFO - __main__ - Step 270 Global step 270 Train loss 2.331217 on epoch=33
06/03/2022 08:38:12 - INFO - __main__ - Step 280 Global step 280 Train loss 2.481318 on epoch=34
06/03/2022 08:38:17 - INFO - __main__ - Step 290 Global step 290 Train loss 2.097777 on epoch=36
06/03/2022 08:38:22 - INFO - __main__ - Step 300 Global step 300 Train loss 1.862069 on epoch=37
06/03/2022 08:38:25 - INFO - __main__ - Global step 300 Train loss 2.211616 ACC 0.47580645161290325 on epoch=37
06/03/2022 08:38:30 - INFO - __main__ - Step 310 Global step 310 Train loss 1.930133 on epoch=38
06/03/2022 08:38:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.835759 on epoch=39
06/03/2022 08:38:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.713733 on epoch=41
06/03/2022 08:38:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.477214 on epoch=42
06/03/2022 08:38:50 - INFO - __main__ - Step 350 Global step 350 Train loss 2.247995 on epoch=43
06/03/2022 08:38:53 - INFO - __main__ - Global step 350 Train loss 1.840966 ACC 0.47580645161290325 on epoch=43
06/03/2022 08:38:58 - INFO - __main__ - Step 360 Global step 360 Train loss 1.561691 on epoch=44
06/03/2022 08:39:03 - INFO - __main__ - Step 370 Global step 370 Train loss 1.606722 on epoch=46
06/03/2022 08:39:08 - INFO - __main__ - Step 380 Global step 380 Train loss 1.563961 on epoch=47
06/03/2022 08:39:14 - INFO - __main__ - Step 390 Global step 390 Train loss 1.220363 on epoch=48
06/03/2022 08:39:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.301561 on epoch=49
06/03/2022 08:39:21 - INFO - __main__ - Global step 400 Train loss 1.450860 ACC 0.47580645161290325 on epoch=49
06/03/2022 08:39:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.297074 on epoch=51
06/03/2022 08:39:31 - INFO - __main__ - Step 420 Global step 420 Train loss 1.605648 on epoch=52
06/03/2022 08:39:37 - INFO - __main__ - Step 430 Global step 430 Train loss 1.406116 on epoch=53
06/03/2022 08:39:42 - INFO - __main__ - Step 440 Global step 440 Train loss 1.614182 on epoch=54
06/03/2022 08:39:47 - INFO - __main__ - Step 450 Global step 450 Train loss 1.318304 on epoch=56
06/03/2022 08:39:49 - INFO - __main__ - Global step 450 Train loss 1.448265 ACC 0.47580645161290325 on epoch=56
06/03/2022 08:39:55 - INFO - __main__ - Step 460 Global step 460 Train loss 1.596536 on epoch=57
06/03/2022 08:40:00 - INFO - __main__ - Step 470 Global step 470 Train loss 1.198012 on epoch=58
06/03/2022 08:40:05 - INFO - __main__ - Step 480 Global step 480 Train loss 1.308703 on epoch=59
06/03/2022 08:40:10 - INFO - __main__ - Step 490 Global step 490 Train loss 1.004743 on epoch=61
06/03/2022 08:40:15 - INFO - __main__ - Step 500 Global step 500 Train loss 1.091895 on epoch=62
06/03/2022 08:40:18 - INFO - __main__ - Global step 500 Train loss 1.239978 ACC 0.47580645161290325 on epoch=62
06/03/2022 08:40:23 - INFO - __main__ - Step 510 Global step 510 Train loss 1.198483 on epoch=63
06/03/2022 08:40:28 - INFO - __main__ - Step 520 Global step 520 Train loss 1.100835 on epoch=64
06/03/2022 08:40:33 - INFO - __main__ - Step 530 Global step 530 Train loss 1.278750 on epoch=66
06/03/2022 08:40:38 - INFO - __main__ - Step 540 Global step 540 Train loss 1.121988 on epoch=67
06/03/2022 08:40:44 - INFO - __main__ - Step 550 Global step 550 Train loss 1.007474 on epoch=68
06/03/2022 08:40:46 - INFO - __main__ - Global step 550 Train loss 1.141506 ACC 0.47580645161290325 on epoch=68
06/03/2022 08:40:51 - INFO - __main__ - Step 560 Global step 560 Train loss 1.014981 on epoch=69
06/03/2022 08:40:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.991333 on epoch=71
06/03/2022 08:41:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.983696 on epoch=72
06/03/2022 08:41:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.864896 on epoch=73
06/03/2022 08:41:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.798172 on epoch=74
06/03/2022 08:41:14 - INFO - __main__ - Global step 600 Train loss 0.930616 ACC 0.47580645161290325 on epoch=74
06/03/2022 08:41:19 - INFO - __main__ - Step 610 Global step 610 Train loss 0.872769 on epoch=76
06/03/2022 08:41:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.775498 on epoch=77
06/03/2022 08:41:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.971370 on epoch=78
06/03/2022 08:41:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.792283 on epoch=79
06/03/2022 08:41:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.715006 on epoch=81
06/03/2022 08:41:42 - INFO - __main__ - Global step 650 Train loss 0.825385 ACC 0.47580645161290325 on epoch=81
06/03/2022 08:41:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.813572 on epoch=82
06/03/2022 08:41:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.743301 on epoch=83
06/03/2022 08:41:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.739770 on epoch=84
06/03/2022 08:42:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.724297 on epoch=86
06/03/2022 08:42:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.712142 on epoch=87
06/03/2022 08:42:10 - INFO - __main__ - Global step 700 Train loss 0.746616 ACC 0.47580645161290325 on epoch=87
06/03/2022 08:42:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.660679 on epoch=88
06/03/2022 08:42:21 - INFO - __main__ - Step 720 Global step 720 Train loss 0.555745 on epoch=89
06/03/2022 08:42:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.431903 on epoch=91
06/03/2022 08:42:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.685442 on epoch=92
06/03/2022 08:42:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.645291 on epoch=93
06/03/2022 08:42:39 - INFO - __main__ - Global step 750 Train loss 0.595812 ACC 0.47580645161290325 on epoch=93
06/03/2022 08:42:44 - INFO - __main__ - Step 760 Global step 760 Train loss 0.677563 on epoch=94
06/03/2022 08:42:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.563867 on epoch=96
06/03/2022 08:42:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.535091 on epoch=97
06/03/2022 08:42:59 - INFO - __main__ - Step 790 Global step 790 Train loss 0.542188 on epoch=98
06/03/2022 08:43:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.543088 on epoch=99
06/03/2022 08:43:07 - INFO - __main__ - Global step 800 Train loss 0.572359 ACC 0.5725806451612904 on epoch=99
06/03/2022 08:43:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.584620 on epoch=101
06/03/2022 08:43:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.433367 on epoch=102
06/03/2022 08:43:23 - INFO - __main__ - Step 830 Global step 830 Train loss 0.394024 on epoch=103
06/03/2022 08:43:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.496074 on epoch=104
06/03/2022 08:43:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.538230 on epoch=106
06/03/2022 08:43:36 - INFO - __main__ - Global step 850 Train loss 0.489263 ACC 0.6129032258064516 on epoch=106
06/03/2022 08:43:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.469350 on epoch=107
06/03/2022 08:43:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.536472 on epoch=108
06/03/2022 08:43:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.462011 on epoch=109
06/03/2022 08:43:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.538683 on epoch=111
06/03/2022 08:44:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.458176 on epoch=112
06/03/2022 08:44:05 - INFO - __main__ - Global step 900 Train loss 0.492939 ACC 0.47580645161290325 on epoch=112
06/03/2022 08:44:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.500994 on epoch=113
06/03/2022 08:44:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.407194 on epoch=114
06/03/2022 08:44:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.452051 on epoch=116
06/03/2022 08:44:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.395089 on epoch=117
06/03/2022 08:44:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.388473 on epoch=118
06/03/2022 08:44:34 - INFO - __main__ - Global step 950 Train loss 0.428760 ACC 0.5080645161290323 on epoch=118
06/03/2022 08:44:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.491605 on epoch=119
06/03/2022 08:44:44 - INFO - __main__ - Step 970 Global step 970 Train loss 0.374471 on epoch=121
06/03/2022 08:44:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.480900 on epoch=122
06/03/2022 08:44:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.461427 on epoch=123
06/03/2022 08:45:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.549595 on epoch=124
06/03/2022 08:45:01 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:45:01 - INFO - __main__ - Printing 3 examples
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:45:01 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:45:01 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:45:01 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:45:01 - INFO - __main__ - Printing 3 examples
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:45:01 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:01 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:45:01 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:45:01 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:45:03 - INFO - __main__ - Global step 1000 Train loss 0.471599 ACC 0.46774193548387094 on epoch=124
06/03/2022 08:45:03 - INFO - __main__ - save last model!
06/03/2022 08:45:09 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:45:10 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:45:10 - INFO - __main__ - Printing 3 examples
06/03/2022 08:45:10 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:45:10 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:10 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:45:10 - INFO - __main__ - ['neutral']
06/03/2022 08:45:10 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:45:10 - INFO - __main__ - ['entailment']
06/03/2022 08:45:10 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:45:10 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:45:10 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:45:12 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_21_0.0002_8_predictions.txt
06/03/2022 08:45:12 - INFO - __main__ - ACC on test data: 0.6964
06/03/2022 08:45:12 - INFO - __main__ - prefix=superglue-cb_128_21, lr=0.0002, bsz=8, dev_performance=0.6129032258064516, test_performance=0.6964285714285714
06/03/2022 08:45:12 - INFO - __main__ - Running ... prefix=superglue-cb_128_21, lr=0.0001, bsz=8 ...
06/03/2022 08:45:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:45:12 - INFO - __main__ - Starting training!
06/03/2022 08:45:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:45:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: A: uh, but then when you quantify things and might also hold criminal trials for how many years is appropriate, uh, that they might leave it to somebody else who, uh, has expertise in that. B: Right, I agree, too. I don't think the jury should be the ones that put the sentencings down. [SEP] hypothesis: the jury should be the ones that put the sentencings down
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: B: Well, that's kind of the way I feel about rock and roll sometimes, too, I guess. They don't really, has kind of the same sound over and over, and the other thing I don't like about it is they have a tendency to play the instrumental so loud that you can't understand what the lyrics are A: Um. Right. B: you can't understand what they're saying on some of those songs which probably is just as well on some of them, too. A: Yeah. And I can't say that I like a lot of the very modern, uh, rock and roll, [SEP] hypothesis: she likes a lot of the very modern rock and roll
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: B: I think that not only applies inside the public school system, but in society itself. there's been too much negative reinforcement. How much, like, the caught being good slips. How about, just the John Q citizen out there on the street? A: Yeah, well that's true. I think, really though, I mean, that's one thing that, I mean, my kids definitely get spanked when they need to be spanked. But I really do try to use positive, uh, reinforcement with them at home, also. And it really helps. And I mean, they don't get spanked very often, but they do when they deserve it, you know. But, uh, I don't think any kid should be exempt from being spanked. I mean, I think I wouldn't mind if a teacher spanked my child. But, you know, that's just my personal opinion, and that's not going to, I mean, I don't think that law will ever change. [SEP] hypothesis: the law will change
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:45:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:45:13 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:45:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:45:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: B: And, uh, I think they've all developed kind of an interest in reading also. A: That's re-, yeah. B: I'm not saying they read all the right things [SEP] hypothesis: they read all the right things
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: A: Oh, yes. Animals have a way of talking. B: Alfie did. I tell you if I could have gotten a hold of that cat that day. A: I don't know uh, that I'd trade my dog in for the world. [SEP] hypothesis: he would trade his dog in for the world
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ -  [superglue-cb] premise: B: So, let's talk about the, uh, wonderful abuses in the State of Pennsylvania of personal property taxes whereby you can purchase something mail order and after the fact, the State of Pennsylvania can find out about it and send you a bill for the sales tax appropriate to that item that you purchased as well as interest and penalties from the time that you bought it. What do you think? Is Pennsylvania kind of out of line there? A: Well, actually, I do n't think they're out of line. [SEP] hypothesis: they're out of line
06/03/2022 08:45:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:45:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:45:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:45:13 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:45:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:45:26 - INFO - __main__ - Starting training!
06/03/2022 08:45:30 - INFO - __main__ - Step 10 Global step 10 Train loss 24.896042 on epoch=1
06/03/2022 08:45:35 - INFO - __main__ - Step 20 Global step 20 Train loss 21.943367 on epoch=2
06/03/2022 08:45:40 - INFO - __main__ - Step 30 Global step 30 Train loss 16.496428 on epoch=3
06/03/2022 08:45:45 - INFO - __main__ - Step 40 Global step 40 Train loss 12.900431 on epoch=4
06/03/2022 08:45:50 - INFO - __main__ - Step 50 Global step 50 Train loss 11.633308 on epoch=6
06/03/2022 08:46:08 - INFO - __main__ - Global step 50 Train loss 17.573915 ACC 0.3064516129032258 on epoch=6
06/03/2022 08:46:14 - INFO - __main__ - Step 60 Global step 60 Train loss 10.624983 on epoch=7
06/03/2022 08:46:19 - INFO - __main__ - Step 70 Global step 70 Train loss 9.956608 on epoch=8
06/03/2022 08:46:24 - INFO - __main__ - Step 80 Global step 80 Train loss 9.462491 on epoch=9
06/03/2022 08:46:29 - INFO - __main__ - Step 90 Global step 90 Train loss 10.087315 on epoch=11
06/03/2022 08:46:35 - INFO - __main__ - Step 100 Global step 100 Train loss 9.623092 on epoch=12
06/03/2022 08:46:38 - INFO - __main__ - Global step 100 Train loss 9.950898 ACC 0.0 on epoch=12
06/03/2022 08:46:43 - INFO - __main__ - Step 110 Global step 110 Train loss 9.658189 on epoch=13
06/03/2022 08:46:48 - INFO - __main__ - Step 120 Global step 120 Train loss 9.379334 on epoch=14
06/03/2022 08:46:53 - INFO - __main__ - Step 130 Global step 130 Train loss 8.693045 on epoch=16
06/03/2022 08:46:59 - INFO - __main__ - Step 140 Global step 140 Train loss 8.887505 on epoch=17
06/03/2022 08:47:04 - INFO - __main__ - Step 150 Global step 150 Train loss 8.064724 on epoch=18
06/03/2022 08:47:07 - INFO - __main__ - Global step 150 Train loss 8.936560 ACC 0.0 on epoch=18
06/03/2022 08:47:12 - INFO - __main__ - Step 160 Global step 160 Train loss 7.636374 on epoch=19
06/03/2022 08:47:17 - INFO - __main__ - Step 170 Global step 170 Train loss 8.476899 on epoch=21
06/03/2022 08:47:22 - INFO - __main__ - Step 180 Global step 180 Train loss 8.773408 on epoch=22
06/03/2022 08:47:28 - INFO - __main__ - Step 190 Global step 190 Train loss 7.922494 on epoch=23
06/03/2022 08:47:33 - INFO - __main__ - Step 200 Global step 200 Train loss 7.459546 on epoch=24
06/03/2022 08:47:36 - INFO - __main__ - Global step 200 Train loss 8.053743 ACC 0.0 on epoch=24
06/03/2022 08:47:41 - INFO - __main__ - Step 210 Global step 210 Train loss 7.480020 on epoch=26
06/03/2022 08:47:46 - INFO - __main__ - Step 220 Global step 220 Train loss 7.467027 on epoch=27
06/03/2022 08:47:51 - INFO - __main__ - Step 230 Global step 230 Train loss 7.041055 on epoch=28
06/03/2022 08:47:56 - INFO - __main__ - Step 240 Global step 240 Train loss 6.375134 on epoch=29
06/03/2022 08:48:01 - INFO - __main__ - Step 250 Global step 250 Train loss 6.701440 on epoch=31
06/03/2022 08:48:04 - INFO - __main__ - Global step 250 Train loss 7.012935 ACC 0.0 on epoch=31
06/03/2022 08:48:09 - INFO - __main__ - Step 260 Global step 260 Train loss 6.308570 on epoch=32
06/03/2022 08:48:14 - INFO - __main__ - Step 270 Global step 270 Train loss 6.117774 on epoch=33
06/03/2022 08:48:20 - INFO - __main__ - Step 280 Global step 280 Train loss 6.161874 on epoch=34
06/03/2022 08:48:25 - INFO - __main__ - Step 290 Global step 290 Train loss 5.543918 on epoch=36
06/03/2022 08:48:30 - INFO - __main__ - Step 300 Global step 300 Train loss 5.254680 on epoch=37
06/03/2022 08:48:32 - INFO - __main__ - Global step 300 Train loss 5.877363 ACC 0.0 on epoch=37
06/03/2022 08:48:38 - INFO - __main__ - Step 310 Global step 310 Train loss 5.204601 on epoch=38
06/03/2022 08:48:43 - INFO - __main__ - Step 320 Global step 320 Train loss 5.421760 on epoch=39
06/03/2022 08:48:48 - INFO - __main__ - Step 330 Global step 330 Train loss 5.253371 on epoch=41
06/03/2022 08:48:53 - INFO - __main__ - Step 340 Global step 340 Train loss 4.069694 on epoch=42
06/03/2022 08:48:58 - INFO - __main__ - Step 350 Global step 350 Train loss 3.572673 on epoch=43
06/03/2022 08:49:01 - INFO - __main__ - Global step 350 Train loss 4.704420 ACC 0.3709677419354839 on epoch=43
06/03/2022 08:49:07 - INFO - __main__ - Step 360 Global step 360 Train loss 3.557974 on epoch=44
06/03/2022 08:49:12 - INFO - __main__ - Step 370 Global step 370 Train loss 3.629202 on epoch=46
06/03/2022 08:49:17 - INFO - __main__ - Step 380 Global step 380 Train loss 3.090791 on epoch=47
06/03/2022 08:49:22 - INFO - __main__ - Step 390 Global step 390 Train loss 1.343631 on epoch=48
06/03/2022 08:49:27 - INFO - __main__ - Step 400 Global step 400 Train loss 1.008834 on epoch=49
06/03/2022 08:49:29 - INFO - __main__ - Global step 400 Train loss 2.526086 ACC 0.47580645161290325 on epoch=49
06/03/2022 08:49:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.381618 on epoch=51
06/03/2022 08:49:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.427457 on epoch=52
06/03/2022 08:49:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.564556 on epoch=53
06/03/2022 08:49:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.381137 on epoch=54
06/03/2022 08:49:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.769209 on epoch=56
06/03/2022 08:49:58 - INFO - __main__ - Global step 450 Train loss 0.504795 ACC 0.5 on epoch=56
06/03/2022 08:50:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.556280 on epoch=57
06/03/2022 08:50:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.313425 on epoch=58
06/03/2022 08:50:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.229622 on epoch=59
06/03/2022 08:50:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.244907 on epoch=61
06/03/2022 08:50:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.221873 on epoch=62
06/03/2022 08:50:27 - INFO - __main__ - Global step 500 Train loss 0.313222 ACC 0.5887096774193549 on epoch=62
06/03/2022 08:50:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.232495 on epoch=63
06/03/2022 08:50:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.155861 on epoch=64
06/03/2022 08:50:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.167658 on epoch=66
06/03/2022 08:50:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.267350 on epoch=67
06/03/2022 08:50:53 - INFO - __main__ - Step 550 Global step 550 Train loss 0.949197 on epoch=68
06/03/2022 08:50:56 - INFO - __main__ - Global step 550 Train loss 0.354512 ACC 0.6532258064516129 on epoch=68
06/03/2022 08:51:02 - INFO - __main__ - Step 560 Global step 560 Train loss 0.376484 on epoch=69
06/03/2022 08:51:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.349074 on epoch=71
06/03/2022 08:51:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.227097 on epoch=72
06/03/2022 08:51:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.172652 on epoch=73
06/03/2022 08:51:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.365137 on epoch=74
06/03/2022 08:51:25 - INFO - __main__ - Global step 600 Train loss 0.298089 ACC 0.7258064516129032 on epoch=74
06/03/2022 08:51:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.549165 on epoch=76
06/03/2022 08:51:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.243065 on epoch=77
06/03/2022 08:51:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.262849 on epoch=78
06/03/2022 08:51:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.519367 on epoch=79
06/03/2022 08:51:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.401139 on epoch=81
06/03/2022 08:51:54 - INFO - __main__ - Global step 650 Train loss 0.395117 ACC 0.7661290322580645 on epoch=81
06/03/2022 08:52:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.205391 on epoch=82
06/03/2022 08:52:05 - INFO - __main__ - Step 670 Global step 670 Train loss 0.172737 on epoch=83
06/03/2022 08:52:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.115245 on epoch=84
06/03/2022 08:52:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.217141 on epoch=86
06/03/2022 08:52:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.124788 on epoch=87
06/03/2022 08:52:23 - INFO - __main__ - Global step 700 Train loss 0.167060 ACC 0.7741935483870968 on epoch=87
06/03/2022 08:52:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.112780 on epoch=88
06/03/2022 08:52:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.149862 on epoch=89
06/03/2022 08:52:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.052389 on epoch=91
06/03/2022 08:52:44 - INFO - __main__ - Step 740 Global step 740 Train loss 0.160675 on epoch=92
06/03/2022 08:52:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.105419 on epoch=93
06/03/2022 08:52:52 - INFO - __main__ - Global step 750 Train loss 0.116225 ACC 0.782258064516129 on epoch=93
06/03/2022 08:52:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.061538 on epoch=94
06/03/2022 08:53:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.029158 on epoch=96
06/03/2022 08:53:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.071218 on epoch=97
06/03/2022 08:53:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.193061 on epoch=98
06/03/2022 08:53:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.103355 on epoch=99
06/03/2022 08:53:21 - INFO - __main__ - Global step 800 Train loss 0.091666 ACC 0.8225806451612904 on epoch=99
06/03/2022 08:53:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.005717 on epoch=101
06/03/2022 08:53:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.018772 on epoch=102
06/03/2022 08:53:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.112253 on epoch=103
06/03/2022 08:53:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.041723 on epoch=104
06/03/2022 08:53:47 - INFO - __main__ - Step 850 Global step 850 Train loss 0.002976 on epoch=106
06/03/2022 08:53:50 - INFO - __main__ - Global step 850 Train loss 0.036288 ACC 0.8306451612903226 on epoch=106
06/03/2022 08:53:56 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003261 on epoch=107
06/03/2022 08:54:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.131731 on epoch=108
06/03/2022 08:54:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.120322 on epoch=109
06/03/2022 08:54:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.159808 on epoch=111
06/03/2022 08:54:16 - INFO - __main__ - Step 900 Global step 900 Train loss 0.918616 on epoch=112
06/03/2022 08:54:19 - INFO - __main__ - Global step 900 Train loss 0.266748 ACC 0.8225806451612904 on epoch=112
06/03/2022 08:54:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.949224 on epoch=113
06/03/2022 08:54:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.309299 on epoch=114
06/03/2022 08:54:34 - INFO - __main__ - Step 930 Global step 930 Train loss 2.234652 on epoch=116
06/03/2022 08:54:39 - INFO - __main__ - Step 940 Global step 940 Train loss 2.110525 on epoch=117
06/03/2022 08:54:44 - INFO - __main__ - Step 950 Global step 950 Train loss 2.048597 on epoch=118
06/03/2022 08:54:47 - INFO - __main__ - Global step 950 Train loss 1.530459 ACC 0.47580645161290325 on epoch=118
06/03/2022 08:54:52 - INFO - __main__ - Step 960 Global step 960 Train loss 2.349224 on epoch=119
06/03/2022 08:54:57 - INFO - __main__ - Step 970 Global step 970 Train loss 1.605585 on epoch=121
06/03/2022 08:55:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.843414 on epoch=122
06/03/2022 08:55:07 - INFO - __main__ - Step 990 Global step 990 Train loss 1.631601 on epoch=123
06/03/2022 08:55:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.754908 on epoch=124
06/03/2022 08:55:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:55:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:55:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:55:13 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:55:13 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:55:13 - INFO - __main__ - Printing 3 examples
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 08:55:13 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:13 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:55:13 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:55:13 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:55:14 - INFO - __main__ - Global step 1000 Train loss 1.836946 ACC 0.47580645161290325 on epoch=124
06/03/2022 08:55:14 - INFO - __main__ - save last model!
06/03/2022 08:55:21 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 08:55:22 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 08:55:22 - INFO - __main__ - Printing 3 examples
06/03/2022 08:55:22 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 08:55:22 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:22 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 08:55:22 - INFO - __main__ - ['neutral']
06/03/2022 08:55:22 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 08:55:22 - INFO - __main__ - ['entailment']
06/03/2022 08:55:22 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:55:22 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:55:22 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 08:55:23 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_21_0.0001_8_predictions.txt
06/03/2022 08:55:23 - INFO - __main__ - ACC on test data: 0.8214
06/03/2022 08:55:24 - INFO - __main__ - prefix=superglue-cb_128_21, lr=0.0001, bsz=8, dev_performance=0.8306451612903226, test_performance=0.8214285714285714
06/03/2022 08:55:24 - INFO - __main__ - Running ... prefix=superglue-cb_128_42, lr=0.0005, bsz=8 ...
06/03/2022 08:55:25 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:55:25 - INFO - __main__ - Printing 3 examples
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:55:25 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:55:25 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 08:55:25 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 08:55:25 - INFO - __main__ - Printing 3 examples
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 08:55:25 - INFO - __main__ - ['contradiction']
06/03/2022 08:55:25 - INFO - __main__ - Tokenizing Input ...
06/03/2022 08:55:25 - INFO - __main__ - Tokenizing Output ...
06/03/2022 08:55:25 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 08:55:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:55:26 - INFO - __main__ - Starting training!
06/03/2022 08:55:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 08:55:36 - INFO - __main__ - Starting training!
06/03/2022 08:55:41 - INFO - __main__ - Step 10 Global step 10 Train loss 24.329372 on epoch=1
06/03/2022 08:55:45 - INFO - __main__ - Step 20 Global step 20 Train loss 19.601414 on epoch=2
06/03/2022 08:55:50 - INFO - __main__ - Step 30 Global step 30 Train loss 14.734365 on epoch=3
06/03/2022 08:55:55 - INFO - __main__ - Step 40 Global step 40 Train loss 9.692529 on epoch=4
06/03/2022 08:56:00 - INFO - __main__ - Step 50 Global step 50 Train loss 9.360460 on epoch=6
06/03/2022 08:56:02 - INFO - __main__ - Global step 50 Train loss 15.543629 ACC 0.0 on epoch=6
06/03/2022 08:56:08 - INFO - __main__ - Step 60 Global step 60 Train loss 7.414771 on epoch=7
06/03/2022 08:56:13 - INFO - __main__ - Step 70 Global step 70 Train loss 6.769616 on epoch=8
06/03/2022 08:56:18 - INFO - __main__ - Step 80 Global step 80 Train loss 5.748357 on epoch=9
06/03/2022 08:56:23 - INFO - __main__ - Step 90 Global step 90 Train loss 5.369400 on epoch=11
06/03/2022 08:56:29 - INFO - __main__ - Step 100 Global step 100 Train loss 3.386459 on epoch=12
06/03/2022 08:56:31 - INFO - __main__ - Global step 100 Train loss 5.737720 ACC 0.4596774193548387 on epoch=12
06/03/2022 08:56:37 - INFO - __main__ - Step 110 Global step 110 Train loss 2.636419 on epoch=13
06/03/2022 08:56:42 - INFO - __main__ - Step 120 Global step 120 Train loss 1.433589 on epoch=14
06/03/2022 08:56:48 - INFO - __main__ - Step 130 Global step 130 Train loss 1.617789 on epoch=16
06/03/2022 08:56:53 - INFO - __main__ - Step 140 Global step 140 Train loss 1.396064 on epoch=17
06/03/2022 08:56:58 - INFO - __main__ - Step 150 Global step 150 Train loss 2.028831 on epoch=18
06/03/2022 08:57:00 - INFO - __main__ - Global step 150 Train loss 1.822538 ACC 0.47580645161290325 on epoch=18
06/03/2022 08:57:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.249250 on epoch=19
06/03/2022 08:57:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.405522 on epoch=21
06/03/2022 08:57:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.434080 on epoch=22
06/03/2022 08:57:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.319407 on epoch=23
06/03/2022 08:57:27 - INFO - __main__ - Step 200 Global step 200 Train loss 1.166168 on epoch=24
06/03/2022 08:57:29 - INFO - __main__ - Global step 200 Train loss 1.314886 ACC 0.47580645161290325 on epoch=24
06/03/2022 08:57:35 - INFO - __main__ - Step 210 Global step 210 Train loss 1.260099 on epoch=26
06/03/2022 08:57:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.159452 on epoch=27
06/03/2022 08:57:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.978669 on epoch=28
06/03/2022 08:57:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.742310 on epoch=29
06/03/2022 08:57:55 - INFO - __main__ - Step 250 Global step 250 Train loss 1.001632 on epoch=31
06/03/2022 08:57:58 - INFO - __main__ - Global step 250 Train loss 1.028432 ACC 0.47580645161290325 on epoch=31
06/03/2022 08:58:03 - INFO - __main__ - Step 260 Global step 260 Train loss 0.633780 on epoch=32
06/03/2022 08:58:08 - INFO - __main__ - Step 270 Global step 270 Train loss 0.949138 on epoch=33
06/03/2022 08:58:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.736100 on epoch=34
06/03/2022 08:58:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.613139 on epoch=36
06/03/2022 08:58:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.543540 on epoch=37
06/03/2022 08:58:26 - INFO - __main__ - Global step 300 Train loss 0.695139 ACC 0.47580645161290325 on epoch=37
06/03/2022 08:58:31 - INFO - __main__ - Step 310 Global step 310 Train loss 0.685895 on epoch=38
06/03/2022 08:58:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.475955 on epoch=39
06/03/2022 08:58:41 - INFO - __main__ - Step 330 Global step 330 Train loss 0.524000 on epoch=41
06/03/2022 08:58:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.610850 on epoch=42
06/03/2022 08:58:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.613774 on epoch=43
06/03/2022 08:58:54 - INFO - __main__ - Global step 350 Train loss 0.582095 ACC 0.47580645161290325 on epoch=43
06/03/2022 08:58:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.479726 on epoch=44
06/03/2022 08:59:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.495101 on epoch=46
06/03/2022 08:59:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.551947 on epoch=47
06/03/2022 08:59:14 - INFO - __main__ - Step 390 Global step 390 Train loss 0.454403 on epoch=48
06/03/2022 08:59:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.467235 on epoch=49
06/03/2022 08:59:22 - INFO - __main__ - Global step 400 Train loss 0.489682 ACC 0.47580645161290325 on epoch=49
06/03/2022 08:59:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.470652 on epoch=51
06/03/2022 08:59:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.392283 on epoch=52
06/03/2022 08:59:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.470711 on epoch=53
06/03/2022 08:59:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.441298 on epoch=54
06/03/2022 08:59:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.407986 on epoch=56
06/03/2022 08:59:51 - INFO - __main__ - Global step 450 Train loss 0.436586 ACC 0.47580645161290325 on epoch=56
06/03/2022 08:59:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.413628 on epoch=57
06/03/2022 09:00:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.460734 on epoch=58
06/03/2022 09:00:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.461408 on epoch=59
06/03/2022 09:00:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.360627 on epoch=61
06/03/2022 09:00:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.338800 on epoch=62
06/03/2022 09:00:19 - INFO - __main__ - Global step 500 Train loss 0.407039 ACC 0.47580645161290325 on epoch=62
06/03/2022 09:00:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.325757 on epoch=63
06/03/2022 09:00:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.483318 on epoch=64
06/03/2022 09:00:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.353138 on epoch=66
06/03/2022 09:00:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.403416 on epoch=67
06/03/2022 09:00:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.385812 on epoch=68
06/03/2022 09:00:47 - INFO - __main__ - Global step 550 Train loss 0.390288 ACC 0.5 on epoch=68
06/03/2022 09:00:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.302715 on epoch=69
06/03/2022 09:00:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.344133 on epoch=71
06/03/2022 09:01:04 - INFO - __main__ - Step 580 Global step 580 Train loss 0.277077 on epoch=72
06/03/2022 09:01:09 - INFO - __main__ - Step 590 Global step 590 Train loss 0.327965 on epoch=73
06/03/2022 09:01:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.378682 on epoch=74
06/03/2022 09:01:17 - INFO - __main__ - Global step 600 Train loss 0.326115 ACC 0.5725806451612904 on epoch=74
06/03/2022 09:01:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.380562 on epoch=76
06/03/2022 09:01:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.306916 on epoch=77
06/03/2022 09:01:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.312132 on epoch=78
06/03/2022 09:01:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.330899 on epoch=79
06/03/2022 09:01:44 - INFO - __main__ - Step 650 Global step 650 Train loss 0.340088 on epoch=81
06/03/2022 09:01:46 - INFO - __main__ - Global step 650 Train loss 0.334119 ACC 0.47580645161290325 on epoch=81
06/03/2022 09:01:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.332015 on epoch=82
06/03/2022 09:01:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.345316 on epoch=83
06/03/2022 09:02:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.332970 on epoch=84
06/03/2022 09:02:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.338891 on epoch=86
06/03/2022 09:02:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.331578 on epoch=87
06/03/2022 09:02:14 - INFO - __main__ - Global step 700 Train loss 0.336154 ACC 0.47580645161290325 on epoch=87
06/03/2022 09:02:20 - INFO - __main__ - Step 710 Global step 710 Train loss 0.341990 on epoch=88
06/03/2022 09:02:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.339980 on epoch=89
06/03/2022 09:02:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.349678 on epoch=91
06/03/2022 09:02:35 - INFO - __main__ - Step 740 Global step 740 Train loss 0.357989 on epoch=92
06/03/2022 09:02:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.249944 on epoch=93
06/03/2022 09:02:43 - INFO - __main__ - Global step 750 Train loss 0.327916 ACC 0.4596774193548387 on epoch=93
06/03/2022 09:02:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.308352 on epoch=94
06/03/2022 09:02:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.334884 on epoch=96
06/03/2022 09:02:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.274598 on epoch=97
06/03/2022 09:03:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.301024 on epoch=98
06/03/2022 09:03:09 - INFO - __main__ - Step 800 Global step 800 Train loss 0.329409 on epoch=99
06/03/2022 09:03:12 - INFO - __main__ - Global step 800 Train loss 0.309653 ACC 0.45161290322580644 on epoch=99
06/03/2022 09:03:17 - INFO - __main__ - Step 810 Global step 810 Train loss 0.313803 on epoch=101
06/03/2022 09:03:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.305343 on epoch=102
06/03/2022 09:03:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.342164 on epoch=103
06/03/2022 09:03:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.292887 on epoch=104
06/03/2022 09:03:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.271218 on epoch=106
06/03/2022 09:03:40 - INFO - __main__ - Global step 850 Train loss 0.305083 ACC 0.47580645161290325 on epoch=106
06/03/2022 09:03:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.277872 on epoch=107
06/03/2022 09:03:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.257685 on epoch=108
06/03/2022 09:03:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.247251 on epoch=109
06/03/2022 09:04:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.275955 on epoch=111
06/03/2022 09:04:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.261705 on epoch=112
06/03/2022 09:04:08 - INFO - __main__ - Global step 900 Train loss 0.264093 ACC 0.4274193548387097 on epoch=112
06/03/2022 09:04:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.308448 on epoch=113
06/03/2022 09:04:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.283300 on epoch=114
06/03/2022 09:04:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.333471 on epoch=116
06/03/2022 09:04:29 - INFO - __main__ - Step 940 Global step 940 Train loss 0.274316 on epoch=117
06/03/2022 09:04:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.299265 on epoch=118
06/03/2022 09:04:36 - INFO - __main__ - Global step 950 Train loss 0.299760 ACC 0.4435483870967742 on epoch=118
06/03/2022 09:04:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.310732 on epoch=119
06/03/2022 09:04:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.292973 on epoch=121
06/03/2022 09:04:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.284520 on epoch=122
06/03/2022 09:04:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.273107 on epoch=123
06/03/2022 09:05:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.260834 on epoch=124
06/03/2022 09:05:03 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:05:03 - INFO - __main__ - Printing 3 examples
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:05:03 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:05:03 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:05:03 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:05:03 - INFO - __main__ - Printing 3 examples
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:05:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:03 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:05:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:05:04 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:05:05 - INFO - __main__ - Global step 1000 Train loss 0.284433 ACC 0.4435483870967742 on epoch=124
06/03/2022 09:05:05 - INFO - __main__ - save last model!
06/03/2022 09:05:12 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:05:12 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:05:12 - INFO - __main__ - Printing 3 examples
06/03/2022 09:05:12 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:05:12 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:12 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:05:12 - INFO - __main__ - ['neutral']
06/03/2022 09:05:12 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:05:12 - INFO - __main__ - ['entailment']
06/03/2022 09:05:12 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:05:12 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:05:12 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:05:14 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_42_0.0005_8_predictions.txt
06/03/2022 09:05:14 - INFO - __main__ - ACC on test data: 0.4107
06/03/2022 09:05:14 - INFO - __main__ - prefix=superglue-cb_128_42, lr=0.0005, bsz=8, dev_performance=0.5725806451612904, test_performance=0.4107142857142857
06/03/2022 09:05:14 - INFO - __main__ - Running ... prefix=superglue-cb_128_42, lr=0.0003, bsz=8 ...
06/03/2022 09:05:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:05:14 - INFO - __main__ - Starting training!
06/03/2022 09:05:15 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:05:15 - INFO - __main__ - Printing 3 examples
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:05:15 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:05:15 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:05:15 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:05:15 - INFO - __main__ - Printing 3 examples
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:05:15 - INFO - __main__ - ['contradiction']
06/03/2022 09:05:15 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:05:15 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:05:15 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:05:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:05:26 - INFO - __main__ - Starting training!
06/03/2022 09:05:31 - INFO - __main__ - Step 10 Global step 10 Train loss 23.037579 on epoch=1
06/03/2022 09:05:35 - INFO - __main__ - Step 20 Global step 20 Train loss 18.184490 on epoch=2
06/03/2022 09:05:40 - INFO - __main__ - Step 30 Global step 30 Train loss 13.746689 on epoch=3
06/03/2022 09:05:45 - INFO - __main__ - Step 40 Global step 40 Train loss 10.800135 on epoch=4
06/03/2022 09:05:50 - INFO - __main__ - Step 50 Global step 50 Train loss 9.598249 on epoch=6
06/03/2022 09:05:56 - INFO - __main__ - Global step 50 Train loss 15.073426 ACC 0.0967741935483871 on epoch=6
06/03/2022 09:06:02 - INFO - __main__ - Step 60 Global step 60 Train loss 8.884340 on epoch=7
06/03/2022 09:06:07 - INFO - __main__ - Step 70 Global step 70 Train loss 8.332042 on epoch=8
06/03/2022 09:06:12 - INFO - __main__ - Step 80 Global step 80 Train loss 7.505023 on epoch=9
06/03/2022 09:06:17 - INFO - __main__ - Step 90 Global step 90 Train loss 6.885162 on epoch=11
06/03/2022 09:06:22 - INFO - __main__ - Step 100 Global step 100 Train loss 6.766585 on epoch=12
06/03/2022 09:06:25 - INFO - __main__ - Global step 100 Train loss 7.674630 ACC 0.04838709677419355 on epoch=12
06/03/2022 09:06:30 - INFO - __main__ - Step 110 Global step 110 Train loss 5.850656 on epoch=13
06/03/2022 09:06:36 - INFO - __main__ - Step 120 Global step 120 Train loss 4.405980 on epoch=14
06/03/2022 09:06:41 - INFO - __main__ - Step 130 Global step 130 Train loss 4.273428 on epoch=16
06/03/2022 09:06:46 - INFO - __main__ - Step 140 Global step 140 Train loss 2.845149 on epoch=17
06/03/2022 09:06:51 - INFO - __main__ - Step 150 Global step 150 Train loss 2.650209 on epoch=18
06/03/2022 09:06:53 - INFO - __main__ - Global step 150 Train loss 4.005085 ACC 0.47580645161290325 on epoch=18
06/03/2022 09:06:59 - INFO - __main__ - Step 160 Global step 160 Train loss 1.712501 on epoch=19
06/03/2022 09:07:05 - INFO - __main__ - Step 170 Global step 170 Train loss 2.116272 on epoch=21
06/03/2022 09:07:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.926519 on epoch=22
06/03/2022 09:07:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.756452 on epoch=23
06/03/2022 09:07:20 - INFO - __main__ - Step 200 Global step 200 Train loss 1.791745 on epoch=24
06/03/2022 09:07:22 - INFO - __main__ - Global step 200 Train loss 1.860698 ACC 0.47580645161290325 on epoch=24
06/03/2022 09:07:28 - INFO - __main__ - Step 210 Global step 210 Train loss 1.538905 on epoch=26
06/03/2022 09:07:33 - INFO - __main__ - Step 220 Global step 220 Train loss 1.713079 on epoch=27
06/03/2022 09:07:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.700398 on epoch=28
06/03/2022 09:07:43 - INFO - __main__ - Step 240 Global step 240 Train loss 1.215932 on epoch=29
06/03/2022 09:07:48 - INFO - __main__ - Step 250 Global step 250 Train loss 1.623938 on epoch=31
06/03/2022 09:07:51 - INFO - __main__ - Global step 250 Train loss 1.558450 ACC 0.47580645161290325 on epoch=31
06/03/2022 09:07:56 - INFO - __main__ - Step 260 Global step 260 Train loss 1.520304 on epoch=32
06/03/2022 09:08:01 - INFO - __main__ - Step 270 Global step 270 Train loss 1.393824 on epoch=33
06/03/2022 09:08:06 - INFO - __main__ - Step 280 Global step 280 Train loss 1.137739 on epoch=34
06/03/2022 09:08:11 - INFO - __main__ - Step 290 Global step 290 Train loss 1.140885 on epoch=36
06/03/2022 09:08:16 - INFO - __main__ - Step 300 Global step 300 Train loss 1.344356 on epoch=37
06/03/2022 09:08:19 - INFO - __main__ - Global step 300 Train loss 1.307422 ACC 0.47580645161290325 on epoch=37
06/03/2022 09:08:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.886273 on epoch=38
06/03/2022 09:08:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.762239 on epoch=39
06/03/2022 09:08:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.842555 on epoch=41
06/03/2022 09:08:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.578323 on epoch=42
06/03/2022 09:08:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.711423 on epoch=43
06/03/2022 09:08:47 - INFO - __main__ - Global step 350 Train loss 0.756163 ACC 0.6774193548387096 on epoch=43
06/03/2022 09:08:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.880177 on epoch=44
06/03/2022 09:08:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.732272 on epoch=46
06/03/2022 09:09:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.716156 on epoch=47
06/03/2022 09:09:08 - INFO - __main__ - Step 390 Global step 390 Train loss 0.286426 on epoch=48
06/03/2022 09:09:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.231747 on epoch=49
06/03/2022 09:09:15 - INFO - __main__ - Global step 400 Train loss 0.569356 ACC 0.5483870967741935 on epoch=49
06/03/2022 09:09:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.130694 on epoch=51
06/03/2022 09:09:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.092417 on epoch=52
06/03/2022 09:09:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.067674 on epoch=53
06/03/2022 09:09:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.063721 on epoch=54
06/03/2022 09:09:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.038406 on epoch=56
06/03/2022 09:09:44 - INFO - __main__ - Global step 450 Train loss 0.078582 ACC 0.8709677419354839 on epoch=56
06/03/2022 09:09:50 - INFO - __main__ - Step 460 Global step 460 Train loss 0.027864 on epoch=57
06/03/2022 09:09:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.030699 on epoch=58
06/03/2022 09:10:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.023388 on epoch=59
06/03/2022 09:10:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.044487 on epoch=61
06/03/2022 09:10:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.027771 on epoch=62
06/03/2022 09:10:13 - INFO - __main__ - Global step 500 Train loss 0.030842 ACC 0.8467741935483871 on epoch=62
06/03/2022 09:10:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.030293 on epoch=63
06/03/2022 09:10:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.011659 on epoch=64
06/03/2022 09:10:28 - INFO - __main__ - Step 530 Global step 530 Train loss 0.016765 on epoch=66
06/03/2022 09:10:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.051425 on epoch=67
06/03/2022 09:10:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.166229 on epoch=68
06/03/2022 09:10:41 - INFO - __main__ - Global step 550 Train loss 0.055274 ACC 0.8467741935483871 on epoch=68
06/03/2022 09:10:47 - INFO - __main__ - Step 560 Global step 560 Train loss 0.117248 on epoch=69
06/03/2022 09:10:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.278444 on epoch=71
06/03/2022 09:10:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.556637 on epoch=72
06/03/2022 09:11:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.698009 on epoch=73
06/03/2022 09:11:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.610570 on epoch=74
06/03/2022 09:11:09 - INFO - __main__ - Global step 600 Train loss 0.452182 ACC 0.47580645161290325 on epoch=74
06/03/2022 09:11:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.558517 on epoch=76
06/03/2022 09:11:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.425371 on epoch=77
06/03/2022 09:11:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.815400 on epoch=78
06/03/2022 09:11:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.454779 on epoch=79
06/03/2022 09:11:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.402447 on epoch=81
06/03/2022 09:11:37 - INFO - __main__ - Global step 650 Train loss 0.531303 ACC 0.47580645161290325 on epoch=81
06/03/2022 09:11:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.398533 on epoch=82
06/03/2022 09:11:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.385212 on epoch=83
06/03/2022 09:11:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.340886 on epoch=84
06/03/2022 09:11:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.263620 on epoch=86
06/03/2022 09:12:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.286861 on epoch=87
06/03/2022 09:12:05 - INFO - __main__ - Global step 700 Train loss 0.335022 ACC 0.47580645161290325 on epoch=87
06/03/2022 09:12:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.287776 on epoch=88
06/03/2022 09:12:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.297382 on epoch=89
06/03/2022 09:12:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.273254 on epoch=91
06/03/2022 09:12:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.282122 on epoch=92
06/03/2022 09:12:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.295935 on epoch=93
06/03/2022 09:12:33 - INFO - __main__ - Global step 750 Train loss 0.287294 ACC 0.49193548387096775 on epoch=93
06/03/2022 09:12:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.296571 on epoch=94
06/03/2022 09:12:44 - INFO - __main__ - Step 770 Global step 770 Train loss 0.303248 on epoch=96
06/03/2022 09:12:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.274112 on epoch=97
06/03/2022 09:12:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.312516 on epoch=98
06/03/2022 09:12:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.289283 on epoch=99
06/03/2022 09:13:01 - INFO - __main__ - Global step 800 Train loss 0.295146 ACC 0.47580645161290325 on epoch=99
06/03/2022 09:13:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.237597 on epoch=101
06/03/2022 09:13:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.295588 on epoch=102
06/03/2022 09:13:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.273223 on epoch=103
06/03/2022 09:13:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.279417 on epoch=104
06/03/2022 09:13:27 - INFO - __main__ - Step 850 Global step 850 Train loss 0.273037 on epoch=106
06/03/2022 09:13:29 - INFO - __main__ - Global step 850 Train loss 0.271772 ACC 0.4838709677419355 on epoch=106
06/03/2022 09:13:34 - INFO - __main__ - Step 860 Global step 860 Train loss 0.266922 on epoch=107
06/03/2022 09:13:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.308841 on epoch=108
06/03/2022 09:13:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.277371 on epoch=109
06/03/2022 09:13:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.244045 on epoch=111
06/03/2022 09:13:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.241851 on epoch=112
06/03/2022 09:13:57 - INFO - __main__ - Global step 900 Train loss 0.267806 ACC 0.46774193548387094 on epoch=112
06/03/2022 09:14:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.242561 on epoch=113
06/03/2022 09:14:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.255355 on epoch=114
06/03/2022 09:14:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.269194 on epoch=116
06/03/2022 09:14:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.249900 on epoch=117
06/03/2022 09:14:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.244099 on epoch=118
06/03/2022 09:14:25 - INFO - __main__ - Global step 950 Train loss 0.252222 ACC 0.5806451612903226 on epoch=118
06/03/2022 09:14:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.241839 on epoch=119
06/03/2022 09:14:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.250853 on epoch=121
06/03/2022 09:14:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.235266 on epoch=122
06/03/2022 09:14:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.229969 on epoch=123
06/03/2022 09:14:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.267999 on epoch=124
06/03/2022 09:14:52 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:14:52 - INFO - __main__ - Printing 3 examples
06/03/2022 09:14:52 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:14:52 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:52 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:14:52 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:52 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:14:52 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:52 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:14:53 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:14:53 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:14:53 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:14:53 - INFO - __main__ - Printing 3 examples
06/03/2022 09:14:53 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:14:53 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:53 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:14:53 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:53 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:14:53 - INFO - __main__ - ['contradiction']
06/03/2022 09:14:53 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:14:53 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:14:53 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:14:53 - INFO - __main__ - Global step 1000 Train loss 0.245185 ACC 0.47580645161290325 on epoch=124
06/03/2022 09:14:53 - INFO - __main__ - save last model!
06/03/2022 09:15:00 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:15:01 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:15:01 - INFO - __main__ - Printing 3 examples
06/03/2022 09:15:01 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:15:01 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:01 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:15:01 - INFO - __main__ - ['neutral']
06/03/2022 09:15:01 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:15:01 - INFO - __main__ - ['entailment']
06/03/2022 09:15:01 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:15:01 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:15:01 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:15:02 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_42_0.0003_8_predictions.txt
06/03/2022 09:15:02 - INFO - __main__ - ACC on test data: 0.7500
06/03/2022 09:15:02 - INFO - __main__ - prefix=superglue-cb_128_42, lr=0.0003, bsz=8, dev_performance=0.8709677419354839, test_performance=0.75
06/03/2022 09:15:02 - INFO - __main__ - Running ... prefix=superglue-cb_128_42, lr=0.0002, bsz=8 ...
06/03/2022 09:15:03 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:15:03 - INFO - __main__ - Printing 3 examples
06/03/2022 09:15:03 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:15:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:03 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:15:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:03 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:15:03 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:03 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:15:03 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:15:04 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:15:04 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:15:04 - INFO - __main__ - Printing 3 examples
06/03/2022 09:15:04 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:15:04 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:04 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:15:04 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:04 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:15:04 - INFO - __main__ - ['contradiction']
06/03/2022 09:15:04 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:15:04 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:15:04 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:15:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:15:06 - INFO - __main__ - Starting training!
06/03/2022 09:15:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:15:16 - INFO - __main__ - Starting training!
06/03/2022 09:15:21 - INFO - __main__ - Step 10 Global step 10 Train loss 22.805380 on epoch=1
06/03/2022 09:15:25 - INFO - __main__ - Step 20 Global step 20 Train loss 19.746801 on epoch=2
06/03/2022 09:15:30 - INFO - __main__ - Step 30 Global step 30 Train loss 14.415311 on epoch=3
06/03/2022 09:15:35 - INFO - __main__ - Step 40 Global step 40 Train loss 12.076214 on epoch=4
06/03/2022 09:15:41 - INFO - __main__ - Step 50 Global step 50 Train loss 11.193189 on epoch=6
06/03/2022 09:15:46 - INFO - __main__ - Global step 50 Train loss 16.047379 ACC 0.06451612903225806 on epoch=6
06/03/2022 09:15:51 - INFO - __main__ - Step 60 Global step 60 Train loss 9.759787 on epoch=7
06/03/2022 09:15:56 - INFO - __main__ - Step 70 Global step 70 Train loss 9.581917 on epoch=8
06/03/2022 09:16:02 - INFO - __main__ - Step 80 Global step 80 Train loss 8.912157 on epoch=9
06/03/2022 09:16:07 - INFO - __main__ - Step 90 Global step 90 Train loss 9.175476 on epoch=11
06/03/2022 09:16:12 - INFO - __main__ - Step 100 Global step 100 Train loss 8.758622 on epoch=12
06/03/2022 09:16:15 - INFO - __main__ - Global step 100 Train loss 9.237592 ACC 0.03225806451612903 on epoch=12
06/03/2022 09:16:20 - INFO - __main__ - Step 110 Global step 110 Train loss 7.468966 on epoch=13
06/03/2022 09:16:25 - INFO - __main__ - Step 120 Global step 120 Train loss 7.428599 on epoch=14
06/03/2022 09:16:30 - INFO - __main__ - Step 130 Global step 130 Train loss 7.168803 on epoch=16
06/03/2022 09:16:35 - INFO - __main__ - Step 140 Global step 140 Train loss 6.914755 on epoch=17
06/03/2022 09:16:41 - INFO - __main__ - Step 150 Global step 150 Train loss 6.245860 on epoch=18
06/03/2022 09:16:44 - INFO - __main__ - Global step 150 Train loss 7.045396 ACC 0.16129032258064516 on epoch=18
06/03/2022 09:16:50 - INFO - __main__ - Step 160 Global step 160 Train loss 5.407754 on epoch=19
06/03/2022 09:16:56 - INFO - __main__ - Step 170 Global step 170 Train loss 5.597449 on epoch=21
06/03/2022 09:17:01 - INFO - __main__ - Step 180 Global step 180 Train loss 4.922542 on epoch=22
06/03/2022 09:17:06 - INFO - __main__ - Step 190 Global step 190 Train loss 3.511941 on epoch=23
06/03/2022 09:17:11 - INFO - __main__ - Step 200 Global step 200 Train loss 2.999677 on epoch=24
06/03/2022 09:17:15 - INFO - __main__ - Global step 200 Train loss 4.487873 ACC 0.04032258064516129 on epoch=24
06/03/2022 09:17:20 - INFO - __main__ - Step 210 Global step 210 Train loss 2.520607 on epoch=26
06/03/2022 09:17:25 - INFO - __main__ - Step 220 Global step 220 Train loss 2.478364 on epoch=27
06/03/2022 09:17:30 - INFO - __main__ - Step 230 Global step 230 Train loss 2.803358 on epoch=28
06/03/2022 09:17:35 - INFO - __main__ - Step 240 Global step 240 Train loss 2.286331 on epoch=29
06/03/2022 09:17:40 - INFO - __main__ - Step 250 Global step 250 Train loss 2.112719 on epoch=31
06/03/2022 09:17:43 - INFO - __main__ - Global step 250 Train loss 2.440276 ACC 0.47580645161290325 on epoch=31
06/03/2022 09:17:48 - INFO - __main__ - Step 260 Global step 260 Train loss 2.057560 on epoch=32
06/03/2022 09:17:53 - INFO - __main__ - Step 270 Global step 270 Train loss 2.314477 on epoch=33
06/03/2022 09:17:59 - INFO - __main__ - Step 280 Global step 280 Train loss 1.922084 on epoch=34
06/03/2022 09:18:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.968814 on epoch=36
06/03/2022 09:18:09 - INFO - __main__ - Step 300 Global step 300 Train loss 2.498478 on epoch=37
06/03/2022 09:18:11 - INFO - __main__ - Global step 300 Train loss 2.152282 ACC 0.47580645161290325 on epoch=37
06/03/2022 09:18:16 - INFO - __main__ - Step 310 Global step 310 Train loss 2.015643 on epoch=38
06/03/2022 09:18:22 - INFO - __main__ - Step 320 Global step 320 Train loss 1.530433 on epoch=39
06/03/2022 09:18:27 - INFO - __main__ - Step 330 Global step 330 Train loss 2.138134 on epoch=41
06/03/2022 09:18:32 - INFO - __main__ - Step 340 Global step 340 Train loss 1.718510 on epoch=42
06/03/2022 09:18:37 - INFO - __main__ - Step 350 Global step 350 Train loss 1.565153 on epoch=43
06/03/2022 09:18:39 - INFO - __main__ - Global step 350 Train loss 1.793575 ACC 0.47580645161290325 on epoch=43
06/03/2022 09:18:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.830705 on epoch=44
06/03/2022 09:18:50 - INFO - __main__ - Step 370 Global step 370 Train loss 1.789340 on epoch=46
06/03/2022 09:18:55 - INFO - __main__ - Step 380 Global step 380 Train loss 1.929226 on epoch=47
06/03/2022 09:19:00 - INFO - __main__ - Step 390 Global step 390 Train loss 1.650729 on epoch=48
06/03/2022 09:19:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.625177 on epoch=49
06/03/2022 09:19:08 - INFO - __main__ - Global step 400 Train loss 1.765035 ACC 0.47580645161290325 on epoch=49
06/03/2022 09:19:13 - INFO - __main__ - Step 410 Global step 410 Train loss 1.611605 on epoch=51
06/03/2022 09:19:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.394868 on epoch=52
06/03/2022 09:19:23 - INFO - __main__ - Step 430 Global step 430 Train loss 1.609208 on epoch=53
06/03/2022 09:19:28 - INFO - __main__ - Step 440 Global step 440 Train loss 1.338951 on epoch=54
06/03/2022 09:19:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.616414 on epoch=56
06/03/2022 09:19:36 - INFO - __main__ - Global step 450 Train loss 1.514209 ACC 0.47580645161290325 on epoch=56
06/03/2022 09:19:41 - INFO - __main__ - Step 460 Global step 460 Train loss 1.273677 on epoch=57
06/03/2022 09:19:46 - INFO - __main__ - Step 470 Global step 470 Train loss 1.252979 on epoch=58
06/03/2022 09:19:51 - INFO - __main__ - Step 480 Global step 480 Train loss 1.382121 on epoch=59
06/03/2022 09:19:56 - INFO - __main__ - Step 490 Global step 490 Train loss 1.626663 on epoch=61
06/03/2022 09:20:02 - INFO - __main__ - Step 500 Global step 500 Train loss 1.258719 on epoch=62
06/03/2022 09:20:04 - INFO - __main__ - Global step 500 Train loss 1.358832 ACC 0.47580645161290325 on epoch=62
06/03/2022 09:20:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.972355 on epoch=63
06/03/2022 09:20:14 - INFO - __main__ - Step 520 Global step 520 Train loss 0.862796 on epoch=64
06/03/2022 09:20:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.932858 on epoch=66
06/03/2022 09:20:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.959560 on epoch=67
06/03/2022 09:20:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.904988 on epoch=68
06/03/2022 09:20:32 - INFO - __main__ - Global step 550 Train loss 0.926511 ACC 0.47580645161290325 on epoch=68
06/03/2022 09:20:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.383156 on epoch=69
06/03/2022 09:20:43 - INFO - __main__ - Step 570 Global step 570 Train loss 1.040654 on epoch=71
06/03/2022 09:20:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.800470 on epoch=72
06/03/2022 09:20:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.799332 on epoch=73
06/03/2022 09:20:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.900520 on epoch=74
06/03/2022 09:21:00 - INFO - __main__ - Global step 600 Train loss 0.984826 ACC 0.47580645161290325 on epoch=74
06/03/2022 09:21:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.696437 on epoch=76
06/03/2022 09:21:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.774040 on epoch=77
06/03/2022 09:21:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.801238 on epoch=78
06/03/2022 09:21:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.796101 on epoch=79
06/03/2022 09:21:26 - INFO - __main__ - Step 650 Global step 650 Train loss 0.900570 on epoch=81
06/03/2022 09:21:28 - INFO - __main__ - Global step 650 Train loss 0.793677 ACC 0.47580645161290325 on epoch=81
06/03/2022 09:21:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.613384 on epoch=82
06/03/2022 09:21:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.822793 on epoch=83
06/03/2022 09:21:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.677895 on epoch=84
06/03/2022 09:21:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.647942 on epoch=86
06/03/2022 09:21:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.726803 on epoch=87
06/03/2022 09:21:56 - INFO - __main__ - Global step 700 Train loss 0.697764 ACC 0.47580645161290325 on epoch=87
06/03/2022 09:22:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.440777 on epoch=88
06/03/2022 09:22:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.721324 on epoch=89
06/03/2022 09:22:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.573517 on epoch=91
06/03/2022 09:22:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.564396 on epoch=92
06/03/2022 09:22:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.725706 on epoch=93
06/03/2022 09:22:24 - INFO - __main__ - Global step 750 Train loss 0.605144 ACC 0.47580645161290325 on epoch=93
06/03/2022 09:22:30 - INFO - __main__ - Step 760 Global step 760 Train loss 0.734129 on epoch=94
06/03/2022 09:22:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.491048 on epoch=96
06/03/2022 09:22:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.459419 on epoch=97
06/03/2022 09:22:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.483339 on epoch=98
06/03/2022 09:22:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.475037 on epoch=99
06/03/2022 09:22:53 - INFO - __main__ - Global step 800 Train loss 0.528595 ACC 0.47580645161290325 on epoch=99
06/03/2022 09:22:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.584780 on epoch=101
06/03/2022 09:23:03 - INFO - __main__ - Step 820 Global step 820 Train loss 0.634898 on epoch=102
06/03/2022 09:23:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.376152 on epoch=103
06/03/2022 09:23:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.656593 on epoch=104
06/03/2022 09:23:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.534856 on epoch=106
06/03/2022 09:23:21 - INFO - __main__ - Global step 850 Train loss 0.557456 ACC 0.47580645161290325 on epoch=106
06/03/2022 09:23:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.388384 on epoch=107
06/03/2022 09:23:31 - INFO - __main__ - Step 870 Global step 870 Train loss 0.459387 on epoch=108
06/03/2022 09:23:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.570270 on epoch=109
06/03/2022 09:23:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.375819 on epoch=111
06/03/2022 09:23:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.402392 on epoch=112
06/03/2022 09:23:49 - INFO - __main__ - Global step 900 Train loss 0.439250 ACC 0.47580645161290325 on epoch=112
06/03/2022 09:23:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.444484 on epoch=113
06/03/2022 09:23:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.503091 on epoch=114
06/03/2022 09:24:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.467462 on epoch=116
06/03/2022 09:24:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.460378 on epoch=117
06/03/2022 09:24:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.463299 on epoch=118
06/03/2022 09:24:17 - INFO - __main__ - Global step 950 Train loss 0.467743 ACC 0.47580645161290325 on epoch=118
06/03/2022 09:24:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.499240 on epoch=119
06/03/2022 09:24:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.401992 on epoch=121
06/03/2022 09:24:33 - INFO - __main__ - Step 980 Global step 980 Train loss 0.416476 on epoch=122
06/03/2022 09:24:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.512291 on epoch=123
06/03/2022 09:24:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.453608 on epoch=124
06/03/2022 09:24:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:24:45 - INFO - __main__ - Printing 3 examples
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:24:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:24:45 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:24:45 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:24:45 - INFO - __main__ - Printing 3 examples
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:24:45 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:45 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:24:45 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:24:45 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:24:46 - INFO - __main__ - Global step 1000 Train loss 0.456721 ACC 0.4838709677419355 on epoch=124
06/03/2022 09:24:46 - INFO - __main__ - save last model!
06/03/2022 09:24:53 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:24:54 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:24:54 - INFO - __main__ - Printing 3 examples
06/03/2022 09:24:54 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:24:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:54 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:24:54 - INFO - __main__ - ['neutral']
06/03/2022 09:24:54 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:24:54 - INFO - __main__ - ['entailment']
06/03/2022 09:24:54 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:24:54 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:24:54 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:24:55 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_42_0.0002_8_predictions.txt
06/03/2022 09:24:55 - INFO - __main__ - ACC on test data: 0.4821
06/03/2022 09:24:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:24:56 - INFO - __main__ - Starting training!
06/03/2022 09:24:56 - INFO - __main__ - prefix=superglue-cb_128_42, lr=0.0002, bsz=8, dev_performance=0.4838709677419355, test_performance=0.48214285714285715
06/03/2022 09:24:56 - INFO - __main__ - Running ... prefix=superglue-cb_128_42, lr=0.0001, bsz=8 ...
06/03/2022 09:24:57 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:24:57 - INFO - __main__ - Printing 3 examples
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: A: so it's nice to get away. It's just amazing, how much you miss. B: Yeah, it,  Yeah, it, yeah, it really is. I mean, I don't think I ever see the Little Dipper, [SEP] hypothesis: she has seen the Little Dipper
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: I'm sorry, I 've put you in an invidious position. If you're being run by Morton, he 'll want to hear all this. It won't do any harm but I 'd rather not give him food for thought because I consider him an idiot and I don't think he's capable of interpreting it correctly. [SEP] hypothesis: Morton is capable of interpreting this food for thought correctly
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:24:57 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:24:57 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:24:57 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:24:57 - INFO - __main__ - Printing 3 examples
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: A: And, uh, I got to stay home with my kids, which I really wanted to do, but now I could not go back and do it. B: Yeah. A: I really couldn't, I don't think I could stay home all the time and do nothing. [SEP] hypothesis: he could stay home all the time and do nothing
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: In finding ``good news'' in Berkeley's new freshman admissions plan (``The Privileged Class,'' editorial, Sept. 20), you're reading the headline but not the story. The plan indeed raises from 40% to 50% the number of freshmen applicants admitted strictly by academic criteria. But that doesn't mean ``half of the students attending Berkeley'' will be admitted this way. [SEP] hypothesis: half of the students attending Berkeley will be admitted this way
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ -  [superglue-cb] premise: B: Uh, I have, uh, I guess a lot of thoughts about the Vietnam War, um, I guess I feel like I was pretty young while it was going on and so there's probably a lot of things I remember and a lot of things that I really didn't have a clue as to what was happening. B: Yeah. A: Um, looking back, like maybe some of the things that I know now, I'm not sure I do believe it was worth the cost in dollars and lives. That was one of the questions that she asked us to think about, because we never went to war. I don't think we were committed to winning it and getting out [SEP] hypothesis: they were committed to winning the Vietnam War and getting out
06/03/2022 09:24:57 - INFO - __main__ - ['contradiction']
06/03/2022 09:24:57 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:24:57 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:24:57 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:25:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:25:10 - INFO - __main__ - Starting training!
06/03/2022 09:25:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.053619 on epoch=1
06/03/2022 09:25:19 - INFO - __main__ - Step 20 Global step 20 Train loss 21.950159 on epoch=2
06/03/2022 09:25:24 - INFO - __main__ - Step 30 Global step 30 Train loss 16.358959 on epoch=3
06/03/2022 09:25:29 - INFO - __main__ - Step 40 Global step 40 Train loss 11.994748 on epoch=4
06/03/2022 09:25:35 - INFO - __main__ - Step 50 Global step 50 Train loss 12.126293 on epoch=6
06/03/2022 09:25:42 - INFO - __main__ - Global step 50 Train loss 17.296755 ACC 0.008064516129032258 on epoch=6
06/03/2022 09:25:48 - INFO - __main__ - Step 60 Global step 60 Train loss 10.707510 on epoch=7
06/03/2022 09:25:53 - INFO - __main__ - Step 70 Global step 70 Train loss 10.684870 on epoch=8
06/03/2022 09:25:58 - INFO - __main__ - Step 80 Global step 80 Train loss 10.573422 on epoch=9
06/03/2022 09:26:04 - INFO - __main__ - Step 90 Global step 90 Train loss 10.689451 on epoch=11
06/03/2022 09:26:09 - INFO - __main__ - Step 100 Global step 100 Train loss 9.595850 on epoch=12
06/03/2022 09:26:12 - INFO - __main__ - Global step 100 Train loss 10.450220 ACC 0.0 on epoch=12
06/03/2022 09:26:17 - INFO - __main__ - Step 110 Global step 110 Train loss 8.939314 on epoch=13
06/03/2022 09:26:22 - INFO - __main__ - Step 120 Global step 120 Train loss 8.899908 on epoch=14
06/03/2022 09:26:27 - INFO - __main__ - Step 130 Global step 130 Train loss 9.460616 on epoch=16
06/03/2022 09:26:32 - INFO - __main__ - Step 140 Global step 140 Train loss 8.789194 on epoch=17
06/03/2022 09:26:38 - INFO - __main__ - Step 150 Global step 150 Train loss 8.276406 on epoch=18
06/03/2022 09:26:40 - INFO - __main__ - Global step 150 Train loss 8.873089 ACC 0.0 on epoch=18
06/03/2022 09:26:45 - INFO - __main__ - Step 160 Global step 160 Train loss 8.462529 on epoch=19
06/03/2022 09:26:51 - INFO - __main__ - Step 170 Global step 170 Train loss 8.533133 on epoch=21
06/03/2022 09:26:56 - INFO - __main__ - Step 180 Global step 180 Train loss 7.883465 on epoch=22
06/03/2022 09:27:01 - INFO - __main__ - Step 190 Global step 190 Train loss 7.767089 on epoch=23
06/03/2022 09:27:06 - INFO - __main__ - Step 200 Global step 200 Train loss 7.187053 on epoch=24
06/03/2022 09:27:09 - INFO - __main__ - Global step 200 Train loss 7.966653 ACC 0.008064516129032258 on epoch=24
06/03/2022 09:27:14 - INFO - __main__ - Step 210 Global step 210 Train loss 7.536242 on epoch=26
06/03/2022 09:27:19 - INFO - __main__ - Step 220 Global step 220 Train loss 7.243269 on epoch=27
06/03/2022 09:27:25 - INFO - __main__ - Step 230 Global step 230 Train loss 7.140057 on epoch=28
06/03/2022 09:27:30 - INFO - __main__ - Step 240 Global step 240 Train loss 6.499084 on epoch=29
06/03/2022 09:27:35 - INFO - __main__ - Step 250 Global step 250 Train loss 6.848194 on epoch=31
06/03/2022 09:27:38 - INFO - __main__ - Global step 250 Train loss 7.053369 ACC 0.008064516129032258 on epoch=31
06/03/2022 09:27:43 - INFO - __main__ - Step 260 Global step 260 Train loss 6.295842 on epoch=32
06/03/2022 09:27:48 - INFO - __main__ - Step 270 Global step 270 Train loss 5.544331 on epoch=33
06/03/2022 09:27:53 - INFO - __main__ - Step 280 Global step 280 Train loss 5.492945 on epoch=34
06/03/2022 09:27:59 - INFO - __main__ - Step 290 Global step 290 Train loss 5.589422 on epoch=36
06/03/2022 09:28:04 - INFO - __main__ - Step 300 Global step 300 Train loss 5.105151 on epoch=37
06/03/2022 09:28:07 - INFO - __main__ - Global step 300 Train loss 5.605539 ACC 0.008064516129032258 on epoch=37
06/03/2022 09:28:12 - INFO - __main__ - Step 310 Global step 310 Train loss 4.408380 on epoch=38
06/03/2022 09:28:17 - INFO - __main__ - Step 320 Global step 320 Train loss 4.258244 on epoch=39
06/03/2022 09:28:22 - INFO - __main__ - Step 330 Global step 330 Train loss 4.169518 on epoch=41
06/03/2022 09:28:27 - INFO - __main__ - Step 340 Global step 340 Train loss 3.792948 on epoch=42
06/03/2022 09:28:33 - INFO - __main__ - Step 350 Global step 350 Train loss 3.290427 on epoch=43
06/03/2022 09:28:35 - INFO - __main__ - Global step 350 Train loss 3.983903 ACC 0.3709677419354839 on epoch=43
06/03/2022 09:28:41 - INFO - __main__ - Step 360 Global step 360 Train loss 2.816262 on epoch=44
06/03/2022 09:28:46 - INFO - __main__ - Step 370 Global step 370 Train loss 2.311386 on epoch=46
06/03/2022 09:28:52 - INFO - __main__ - Step 380 Global step 380 Train loss 3.202292 on epoch=47
06/03/2022 09:28:57 - INFO - __main__ - Step 390 Global step 390 Train loss 2.631919 on epoch=48
06/03/2022 09:29:02 - INFO - __main__ - Step 400 Global step 400 Train loss 2.348958 on epoch=49
06/03/2022 09:29:04 - INFO - __main__ - Global step 400 Train loss 2.662164 ACC 0.49193548387096775 on epoch=49
06/03/2022 09:29:10 - INFO - __main__ - Step 410 Global step 410 Train loss 2.427608 on epoch=51
06/03/2022 09:29:16 - INFO - __main__ - Step 420 Global step 420 Train loss 1.978545 on epoch=52
06/03/2022 09:29:21 - INFO - __main__ - Step 430 Global step 430 Train loss 2.160420 on epoch=53
06/03/2022 09:29:26 - INFO - __main__ - Step 440 Global step 440 Train loss 2.287056 on epoch=54
06/03/2022 09:29:32 - INFO - __main__ - Step 450 Global step 450 Train loss 2.249591 on epoch=56
06/03/2022 09:29:34 - INFO - __main__ - Global step 450 Train loss 2.220644 ACC 0.47580645161290325 on epoch=56
06/03/2022 09:29:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.507721 on epoch=57
06/03/2022 09:29:44 - INFO - __main__ - Step 470 Global step 470 Train loss 1.717675 on epoch=58
06/03/2022 09:29:50 - INFO - __main__ - Step 480 Global step 480 Train loss 1.953091 on epoch=59
06/03/2022 09:29:55 - INFO - __main__ - Step 490 Global step 490 Train loss 1.981894 on epoch=61
06/03/2022 09:30:00 - INFO - __main__ - Step 500 Global step 500 Train loss 2.252475 on epoch=62
06/03/2022 09:30:02 - INFO - __main__ - Global step 500 Train loss 1.882571 ACC 0.47580645161290325 on epoch=62
06/03/2022 09:30:08 - INFO - __main__ - Step 510 Global step 510 Train loss 1.688878 on epoch=63
06/03/2022 09:30:13 - INFO - __main__ - Step 520 Global step 520 Train loss 1.999504 on epoch=64
06/03/2022 09:30:18 - INFO - __main__ - Step 530 Global step 530 Train loss 1.665132 on epoch=66
06/03/2022 09:30:23 - INFO - __main__ - Step 540 Global step 540 Train loss 1.245073 on epoch=67
06/03/2022 09:30:29 - INFO - __main__ - Step 550 Global step 550 Train loss 1.217404 on epoch=68
06/03/2022 09:30:31 - INFO - __main__ - Global step 550 Train loss 1.563198 ACC 0.47580645161290325 on epoch=68
06/03/2022 09:30:36 - INFO - __main__ - Step 560 Global step 560 Train loss 1.578047 on epoch=69
06/03/2022 09:30:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.933952 on epoch=71
06/03/2022 09:30:47 - INFO - __main__ - Step 580 Global step 580 Train loss 1.259877 on epoch=72
06/03/2022 09:30:52 - INFO - __main__ - Step 590 Global step 590 Train loss 1.683364 on epoch=73
06/03/2022 09:30:57 - INFO - __main__ - Step 600 Global step 600 Train loss 2.017212 on epoch=74
06/03/2022 09:30:59 - INFO - __main__ - Global step 600 Train loss 1.694491 ACC 0.47580645161290325 on epoch=74
06/03/2022 09:31:05 - INFO - __main__ - Step 610 Global step 610 Train loss 1.514299 on epoch=76
06/03/2022 09:31:10 - INFO - __main__ - Step 620 Global step 620 Train loss 1.458464 on epoch=77
06/03/2022 09:31:15 - INFO - __main__ - Step 630 Global step 630 Train loss 1.775090 on epoch=78
06/03/2022 09:31:21 - INFO - __main__ - Step 640 Global step 640 Train loss 1.567971 on epoch=79
06/03/2022 09:31:26 - INFO - __main__ - Step 650 Global step 650 Train loss 1.725555 on epoch=81
06/03/2022 09:31:28 - INFO - __main__ - Global step 650 Train loss 1.608276 ACC 0.47580645161290325 on epoch=81
06/03/2022 09:31:33 - INFO - __main__ - Step 660 Global step 660 Train loss 1.296973 on epoch=82
06/03/2022 09:31:39 - INFO - __main__ - Step 670 Global step 670 Train loss 1.724420 on epoch=83
06/03/2022 09:31:44 - INFO - __main__ - Step 680 Global step 680 Train loss 1.803081 on epoch=84
06/03/2022 09:31:49 - INFO - __main__ - Step 690 Global step 690 Train loss 1.242895 on epoch=86
06/03/2022 09:31:54 - INFO - __main__ - Step 700 Global step 700 Train loss 1.379841 on epoch=87
06/03/2022 09:31:57 - INFO - __main__ - Global step 700 Train loss 1.489442 ACC 0.47580645161290325 on epoch=87
06/03/2022 09:32:02 - INFO - __main__ - Step 710 Global step 710 Train loss 1.002669 on epoch=88
06/03/2022 09:32:07 - INFO - __main__ - Step 720 Global step 720 Train loss 1.633003 on epoch=89
06/03/2022 09:32:13 - INFO - __main__ - Step 730 Global step 730 Train loss 1.517137 on epoch=91
06/03/2022 09:32:18 - INFO - __main__ - Step 740 Global step 740 Train loss 1.232408 on epoch=92
06/03/2022 09:32:23 - INFO - __main__ - Step 750 Global step 750 Train loss 1.249913 on epoch=93
06/03/2022 09:32:26 - INFO - __main__ - Global step 750 Train loss 1.327026 ACC 0.47580645161290325 on epoch=93
06/03/2022 09:32:31 - INFO - __main__ - Step 760 Global step 760 Train loss 1.186510 on epoch=94
06/03/2022 09:32:36 - INFO - __main__ - Step 770 Global step 770 Train loss 1.125487 on epoch=96
06/03/2022 09:32:41 - INFO - __main__ - Step 780 Global step 780 Train loss 1.143621 on epoch=97
06/03/2022 09:32:46 - INFO - __main__ - Step 790 Global step 790 Train loss 1.228127 on epoch=98
06/03/2022 09:32:52 - INFO - __main__ - Step 800 Global step 800 Train loss 1.192697 on epoch=99
06/03/2022 09:32:54 - INFO - __main__ - Global step 800 Train loss 1.175288 ACC 0.47580645161290325 on epoch=99
06/03/2022 09:32:59 - INFO - __main__ - Step 810 Global step 810 Train loss 1.114594 on epoch=101
06/03/2022 09:33:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.914605 on epoch=102
06/03/2022 09:33:10 - INFO - __main__ - Step 830 Global step 830 Train loss 1.127796 on epoch=103
06/03/2022 09:33:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.773446 on epoch=104
06/03/2022 09:33:20 - INFO - __main__ - Step 850 Global step 850 Train loss 1.197973 on epoch=106
06/03/2022 09:33:22 - INFO - __main__ - Global step 850 Train loss 1.025683 ACC 0.47580645161290325 on epoch=106
06/03/2022 09:33:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.884912 on epoch=107
06/03/2022 09:33:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.906728 on epoch=108
06/03/2022 09:33:38 - INFO - __main__ - Step 880 Global step 880 Train loss 1.079687 on epoch=109
06/03/2022 09:33:43 - INFO - __main__ - Step 890 Global step 890 Train loss 1.073516 on epoch=111
06/03/2022 09:33:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.865038 on epoch=112
06/03/2022 09:33:50 - INFO - __main__ - Global step 900 Train loss 0.961976 ACC 0.6935483870967742 on epoch=112
06/03/2022 09:33:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.770212 on epoch=113
06/03/2022 09:34:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.838247 on epoch=114
06/03/2022 09:34:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.836482 on epoch=116
06/03/2022 09:34:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.734564 on epoch=117
06/03/2022 09:34:17 - INFO - __main__ - Step 950 Global step 950 Train loss 0.733219 on epoch=118
06/03/2022 09:34:19 - INFO - __main__ - Global step 950 Train loss 0.782545 ACC 0.717741935483871 on epoch=118
06/03/2022 09:34:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.998064 on epoch=119
06/03/2022 09:34:30 - INFO - __main__ - Step 970 Global step 970 Train loss 0.498611 on epoch=121
06/03/2022 09:34:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.652560 on epoch=122
06/03/2022 09:34:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.771279 on epoch=123
06/03/2022 09:34:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.105179 on epoch=124
06/03/2022 09:34:46 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:34:46 - INFO - __main__ - Printing 3 examples
06/03/2022 09:34:46 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:34:46 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:46 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:34:46 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:46 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:34:46 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:46 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:34:47 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:34:47 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:34:47 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:34:47 - INFO - __main__ - Printing 3 examples
06/03/2022 09:34:47 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:34:47 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:47 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:34:47 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:47 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:34:47 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:47 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:34:47 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:34:47 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:34:48 - INFO - __main__ - Global step 1000 Train loss 0.805138 ACC 0.7338709677419355 on epoch=124
06/03/2022 09:34:49 - INFO - __main__ - save last model!
06/03/2022 09:34:55 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:34:56 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:34:56 - INFO - __main__ - Printing 3 examples
06/03/2022 09:34:56 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:34:56 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:56 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:34:56 - INFO - __main__ - ['neutral']
06/03/2022 09:34:56 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:34:56 - INFO - __main__ - ['entailment']
06/03/2022 09:34:56 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:34:56 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:34:56 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:34:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:34:57 - INFO - __main__ - Starting training!
06/03/2022 09:34:58 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_42_0.0001_8_predictions.txt
06/03/2022 09:34:58 - INFO - __main__ - ACC on test data: 0.6786
06/03/2022 09:34:58 - INFO - __main__ - prefix=superglue-cb_128_42, lr=0.0001, bsz=8, dev_performance=0.7338709677419355, test_performance=0.6785714285714286
06/03/2022 09:34:58 - INFO - __main__ - Running ... prefix=superglue-cb_128_87, lr=0.0005, bsz=8 ...
06/03/2022 09:34:59 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:34:59 - INFO - __main__ - Printing 3 examples
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:34:59 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:34:59 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:34:59 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:34:59 - INFO - __main__ - Printing 3 examples
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:34:59 - INFO - __main__ - ['contradiction']
06/03/2022 09:34:59 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:34:59 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:34:59 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:35:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:35:10 - INFO - __main__ - Starting training!
06/03/2022 09:35:14 - INFO - __main__ - Step 10 Global step 10 Train loss 24.589394 on epoch=1
06/03/2022 09:35:19 - INFO - __main__ - Step 20 Global step 20 Train loss 14.768664 on epoch=2
06/03/2022 09:35:25 - INFO - __main__ - Step 30 Global step 30 Train loss 9.828675 on epoch=3
06/03/2022 09:35:30 - INFO - __main__ - Step 40 Global step 40 Train loss 7.985811 on epoch=4
06/03/2022 09:35:35 - INFO - __main__ - Step 50 Global step 50 Train loss 7.892876 on epoch=6
06/03/2022 09:35:38 - INFO - __main__ - Global step 50 Train loss 13.013086 ACC 0.008064516129032258 on epoch=6
06/03/2022 09:35:44 - INFO - __main__ - Step 60 Global step 60 Train loss 6.827940 on epoch=7
06/03/2022 09:35:49 - INFO - __main__ - Step 70 Global step 70 Train loss 5.108945 on epoch=8
06/03/2022 09:35:54 - INFO - __main__ - Step 80 Global step 80 Train loss 3.141021 on epoch=9
06/03/2022 09:35:59 - INFO - __main__ - Step 90 Global step 90 Train loss 2.851944 on epoch=11
06/03/2022 09:36:04 - INFO - __main__ - Step 100 Global step 100 Train loss 2.532812 on epoch=12
06/03/2022 09:36:06 - INFO - __main__ - Global step 100 Train loss 4.092532 ACC 0.47580645161290325 on epoch=12
06/03/2022 09:36:13 - INFO - __main__ - Step 110 Global step 110 Train loss 1.988611 on epoch=13
06/03/2022 09:36:18 - INFO - __main__ - Step 120 Global step 120 Train loss 1.810456 on epoch=14
06/03/2022 09:36:23 - INFO - __main__ - Step 130 Global step 130 Train loss 1.599579 on epoch=16
06/03/2022 09:36:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.919397 on epoch=17
06/03/2022 09:36:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.627146 on epoch=18
06/03/2022 09:36:35 - INFO - __main__ - Global step 150 Train loss 1.789038 ACC 0.47580645161290325 on epoch=18
06/03/2022 09:36:41 - INFO - __main__ - Step 160 Global step 160 Train loss 1.329166 on epoch=19
06/03/2022 09:36:46 - INFO - __main__ - Step 170 Global step 170 Train loss 1.095006 on epoch=21
06/03/2022 09:36:51 - INFO - __main__ - Step 180 Global step 180 Train loss 1.201337 on epoch=22
06/03/2022 09:36:56 - INFO - __main__ - Step 190 Global step 190 Train loss 1.051396 on epoch=23
06/03/2022 09:37:01 - INFO - __main__ - Step 200 Global step 200 Train loss 1.302497 on epoch=24
06/03/2022 09:37:04 - INFO - __main__ - Global step 200 Train loss 1.195880 ACC 0.6209677419354839 on epoch=24
06/03/2022 09:37:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.837420 on epoch=26
06/03/2022 09:37:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.519537 on epoch=27
06/03/2022 09:37:20 - INFO - __main__ - Step 230 Global step 230 Train loss 0.446651 on epoch=28
06/03/2022 09:37:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.268063 on epoch=29
06/03/2022 09:37:30 - INFO - __main__ - Step 250 Global step 250 Train loss 0.856458 on epoch=31
06/03/2022 09:37:33 - INFO - __main__ - Global step 250 Train loss 0.585625 ACC 0.5725806451612904 on epoch=31
06/03/2022 09:37:38 - INFO - __main__ - Step 260 Global step 260 Train loss 0.732119 on epoch=32
06/03/2022 09:37:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.031908 on epoch=33
06/03/2022 09:37:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.762089 on epoch=34
06/03/2022 09:37:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.676348 on epoch=36
06/03/2022 09:37:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.784559 on epoch=37
06/03/2022 09:38:01 - INFO - __main__ - Global step 300 Train loss 0.797405 ACC 0.47580645161290325 on epoch=37
06/03/2022 09:38:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.605509 on epoch=38
06/03/2022 09:38:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.584365 on epoch=39
06/03/2022 09:38:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.454835 on epoch=41
06/03/2022 09:38:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.624093 on epoch=42
06/03/2022 09:38:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.525621 on epoch=43
06/03/2022 09:38:30 - INFO - __main__ - Global step 350 Train loss 0.558884 ACC 0.7661290322580645 on epoch=43
06/03/2022 09:38:36 - INFO - __main__ - Step 360 Global step 360 Train loss 0.237320 on epoch=44
06/03/2022 09:38:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.383357 on epoch=46
06/03/2022 09:38:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.255174 on epoch=47
06/03/2022 09:38:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.269792 on epoch=48
06/03/2022 09:38:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.270673 on epoch=49
06/03/2022 09:38:59 - INFO - __main__ - Global step 400 Train loss 0.283263 ACC 0.532258064516129 on epoch=49
06/03/2022 09:39:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.121953 on epoch=51
06/03/2022 09:39:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.095818 on epoch=52
06/03/2022 09:39:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.104480 on epoch=53
06/03/2022 09:39:20 - INFO - __main__ - Step 440 Global step 440 Train loss 0.216214 on epoch=54
06/03/2022 09:39:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.098583 on epoch=56
06/03/2022 09:39:28 - INFO - __main__ - Global step 450 Train loss 0.127409 ACC 0.8064516129032258 on epoch=56
06/03/2022 09:39:33 - INFO - __main__ - Step 460 Global step 460 Train loss 0.042272 on epoch=57
06/03/2022 09:39:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.057656 on epoch=58
06/03/2022 09:39:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.034420 on epoch=59
06/03/2022 09:39:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.056363 on epoch=61
06/03/2022 09:39:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.016544 on epoch=62
06/03/2022 09:39:57 - INFO - __main__ - Global step 500 Train loss 0.041451 ACC 0.8870967741935484 on epoch=62
06/03/2022 09:40:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.005072 on epoch=63
06/03/2022 09:40:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.005467 on epoch=64
06/03/2022 09:40:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.009068 on epoch=66
06/03/2022 09:40:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.002705 on epoch=67
06/03/2022 09:40:23 - INFO - __main__ - Step 550 Global step 550 Train loss 0.005510 on epoch=68
06/03/2022 09:40:26 - INFO - __main__ - Global step 550 Train loss 0.005564 ACC 0.8064516129032258 on epoch=68
06/03/2022 09:40:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.002346 on epoch=69
06/03/2022 09:40:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.002812 on epoch=71
06/03/2022 09:40:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.006965 on epoch=72
06/03/2022 09:40:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.003575 on epoch=73
06/03/2022 09:40:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.008064 on epoch=74
06/03/2022 09:40:54 - INFO - __main__ - Global step 600 Train loss 0.004753 ACC 0.7419354838709677 on epoch=74
06/03/2022 09:40:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.000624 on epoch=76
06/03/2022 09:41:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.000705 on epoch=77
06/03/2022 09:41:10 - INFO - __main__ - Step 630 Global step 630 Train loss 0.002525 on epoch=78
06/03/2022 09:41:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.001448 on epoch=79
06/03/2022 09:41:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.000275 on epoch=81
06/03/2022 09:41:23 - INFO - __main__ - Global step 650 Train loss 0.001116 ACC 0.7741935483870968 on epoch=81
06/03/2022 09:41:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.004219 on epoch=82
06/03/2022 09:41:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.009566 on epoch=83
06/03/2022 09:41:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.001238 on epoch=84
06/03/2022 09:41:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.000385 on epoch=86
06/03/2022 09:41:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.000204 on epoch=87
06/03/2022 09:41:51 - INFO - __main__ - Global step 700 Train loss 0.003122 ACC 0.7419354838709677 on epoch=87
06/03/2022 09:41:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.000289 on epoch=88
06/03/2022 09:42:02 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000191 on epoch=89
06/03/2022 09:42:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.000498 on epoch=91
06/03/2022 09:42:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.000288 on epoch=92
06/03/2022 09:42:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.000673 on epoch=93
06/03/2022 09:42:20 - INFO - __main__ - Global step 750 Train loss 0.000388 ACC 0.7661290322580645 on epoch=93
06/03/2022 09:42:25 - INFO - __main__ - Step 760 Global step 760 Train loss 0.000153 on epoch=94
06/03/2022 09:42:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000353 on epoch=96
06/03/2022 09:42:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000068 on epoch=97
06/03/2022 09:42:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.002980 on epoch=98
06/03/2022 09:42:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000261 on epoch=99
06/03/2022 09:42:49 - INFO - __main__ - Global step 800 Train loss 0.000763 ACC 0.7661290322580645 on epoch=99
06/03/2022 09:42:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.000093 on epoch=101
06/03/2022 09:42:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.000059 on epoch=102
06/03/2022 09:43:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000100 on epoch=103
06/03/2022 09:43:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.000127 on epoch=104
06/03/2022 09:43:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000054 on epoch=106
06/03/2022 09:43:18 - INFO - __main__ - Global step 850 Train loss 0.000087 ACC 0.7580645161290323 on epoch=106
06/03/2022 09:43:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.000055 on epoch=107
06/03/2022 09:43:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000120 on epoch=108
06/03/2022 09:43:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.000507 on epoch=109
06/03/2022 09:43:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000781 on epoch=111
06/03/2022 09:43:43 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000264 on epoch=112
06/03/2022 09:43:46 - INFO - __main__ - Global step 900 Train loss 0.000345 ACC 0.7903225806451613 on epoch=112
06/03/2022 09:43:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000089 on epoch=113
06/03/2022 09:43:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000229 on epoch=114
06/03/2022 09:44:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000049 on epoch=116
06/03/2022 09:44:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000110 on epoch=117
06/03/2022 09:44:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000134 on epoch=118
06/03/2022 09:44:15 - INFO - __main__ - Global step 950 Train loss 0.000122 ACC 0.8225806451612904 on epoch=118
06/03/2022 09:44:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000061 on epoch=119
06/03/2022 09:44:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000059 on epoch=121
06/03/2022 09:44:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000041 on epoch=122
06/03/2022 09:44:35 - INFO - __main__ - Step 990 Global step 990 Train loss 0.000017 on epoch=123
06/03/2022 09:44:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000296 on epoch=124
06/03/2022 09:44:42 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:44:42 - INFO - __main__ - Printing 3 examples
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:44:42 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:44:42 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:44:42 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:44:42 - INFO - __main__ - Printing 3 examples
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:44:42 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:42 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:44:42 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:44:42 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:44:43 - INFO - __main__ - Global step 1000 Train loss 0.000095 ACC 0.8306451612903226 on epoch=124
06/03/2022 09:44:43 - INFO - __main__ - save last model!
06/03/2022 09:44:50 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:44:51 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:44:51 - INFO - __main__ - Printing 3 examples
06/03/2022 09:44:51 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:44:51 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:51 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:44:51 - INFO - __main__ - ['neutral']
06/03/2022 09:44:51 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:44:51 - INFO - __main__ - ['entailment']
06/03/2022 09:44:51 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:44:51 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:44:51 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:44:52 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_87_0.0005_8_predictions.txt
06/03/2022 09:44:52 - INFO - __main__ - ACC on test data: 0.9107
06/03/2022 09:44:53 - INFO - __main__ - prefix=superglue-cb_128_87, lr=0.0005, bsz=8, dev_performance=0.8870967741935484, test_performance=0.9107142857142857
06/03/2022 09:44:53 - INFO - __main__ - Running ... prefix=superglue-cb_128_87, lr=0.0003, bsz=8 ...
06/03/2022 09:44:54 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:44:54 - INFO - __main__ - Printing 3 examples
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:44:54 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:44:54 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:44:54 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:44:54 - INFO - __main__ - Printing 3 examples
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:44:54 - INFO - __main__ - ['contradiction']
06/03/2022 09:44:54 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:44:54 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:44:54 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:44:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:44:55 - INFO - __main__ - Starting training!
06/03/2022 09:45:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:45:06 - INFO - __main__ - Starting training!
06/03/2022 09:45:11 - INFO - __main__ - Step 10 Global step 10 Train loss 24.445353 on epoch=1
06/03/2022 09:45:16 - INFO - __main__ - Step 20 Global step 20 Train loss 16.001202 on epoch=2
06/03/2022 09:45:21 - INFO - __main__ - Step 30 Global step 30 Train loss 11.353357 on epoch=3
06/03/2022 09:45:26 - INFO - __main__ - Step 40 Global step 40 Train loss 9.920820 on epoch=4
06/03/2022 09:45:31 - INFO - __main__ - Step 50 Global step 50 Train loss 9.709279 on epoch=6
06/03/2022 09:45:34 - INFO - __main__ - Global step 50 Train loss 14.286003 ACC 0.0 on epoch=6
06/03/2022 09:45:39 - INFO - __main__ - Step 60 Global step 60 Train loss 8.992716 on epoch=7
06/03/2022 09:45:44 - INFO - __main__ - Step 70 Global step 70 Train loss 8.637860 on epoch=8
06/03/2022 09:45:50 - INFO - __main__ - Step 80 Global step 80 Train loss 7.575316 on epoch=9
06/03/2022 09:45:55 - INFO - __main__ - Step 90 Global step 90 Train loss 7.198776 on epoch=11
06/03/2022 09:46:00 - INFO - __main__ - Step 100 Global step 100 Train loss 6.846626 on epoch=12
06/03/2022 09:46:03 - INFO - __main__ - Global step 100 Train loss 7.850259 ACC 0.0 on epoch=12
06/03/2022 09:46:08 - INFO - __main__ - Step 110 Global step 110 Train loss 5.535731 on epoch=13
06/03/2022 09:46:13 - INFO - __main__ - Step 120 Global step 120 Train loss 4.511558 on epoch=14
06/03/2022 09:46:18 - INFO - __main__ - Step 130 Global step 130 Train loss 3.797090 on epoch=16
06/03/2022 09:46:23 - INFO - __main__ - Step 140 Global step 140 Train loss 2.866469 on epoch=17
06/03/2022 09:46:28 - INFO - __main__ - Step 150 Global step 150 Train loss 2.756580 on epoch=18
06/03/2022 09:46:31 - INFO - __main__ - Global step 150 Train loss 3.893485 ACC 0.47580645161290325 on epoch=18
06/03/2022 09:46:36 - INFO - __main__ - Step 160 Global step 160 Train loss 2.228414 on epoch=19
06/03/2022 09:46:41 - INFO - __main__ - Step 170 Global step 170 Train loss 1.409901 on epoch=21
06/03/2022 09:46:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.531272 on epoch=22
06/03/2022 09:46:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.340659 on epoch=23
06/03/2022 09:46:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.325459 on epoch=24
06/03/2022 09:46:59 - INFO - __main__ - Global step 200 Train loss 0.967141 ACC 0.47580645161290325 on epoch=24
06/03/2022 09:47:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.325945 on epoch=26
06/03/2022 09:47:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.204285 on epoch=27
06/03/2022 09:47:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.229518 on epoch=28
06/03/2022 09:47:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.227283 on epoch=29
06/03/2022 09:47:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.647268 on epoch=31
06/03/2022 09:47:28 - INFO - __main__ - Global step 250 Train loss 0.326860 ACC 0.7338709677419355 on epoch=31
06/03/2022 09:47:33 - INFO - __main__ - Step 260 Global step 260 Train loss 0.152859 on epoch=32
06/03/2022 09:47:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.103636 on epoch=33
06/03/2022 09:47:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.109659 on epoch=34
06/03/2022 09:47:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.124889 on epoch=36
06/03/2022 09:47:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.070156 on epoch=37
06/03/2022 09:47:56 - INFO - __main__ - Global step 300 Train loss 0.112240 ACC 0.7903225806451613 on epoch=37
06/03/2022 09:48:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.053887 on epoch=38
06/03/2022 09:48:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.036632 on epoch=39
06/03/2022 09:48:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.023447 on epoch=41
06/03/2022 09:48:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.010944 on epoch=42
06/03/2022 09:48:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.008047 on epoch=43
06/03/2022 09:48:26 - INFO - __main__ - Global step 350 Train loss 0.026591 ACC 0.782258064516129 on epoch=43
06/03/2022 09:48:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.011901 on epoch=44
06/03/2022 09:48:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.011846 on epoch=46
06/03/2022 09:48:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.003264 on epoch=47
06/03/2022 09:48:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.004543 on epoch=48
06/03/2022 09:48:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.017038 on epoch=49
06/03/2022 09:48:54 - INFO - __main__ - Global step 400 Train loss 0.009718 ACC 0.8145161290322581 on epoch=49
06/03/2022 09:49:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.009621 on epoch=51
06/03/2022 09:49:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.008073 on epoch=52
06/03/2022 09:49:10 - INFO - __main__ - Step 430 Global step 430 Train loss 2.015178 on epoch=53
06/03/2022 09:49:15 - INFO - __main__ - Step 440 Global step 440 Train loss 2.267891 on epoch=54
06/03/2022 09:49:21 - INFO - __main__ - Step 450 Global step 450 Train loss 1.520736 on epoch=56
06/03/2022 09:49:23 - INFO - __main__ - Global step 450 Train loss 1.164300 ACC 0.47580645161290325 on epoch=56
06/03/2022 09:49:28 - INFO - __main__ - Step 460 Global step 460 Train loss 1.452593 on epoch=57
06/03/2022 09:49:33 - INFO - __main__ - Step 470 Global step 470 Train loss 1.482465 on epoch=58
06/03/2022 09:49:39 - INFO - __main__ - Step 480 Global step 480 Train loss 1.060134 on epoch=59
06/03/2022 09:49:44 - INFO - __main__ - Step 490 Global step 490 Train loss 1.465410 on epoch=61
06/03/2022 09:49:49 - INFO - __main__ - Step 500 Global step 500 Train loss 1.133629 on epoch=62
06/03/2022 09:49:51 - INFO - __main__ - Global step 500 Train loss 1.318846 ACC 0.47580645161290325 on epoch=62
06/03/2022 09:49:57 - INFO - __main__ - Step 510 Global step 510 Train loss 1.178388 on epoch=63
06/03/2022 09:50:02 - INFO - __main__ - Step 520 Global step 520 Train loss 1.185970 on epoch=64
06/03/2022 09:50:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.770676 on epoch=66
06/03/2022 09:50:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.680397 on epoch=67
06/03/2022 09:50:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.714477 on epoch=68
06/03/2022 09:50:20 - INFO - __main__ - Global step 550 Train loss 0.905981 ACC 0.7580645161290323 on epoch=68
06/03/2022 09:50:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.748540 on epoch=69
06/03/2022 09:50:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.908049 on epoch=71
06/03/2022 09:50:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.544810 on epoch=72
06/03/2022 09:50:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.424902 on epoch=73
06/03/2022 09:50:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.494492 on epoch=74
06/03/2022 09:50:48 - INFO - __main__ - Global step 600 Train loss 0.624158 ACC 0.7903225806451613 on epoch=74
06/03/2022 09:50:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.309982 on epoch=76
06/03/2022 09:50:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.301140 on epoch=77
06/03/2022 09:51:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.327134 on epoch=78
06/03/2022 09:51:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.325488 on epoch=79
06/03/2022 09:51:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.060951 on epoch=81
06/03/2022 09:51:17 - INFO - __main__ - Global step 650 Train loss 0.264939 ACC 0.8709677419354839 on epoch=81
06/03/2022 09:51:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.110298 on epoch=82
06/03/2022 09:51:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.129910 on epoch=83
06/03/2022 09:51:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.123581 on epoch=84
06/03/2022 09:51:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.053593 on epoch=86
06/03/2022 09:51:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.008780 on epoch=87
06/03/2022 09:51:46 - INFO - __main__ - Global step 700 Train loss 0.085233 ACC 0.9435483870967742 on epoch=87
06/03/2022 09:51:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.006444 on epoch=88
06/03/2022 09:51:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.000863 on epoch=89
06/03/2022 09:52:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.003308 on epoch=91
06/03/2022 09:52:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.011794 on epoch=92
06/03/2022 09:52:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.001173 on epoch=93
06/03/2022 09:52:15 - INFO - __main__ - Global step 750 Train loss 0.004716 ACC 0.8790322580645161 on epoch=93
06/03/2022 09:52:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.011298 on epoch=94
06/03/2022 09:52:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.000601 on epoch=96
06/03/2022 09:52:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.000285 on epoch=97
06/03/2022 09:52:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.000526 on epoch=98
06/03/2022 09:52:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.000934 on epoch=99
06/03/2022 09:52:44 - INFO - __main__ - Global step 800 Train loss 0.002729 ACC 0.8790322580645161 on epoch=99
06/03/2022 09:52:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.001555 on epoch=101
06/03/2022 09:52:54 - INFO - __main__ - Step 820 Global step 820 Train loss 0.007724 on epoch=102
06/03/2022 09:52:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.000369 on epoch=103
06/03/2022 09:53:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.001712 on epoch=104
06/03/2022 09:53:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.000194 on epoch=106
06/03/2022 09:53:12 - INFO - __main__ - Global step 850 Train loss 0.002311 ACC 0.8709677419354839 on epoch=106
06/03/2022 09:53:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.116302 on epoch=107
06/03/2022 09:53:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.000399 on epoch=108
06/03/2022 09:53:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.001055 on epoch=109
06/03/2022 09:53:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.000136 on epoch=111
06/03/2022 09:53:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.000067 on epoch=112
06/03/2022 09:53:41 - INFO - __main__ - Global step 900 Train loss 0.023592 ACC 0.9435483870967742 on epoch=112
06/03/2022 09:53:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.000102 on epoch=113
06/03/2022 09:53:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.000509 on epoch=114
06/03/2022 09:53:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.000977 on epoch=116
06/03/2022 09:54:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.000092 on epoch=117
06/03/2022 09:54:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.000079 on epoch=118
06/03/2022 09:54:09 - INFO - __main__ - Global step 950 Train loss 0.000352 ACC 0.9516129032258065 on epoch=118
06/03/2022 09:54:15 - INFO - __main__ - Step 960 Global step 960 Train loss 0.000237 on epoch=119
06/03/2022 09:54:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.000191 on epoch=121
06/03/2022 09:54:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.000059 on epoch=122
06/03/2022 09:54:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001701 on epoch=123
06/03/2022 09:54:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.000050 on epoch=124
06/03/2022 09:54:37 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:54:37 - INFO - __main__ - Printing 3 examples
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:54:37 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:54:37 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:54:37 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:54:37 - INFO - __main__ - Printing 3 examples
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:54:37 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:37 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:54:37 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:54:38 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:54:39 - INFO - __main__ - Global step 1000 Train loss 0.000448 ACC 0.9516129032258065 on epoch=124
06/03/2022 09:54:39 - INFO - __main__ - save last model!
06/03/2022 09:54:46 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 09:54:46 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 09:54:46 - INFO - __main__ - Printing 3 examples
06/03/2022 09:54:46 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 09:54:46 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:46 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 09:54:46 - INFO - __main__ - ['neutral']
06/03/2022 09:54:46 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 09:54:46 - INFO - __main__ - ['entailment']
06/03/2022 09:54:46 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:54:47 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:54:47 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 09:54:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:54:48 - INFO - __main__ - Starting training!
06/03/2022 09:54:48 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_87_0.0003_8_predictions.txt
06/03/2022 09:54:48 - INFO - __main__ - ACC on test data: 0.9107
06/03/2022 09:54:48 - INFO - __main__ - prefix=superglue-cb_128_87, lr=0.0003, bsz=8, dev_performance=0.9516129032258065, test_performance=0.9107142857142857
06/03/2022 09:54:48 - INFO - __main__ - Running ... prefix=superglue-cb_128_87, lr=0.0002, bsz=8 ...
06/03/2022 09:54:49 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:54:49 - INFO - __main__ - Printing 3 examples
06/03/2022 09:54:49 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 09:54:49 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:49 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 09:54:49 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:49 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 09:54:49 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:49 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:54:49 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:54:50 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 09:54:50 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 09:54:50 - INFO - __main__ - Printing 3 examples
06/03/2022 09:54:50 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 09:54:50 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:50 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 09:54:50 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:50 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 09:54:50 - INFO - __main__ - ['contradiction']
06/03/2022 09:54:50 - INFO - __main__ - Tokenizing Input ...
06/03/2022 09:54:50 - INFO - __main__ - Tokenizing Output ...
06/03/2022 09:54:50 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 09:55:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 09:55:01 - INFO - __main__ - Starting training!
06/03/2022 09:55:05 - INFO - __main__ - Step 10 Global step 10 Train loss 24.904907 on epoch=1
06/03/2022 09:55:10 - INFO - __main__ - Step 20 Global step 20 Train loss 20.626015 on epoch=2
06/03/2022 09:55:15 - INFO - __main__ - Step 30 Global step 30 Train loss 14.689441 on epoch=3
06/03/2022 09:55:20 - INFO - __main__ - Step 40 Global step 40 Train loss 11.677746 on epoch=4
06/03/2022 09:55:25 - INFO - __main__ - Step 50 Global step 50 Train loss 10.889633 on epoch=6
06/03/2022 09:55:50 - INFO - __main__ - Global step 50 Train loss 16.557549 ACC 0.20967741935483872 on epoch=6
06/03/2022 09:55:56 - INFO - __main__ - Step 60 Global step 60 Train loss 9.839466 on epoch=7
06/03/2022 09:56:01 - INFO - __main__ - Step 70 Global step 70 Train loss 9.282076 on epoch=8
06/03/2022 09:56:06 - INFO - __main__ - Step 80 Global step 80 Train loss 8.184989 on epoch=9
06/03/2022 09:56:11 - INFO - __main__ - Step 90 Global step 90 Train loss 8.879372 on epoch=11
06/03/2022 09:56:16 - INFO - __main__ - Step 100 Global step 100 Train loss 8.162828 on epoch=12
06/03/2022 09:56:21 - INFO - __main__ - Global step 100 Train loss 8.869745 ACC 0.1935483870967742 on epoch=12
06/03/2022 09:56:26 - INFO - __main__ - Step 110 Global step 110 Train loss 7.704280 on epoch=13
06/03/2022 09:56:31 - INFO - __main__ - Step 120 Global step 120 Train loss 7.724116 on epoch=14
06/03/2022 09:56:37 - INFO - __main__ - Step 130 Global step 130 Train loss 7.017460 on epoch=16
06/03/2022 09:56:42 - INFO - __main__ - Step 140 Global step 140 Train loss 6.496813 on epoch=17
06/03/2022 09:56:47 - INFO - __main__ - Step 150 Global step 150 Train loss 5.648727 on epoch=18
06/03/2022 09:56:51 - INFO - __main__ - Global step 150 Train loss 6.918280 ACC 0.12096774193548387 on epoch=18
06/03/2022 09:56:56 - INFO - __main__ - Step 160 Global step 160 Train loss 5.004251 on epoch=19
06/03/2022 09:57:01 - INFO - __main__ - Step 170 Global step 170 Train loss 5.249307 on epoch=21
06/03/2022 09:57:07 - INFO - __main__ - Step 180 Global step 180 Train loss 3.855490 on epoch=22
06/03/2022 09:57:12 - INFO - __main__ - Step 190 Global step 190 Train loss 2.929681 on epoch=23
06/03/2022 09:57:17 - INFO - __main__ - Step 200 Global step 200 Train loss 3.137754 on epoch=24
06/03/2022 09:57:20 - INFO - __main__ - Global step 200 Train loss 4.035296 ACC 0.5080645161290323 on epoch=24
06/03/2022 09:57:25 - INFO - __main__ - Step 210 Global step 210 Train loss 2.564602 on epoch=26
06/03/2022 09:57:31 - INFO - __main__ - Step 220 Global step 220 Train loss 2.434060 on epoch=27
06/03/2022 09:57:36 - INFO - __main__ - Step 230 Global step 230 Train loss 2.147469 on epoch=28
06/03/2022 09:57:41 - INFO - __main__ - Step 240 Global step 240 Train loss 2.149662 on epoch=29
06/03/2022 09:57:46 - INFO - __main__ - Step 250 Global step 250 Train loss 2.299343 on epoch=31
06/03/2022 09:57:49 - INFO - __main__ - Global step 250 Train loss 2.319027 ACC 0.47580645161290325 on epoch=31
06/03/2022 09:57:54 - INFO - __main__ - Step 260 Global step 260 Train loss 2.085221 on epoch=32
06/03/2022 09:57:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.767995 on epoch=33
06/03/2022 09:58:04 - INFO - __main__ - Step 280 Global step 280 Train loss 2.025222 on epoch=34
06/03/2022 09:58:09 - INFO - __main__ - Step 290 Global step 290 Train loss 1.873092 on epoch=36
06/03/2022 09:58:14 - INFO - __main__ - Step 300 Global step 300 Train loss 1.546467 on epoch=37
06/03/2022 09:58:17 - INFO - __main__ - Global step 300 Train loss 1.859599 ACC 0.47580645161290325 on epoch=37
06/03/2022 09:58:22 - INFO - __main__ - Step 310 Global step 310 Train loss 1.607878 on epoch=38
06/03/2022 09:58:27 - INFO - __main__ - Step 320 Global step 320 Train loss 1.443227 on epoch=39
06/03/2022 09:58:32 - INFO - __main__ - Step 330 Global step 330 Train loss 1.486281 on epoch=41
06/03/2022 09:58:37 - INFO - __main__ - Step 340 Global step 340 Train loss 1.560665 on epoch=42
06/03/2022 09:58:42 - INFO - __main__ - Step 350 Global step 350 Train loss 1.270265 on epoch=43
06/03/2022 09:58:45 - INFO - __main__ - Global step 350 Train loss 1.473663 ACC 0.47580645161290325 on epoch=43
06/03/2022 09:58:50 - INFO - __main__ - Step 360 Global step 360 Train loss 1.233979 on epoch=44
06/03/2022 09:58:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.433067 on epoch=46
06/03/2022 09:59:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.567079 on epoch=47
06/03/2022 09:59:06 - INFO - __main__ - Step 390 Global step 390 Train loss 1.226271 on epoch=48
06/03/2022 09:59:11 - INFO - __main__ - Step 400 Global step 400 Train loss 1.436520 on epoch=49
06/03/2022 09:59:13 - INFO - __main__ - Global step 400 Train loss 1.379383 ACC 0.47580645161290325 on epoch=49
06/03/2022 09:59:18 - INFO - __main__ - Step 410 Global step 410 Train loss 1.333413 on epoch=51
06/03/2022 09:59:24 - INFO - __main__ - Step 420 Global step 420 Train loss 1.090227 on epoch=52
06/03/2022 09:59:29 - INFO - __main__ - Step 430 Global step 430 Train loss 1.169403 on epoch=53
06/03/2022 09:59:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.989062 on epoch=54
06/03/2022 09:59:39 - INFO - __main__ - Step 450 Global step 450 Train loss 1.172276 on epoch=56
06/03/2022 09:59:41 - INFO - __main__ - Global step 450 Train loss 1.150876 ACC 0.47580645161290325 on epoch=56
06/03/2022 09:59:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.952964 on epoch=57
06/03/2022 09:59:52 - INFO - __main__ - Step 470 Global step 470 Train loss 1.035196 on epoch=58
06/03/2022 09:59:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.823497 on epoch=59
06/03/2022 10:00:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.898907 on epoch=61
06/03/2022 10:00:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.670442 on epoch=62
06/03/2022 10:00:10 - INFO - __main__ - Global step 500 Train loss 0.876201 ACC 0.5241935483870968 on epoch=62
06/03/2022 10:00:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.700415 on epoch=63
06/03/2022 10:00:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.755026 on epoch=64
06/03/2022 10:00:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.757929 on epoch=66
06/03/2022 10:00:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.580913 on epoch=67
06/03/2022 10:00:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.931152 on epoch=68
06/03/2022 10:00:39 - INFO - __main__ - Global step 550 Train loss 0.745087 ACC 0.7580645161290323 on epoch=68
06/03/2022 10:00:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.592634 on epoch=69
06/03/2022 10:00:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.459296 on epoch=71
06/03/2022 10:00:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.376303 on epoch=72
06/03/2022 10:01:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.222024 on epoch=73
06/03/2022 10:01:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.313208 on epoch=74
06/03/2022 10:01:08 - INFO - __main__ - Global step 600 Train loss 0.392693 ACC 0.8145161290322581 on epoch=74
06/03/2022 10:01:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.220685 on epoch=76
06/03/2022 10:01:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.191842 on epoch=77
06/03/2022 10:01:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.184401 on epoch=78
06/03/2022 10:01:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.177732 on epoch=79
06/03/2022 10:01:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.389013 on epoch=81
06/03/2022 10:01:37 - INFO - __main__ - Global step 650 Train loss 0.232735 ACC 0.8467741935483871 on epoch=81
06/03/2022 10:01:43 - INFO - __main__ - Step 660 Global step 660 Train loss 0.301166 on epoch=82
06/03/2022 10:01:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.154425 on epoch=83
06/03/2022 10:01:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.147385 on epoch=84
06/03/2022 10:01:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.093925 on epoch=86
06/03/2022 10:02:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.125014 on epoch=87
06/03/2022 10:02:06 - INFO - __main__ - Global step 700 Train loss 0.164383 ACC 0.7258064516129032 on epoch=87
06/03/2022 10:02:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.050825 on epoch=88
06/03/2022 10:02:17 - INFO - __main__ - Step 720 Global step 720 Train loss 0.050584 on epoch=89
06/03/2022 10:02:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.107316 on epoch=91
06/03/2022 10:02:27 - INFO - __main__ - Step 740 Global step 740 Train loss 0.134950 on epoch=92
06/03/2022 10:02:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.043614 on epoch=93
06/03/2022 10:02:35 - INFO - __main__ - Global step 750 Train loss 0.077458 ACC 0.8790322580645161 on epoch=93
06/03/2022 10:02:41 - INFO - __main__ - Step 760 Global step 760 Train loss 0.027147 on epoch=94
06/03/2022 10:02:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.040209 on epoch=96
06/03/2022 10:02:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.032800 on epoch=97
06/03/2022 10:02:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.019880 on epoch=98
06/03/2022 10:03:01 - INFO - __main__ - Step 800 Global step 800 Train loss 0.019278 on epoch=99
06/03/2022 10:03:04 - INFO - __main__ - Global step 800 Train loss 0.027863 ACC 0.9193548387096774 on epoch=99
06/03/2022 10:03:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.030990 on epoch=101
06/03/2022 10:03:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.040247 on epoch=102
06/03/2022 10:03:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.007709 on epoch=103
06/03/2022 10:03:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.015041 on epoch=104
06/03/2022 10:03:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.004933 on epoch=106
06/03/2022 10:03:33 - INFO - __main__ - Global step 850 Train loss 0.019784 ACC 0.9354838709677419 on epoch=106
06/03/2022 10:03:39 - INFO - __main__ - Step 860 Global step 860 Train loss 0.003842 on epoch=107
06/03/2022 10:03:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.009838 on epoch=108
06/03/2022 10:03:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.006712 on epoch=109
06/03/2022 10:03:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.013110 on epoch=111
06/03/2022 10:04:00 - INFO - __main__ - Step 900 Global step 900 Train loss 0.004808 on epoch=112
06/03/2022 10:04:03 - INFO - __main__ - Global step 900 Train loss 0.007662 ACC 0.8951612903225806 on epoch=112
06/03/2022 10:04:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.013229 on epoch=113
06/03/2022 10:04:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.008653 on epoch=114
06/03/2022 10:04:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.011409 on epoch=116
06/03/2022 10:04:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.007666 on epoch=117
06/03/2022 10:04:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.014002 on epoch=118
06/03/2022 10:04:31 - INFO - __main__ - Global step 950 Train loss 0.010992 ACC 0.9193548387096774 on epoch=118
06/03/2022 10:04:36 - INFO - __main__ - Step 960 Global step 960 Train loss 0.001716 on epoch=119
06/03/2022 10:04:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.004862 on epoch=121
06/03/2022 10:04:47 - INFO - __main__ - Step 980 Global step 980 Train loss 0.012727 on epoch=122
06/03/2022 10:04:52 - INFO - __main__ - Step 990 Global step 990 Train loss 0.001941 on epoch=123
06/03/2022 10:04:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.004492 on epoch=124
06/03/2022 10:04:58 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 10:04:58 - INFO - __main__ - Printing 3 examples
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:04:58 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:04:58 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 10:04:58 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 10:04:58 - INFO - __main__ - Printing 3 examples
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 10:04:58 - INFO - __main__ - ['contradiction']
06/03/2022 10:04:58 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:04:58 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:04:59 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 10:05:00 - INFO - __main__ - Global step 1000 Train loss 0.005148 ACC 0.9112903225806451 on epoch=124
06/03/2022 10:05:00 - INFO - __main__ - save last model!
06/03/2022 10:05:07 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 10:05:07 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 10:05:07 - INFO - __main__ - Printing 3 examples
06/03/2022 10:05:07 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 10:05:07 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:07 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 10:05:07 - INFO - __main__ - ['neutral']
06/03/2022 10:05:07 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 10:05:07 - INFO - __main__ - ['entailment']
06/03/2022 10:05:07 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:05:07 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:05:07 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 10:05:09 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_87_0.0002_8_predictions.txt
06/03/2022 10:05:09 - INFO - __main__ - ACC on test data: 0.8929
06/03/2022 10:05:09 - INFO - __main__ - prefix=superglue-cb_128_87, lr=0.0002, bsz=8, dev_performance=0.9354838709677419, test_performance=0.8928571428571429
06/03/2022 10:05:09 - INFO - __main__ - Running ... prefix=superglue-cb_128_87, lr=0.0001, bsz=8 ...
06/03/2022 10:05:10 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 10:05:10 - INFO - __main__ - Printing 3 examples
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: B: Yeah. How about Mister Rogers, is he still around? A: Yes. Yeah. They still show Mister Rogers. I don't think he's making new ones, [SEP] hypothesis: Mister Rogers is making new Mister Rogers
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: A: It was just a side benefit. B: Yeah, yeah, because, I'm not big or anything, but I'm not in great shape, But when I worked out, I got in pretty good shape. I didn't build up muscle, though, I just got real good and toned. A: Yeah. B: I don't think women look good with muscles. [SEP] hypothesis: women look good with muscles
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: B: Oh, well that's good. A: but she really doesn't. Nobody thought she would adjust, [SEP] hypothesis: she would adjust
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:05:10 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:05:10 - INFO - __main__ - Loaded 124 examples from train data
06/03/2022 10:05:10 - INFO - __main__ - Start tokenizing ... 124 instances
06/03/2022 10:05:10 - INFO - __main__ - Printing 3 examples
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: B: She says that when her husband died oh, that my uncle had said that he would never put her in a rest home. So it's kind of, uh, I don't know. I mean, I don't think my parents would but she is getting pretty bad like she has to have like a little toilet right by her bed and, it's, A: Uh-huh. B: and my mom has to take care of her pretty much so it gets, I don't know. it's a hard decision, but I don't think I would do it to my parents personally. [SEP] hypothesis: she would do it to her parents
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: B: but, uh, I can definitely, uh, see on down the road, you know, where we do have kids and are getting to that age, that's going to be a definite concern. A: Yeah, you talked before, about the school funding. I think there's only going to be one solution to school funding which I don't think will be necessarily the best way [SEP] hypothesis: the one solution to school funding will be necessarily the best way
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ -  [superglue-cb] premise: A: But, uh, B: Okay.  Uh, uh, I've had one or two American cars I think, and they were okay. I had a Pontiac once and I never had a problem with it, but, uh, my mother had a Dodge at one point and I had driven it a few times and I really did not feel that I would buy a Dodge just from, [SEP] hypothesis: she would buy a Dodge
06/03/2022 10:05:10 - INFO - __main__ - ['contradiction']
06/03/2022 10:05:10 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:05:10 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:05:11 - INFO - __main__ - Loaded 124 examples from dev data
06/03/2022 10:05:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 10:05:11 - INFO - __main__ - Starting training!
06/03/2022 10:05:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.87M parameters
06/03/2022 10:05:23 - INFO - __main__ - Starting training!
06/03/2022 10:05:27 - INFO - __main__ - Step 10 Global step 10 Train loss 23.577749 on epoch=1
06/03/2022 10:05:32 - INFO - __main__ - Step 20 Global step 20 Train loss 21.073116 on epoch=2
06/03/2022 10:05:37 - INFO - __main__ - Step 30 Global step 30 Train loss 18.072657 on epoch=3
06/03/2022 10:05:42 - INFO - __main__ - Step 40 Global step 40 Train loss 14.725904 on epoch=4
06/03/2022 10:05:48 - INFO - __main__ - Step 50 Global step 50 Train loss 12.526375 on epoch=6
06/03/2022 10:06:13 - INFO - __main__ - Global step 50 Train loss 17.995159 ACC 0.008064516129032258 on epoch=6
06/03/2022 10:06:18 - INFO - __main__ - Step 60 Global step 60 Train loss 11.879367 on epoch=7
06/03/2022 10:06:23 - INFO - __main__ - Step 70 Global step 70 Train loss 11.319773 on epoch=8
06/03/2022 10:06:29 - INFO - __main__ - Step 80 Global step 80 Train loss 11.323542 on epoch=9
06/03/2022 10:06:34 - INFO - __main__ - Step 90 Global step 90 Train loss 11.168344 on epoch=11
06/03/2022 10:06:39 - INFO - __main__ - Step 100 Global step 100 Train loss 10.305786 on epoch=12
06/03/2022 10:06:43 - INFO - __main__ - Global step 100 Train loss 11.199362 ACC 0.0 on epoch=12
06/03/2022 10:06:48 - INFO - __main__ - Step 110 Global step 110 Train loss 10.120875 on epoch=13
06/03/2022 10:06:53 - INFO - __main__ - Step 120 Global step 120 Train loss 9.656104 on epoch=14
06/03/2022 10:06:59 - INFO - __main__ - Step 130 Global step 130 Train loss 9.760061 on epoch=16
06/03/2022 10:07:04 - INFO - __main__ - Step 140 Global step 140 Train loss 8.536016 on epoch=17
06/03/2022 10:07:09 - INFO - __main__ - Step 150 Global step 150 Train loss 9.289129 on epoch=18
06/03/2022 10:07:13 - INFO - __main__ - Global step 150 Train loss 9.472437 ACC 0.0 on epoch=18
06/03/2022 10:07:19 - INFO - __main__ - Step 160 Global step 160 Train loss 8.241757 on epoch=19
06/03/2022 10:07:24 - INFO - __main__ - Step 170 Global step 170 Train loss 9.194077 on epoch=21
06/03/2022 10:07:29 - INFO - __main__ - Step 180 Global step 180 Train loss 8.274329 on epoch=22
06/03/2022 10:07:34 - INFO - __main__ - Step 190 Global step 190 Train loss 7.869205 on epoch=23
06/03/2022 10:07:39 - INFO - __main__ - Step 200 Global step 200 Train loss 7.539983 on epoch=24
06/03/2022 10:07:44 - INFO - __main__ - Global step 200 Train loss 8.223870 ACC 0.0 on epoch=24
06/03/2022 10:07:49 - INFO - __main__ - Step 210 Global step 210 Train loss 8.304532 on epoch=26
06/03/2022 10:07:54 - INFO - __main__ - Step 220 Global step 220 Train loss 7.690021 on epoch=27
06/03/2022 10:07:59 - INFO - __main__ - Step 230 Global step 230 Train loss 7.120952 on epoch=28
06/03/2022 10:08:04 - INFO - __main__ - Step 240 Global step 240 Train loss 6.800559 on epoch=29
06/03/2022 10:08:10 - INFO - __main__ - Step 250 Global step 250 Train loss 7.166499 on epoch=31
06/03/2022 10:08:13 - INFO - __main__ - Global step 250 Train loss 7.416512 ACC 0.0 on epoch=31
06/03/2022 10:08:18 - INFO - __main__ - Step 260 Global step 260 Train loss 6.864641 on epoch=32
06/03/2022 10:08:23 - INFO - __main__ - Step 270 Global step 270 Train loss 6.491129 on epoch=33
06/03/2022 10:08:28 - INFO - __main__ - Step 280 Global step 280 Train loss 6.767685 on epoch=34
06/03/2022 10:08:33 - INFO - __main__ - Step 290 Global step 290 Train loss 6.446502 on epoch=36
06/03/2022 10:08:38 - INFO - __main__ - Step 300 Global step 300 Train loss 5.980579 on epoch=37
06/03/2022 10:08:41 - INFO - __main__ - Global step 300 Train loss 6.510108 ACC 0.016129032258064516 on epoch=37
06/03/2022 10:08:47 - INFO - __main__ - Step 310 Global step 310 Train loss 5.610776 on epoch=38
06/03/2022 10:08:52 - INFO - __main__ - Step 320 Global step 320 Train loss 4.888856 on epoch=39
06/03/2022 10:08:57 - INFO - __main__ - Step 330 Global step 330 Train loss 5.337121 on epoch=41
06/03/2022 10:09:02 - INFO - __main__ - Step 340 Global step 340 Train loss 4.723590 on epoch=42
06/03/2022 10:09:07 - INFO - __main__ - Step 350 Global step 350 Train loss 4.312994 on epoch=43
06/03/2022 10:09:10 - INFO - __main__ - Global step 350 Train loss 4.974668 ACC 0.03225806451612903 on epoch=43
06/03/2022 10:09:16 - INFO - __main__ - Step 360 Global step 360 Train loss 4.318336 on epoch=44
06/03/2022 10:09:21 - INFO - __main__ - Step 370 Global step 370 Train loss 4.252438 on epoch=46
06/03/2022 10:09:26 - INFO - __main__ - Step 380 Global step 380 Train loss 3.404554 on epoch=47
06/03/2022 10:09:31 - INFO - __main__ - Step 390 Global step 390 Train loss 3.204175 on epoch=48
06/03/2022 10:09:36 - INFO - __main__ - Step 400 Global step 400 Train loss 2.674196 on epoch=49
06/03/2022 10:09:39 - INFO - __main__ - Global step 400 Train loss 3.570740 ACC 0.016129032258064516 on epoch=49
06/03/2022 10:09:44 - INFO - __main__ - Step 410 Global step 410 Train loss 3.332130 on epoch=51
06/03/2022 10:09:49 - INFO - __main__ - Step 420 Global step 420 Train loss 2.612798 on epoch=52
06/03/2022 10:09:54 - INFO - __main__ - Step 430 Global step 430 Train loss 2.189892 on epoch=53
06/03/2022 10:09:59 - INFO - __main__ - Step 440 Global step 440 Train loss 1.844326 on epoch=54
06/03/2022 10:10:04 - INFO - __main__ - Step 450 Global step 450 Train loss 2.207376 on epoch=56
06/03/2022 10:10:07 - INFO - __main__ - Global step 450 Train loss 2.437305 ACC 0.47580645161290325 on epoch=56
06/03/2022 10:10:13 - INFO - __main__ - Step 460 Global step 460 Train loss 1.978724 on epoch=57
06/03/2022 10:10:18 - INFO - __main__ - Step 470 Global step 470 Train loss 2.138437 on epoch=58
06/03/2022 10:10:23 - INFO - __main__ - Step 480 Global step 480 Train loss 2.147151 on epoch=59
06/03/2022 10:10:28 - INFO - __main__ - Step 490 Global step 490 Train loss 2.329970 on epoch=61
06/03/2022 10:10:33 - INFO - __main__ - Step 500 Global step 500 Train loss 1.792244 on epoch=62
06/03/2022 10:10:36 - INFO - __main__ - Global step 500 Train loss 2.077305 ACC 0.47580645161290325 on epoch=62
06/03/2022 10:10:41 - INFO - __main__ - Step 510 Global step 510 Train loss 2.185758 on epoch=63
06/03/2022 10:10:46 - INFO - __main__ - Step 520 Global step 520 Train loss 1.970474 on epoch=64
06/03/2022 10:10:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.492961 on epoch=66
06/03/2022 10:10:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.993207 on epoch=67
06/03/2022 10:11:01 - INFO - __main__ - Step 550 Global step 550 Train loss 1.668226 on epoch=68
06/03/2022 10:11:04 - INFO - __main__ - Global step 550 Train loss 1.862125 ACC 0.47580645161290325 on epoch=68
06/03/2022 10:11:09 - INFO - __main__ - Step 560 Global step 560 Train loss 1.532523 on epoch=69
06/03/2022 10:11:14 - INFO - __main__ - Step 570 Global step 570 Train loss 2.156636 on epoch=71
06/03/2022 10:11:19 - INFO - __main__ - Step 580 Global step 580 Train loss 2.018204 on epoch=72
06/03/2022 10:11:24 - INFO - __main__ - Step 590 Global step 590 Train loss 1.380426 on epoch=73
06/03/2022 10:11:29 - INFO - __main__ - Step 600 Global step 600 Train loss 1.626673 on epoch=74
06/03/2022 10:11:32 - INFO - __main__ - Global step 600 Train loss 1.742893 ACC 0.47580645161290325 on epoch=74
06/03/2022 10:11:37 - INFO - __main__ - Step 610 Global step 610 Train loss 1.300977 on epoch=76
06/03/2022 10:11:42 - INFO - __main__ - Step 620 Global step 620 Train loss 1.386749 on epoch=77
06/03/2022 10:11:47 - INFO - __main__ - Step 630 Global step 630 Train loss 1.897885 on epoch=78
06/03/2022 10:11:52 - INFO - __main__ - Step 640 Global step 640 Train loss 1.655096 on epoch=79
06/03/2022 10:11:57 - INFO - __main__ - Step 650 Global step 650 Train loss 1.752002 on epoch=81
06/03/2022 10:12:00 - INFO - __main__ - Global step 650 Train loss 1.598542 ACC 0.47580645161290325 on epoch=81
06/03/2022 10:12:05 - INFO - __main__ - Step 660 Global step 660 Train loss 1.691866 on epoch=82
06/03/2022 10:12:10 - INFO - __main__ - Step 670 Global step 670 Train loss 1.650724 on epoch=83
06/03/2022 10:12:15 - INFO - __main__ - Step 680 Global step 680 Train loss 1.644800 on epoch=84
06/03/2022 10:12:20 - INFO - __main__ - Step 690 Global step 690 Train loss 1.577068 on epoch=86
06/03/2022 10:12:25 - INFO - __main__ - Step 700 Global step 700 Train loss 1.498792 on epoch=87
06/03/2022 10:12:27 - INFO - __main__ - Global step 700 Train loss 1.612650 ACC 0.47580645161290325 on epoch=87
06/03/2022 10:12:32 - INFO - __main__ - Step 710 Global step 710 Train loss 1.389209 on epoch=88
06/03/2022 10:12:38 - INFO - __main__ - Step 720 Global step 720 Train loss 1.335807 on epoch=89
06/03/2022 10:12:43 - INFO - __main__ - Step 730 Global step 730 Train loss 1.583881 on epoch=91
06/03/2022 10:12:48 - INFO - __main__ - Step 740 Global step 740 Train loss 1.380636 on epoch=92
06/03/2022 10:12:53 - INFO - __main__ - Step 750 Global step 750 Train loss 1.328458 on epoch=93
06/03/2022 10:12:56 - INFO - __main__ - Global step 750 Train loss 1.403598 ACC 0.47580645161290325 on epoch=93
06/03/2022 10:13:01 - INFO - __main__ - Step 760 Global step 760 Train loss 1.201815 on epoch=94
06/03/2022 10:13:06 - INFO - __main__ - Step 770 Global step 770 Train loss 1.059984 on epoch=96
06/03/2022 10:13:11 - INFO - __main__ - Step 780 Global step 780 Train loss 1.306464 on epoch=97
06/03/2022 10:13:16 - INFO - __main__ - Step 790 Global step 790 Train loss 1.469974 on epoch=98
06/03/2022 10:13:21 - INFO - __main__ - Step 800 Global step 800 Train loss 1.190964 on epoch=99
06/03/2022 10:13:24 - INFO - __main__ - Global step 800 Train loss 1.245840 ACC 0.47580645161290325 on epoch=99
06/03/2022 10:13:29 - INFO - __main__ - Step 810 Global step 810 Train loss 1.243034 on epoch=101
06/03/2022 10:13:34 - INFO - __main__ - Step 820 Global step 820 Train loss 1.250668 on epoch=102
06/03/2022 10:13:39 - INFO - __main__ - Step 830 Global step 830 Train loss 1.166507 on epoch=103
06/03/2022 10:13:44 - INFO - __main__ - Step 840 Global step 840 Train loss 0.834608 on epoch=104
06/03/2022 10:13:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.964956 on epoch=106
06/03/2022 10:13:52 - INFO - __main__ - Global step 850 Train loss 1.091954 ACC 0.47580645161290325 on epoch=106
06/03/2022 10:13:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.988815 on epoch=107
06/03/2022 10:14:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.836091 on epoch=108
06/03/2022 10:14:07 - INFO - __main__ - Step 880 Global step 880 Train loss 1.077520 on epoch=109
06/03/2022 10:14:12 - INFO - __main__ - Step 890 Global step 890 Train loss 1.092409 on epoch=111
06/03/2022 10:14:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.911956 on epoch=112
06/03/2022 10:14:20 - INFO - __main__ - Global step 900 Train loss 0.981358 ACC 0.6129032258064516 on epoch=112
06/03/2022 10:14:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.952206 on epoch=113
06/03/2022 10:14:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.910255 on epoch=114
06/03/2022 10:14:36 - INFO - __main__ - Step 930 Global step 930 Train loss 1.196967 on epoch=116
06/03/2022 10:14:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.781011 on epoch=117
06/03/2022 10:14:46 - INFO - __main__ - Step 950 Global step 950 Train loss 1.063043 on epoch=118
06/03/2022 10:14:48 - INFO - __main__ - Global step 950 Train loss 0.980696 ACC 0.5725806451612904 on epoch=118
06/03/2022 10:14:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.843364 on epoch=119
06/03/2022 10:14:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.804812 on epoch=121
06/03/2022 10:15:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.984869 on epoch=122
06/03/2022 10:15:09 - INFO - __main__ - Step 990 Global step 990 Train loss 1.016333 on epoch=123
06/03/2022 10:15:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.816992 on epoch=124
06/03/2022 10:15:16 - INFO - __main__ - Global step 1000 Train loss 0.893274 ACC 0.7419354838709677 on epoch=124
06/03/2022 10:15:17 - INFO - __main__ - save last model!
06/03/2022 10:15:25 - INFO - __main__ - Loading checkpoint on the fly
06/03/2022 10:15:25 - INFO - __main__ - Start tokenizing ... 56 instances
06/03/2022 10:15:25 - INFO - __main__ - Printing 3 examples
06/03/2022 10:15:25 - INFO - __main__ -  [superglue-cb] premise: Valence the void-brain, Valence the virtuous valet. Why couldn't the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? [SEP] hypothesis: Valence was helping
06/03/2022 10:15:25 - INFO - __main__ - ['contradiction']
06/03/2022 10:15:25 - INFO - __main__ -  [superglue-cb] premise: ``Who knows? The point is, do we go with it or not?'' Do we assume there is a shipment? [SEP] hypothesis: there is a shipment
06/03/2022 10:15:25 - INFO - __main__ - ['neutral']
06/03/2022 10:15:25 - INFO - __main__ -  [superglue-cb] premise: ``But my father always taught me never to be afraid of pointing out the obvious. I'm sure you have noticed the implication of the letter, that the writer has in fact observed Jenny undressing for bed?'' I just wondered if you also knew as I'm sure you do that her bedroom's at the rear of the house? [SEP] hypothesis: Jenny's bedroom's at the rear of the house
06/03/2022 10:15:25 - INFO - __main__ - ['entailment']
06/03/2022 10:15:25 - INFO - __main__ - Tokenizing Input ...
06/03/2022 10:15:25 - INFO - __main__ - Tokenizing Output ...
06/03/2022 10:15:25 - INFO - __main__ - Loaded 56 examples from test data
06/03/2022 10:15:27 - INFO - __main__ - Saved prediction in models/T5-large-ft-cls2cls-down128shot/singletask-superglue-cb/superglue-cb_128_87_0.0001_8_predictions.txt
06/03/2022 10:15:27 - INFO - __main__ - ACC on test data: 0.6607
06/03/2022 10:15:27 - INFO - __main__ - prefix=superglue-cb_128_87, lr=0.0001, bsz=8, dev_performance=0.7419354838709677, test_performance=0.6607142857142857
